id,Title,Abstract,Teacher Id,Link,Dblp,Schoolar
4,Comparison Between SVM and Back Propagation Neural Network in Building IDS,"Recently, applying the novel data mining techniques for anomaly detection-an element in Intrusion Detection System has received much research alternation. Support Vector Machine (SVM) and Back Propagation Neural (BPN) network has been applied successfully in many areas with excellent generalization results, such as rule extraction, classification and evaluation. In this paper, we use an approach that is entropy based analysis method to characterize some common types of attack like scanning attack. A model based on SVM with Gaussian RBF kernel is also proposed here for building anomaly detection system. BPN network is considered one of the simplest and most general methods used for supervised training of multilayered neural network. The comparative results show that with attack scenarios that we create and through the differences between the performance measures, we found that SVM gives higher precision and lower error rate than BPN method.",1,https://link.springer.com/chapter/10.1007/978-94-007-6738-6_138,1,
5,"Book cover
Multimedia and Ubiquitous Engineering pp 1129–1136Cite as

Anomaly Detection with Multinomial Logistic Regression and Naïve Bayesian","Intrusion Detection by automated means is gaining widespread interest due to the serious impact of Intrusions on computer system or network. Several techniques have been introduced in an effort to minimize up to some extent the risk associated with Intrusion attack. In this paper, we have used two novel Machine Learning techniques including Multinomial Logistic Regression and Naïve Bayesian in building Anomaly-based Intrusion Detection System (IDS). Also, we create our own dataset based on four attack scenarios including TCP flood, ICMP flood, UDP flood and Scan port. Then, we will test the system’s ability of detecting anomaly-based intrusion activities using these two methods. Furthermore we will make the comparison of classification performance between the Multinomial Logistic Regression and Naïve Bayesian.",1,https://link.springer.com/chapter/10.1007/978-94-007-6738-6_139,1,
6,Intrusion detection under covariate shift using modified support vector machine and modified backpropagation,"In this paper, we address the dataset shift problem in building intrusion detection systems by assuming that network traffic variants follow the covariate shift model. Based on two recent works on direct density ratio estimation which are kernel mean matching and unconstrained least squares importance fitting, we propose to modify two well-known classification techniques: neural networks with back propagation and support vector machine in order to make these techniques work better under covariate shift effect. We evaluated the modified techniques on a benchmark intrusion detection dataset, the KDD Cup 1999, and got higher results on predication accuracy of network behaviors compared with the original techniques.",1,https://dl.acm.org/doi/10.1145/2350716.2350756,1,
7,A LSTM based framework for handling multiclass imbalance in DGA botnet detection,"In recent years, botnets have become a major threat on the Internet. Most sophisticated bots use Domain Generation Algorithms (DGA) to pseudo-randomly generate a large number of domains and select a subset in order to communicate with Command and Control (C&C) server. The basic aim is to avoid blacklisting, sinkholing and evade the security systems. Long Short-Term Memory network (LSTM) provides a mean to combat this botnet type. It operates on raw domains and is amenable to immediate applications. LSTM is however prone to multiclass imbalance problem, which becomes even more significant in DGA malware detection. This is due the fact that many DGA classes have a very little support in the training dataset. This paper presents a novel LSTM.MI algorithm to combine both binary and multiclass classification models, where the original LSTM is adapted to be cost-sensitive. The cost items are …",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:9yKSN-GCB0IC,,1
8,Pre-delivery of content to a user device,"Systems and methods for delivering content to user devices before the content is selected or requested (eg, a pre-delivery of content) are described. In some embodiments, the system and methods receive, from a content server, information associated with content items available for retrieval from the content server and associated with one or more applications resident on a user device, select a subset of content items from the content items available for retrieval to deliver to the user device based on content usage information associated with the user device, and cause the user device to retrieve at least a portion of the selected subset of content items from the content server.",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:4TOpqqG69KYC,,1
9,Lipid bilayer-coated curcumin-based mesoporous organosilica nanoparticles for cellular delivery,"Effective and controlled drug delivery systems with on-demand release abilities and biocompatible properties receive enormous attention for biomedical applications. Here, we describe a novel inorganic–organic hybrid material with a strikingly high organic content of almost 50 wt%. The colloidal periodic mesoporous organosilica (PMO) nanoparticles synthesized in this work consist entirely of curcumin and ethane derivatives serving as constituents that are crosslinked by siloxane bridges, without any added silica. These mesoporous curcumin nanoparticles (MCNs) exhibit very high surface areas (ca. 1000 m2/g), narrow particle size distribution (around 200 nm) and a strikingly high stability in simulated biological media. Additionally, the MCNs show high autofluorescence and were used as a cargo delivery system in live-cell experiments. A supported lipid bilayer (SLB) efficiently seals the pores and releases …",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:NMxIlDl6LWMC,,1
10,DGA botnet detection using supervised learning methods,"Modern botnets are based on Domain Generation Algorithms (DGAs) to build a resilient communication between bots and Command and Control (C&C) server. The basic aim is to avoid blacklisting and evade the Intrusion Protection Systems (IPS). Given the prevalence of this mechanism, numerous solutions have been developed in the literature. In particular, supervised learning has received an increased interest as it is able to operate on the raw domains and is amenable to real-time applications. Hidden Markov Model, C4. 5 decision tree, Extreme Learning Machine, Long Short-Term Memory networks have become the state of the art in DGA botnet detection. There also exist several advanced supervised learning methods, namely Support Vector Machine (SVM), Recurrent SVM, CNN+ LSTM and Bidirectional LSTM, which have not been suitably appropriated in such domain. This paper presents a first attempt …",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:qjMakFHDy7sC,,1
11,A method for detecting DGA Botnet based on semantic and cluster analysis,"Botnets play major roles in a vast number of threats to network security, such as DDoS attacks, generation of spam emails, information theft. Detecting Botnets is a difficult task in due to the complexity and performance issues when analyzing the huge amount of data from real large-scale networks. In major Botnet malware, the use of Domain Generation Algorithms allows to decrease possibility to be detected using white list-blacklist scheme and thus DGA Botnets have higher survival. This paper proposes a DGA Botnet detection scheme based on DNS traffic analysis which utilizes semantic measures such as entropy, meaning the level of the domain, frequency of n-gram appearances and Mahalanobis distance for domain classification. The proposed method is an improvement of Phoenix botnet detection mechanism, where in the classification phase, the modified Mahalanobis distance is used instead of the …",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:hqOjcs7Dif8C,,1
12,DGA botnet detection using collaborative filtering and density-based clustering,"In recent years, the botnet phenomenon is one of the most dangerous threat to Internet security, which supports a wide range of criminal activities, including distributed denial of service (DDoS) attacks, click fraud, phishing, malware distribution, spam emails, etc. An increasing number of botnets use Domain Generation Algorithms (DGAs) to avoid detection and exclusion by the traditional methods. By dynamically and frequently generating a large number of random domain names for candidate command and control (C&C) server, botnet can be still survive even when a C&C server domain is identified and taken down. This paper presents a novel method to detect DGA botnets using Collaborative Filtering and Density-Based Clustering. We propose a combination of clustering and classification algorithm that relies on the similarity in characteristic distribution of domain names to remove noise and group similar …",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:IjCSPb-OGe4C,,1
13,Factors affecting employee engagement at not-for-profit organizations: A case in Vietnam,"The paper examines the determinants of employee engagement in the not-for-profit organization (NFPO) sector. A structured questionnaire survey of 205 NFPO employees in Vietnam was employed, preceded by in-depth interviews with NFPO associates and managers. After a multiple regression analysis to study the impact of five independent variables on one dependent variable, the study reveals that Work design and Work-life balance are positively and significantly associated with employee engagement. Of which, Work design is measured by the level of job fit, job autonomy, job challenge, and job meaningfulness. On the other hand, Work-life balance involves measurement aspects consisting of how reasonable the workload is and organizational support. Meanwhile, different from the business world, Leadership, Learning and Development, and Recognition do not show statistically positive impacts on employee engagement in NFPOs. Still, they are crucial back-supporting factors to ensure employee experience at work. Further, the study finds that personal growth contributes a great part to keep employees in the sector motivated. This paper brings the area of study to the forefront in an effort to benefit the practice of human resources management in NFPOs as well as support employees in the sector with a higher quality working experience.",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:EUQCXRtRnyEC,,1
14,Playback of content pre-delivered to a user device,"Systems and methods for displaying content pre-delivered to a user device, playing back content pre-delivered to a user device, and/or pre-delivering content to a user device during concurrent content playback, are described. In some embodiments, the systems and methods include or interact with a mobile application that displays descriptions of content available for playback via the mobile application along with indicators that represent a state of delivery (eg, a state of pre-delivery) for the content items.",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:kNdYIx-mwKoC,,1
15,Improving the video transmission quality over IP network,"Nowadays, the application of Video transmission on Internet tends to require large bandwidth, which makes the network overloaded. Furthermore, parameters such as packet delay, jitter, and low throughput have significantly reduced the video quality of the receiver, which reduces the end-user's efficiency of work or entertainment. In this paper, we propose a solution to improve the queue management mechanism BLUE to ensure the quality of the video data stream which always requires large resources when the state of network transmission is unstable. We carry out this by prioritizing video packet and ensuring network resources for more powerful video packets than other common packets. Experimental simulation based on NS-2 shows that our proposal has significantly improved video quality at the receiver while maintaining the flow of data transmitted on the network in an acceptable range.",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:Y0pCki6q_DkC,,1
16,Content source discovery,"Systems and methods for discovering content sources and/or delivering content to applications resident on mobile devices are described. In some embodiments, the systems and methods transmit information identifying one or more applications resident on a mobile device to a server, receive, from the server, information associated with content items available for retrieval from a content server and associated with the identified one or more applications, and cause the mobile device to retrieve at least one of the content items available for retrieval from the content server.",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:Zph67rFs4hoC,,1
17,Labour provisions in preferential trade agreements: potential opportunities or challenges to Vietnam?,"In this research, we discuss the impacts of labour provisions in preferential trade agreements (PTAs) to which Vietnam is a party in aspects of politics, institutions, law and economy. We find that the impacts are a mixture of both positives and negatives. Positive effects include a more and more developed and well-established domestic labour legal framework, stronger domestic political commitment to labour reforms, more stabilized political environment, higher level of labour protection, higher wages, better working conditions and better life, higher productivity and competitiveness of goods and services and more foreign direct investment. Negative effects include potential political risks of state sovereignty weakening, reduction in competitiveness because of rising labour costs and undermined trade and economic growth resulting from trade-based sanctions in labour disputes. Both positive effects and negative effects exist as possibilities and whether they are materialized and which of them outweighs depend on reactions taken by Vietnam. Proposals for Vietnam to benefit from labour commitments include improving the domestic labour law system and efficiency of state administration and enforcement of domestic labour law, changing the economy from factor-based to efficiency-based status, making relevant institutional reforms, introducing the trade-related labour topic into the academic setting, performing studies on the impacts of labour provisions in PTAs, set up strategies and tactics in PTAs’ labour negotiations, joining forces with other trade partners of the same interest, effective exploitation of cooperation mechanisms in PTAs and proposing …",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:aqlVkmm33-oC,,1
18,SHIOT: A novel SDN-based framework for the heterogeneous Internet of Things,"A new measurable and quantifiable world is created by the Internet of Things. However, the variety of IoT components, ie, devices, access technologies and applications, which are deployed on the same core infrastructure with a common network policy have led to an unexpected issue of heterogeneity. Such issue directly dismisses the interoperability in the IoT, and hence, significantly decreasing the QoS of a given IoT service. In this paper, we develop a SDN-based framework, called SHIOT to address the above challenge. SHIOT relies on the ontology for examining the end-user requests and applies a SDN controller to classify flow scheduling over the task level. We also utilize the Lagrange relaxation theory to optimize the routing mechanism. Extensive experiments demonstrate that SHIOT is able to support stressed networks and offers a significant advantage over the traditional framework that is integrated without SDN.",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:KlAtU1dfN6UC,,1
19,A novel SDN controller based on ontology and global optimization for heterogeneous IoT architecture,"Today, our modern living world is covered by ubiquitous systems that offer the ability to assess, understand and handle the environmental indicators of our urban environment as well as our delicate ecology and natural resources. The sudden increase of such systems creates the notion of Internet of Things (IoT). However, the variety of components and wide-area deployments in IoT create a disadvantage point: the heterogeneity problem. Such system has multiple heterogeneous wireless communication solutions with multiple access technologies such as bluetooth, wifi, zigbee, cellular, MANET, etc. Concretely, the effectiveness is revealed by a variety of access technologies that are working on a common core network with a common policy for every type of access network. The challenge is how to manage this heterogeneous network in a dynamic context with an open and distributed infrastructure. One of the most …",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:u-x6o8ySG0sC,,1
20,Does seller status matter in inter-corporate asset sales?,"This paper examines the returns of asset acquirers when sellers have different statuses. We find that private sellers create lower returns for acquirers and receive higher premiums than public sellers. Both private equity and private operating sellers generate lower returns for acquirers than public sellers, but their relative gain differences are not significantly different. In addition, the gain difference cannot be explained by acquirer characteristics, sample selection effects, or means of payments, but it increases with sellers’ director ownership. We examine alternative theories to explain our results. While we do not find supportive evidence for the synergy creation and information symmetry hypothesis, we find ample evidence for the manager discretion hypothesis.",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:0EnyYjriUFMC,,1
23,End to end message encryption using Poly1305 and XSalsa20 in low power and lossy networks,"Security has been considered as a key concern in real world deployments of Low power and Lossy networks (LLNs). It has been challenging to ensure a secure communication between the constrained devices. IPSec and DTLS are known not to be amenable to immediate LLN applications due to their memory footprint and the amount of traffic established during their session. In this paper, we propose to use the so-called Authenticated Encryption (AE) that is based on Poly1305 and XSalsa20 to provide confidentiality, integrity and authenticity over the data packets. Experiments are carried out using Contiki OS, an open source operating system for the Internet of Things. They demonstrate that AE has extremely high speed, requires very little resource, and can be implemented in constrained environments.",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:hMod-77fHWUC,,1
24,Takeover pressure and corporate diversification,"This paper shows theoretically and empirically that the takeover market is an effective external force of discipline for corporate diversification. First, we derive a simple model that highlights the managers’ incentives to overdiversify their firm. In the absence of a takeover threat, managers may structure their firm suboptimally in pursuit of private benefits. However, facing a threat of takeover, managers will de-diversify to maximize firms’ value in fear of being acquired and replaced. We also discuss the discipline role of the takeover market in competitive and non-competitive environment, and its interactions with other monitoring mechanisms. Second, we test three hypotheses generated from the model:(1) state anti-takeover index increases diversification;(2) the disciplinary effect is more pronounced in non-competitive industries;(3) the effect is lower when firms are monitored more intensively. The empirical results are strongly consistent with these predictions, and robust to alternative measurements diversification, and censored and truncated data.",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:dhFuZR0502QC,,1
25,Supervisory Shopping in the Banking Sector: When Is It Socially Desirable?,"This paper analyzes the welfare impact of supervisory shopping in the banking sector. Supervisory shopping leads to a welfare-increasing"" race to the top"" among supervisors, if strong supervision increases banks' access to deposits by signaling that banks have a less risky balance sheet. However, if deposits are subsidized through an underpriced financial safety net, banks shop for a weak supervisor in order to maximize gains from risk-shifting. Supervisory shopping then precipitates a"" race to the bottom."" In an extension of the framework, contingent convertible bonds (CoCos) prevent banks from risk-shifting and supervisory shopping becomes socially desirable again.",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:8k81kl-MbHgC,,1
26,Pre-delivery of content to a user device,"Systems and methods for delivering content to user devices before the content is selected or requested (eg, a pre-delivery of content) are described. In some embodiments, the system and methods receive, from a content server, information associated with content items available for retrieval from the content server and associated with one or more applications resident on a user device, select a subset of content items from the content items available for retrieval to deliver to the user device based on content usage information associated with the user device, and cause the user device to retrieve at least a portion of the selected subset of content items from the content server.",1,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=x5RNHYgAAAAJ&citation_for_view=x5RNHYgAAAAJ:M3ejUd6NZC8C,,1
27,Analyzing Kleinberg's (and other) small-world models,"We analyze the properties of Small-World networks, where links are much more likely to connect"" neighbor nodes"" than distant nodes. In particular, our analysis provides new results for Kleinberg's Small-World model and its extensions. Kleinberg adds a number of directed long-range random links to an nxn lattice network (vertices as nodes of a grid, undirected edges between any two adjacent nodes). Links have a non-uniform distribution that favors arcs to close nodes over more distant ones. He shows that the following phenomenon occurs: between any two nodes a path with expected length O (log 2 n) can be found using a simple greedy algorithm which has no global knowledge of long-range links. We show that Kleinberg's analysis is tight: his algorithm achieves θ (log 2 n) delivery time. Moreover, we show that the expected diameter of the graph is θlog n), a log n factor smaller. We also extend our results to …",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:HDshCWvjkbEC,,1
28,Analyzing and characterizing small-world graphs,"We study variants of Kleinberg's small-world model where we start with a k-dimensional grid and add a random directed edge from each node. The probability u's random edge is to v is proportional to d (u, v)-r where d (u, v) is the lattice distance and r is a parameter of the model. For a k-dimensional grid, we show that these graphs have poly-log expected diameter when k< r< 2k, but have polynomial expected diameter when r> 2k. This shows an interesting phase-transition between small-world and"" large-world"" graphs. We also present a general framework to construct classes of small-world graphs with Θ (log n) expected diameter, which includes several existing settings such as Kleinberg's grid-based and tree-based settings [15]. We also generalize the idea of'adding links with probability α the inverse distance'to design small-world graphs. We use semi-metric and metric functions to abstract distance to create a …",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:4JMBOYKVnBMC,,1
29,Distributed hole-bypassing protocol in WSNs with constant stretch and load balancing,"Geographic routing has been widely used in wireless sensor networks because of its simplicity and efficiency resulting from its local and stateless nature. However, when subjected to routing holes (ie, regions without sensor nodes that have communication capability), geographic routing suffers from the so-called local minimum phenomenon, where packets are stopped at the hole boundary. This local minimum phenomenon results in problems of load imbalance (ie, a higher traffic intensity around the hole boundary) and routing path enlargement due to the long hole detour paths. Although several protocols have been proposed to address these issues, the load imbalance problem has not been solved thoroughly, and none of the existing protocols can solve both of these problems. In this article, we propose a distributed hole-bypassing routing protocol named ACOBA (Adaptive forbidden area-based COnstant …",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:_FxGoFyzp5QC,,1
30,An energy efficient and load balanced distributed routing scheme for wireless sensor networks with holes,"In this paper we present a new approach to route packets in the presence of routing holes. In our proposal, nodes cooperate to determine the approximate polygon of a specific hole and then exchange information about the approximate polygon. Based on the hole covering parallelogram and the hole view angle of a specific node, packets can be forwarded along an escape route that bends around the hole. We rigorously prove that the Euclidean stretch of an escape route is bounded. Simulation results show that the proposed scheme can save more than 16% of the energy consumption and 7% of the network lifetime in the comparison with existing routing algorithms. The average length of routing paths in our approach is less than 60% of other routing schemes.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:ILKRHgRFtOwC,,1
31,A multiplicative finite strain crystal plasticity formulation based on additive elastic corrector rates: Theory and numerical implementation,"The purpose of continuum plasticity models is to efficiently predict the behavior of structures beyond their elastic limits. The purpose of multiscale materials science models, among them crystal plasticity models, is to understand the material behavior and design the material for a given target. The current successful continuum hyperelastoplastic models are based in the multiplicative decomposition from crystal plasticity, but significant differences in the computational frameworks of both approaches remain, making comparisons not straightforward.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:gsN89kCJA0AC,,1
32,Distributed shortcut networks: Low-latency low-degree non-random topologies targeting the diameter and cable length trade-off,"Low communication latency becomes a main concern in highly parallel computers and supercomputers that reach millions of processing cores. Random network topologies are better suited to achieve low average shortest path length and low diameter in terms of the hop counts between nodes. However, random topologies lead to two problems: (1) increased aggregate cable length on a machine room floor that would become dominant for communication latency in next-generation custom supercomputers, and (2) high routing complexity that typically requires a routing table at each node (e.g., topology-agnostic deadlock-free routing). In this context, we first propose low-degree non-random topologies that exploit the small-world effect, which has been well modeled by some random network models. Our main idea is to carefully design a set of various-length shortcuts that keep the diameter small while maintaining a …",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:0EnyYjriUFMC,,1
33,Computational modeling of dislocation slip mechanisms in crystal plasticity: a short review,"The bridge between classical continuum plasticity and crystal plasticity is becoming narrower with continuously improved computational power and with engineers’ desire to obtain more information and better accuracy from their simulations, incorporating at the same time more effects about the microstructure of the material. This paper presents a short overview of the main current techniques employed in crystal plasticity formulations for finite element analysis, as to serve as a point of departure for researchers willing to incorporate microstructure effects in elastoplastic simulations. We include both classical and novel crystal plasticity formulations, as well as the different approaches to model dislocations in crystals.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:MLfJN-KU85MC,,1
34,Distributed shortcut networks: Layout-aware low-degree topologies exploiting small-world effect,"Low communication latency becomes a main concern in highly parallel computers and supercomputers. Random network topologies are best to achieve low average shortest path length and low diameter in hop counts between nodes and thus low communication latency. However, random topologies lead to a problem of increased aggregate cable length on a machine room floor. In this context we propose low-degree non-random topologies that exploit the small-world effect, which has been typically well modeled by some random network models. Our main idea is to carefully design a set of various-length shortcuts that keep the diameter small while maintain an economical cable length. Our experimental graph analysis showed that our proposed topology has low diameter and low average shortest path length, which is considerably better than those of a counterpart 2-D torus and is near to those of a counterpart …",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:isC4tDSrTZIC,,1
35,Efficient approximation of routing holes in wireless sensor networks,"Routing holes in wireless sensor networks (WSN) do occur due to several reasons, including cases caused by natural obstacles or disaster suffered areas. Computing the exact boundary of a hole is possible [5], but can be impractical with large holes. In this paper, we consider the problem of how to approximate a hole by a simplistic shape, covering polygon such that if demanded either the approximation error or the memory space needed for reporting this polygon shape can be made as small as desired. We present an efficient algorithmic scheme that can produce cost-effective and flexible solutions that can be customized to fit to specific situations. Based on the basic BoundHole algorithm for computing the exact hole boundary [5], we propose a two-folded approximation technique:(1) covering the hole with a minimal polygon with edges aligned to a given square grid;(2) dynamically trimming-off this polygon …",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:zYLM7Y9cAGgC,,1
36,Synflood spoof source DDoS attack defence based on packet ID anomaly detection-PIDAD,"A distributed denial-of-service (DDoS) attack characterized by flooding SYN packets is one of the network attacks to make the information system unavailable. This kind of attack becomes dangerous and more difficult to prevent and defense when attackers try to send flood SYN packets with spoof source, especially, there packets have information fields as the normal SYN packets. In this study, we propose a method called Packet Identification Anomaly Detection - PIDAD used to defense type of DDoS attack mentioned above. This method based on abnormal information of identification field in IP Header when observing the set of packets received in the victim system.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:8k81kl-MbHgC,,1
37,A distributed protocol for detecting and updating hole boundary in wireless sensor networks,"Holes in sensor network are regions without operating nodes which may occur due to several reasons, including cases caused by natural obstacles or disaster suffered areas. Determining the location and shape of holes can help to monitor these disaster events (such as volcano, tsunami, etc.) or help to make smart, early routing decisions for circumventing a hole. There are many hole determination algorithms have been proposed in the literature. However, all of these algorithms only consider the networks with static holes. Moreover, most of these protocols are conducted in a centralized manner which is not suitable with the resource constraint of sensor nodes. In this paper, we consider the networks with dynamic holes and propose a distributed algorithm which can determine and update the hole boundary very quickly. We also conduct simulation to evaluate performance of proposed protocol.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:9yKSN-GCB0IC,,1
38,Load balanced routing with constant stretch for wireless sensor network with holes,"Because of its simplicity and scalability, geographic routing is a popular approach in wireless sensor networks, which can achieve a near-optimal routing path in the networks without of holes. With the occurrence of holes, however, geographic routing faces the problems of hole diffusion and routing path enlargement. Several recent proposals attempt to fix these issues by deploying a special, forbidding area around the hole, which helps to improve the congestion on the hole boundary but still causes significant load imbalancing due to static detour routes bypassing this fixed forbidding area. Also, a significant enlargement on routing path is still possible due to the possibly significant difference between this forbidding area and the hole. Another recent approach can achieve a low route stretch (constant bounded) but still, the load imbalancing due to the holes is a concern. In this paper, we introduce a novel approach …",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:W7OEmFMy1HYC,,1
39,Environment monitoring for anomaly detection system using smartphones,"Currently, the popularity of smartphones with networking capabilities equipped with various sensors and the low cost of the Internet have opened up great opportunities for the use of smartphones for sensing systems. One of the most popular applications is the monitoring and the detection of anomalies in the environment. In this article, we propose to enhance classic road anomaly detection methods using the Grubbs test on a sliding window to make it adaptive to the local characteristics of the road. This allows more precision in the detection of potholes and also building algorithms that consume less resources on smartphones and adapt better to real conditions by applying statistical outlier tests on current threshold-based anomaly detection methods. We also include a clustering algorithm and a mean shift-based algorithm to aggregate reported anomalies on data to the server. Experiments and simulations allow us to confirm the effectiveness of the proposed methods.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:kz9GbA2Ns4gC,,1
40,Road anomaly detection using smartphone: A brief analysis,"Identification of road anomaly not only helps drivers to reduce the risk, but also support for road maintenance. Arguably, with the popularity of smartphones including multiple sensors, many road anomaly detection systems using mobile phones have been proposed. This paper aims at analyzing a number of typical road anomaly detection methods in terms of resource requirements, energy consumption, fitness conditions. From these measurements, we suggest some improvement directions to build road anomaly detection algorithms appropriate for smartphones.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:M7yex6snE4oC,,1
41,An interconnection network exploiting trade-off between routing table size and path length,"Various parallel applications require a low-latency interconnection network to achieve high performance and high scalability. Recently proposed random network topologies achieve low latency, but they require each switch to have a large number of routing table entries, e.g., larger than N for an N-node network, for implementing a minimal routing. In this study, we propose the use of a random network topology with our new routing scheme so that the required routing table size becomes small, e.g., 528 for 8192 nodes, at each switch. Our main finding is that our routing algorithm cannot always follow the minimal paths, but its average path length is still short when compared to that of existing network topologies.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:2osOgNQ5qMEC,,1
42,Designing low cost networks with short routes and low congestion,"We design network topologies and routing strategies which optimize several measures simultaneously: low cost, small routing diameter, bounded degree and low congestion. This set of design issues is broader than traditional network design and hence, our work is useful and relevant to a set of traditional and emerging design problems. Surprisingly, a simple idea from the research on small-world models, inspires a fruitful approach and useful techniques here.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:BqipwSGYUEgC,,1
43,"Small-world graphs: Models, analysis and applications in network designs","Small-world properties are common in many large-scale real-world networks such as social networks, the Internet, or biological networks. In 2000, Kleinberg produced a new model for a striking aspect of acquaintance networks: that short chains can be found using limited local information only (eg a search based on a first-name basis). This model added random links to a 2D grid, such that the random links were more likely to connect closer nodes.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:dfsIfKJdRG4C,,1
44,On the network orientational affinity assumption in polymers and the micro–macro connection through the chain stretch,"We question the network affinity assumption in modeling chain orientations under polymer deformations, and the use of the stretch measure projected from the right Cauchy–Green deformation tensor (or non-affine micro-stretches derived from that measure) as a basic state variable for the micro–macro transition. These ingredients are standard, taken from the statistical theory of polymers, and used in most micromechanical polymer network and soft tissue models.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:TIZ-Mc8IlK0C,,1
45,Energy-efficient routing in the proximity of a complicated hole in wireless sensor networks,"A quest for geographic routing schemes of wireless sensor networks when sensor nodes are deployed in areas with obstacles has resulted in numerous ingenious proposals and techniques. However, there is a lack of solutions for complicated cases wherein the source or the sink nodes are located close to a specific hole, especially in cavern-like regions of large complex-shaped holes. In this paper, we propose a geographic routing scheme to deal with the existence of complicated-shape holes in an effective manner. Our proposed routing scheme achieves routes around holes with the (1+)-stretch. Experimental results show that our routing scheme yields the highest load balancing and the most extended network lifetime compared to other well-known routing algorithms as well.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:LI9QrySNdTsC,,1
46,FDDA: a framework for fast detecting source attack in web application DDoS attack,"Anomaly detection techniques usually require a training phase in order to capture the normality traits (typical features) of a given system which is considered as in a state free of attack. The efficiency of detecting abnormal signals mainly depends on processing time and the quality of featured data learned from the training phase. In this research, we propose a framework for fast detecting DDoS attack (FDDA) which can improve the speed and efficiency in detecting an on-going DDoS attack to web application, and hence greatly reduce the training time.",5,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=W_EJK0sAAAAJ&citation_for_view=W_EJK0sAAAAJ:hMsQuOkrut0C,,1
48,A novel feature selection method based on normalized mutual information,"In this paper, a novel feature selection method based on the normalization of the well-known mutual information measurement is presented. Our method is derived from an existing approach, the max-relevance and min-redundancy (mRMR) approach. We, however, propose to normalize the mutual information used in the method so that the domination of the relevance or of the redundancy can be eliminated. We borrow some commonly used recognition models including Support Vector Machine (SVM), k-Nearest-Neighbor (kNN), and Linear Discriminant Analysis (LDA) to compare our algorithm with the original (mRMR) and a recently improved version of the mRMR, the Normalized Mutual Information Feature Selection (NMIFS) algorithm. To avoid data-specific statements, we conduct our classification experiments using various datasets from the UCI machine learning repository. The results confirm that our …",9,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3bU8SVcAAAAJ&citation_for_view=3bU8SVcAAAAJ:YsMSGLbcyi4C,,1
49,Semi-Markov conditional random fields for accelerometer-based activity recognition,"Activity recognition is becoming an important research area, and finding its way to many application domains ranging from daily life services to industrial zones. Sensing hardware and learning algorithms are two important components in activity recognition. For sensing devices, we prefer to use accelerometers due to low cost and low power requirement. For learning algorithms, we propose a novel implementation of the semi-Markov Conditional Random Fields (semi-CRF) introduced by Sarawagi and Cohen. Our implementation not only outperforms the original method in terms of computation complexity (at least 10 times faster in our experiments) but also is able to capture the interdependency among labels, which was not possible in the previously proposed model. Our results indicate that the proposed approach works well even for complicated activities like eating and driving a car. The average …",9,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3bU8SVcAAAAJ&citation_for_view=3bU8SVcAAAAJ:IjCSPb-OGe4C,,1
50,Formation of ionospheric irregularities over Southeast Asia during the 2015 St. Patrick's Day storm,"We investigate the geospace response to the 2015 St. Patrick's Day storm leveraging on instruments spread over Southeast Asia (SEA), covering a wide longitudinal sector of the low‐latitude ionosphere. A regional characterization of the storm is provided, identifying the peculiarities of ionospheric irregularity formation. The novelties of this work are the characterization in a broad longitudinal range and the methodology relying on the integration of data acquired by Global Navigation Satellite System (GNSS) receivers, magnetometers, ionosondes, and Swarm satellites. This work is a legacy of the project EquatoRial Ionosphere Characterization in Asia (ERICA). ERICA aimed to capture the features of both crests of the equatorial ionospheric anomaly (EIA) and trough (EIT) by means of a dedicated measurement campaign. The campaign lasted from March to October 2015 and was able to observe the ionospheric …",9,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3bU8SVcAAAAJ&citation_for_view=3bU8SVcAAAAJ:d1gkVwhDpl0C,,1
51,An improved maximum relevance and minimum redundancy feature selection algorithm based on normalized mutual information,"We present in this paper a comprehensive analysis of the mutual information based feature selection algorithms. We point out the limitations of some recent work in this area then propose an improvement to overcome the weak points. The experiment results confirm that we achieve a better feature sets compared with the two recent developed algorithms, which are Maximum Relevance and Minimum Redundancy (mRMR) and Normalized Mutual Information Feature Selection (NMIFS), in terms of the classification accuracy.",9,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3bU8SVcAAAAJ&citation_for_view=3bU8SVcAAAAJ:eQOLeE2rZwMC,,1
52,Analysis of the regional ionosphere at low latitudes in support of the biomass ESA mission,"Biomass is a spaceborn polarimetric P-band (435 MHz) synthetic aperture radar (SAR) in a dawn-dusk low Earth orbit. Its principal objective is to measure biomass content and change in all the Earth's forests. The ionosphere introduces the Faraday rotation on every pulse emitted by low-frequency SAR and scintillations when the pulse traverses a region of plasma irregularities, consequently impacting the quality of the imaging. Some of these effects are due to total electron content (TEC) and its gradients along the propagation path. Therefore, an accurate assessment of the ionospheric morphology and dynamics is necessary to properly understand the impact on image quality, especially in the equatorial and tropical regions. To this scope, we have conducted an in-depth investigation of the significant noise budget introduced by the two crests of the equatorial ionospheric anomaly (EIA) over Brazil and Southeast …",9,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3bU8SVcAAAAJ&citation_for_view=3bU8SVcAAAAJ:9yKSN-GCB0IC,,1
53,A novel residual frequency estimation method for GNSS receivers,"In Global Navigation Satellite System (GNSS) receivers, residual frequency estimation methods are traditionally applied in the synchronization block to reduce the transient time from acquisition to tracking, or they are used within the frequency estimator to improve its accuracy in open-loop architectures. There are several disadvantages in the current estimation methods, including sensitivity to noise and wide search space size. This paper proposes a new residual frequency estimation method depending on differential processing. Although the complexity of the proposed method is higher than the one of traditional methods, it can lead to more accurate estimates, without increasing the size of the search space.",9,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3bU8SVcAAAAJ&citation_for_view=3bU8SVcAAAAJ:WF5omc3nYNoC,,1
55,Visualizations of human activities in sensor-enabled ubiquitous environments,"Sensor network ubiquitous environments may generate a lot of data including heterogeneous `raw' sensor data, low-level feature and/or trend data and higher-level context and inferenced information. This paper considers the visualization of such large, heterogeneous and complex integrated information, especially for real-time deployments facilitating rapid understanding leading to decision making. Visualizations are contextually structured according to the newly proposed serviceable visualizations paradigm for service-based and cloud-enabled visualizations. Specific visualizations of human activities are subsequently developed. These visualizations are based on the data provided via a secured WSN-integrated cloud computing for u-Health Care (SC3) architecture that is under development.",9,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3bU8SVcAAAAJ&citation_for_view=3bU8SVcAAAAJ:Tyk-4Ss8FVUC,,1
59,Video Based Human Activity Recognition using Wavelet Transform and Hidden Conditional Random Fields (HCRF),"In this research, we proposed testing and validating the accuracy of employing wavelet transform and Hidden Conditional Random Field (HRCF) for video based activity recognition. For feature extraction, Symlet wavelet was tested and decomposed up to 4 levels, and some of the highest coefficients were extracted from each level of decomposition. These coefficients were based on the average frequency of each video frame and the time difference between each frame. Finally, a novel HRCF model was applied for recognition. The proposed method was tested on a database of ten activities, where the data were collected from nine different people, and compared with one of the existing techniques. The overall recognition rate, using the symlet wavelet family (Symlet 4), was 93% that showed an improvement of 13% in performance.",9,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3bU8SVcAAAAJ&citation_for_view=3bU8SVcAAAAJ:zYLM7Y9cAGgC,,1
61,Detecting selective forwarding attacks in wireless sensor networks using two-hops neighbor knowledge,"Wireless sensor networks have many potential applications for both civil and military tasks. However, WSNs are susceptible to many types of attacks because they are deployed in open and unprotected environment. Selective forwarding attack is one of the easiest implement and damaged attacks in multi-hop routing protocols. In this paper, we proposed a lightweight detection algorithm based only on the neighborhood information. Our detection algorithm can detect selective forwarding attack with high accuracy and little overhead imposed on detection modules than previous works.",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:9yKSN-GCB0IC,,1
62,A lightweight intrusion detection framework for wireless sensor networks,"In recent years, Wireless Sensor Networks (WSNs) have demonstrated successful applications for both civil and military tasks. However, sensor networks are susceptible to multiple types of attacks because they are randomly deployed in open and unprotected environments. It is necessary to utilize effective mechanisms to protect sensor networks against multiple types of attacks on routing protocols. In this paper, we propose a lightweight intrusion detection framework integrated for clustered sensor networks. Furthermore, we provide algorithms to minimize the triggered intrusion modules in clustered WSNs by using an over‐hearing mechanism to reduce the sending alert packets. Our scheme can prevent most routing attacks on sensor networks. In in‐depth simulation, the proposed scheme shows less energy consumption in intrusion detection than other schemes. Copyright © 2009 John Wiley & Sons, Ltd.",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:u5HHmVD_uO8C,,1
63,Hybrid intrusion detection system for wireless sensor networks,"In recent years, wireless sensor networks have many potential applications for both civil and military tasks. However, sensor networks are susceptible to many types of attacks because they are deployed in open and unprotected environment. So it is necessary to use effective mechanisms to protect sensor networks against many types of attacks on routing protocols. Intrusion detection is one of the major and efficient defense methods against attacks in a computer network and system. Because of different characteristics of sensor networks, security solutions have to be designed with limited usage of computation and resources. In this paper, we proposed a hybrid, lightweight intrusion detection system integrated for sensor networks. Our intrusion detection scheme take advantage of cluster-based protocol to build a hierarchical network and provide an intrusion framework based both on anomaly and misuse …",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:2osOgNQ5qMEC,,1
64,Optimal selection and activation of intrusion detection agents for wireless sensor networks,"Intrusion detection is an efficient method against attacks in computer network and system. Most intrusion detection system for sensor networks and ad hoc networks are based on the cooperative of intrusion detection agents and suffered from a computation and memory usage overload. In this paper, we address this problem by proposing two algorithms to optimal select and activate the intrusion detection agents for sensor networks.",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:u-x6o8ySG0sC,,1
65,Enhanced group-based key management scheme for wireless sensor networks using deployment knowledge,"Key establishment plays a central role in authentication and encryption in wireless sensor networks, especially when they are mainly deployed in hostile environments. Because of the strict constraints in power, processing and storage, designing an efficient key establishment protocol is not a trivial task. Compared with public key cryptography, symmetric key cryptographic with key predistribution mechanism is more suitable for large-scale wireless sensor networks. Most of previous solutions have some issues on performance and security capabilities. In this paper, we propose a novel key predistribution model using pre-deployment knowledge and random values in pairwise key generation to take advantage in terms of network connectivity, memory cost, energy for transmission and strong resilience against node capture attacks.",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:UeHWp8X0CEIC,,1
66,Double-sided auctions applied to vertical handover for mobility management in wireless networks,"In the next generation Internet, we have seen the convergence of multimedia services and Internet with the mobility of users. Vertical handover decision (VHD) algorithm are essential components of the mobility management architecture in mobile wireless networks. VHD algorithms help mobile users to choose the best mobile network to connect among available candidates. It also can help the network manager to optimize easily the limited resources shared among the network providers and the users. In this article, we formulate VHD algorithm as a resource allocation problem for down-link transmission power in multiple W-CDMA networks and show how combinatorial double-sided auctions can be applied to this specific problem. The proposed pricing schemes make use of the signal interference to noise ratio, achievable data rates, power allocation at mobile networks, and monetary cost as decision criteria and …",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:IjCSPb-OGe4C,,1
67,Inter‐domain pricing: challenges and possible approaches,"Telecommunication networks are now an interconnection of competitive operators that need to cooperate to ensure end‐to‐end delivery of traffic. Inter‐domain agreements have to be performed, and pricing is seen as a relevant way to provide an incentive to intermediate domains for forwarding the traffic of others. In this paper, we first review propositions from the literature. Greatest consideration is given to the Vickrey–Clarke–Groves (VCG) auction because it provides appropriate incentives, leads to an efficient use of the network and satisfies other relevant properties. We highlight, on the other hand, that it is neither budget‐balanced nor robust to collusion—something forgotten in the inter‐domain context. This brings us to review the list of properties that we would ideally like the inter‐domain pricing scheme to satisfy, and claim that no mechanism can satisfy all of them together. We then adapt to inter‐domain …",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:W7OEmFMy1HYC,,1
68,The Practice of Cloud-based Navigation System for Indoor Robot,"The indoor positioning system has seen an explosion of advances in recent years, particularly in demands for location-based services. One such important location-based service is indoor navigation for robots based on an indoor positioning system, which is used to guide blind people and locate patients in a hospital. However, most indoor navigation systems are based on location information depended on respect signal from at least three reference access points. In this paper, we design a cloud-based navigation system for indoor robot through a Wi-Fi located at the one-hop wireless network. We compare our proposal and baseline techniques in the real-world indoor environment. The practical results show that the robot using the fixed edge cloud can move in accuracy achieved position and efficiently reach to the given destination position. Moreover, an indoor robot using a single access point and cloud service …",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:roLk4NBRz8UC,,1
69,Minimizing the intrusion detection modules in wireless sensor networks,"Intrusion detection is an efficient method against attacks in computer network and system. Most intrusion detection system for sensor networks and ad hoc networks are based on the cooperative of intrusion detection agents and suffered from a computation and memory usage overload. In this paper, we address this problem by proposing a lightweight algorithm to select and activate the intrusion detection modules on particular sensor nodes. We also apply over-hearing mechanism to reduce the redundant sending alert packets thus enhance network lifetimes.",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:qjMakFHDy7sC,,1
70,Network anomaly detection based on late fusion of several machine learning algorithms,"Today's Internet and enterprise networks are so popular as they can easily provide multimedia and e-commerce services to millions of users over the Internet in our daily lives. Since then, security has been a challenging problem in the Internet's world. That issue is called Cyberwar, in which attackers can aim or raise Distributed Denial of Service (DDoS) to others to take down the operation of enterprises Intranet. Therefore, the need of applying an Intrusion Detection System (IDS) is very important to enterprise networks. In this paper, we propose a smarter solution to detect network anomalies in Cyberwar using Stacking techniques in which we apply three popular machine learning models: k-nearest neighbor algorithm (KNN), Adaptive Boosting (AdaBoost), and Random Decision Forests (RandomForest). Our proposed scheme uses the Logistic Regression method to automatically search for better parameters to the Stacking model. We do the performance evaluation of our proposed scheme on the latest data set NSL-KDD 2019 dataset. We also compare the achieved results with individual machine learning models to show that our proposed model achieves much higher accuracy than previous works.",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:UebtZRa9Y70C,,1
71,Human extraction from a sequence of depth images using segmentation and foreground detection,"This paper investigates on an approach of how to extract and track multiple subjects from a sequence of depth images. The Kinect camera is used to obtain a depth image revealing the depth information of a scene. Our proposed system includes the object clustering module to segment different isolated regions correspondent to objects in a depth image and foreground detection module to find moving regions from a sequence of frames. The combination of the two modules let us know which object is moving within a sequence of frames to locate a human subject. In order to extract the depth silhouettes of multiple subjects during time, we propose the use of matching algorithm between two consecutive frames to track their movements. We evaluate the algorithm with a long sequence of frames within a complex environment containing backgrounds with furniture and show how the algorithm is able to precisely extract …",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:ufrVoPGSRksC,,1
72,Combinatorial double-sided auctions for network bandwidth allocation: a budget-balanced and decentralized approach,"Telecommunication networks are now an interconnection of competitive operators that need to cooperate to ensure end-to-end delivery of traffic. Inter-domain agreements have to be performed, and pricing is seen as a relevant way to reward intermediate domains for forwarding the traffic of others. In previous works, Vickrey–Clark–Groves (VCG) double-sided auctions have been applied because they provide proper incentives, lead to an efficient use of the network, and verify other relevant characteristics. However, it has been highlighted that the resource allocation schemes applying VCG auction are neither budget-balanced nor solvable in a decentralized way. In this paper, we apply combinatorial double-sided auction to allocate the bandwidth resources over nodes. While previous works were using a centralized algorithm, we use here a new pricing rule, leading to a new budget-balanced pricing …",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:Tyk-4Ss8FVUC,,1
73,State-space modeling based on principal component analysis and oxygenated-deoxygenated correlation to improve near-infrared spectroscopy signals,"Near infrared spectroscopy (NIRS) is currently becoming an effective technique for noninvasive functional brain imaging. Therefore, the methods to improve the quality of measured NIRS signals play an important role to make NIRS broadly accepted in practical applications. Previously, there have been approaches using state-space modeling to recover the NIRS signals from basic component signals to eliminate the artifacts presented in the NIRS measurements. However, the proposed approach requires us an onset vector to determine the starting position of stimulus that is not always available in practical situation. In this work, we provide a new way to find the basic components for efficient implementations of the state-space modeling. We apply principal component analysis to estimate eigenvector-based basis that presents the compact information of the whole signals. We utilize the oxygenated-deoxygenated …",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:WF5omc3nYNoC,,1
74,Architecture for IDS log processing using spark streaming,"In a large network enterprise system, the use network intrusion detection system (N-IDS) become popular since it has very important role and a challenging task to the network manager in term of security management. Existing network systems develop and expand both in terms of network size, load, and application traffic so the processing of a single IDS is not enough and imposed a high overload on the system. Therefore, there is a need for upgrading a novel IDS system to adapt to the new challenges. To improve the performance of the entire N-IDS system, the traditional way is to replace it with a higher performance server to meet the requirements of processing and storage or using several N-IDS systems. However, in those types of systems, the cost is often expensive but the processing and representation data real-time is still very limited and it does not meet the urgent requirements of security manager. In this …",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:kNdYIx-mwKoC,,1
75,A Pricing Model for Sharing Cloudlets in Mobile Cloud Computing,"Driven by pervasive mobile devices and ubiquitous wireless communication networks, mobile cloud computing emerges as an appealing paradigm to accommodate demands for running power-hungry or computation-intensive applications over resource-constrained mobile devices. Cloudlets that move available resources closer to the network edge offer a promising architecture to support real-time applications, such as online gaming and speech recognition. To stimulate service provisioning by cloudlets, it is essential to design an incentive mechanism that charges mobile devices and rewards cloudlets. Although auction has been considered as a promising form for incentive, it is challenging to design an auction mechanism that holds certain desirable properties for the cloudlet scenario. In the paper, we propose an appropriate auction mechanism for the allocation of resource between mobile devices as service …",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:LkGwnXOMwfcC,,1
76,A distributed auction-based algorithm to allocate bandwidth over paths,"In the literature, Vickrey-Clark-Groves (VCG) double-sided auctions have been applied to inter-domain traffic exchange because they provide incentives to be truthful and lead to an efficient use of the network, among relevant properties of mechanism design. Unfortunately, the resulting resource allocation scheme is neither budget-balanced nor solvable in a decentralized way, two important properties. We present a different but more realistic auction-based algorithm for allocating bandwidth over paths to end users or ISPs, leading to a new budget-balanced pricing scheme for which allocations and charges can be computed in a decentralized way.",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:Y0pCki6q_DkC,,1
77,A Practical Comparison of Deep Learning Methods for Network Intrusion Detection,"Cybersecurity is essential nowadays due to the vast development of Internet leading to miscellaneous attacks to the cyber systems. Numerous measures have been proposed to prevent those attacks including Intrusion Detection System (IDS). In response to the rise of both quantity and quality of the invasion, the IDS has been constantly enhanced, and integrating knowledge base into existing IDS is the solution of interest. Among the implementations of such solution, Deep Learning is highly appreciated and has the potential to expand in the future, showing higher accuracy detecting network anomalies in comparison with Machine Learning. In this paper, a comparison is drawn between the efficiency of three novel Deep Learning approaches: RNN (Recurrent Neural Network), LSTM (Long short-term memory) and GRU (Gated Recurrent Unit), tackling the problem of anomaly detection on two datasets CICIDS2017 …",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:KlAtU1dfN6UC,,1
78,Performance Evaluation of AODV and AOMDV Routing Protocols Under Collaborative Blackhole and Wormhole Attacks,"Mobile Ad hoc Network (MANET) is easy to be attacked than wired networks due to its characteristics of open network topology, high mobility, lack of physical security and independent management. This paper analyzes the impact of routing attacks on the performance of the AODV and AOMDV routing protocol under several collaborative routing attack scenarios (blackhole and wormhole). We conclude that AOMDV is better than AODV in case of collaborative attacks and we show that collaborative Blackhole and Wormhole attacks can affect much performance of the network, especially in the case when we do not recognize the location area of malicious nodes.",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:Se3iqnhoufwC,,1
79,Impact of Vehicle Speed on Task Offloading of Vehicular Edge Computing,"Due to the dynamic topology character, vehicular edge networks are facing the challenges to support either delay-sensitive applications or throughput-sensitive applications. To address these problems, vehicular edge computing is explored by employing computing resources at the edge of vehicular edge networks. Moreover, the vehicle speed is an important performance metric to indicate the task offloading decisions of vehicular edge computing because of space and time dynamics of moving vehicles. Therefore, in this paper, we analyze the impact of vehicle speed on task offloading of vehicle edge computing in terms of total task completed latency of the application and packet loss ratio. Finally, we suggest a potential solution to address this issue in future work by employing the 5G-VANET heterogeneous networks.",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:hqOjcs7Dif8C,,1
80,Vehicular Cloud Computing Considering the 802.11 p-based MAC protocol,"With recent advancements in edge computing and vehicular ad hoc network (VANET), the vehicular edge cloud (VEC) is promising new paradigm that is utilized to speed up the running of vehicular computation workload by deploying VEC servers in close proximity to vehicles. In addition, medium access control (MAC) protocol plays a role in providing an efficient broadcast service. In this paper, we propose a model to combine vehicular cloud computing considering the 802.11 p-based MAC protocol environment. This model can improve the optimization problem in VEC adopted the contention based IEEE 802.11 p standard.",11,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=nIGjockAAAAJ&citation_for_view=nIGjockAAAAJ:MXK_kJrjxJIC,,1
87,An extended SDN controller for handover in heterogeneous wireless network,"In software defined networks, a centralized control is commonly used to provide a global view of the network and to simplify the network operation as well as network programmability. However, in the centralized model, the control path between the network devices and the controller can be a single point of failure or a potential bottleneck. Therefore, in this paper, the traditional SDN controller has been extended and then being embedded in every node, aiming to reduce the risk of losing the connection to the controller and the delay in passing commands from the controller to the devices. This model also enables the benefit of SDN to be utilized in wireless environment. For example, the system resilience has been improved by switching an ongoing connection from a physical interface to another one. The performance of the system was evaluated in a real testbed and the experiment results show that the system …",15,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=d_etJHwAAAAJ&citation_for_view=d_etJHwAAAAJ:u5HHmVD_uO8C,,1
88,An SDN Approach for an Energy Efficient Heterogeneous Communication Network in Disaster Scenarios,"Wireless access technologies have been extensively developed aiming to give users the ability to connect to their expected networks anytime, anywhere. This leads to an increment of the number of wireless interfaces integrated into a single mobile device, hence, it allows the device to be able to connect to multiple access networks. However, in some specific cases such as natural disasters, having an uncorrupted and timely information exchanging means is critical for affected victims to survive or to connect to the outside world. This is because the essential network infrastructures in these cases could be destroyed causing a large number of systems to stop working. In that cases, the victims need a heterogeneous communications network in which they can communicate, without a doubt, by using different wireless access technologies, i.e., Bluetooth or Wi-Fi. The network must also be able to smoothly change the access technologies, or called a vertical handover, to ensure QoS for ongoing applications. In addition, the network must have a mechanism to save energy. For hese reasons, an SDN approach, which has been proposed in a previous work, is considered. The performance of the system has been validated by a set of experiments in a real testbed. The obtained results show that the proposed vertical handover can save at least 24.42 per cent of the energy consumed by the wireless communication. The handover delay with different UDP traffic is less than 150ms. Moreover, the network allows a device using Bluetooth to talk with another one using Wi-Fi over a heterogeneous connection where the end-to-end jitter is mainly below 20ms and …",15,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=d_etJHwAAAAJ&citation_for_view=d_etJHwAAAAJ:IjCSPb-OGe4C,,1
89,An energy-efficient mobile-controlled vertical handover management for real time services,"Multiple wireless access technology has been embedded into a single mobile device as a fundamental feature, aiming to give end users ubiquitous access at any time. To allow the users to enjoy the ubiquitous connectivity, the mobile device has to consume higher energy for the simultaneous activation of multiple wireless interface and the continuous connectivity. In addition, a seamless vertical handover between the access technologies is a mandatory requirement to insure the quality, reliability and continuity of real time services. In this paper, the continuity of real time services as well as energy saving for mobile devices has been taken into account. The conceptual idea is that whenever traffic rate is lower than a threshold it will be smoothly handed over to a low energy consumption technology, i.e., Bluetooth. When the traffic exceeds the limitation of Bluetooth bandwidth, it will be handed over to a wider bandwidth technology, i.e., Wi-Fi. In the considered scenarios, the technologies are not interconnected; hence, the vertical handover management must be fully controlled by the mobile devices. The performance of the system including energy saving and maintaining the continuity of real time services has been evaluated by direct measurements in a real testbed.",15,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=d_etJHwAAAAJ&citation_for_view=d_etJHwAAAAJ:2osOgNQ5qMEC,,1
90,Investigating the performance of link aggregation on OpenFlow switches,"In this paper, we extensively explore the operation of Link Aggregation (LA) on OpenFlow switches in comparison to the LA in conventional switches. The comparison of two LA implementations has been conducted in a real testbed under the UDP and TCP traffic loads. The testbed includes Pica8 P-3925 switches, which support two modes: an OpenFlow switch (i.e., using Open vSwitch) and a conventional switch (i.e., using the operating system called XorPlus). The evaluation results show that two LA implementations achieve similar performance in improving throughput. However, the XorPlus implementation provides a better resilience than the other. Specifically, the LA implementation on XorPlus spends less than 1.49538 seconds to switch the TCP traffic on the faulty link to the other links of a Link Aggregation Group (LAG) while the switchover time is four times longer on the Open vSwitch. In the case of …",15,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=d_etJHwAAAAJ&citation_for_view=d_etJHwAAAAJ:Y0pCki6q_DkC,,1
91,Feasibility of SDN-based vertical handover between bluetooth and Wi-Fi,"In natural disasters, when essential network infrastructures are destroyed, Bluetooth networking can be a potential candidate to connect devices directly at a low cost and with little energy consumption. However, it typically permits short range communications, i.e., 10 meters. To communicate over a longer distance, the network needs more nodes or the nodes use another technology such as Wi-Fi. The Wi-Fi technology offers a wider communication coverage but it consumes more energy. Therefore, when two nodes change their connection from one wireless network to another due to the power limitation and the communication coverage, a vertical handover is necessary to maintain the ongoing session between them. In this paper, the performance of handover between Bluetooth and Wi-Fi is evaluated in a real testbed. The communication behavior of the system components is also described in order to explain why …",15,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=d_etJHwAAAAJ&citation_for_view=d_etJHwAAAAJ:zYLM7Y9cAGgC,,1
92,A self-adaptive and energy-efficient wireless multi-hop network,"In wireless multi-hop networks, mobiles devices communicate with the others without relying on any infrastructure networks or centralized control. When two end devices are communicating, they commonly require intermediate devices to cooperate for maintaining the network connectivity. Although the communication is for only two devices, the other devices have to consume their battery power on relaying messages. As a result, the network-wide energy consumption is much higher than the energy consumed by only two end devices. To reduce the network-wide energy consumption, one potential way is to tradeoff between throughput and energy consumption on the intermediate devices. For example, when the throughput is not too high, i.e., less than 1Mbps, the intermediate devices can turn off the high-speed wireless network interface and switch to a low energy consumption one, called an energy-efficient …",15,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=d_etJHwAAAAJ&citation_for_view=d_etJHwAAAAJ:9yKSN-GCB0IC,,1
94,Experimental study on the performance of Linux Ethernet bonding,"Linux bonding is a feature allowing to group multiple physical network interfaces into a logical one on Linux machines. Known as a low-cost method to improve fault tolerance and network throughput, the Linux bonding with seven supported modes is increasingly deployed in various scenarios such as datacenters, home networks, etc. However, the strengths and weaknesses of different modes have not been well investigated. While previous works mostly pay attention on the performance of the popular round-robin mode, this work extensively and additionally evaluates other modes based on three major criteria: throughput improvement, load balancing, and fault tolerance. To the best of our knowledge, this is the first work investigating the capabilities of fault tolerance using Linux bonding. The evaluation results show that the active-backup mode achieves the flow switch-over time, which is the duration of …",15,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=d_etJHwAAAAJ&citation_for_view=d_etJHwAAAAJ:W7OEmFMy1HYC,,1
95,FFCDH: Solution to enable face-to-face conversation between deaf and hearing people,"A real time communication between deaf and hearing people is still a barrier that isolates the deaf people from the hearing world. Over ninety percent of deaf children are born to hearing parents. However, most of them can only learn how to communicate using sign language at school. One of the reasons is that the hearings parents have neither enough time nor support to learn sign language to communicate and support their children. Not surprisingly, the deaf finds difficulties in the oral-only education. Since many other hearing pupils do not even know about the existence of sign language, they cannot communicate directly with the deaf without a sign language interpreter. Therefore, to enable a face-to-face conversation between deaf and hearing people, it is important not only to sustain real time conversation between the deaf and their hearing counterparts but also to equip the hearing with basics of sign language. However, speech to sign conversion remains a challenge due to dialectal and sign language variation, speech utterance and lack of sign language written form. In this paper, a solution named Face-to-Face Conversation Deaf and Hearing people—FFCDH is proposed to address above issues. FFCDH supports real time conversation and also allows the hearing to learn the signs with the same meaning as the deaf understand. Moreover, FFCDH records the speech of the hearing and converts it into signs for the deaf. It also provides deaf with an option to adjust volume of their speech by displaying volume of their voice. The performance of the system in supporting the deaf has been evaluated by using a real test-bed. The obtained …",15,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=d_etJHwAAAAJ&citation_for_view=d_etJHwAAAAJ:qjMakFHDy7sC,,1
96,Visualization of spoken language for deaf people,"The academic achievement of the deaf is related to the difficulty in communication, not to the cognitive abilities. Without communication, their needs are generally misinterpreted and ignored. Accumulated years of misunderstanding will impair their academic as well as social success. Since the hearing world uses spoken language to communicate, the deaf are commonly isolated because they can use only sign language and lip-reading. The deaf ultimately go to special need schools where they can find a community with common ground and they can use sign language in their daily conversation. The special need schools are either primary or secondary ones, which offer free education to all children. However, special need schools often cannot offer the same broad curriculum as the general schools do. Therefore, in this work, a system that can support the deaf in general education is proposed. The system operates based on the cooperation between smart devices (including mobile devices) and wearable devices. The performance of the system has been validated based on a real test bed. The obtained results show that the system can support the deaf in realtime communication.",15,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=d_etJHwAAAAJ&citation_for_view=d_etJHwAAAAJ:UeHWp8X0CEIC,,1
97,A Pure Network-Based Approach to Achieve Always Best Quality Video Streaming,"The demand for video streaming has been more and more increasing, causing the streaming technology and the related technologies to be improved to meet the requirement of the best quality of experience (QoE) from various users. A lot of research has been focusing on studying users’ behavior or developing streaming client and/or server application. These works estimate the network state passively and are lack of a global view of the network. As a result, they meet difficulty in bandwidth competition, QoE fairness scenarios. Some works optimize routing mechanism to improve video quality and QoE. This work also proposes a pure network-based approach, however taking into account the characteristics of video streaming application, to support an always best QoE to end-users. The proposed approach leverages the advantage of SDN network to convert the explored characteristic of streaming application into …",15,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=d_etJHwAAAAJ&citation_for_view=d_etJHwAAAAJ:ufrVoPGSRksC,,1
98,To verify the correctness of IoT sensor data in real-time,"Internet of things (IoT) systems are collecting an enormous amount of sensor data which in turn play a critical role in their decision-making process. However, the more sensor data are collected, the higher probability the faulty data occurs. The existence of faulty data may drive any system to make incorrect decisions and actions. It is clear that the faulty data should be detected and corrected as earlier as possible. In other words, the sensor data should be verified in the collecting steps. To best of our knowledge, the sensor data verification in the collecting process has not been well investigated. This research proposes a framework to address this issue right before the sensor data are stored. The verification makes use of a data forecasting technique to estimate the correct range of values for each received point of sensor data. The proposed system was implemented on a real test-bed on a Raspberry Pi 3. The …",15,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=d_etJHwAAAAJ&citation_for_view=d_etJHwAAAAJ:WF5omc3nYNoC,,1
99,Qbf: A query broker framework for adaptable query evaluation,"We present QBF (Query Broker Framework), a query evaluation framework that facilitates reuse and design of flexible query processors. QBF integrates existing optimization and execution mechanisms in a uniform way. It allows the use of multiple mechanisms to efficiently and adaptively evaluate queries according to application requirements. The smallest query evaluation unit built from QBF is called a Query Broker. Query brokers can be organized in hierarchies for evaluating queries. The paper describes the QBF components and the way they interact. It also introduces one implementation of these components so as illustrating query processing strategies supported by QBF and their use.",14,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=68XMiSkAAAAJ&citation_for_view=68XMiSkAAAAJ:d1gkVwhDpl0C,,1
100,Towards a mediation system framework for transparent access to largely distributed sources,"This paper presents the MediaGrid project whose goal is the definition of a mediation framework for transparent access to largely distributed sources. Frameworks are reusable pieces of design being expressed as a set of interfaces and components together with the description of their collaboration. Research topics addressed by the MediaGrid project include meta-data design, generation of mediation queries and adaptive and interactive query evaluation.",14,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=68XMiSkAAAAJ&citation_for_view=68XMiSkAAAAJ:qjMakFHDy7sC,,1
101,Adaptable query evaluation using qbf,"This work shows how to provide adaptable query evaluation using QBF, a query broker framework that provides a set of tools facilitating the design and implementation of query function in data-intensive distributed applications. The framework allows the use of multiple mechanisms to adaptively evaluate queries according to application requirements and its execution context. The smallest query evaluation unit built on the framework is called a query broker. Query brokers can be organized in hierarchies for evaluating distributed queries. This paper concentrates on the adaptability of query brokers so as to illustrate query processing strategies supported by QBF.",14,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=68XMiSkAAAAJ&citation_for_view=68XMiSkAAAAJ:u-x6o8ySG0sC,,1
102,An ensemble system with random projection and dynamic ensemble selection,"In this paper, we propose using dynamic ensemble selection (DES) method on ensemble generated based on random projection. We first construct the homogeneous ensemble in which a set of base classifier is obtained by a learning algorithm on different training schemes generated by projecting the original training set to lower dimensional down spaces. We then develop a DES method on those base classifiers so that a subset of base classifiers is selected to predict label for each test sample. Here competence of a classifier is evaluated based on its prediction results on the test sample’s  nearest neighbors obtaining from the projected data of validation set. Our proposed method, therefore, gains the benefits not only from the random projection in dimensionality reduction and diverse training schemes generation but also from DES method in choosing an appropriate subset of base classifiers for each …",14,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=68XMiSkAAAAJ&citation_for_view=68XMiSkAAAAJ:zYLM7Y9cAGgC,,1
103,Multiple Peer Chord Rings Approach for Device Discovery in IoT Environment,"Recently, with the advances of information and electrical technologies, Internet of Thing (IoT) has moved from being a far vision to an increasing market reality. IoT promises to connect all physical “things” via a dynamic global network infrastructure and create intelligent behaviors in different contexts without human manipulations. In line with this development, the number of IoT solutions also have augmented exponentially covering many domains from health care, urban management to life quality improvement and so forth. However, separate appearances of these solutions today has led to barriers in enlarging smart environments because they lack a mechanism for integration. It is well-known that in the very near future, IoT requires the capabilities of high scale and dynamic adaptation with linking many new and existing smart contexts together. Hence, the need of having a comprehensive management method for …",14,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=68XMiSkAAAAJ&citation_for_view=68XMiSkAAAAJ:IjCSPb-OGe4C,,1
104,Query Brokers for Distributed and Flexible Query Evaluation.,"This paper presents our work on supporting flexi-ble query evaluation over large distributed, heterogeneous, and autonomous sources. Flexibility means that the query evaluation process can be configured according to application contextspecific, resources constraints and also can interact with its execution environment.",14,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=68XMiSkAAAAJ&citation_for_view=68XMiSkAAAAJ:9yKSN-GCB0IC,,1
106,A strategy to develop adaptive and interactive query brokers,"This paper focuses on the software architecture and components for supporting core adaptive and interactive query processing. The architecture permits the reuse of most of the existing adaptation techniques and facilitates the development of new ones. Developers will use it to program adaptable and interactive query evaluators, called Query Brokers. Three adaptability levels are considered:(i) static adaptability, achieved at query broker building time,(ii) personalization, achieved for each (user) query having specific needs, and (iii) dynamic adaptability, achieved during query evaluation.",14,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=68XMiSkAAAAJ&citation_for_view=68XMiSkAAAAJ:u5HHmVD_uO8C,,1
109,Improved cuckoo search and chaotic flower pollination optimization algorithm for maximizing area coverage in wireless sensor networks,"The popularity of Wireless Sensor Networks (WSNs) is rapidly growing due to its wide-ranged applications such as industrial diagnostics, environment monitoring or surveillance. High-quality construction of WSNs is increasingly demanding due to the ubiquity of WSNs. The current work is focused on improving one of the most crucial criteria that appear to exert an enormous impact on the WSNs performance, namely the area coverage. The proposed model is involved with sensor nodes deployment which maximizes the area coverage. This problem is proved to be NP-hard. Although such algorithms to handle this problem with fairly acceptable solutions had been introduced, most of them still heavily suffer from several issues including the large computation time and solution instability. Hence, the existing work proposed ways to overcome such difficulties by proposing two nature-based algorithms, namely …",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:_FxGoFyzp5QC,,1
110,Evolutionary algorithms to optimize task scheduling problem for the IoT based bag-of-tasks application in cloud–fog computing environment,"In recent years, constant developments in Internet of Things (IoT) generate large amounts of data, which put pressure on Cloud computing’s infrastructure. The proposed Fog computing architecture is considered the next generation of Cloud Computing for meeting the requirements posed by the device network of IoT. One of the obstacles of Fog Computing is distribution of computing resources to minimize completion time and operating cost. The following study introduces a new approach to optimize task scheduling problem for Bag-of-Tasks applications in Cloud–Fog environment in terms of execution time and operating costs. The proposed algorithm named TCaS was tested on 11 datasets varying in size. The experimental results show an improvement of 15.11% compared to the Bee Life Algorithm (BLA) and 11.04% compared to Modified Particle Swarm Optimization (MPSO), while achieving balance between completing time and operating cost.",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:738O_yMBCRsC,,1
111,An efficient genetic algorithm for maximizing area coverage in wireless sensor networks,"Wireless sensor networks collect and transfer environmental data from a predefined region to a base station to be processed and analyzed. A major problem when designing these networks is deploying sensors such that their area coverage is maximized. Given a number of sensors with heterogeneous sensing ranges, the problem of coverage maximization is known to be NP-hard. As such, prevailing methods often rely on metaheuristic techniques while employing approximated fitness functions, resulting in modest solution quality and stability. This paper proposes a novel and efficient metaheuristic in the form of a genetic algorithm, which overcomes several weaknesses of existing metaheuristics, along with an exact method for calculating the fitness function for this problem. The proposed genetic algorithm includes a heuristic population initialization procedure, the proposed exact integral area calculation for the …",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:a0OBvERweLwC,,1
112,Biosynthesis of silver and gold nanoparticles using aqueous extract from Crinum latifolium leaf and their applications forward antibacterial effect and wastewater treatment,"Crinum latifolium (CL) leaf is a source of various biologically active compounds such as alkaloid and phenolic compounds, which exhibit anti-inflammatory, antitumor, and antimicrobial effects. In the purpose of expanding applications for the field of bionanotechnology, we report biosynthesis of silver nanoparticles (AgNPs) and gold nanoparticles (AuNPs) by using aqueous extract from C. latifolium leaf and explore antibacterial activity and catalytic performance for degradation of pollutants. The formation of CL-AgNPs and CL-AuNPs is confirmed and optimized by UV-visible spectroscopy with surface plasmon resonance (SPR) peaks at around 402 and 539 nm, respectively. The spherical CL-AgNPs have an average diameter of 20.5 nm and the multishaped CL-AuNPs possess an average size of 17.6 nm. The actions of four bacterial strains were strongly inhibited by using the CL-AgNPs. Furthermore, the biosynthesized metallic nanoparticles (MNPs) exhibited the excellent catalytic degradation performance of pollutants.",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:KxtntwgDAa4C,,1
113,An evolutionary algorithm for solving task scheduling problem in cloud-fog computing environment,"Recently, IoT (Internet of Things) has grown steadily, which generates a tremendous amount of data and puts pressure on the cloud computing infrastructures. Fog computing architecture is proposed to be the next generation of the cloud computing to meet the requirements of the IoT network. One of the big challenges of fog computing is resource management and operating function, as task scheduling, which guarantees a high-performance and cost-effective service. We propose TCaS-an evolutionary algorithm to deal with Bag-of-Tasks application in cloud-fog computing environment. By addressing the tasks in this distributed system, our proposed approach aimed at achieving the optimal tradeoff between the execution time and operating costs. We verify our proposal by extensive simulation with various size of data set, and the experimental results demonstrate that our scheduling algorithm outperforms 38.6 …",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:4OULZ7Gr8RgC,,1
114,Poisongan: Generative poisoning attacks against federated learning in edge computing systems,"Edge computing is a key-enabling technology that meets continuously increasing requirements for the intelligent Internet-of-Things (IoT) applications. To cope with the increasing privacy leakages of machine learning while benefiting from unbalanced data distributions, federated learning has been wildly adopted as a novel intelligent edge computing framework with a localized training mechanism. However, recent studies found that the federated learning framework exhibits inherent vulnerabilities on active attacks, and poisoning attack is one of the most powerful and secluded attacks where the functionalities of the global model could be damaged through attacker’s well-crafted local updates. In this article, we give a comprehensive exploration of the poisoning attack mechanisms in the context of federated learning. We first present a poison data generation method, named  Data_Gen , based on the generative …",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:wbdj-CoPYUoC,,1
115,Kinetics of charge carrier recombination in  crystals,"Cathodoluminescence (CL) spectra were measured to determine the characteristics of luminescence bands and carrier dynamics in β− Ga 2 O 3 bulk single crystals. The CL emission was found to be dominated by a broad UV emission peaked at 3.40 eV, which exhibits strong quenching with increasing temperature; however, its spectral shape and energy position remain virtually unchanged. We observed a superlinear increase of CL intensity with excitation density; this kinetics of carrier recombination can be explained in terms of carrier trapping and charge transfer at Fe impurity centers. The temperature-dependent properties of this UV band are consistent with weakly bound electrons in self-trapped excitons with an activation energy of 48±10 meV. In addition to the self-trapped exciton emission, a blue luminescence (BL) band is shown to be related to a donor-like defect, which increases significantly in …",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:bnK-pcrLprsC,,1
116,New approach to solving the clustered shortest-path tree problem based on reducing the search space of evolutionary algorithm,"Along with the development of manufacture and services, the problem of distribution network optimization has been growing in importance, thus receiving much attention from the research community. One of the most recently introduced network optimization problems is the Clustered Shortest-Path Tree Problem (CluSTP). Since the problem is NP-Hard, recent approaches often prefer to use approximation algorithms to solve it, several of which used Evolutionary Algorithms (EAs) and have been proven to be effective. However, most of the prior studies directly applied EAs to the whole CluSTP problem, which leads to a great amount of resource consumption, especially when the problem size is large. To overcome these limitations, this paper suggests a method for reducing the search space of the EAs applied to CluSTP by decomposing the original problem into two sub-problems, the solution to one of which is found …",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:abG-DnoFyZgC,,1
117,Metaheuristics for maximization of obstacles constrained area coverage in heterogeneous wireless sensor networks,"Wireless Sensor Networks (WSNs) collect and transfer environmental data from a predefined field to a base station to be processed and analyzed. A major problem in designing WSNs is coverage maximization, in which a given number of sensor nodes must be deployed in a way that maximizes area coverage of a given network, without violating practical constraints. This is a known NP-hard problem and thus requires metaheuristic approaches for practical problem sizes.",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:SP6oXDckpogC,,1
118,Effective multifactorial evolutionary algorithm for solving the cluster shortest path tree problem,"Arising from the need of all time for optimization of irrigation systems, distribution network and cable network, the Cluster Shortest Path Tree Problem (CSTP) has been attracting a lot of attention and interest from the research community. For such an NP-Hard problem with a great dimensionality, the approximation approach is usually taken. Evolutionary Algorithms, based on biological evolution, has been proved to be effective in finding approximate solutions to problems of various fields. The multifactorial evolutionary algorithm (MFEA) is one of the most recently exploited realms of EAs and its performance in solving optimization problems has been very promising. The main difference between the MFEA and the traditional Genetic Algorithm (GA) is that the former can solve multiple tasks at the same time and take advantage of implicit genetic transfer in a multitasking problem, while the latter solves one problem and …",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:WbkHhVStYXYC,,1
119,An effective representation scheme in multifactorial evolutionary algorithm for solving cluster shortest-path tree problem,"The wide range of applications of Cluster Tree Problems has been motivating extensive research into various algorithms and techniques with a view to promoting both efficiency of the solving and qualities of solutions. A representative of Cluster Tree Problems, the Cluster Shortest-Path Tree Problem (CSTP) arose from the practical need to optimize network systems such as irrigation systems, network cables and distribution systems. In this paper, we proposed the Multifactorial Evolutionary Algorithm (MFEA) to approach the CSTP with a representation scheme based on the Cayley Code. The proposed algorithm exploit advantages of Cayley Code for improving the MFEAs performance and quality solutions. This approach also applied new decoding method to transform the solution from the unified search space to the tasks. Experiments were conducted to compare the performances of the proposed to another …",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:geHnlv5EZngC,,1
120,An efficient framework for pixel-wise building segmentation from aerial images,Detection of buildings in aerial images is an important and challenging task in computer vision and aerial image interpretation. This paper presents an efficient approach that combines Random forest (RF) and a fully connected conditional random field (CRF) on various features for the detection and segmentation of buildings at pixel level. RF allows one to learn extremely fast on big aerial image data. The unary potentials given by RF are then combined in a fully connected conditional random field model for pixel-wise classification. The use of high dimensional Gaussian filter for pairwise potentials makes the inference tractable while obtaining high classification accuracy. Experiments have been conducted on a challenging aerial image dataset from a recent ISPRS Semantic Labeling Contest [9]. We obtained state-of-the-art accuracy with a reasonable computation time.,17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:hqOjcs7Dif8C,,1
121,An efficient strategy for using multifactorial optimization to solve the clustered shortest path tree problem,"Arising from the need of all time for optimization of irrigation systems, distribution network and cable network, Clustered Shortest-Path Tree Problem (CluSPT) has been attracting a lot of attention and interest from the research community. On the other hand, the Multifactorial Evolutionary Algorithm (MFEA) is one of the most recently exploited realms of Evolutionary Algorithms (EAs) and its performance in solving optimization problems has been very promising. Considering these characteristics, this paper describes a new approach using the MFEA for solving the CluSPT. The MFEA has two tasks: the goal of the first task is to determine the best tree (w.r.t. cost minimization) which envelops all vertices of the CluSPT while the goal of the second task is to find the fittest solution possible for the problem. The purpose of the second task is to find good materials for implicit genetic transfer process in MFEA to improve the …",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:u9iWguZQMMsC,,1
122,GAN-DP: Generative adversarial net driven differentially privacy-preserving big data publishing,"Increasing massive volume of data are generated every single second in this big data era. With big data from multiple sources, adversaries continuously mine private information for potential benefits. Motivated by this, we propose a generative adversarial net (GAN) driven noise generation method under the framework of differential privacy. We add one more perceptron, which is a specifically devised differential privacy identifier. After the generator produces the noise, the discriminator and the proposed identifier game with each other to derive the Nash Equilibrium. Extensive experimental results demonstrate the proposed model meets differential privacy constraints and upgrade data utility simultaneously.",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:xtRiw3GOFMkC,,1
123,Soft computing in wireless sensor networks,This book focuses on the suitable methods to solve optimization problems in wireless network system utilizing digital sensors like Wireless Sensor Network. This kind of system has been emerging as the cornerstone technology for all new smart devices and its direct application in many fields in life.,17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:fPk4N6BV_jEC,,1
124,Online load balancing for network functions virtualization,"Network Functions Virtualization (NFV) aims to support service providers to deploy various services in a more agile and cost-effective way. However, the softwarization and cloudification of network functions can result in severe congestion and low network performance. In this paper, we propose a solution to address this issue. We analyze and solve the online load balancing problem using multipath routing in NFV to optimize network performance in response to the dynamic changes of user demands. In particular, we first formulate the optimization problem of load balancing as a mixed integer linear program for achieving the optimal solution. We then develop the ORBIT algorithm that solves the online load balancing problem. The performance guarantee of ORBIT is analytically proved in comparison with the optimal offline solution. The experiment results on real-world datasets show that ORBIT performs very well …",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:hFOr9nPyWt4C,,1
125,An improved genetic algorithm for maximizing area coverage in wireless sensor networks,"In recent years, Wireless Sensor Networks (WSNs) have proved their power, but they also faced many practical challenges. One of such challenges is covering issue. This paper considers the maximum coverage deployment problem in WSNs. Particularly, with the given number of sensors having various sensing ranges, how to deploy these sensors in a specified domain so that their coverage on which is maximum. This is a NP-hard problem. We propose a new genetic algorithm with some improvements compared to an existing genetic one. These improvements include the definition of a new concept--the overlapping--for the fitness function, using a heuristic technique to initialize population and a dynamic mutation. Our algorithm is experimented on 15 instances constructed for this problem. The experimental results show that our proposed algorithm is effective in all terms of the computational complexity, quality of …",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:9yKSN-GCB0IC,,1
126,A survey on hybridizing genetic algorithm with dynamic programming for solving the traveling salesman problem,Traveling Salesman Problem (TSP) is a well-known NP-hard problem. Many algorithms were developed to solve this problem and gave the nearly optimal solutions within reasonable time. This paper presents a survey about the combination Genetic Algorithm (GA) with Dynamic Programming (DP) for solving TSP. We also setup a combination between GA and DP for this problem and experimented on 7 Euclidean instances derived from TSP-lib. Experimental results are reported to show the efficiency of the experimented algorithm comparing to the genetic algorithm.,17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:_Qo2XoVZTnwC,,1
127,A new hybrid genetic algorithm for solving the bounded diameter minimum spanning tree problem,"In this paper, a new hybrid genetic algorithm - known as HGA - is proposed for solving the bounded diameter minimum spanning tree (BDMST) problem. We experiment with HGA on two sets of benchmark problem instances, both Euclidean and Non-Euclidean. On the Euclidean problem instances, HGA is shown to outperform the previous best two genetic algorithms (GAs) reported in the BDMST literature, while on the Non-Euclidean problem instance, HGA performs comparably with these two GAs.",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:W7OEmFMy1HYC,,1
128,Novel biogenic silver nanoparticles used for antibacterial effect and catalytic degradation of contaminants,"The study reports a versatile, cost-efficient and ecofriendly protocol for the synthesis of biogenic silver nanoparticles (AgNPs) using the aqueous extracts of Quao Binh Chau, Stereospermum binhchauensis and Che Vang, Jasminum subtriplinerve and their application in antibacterial activity and catalysis. The AgNPs with varying morphology and physical properties have been optimized using the absorption measurements. The biogenic AgNPs have been characterized by Fourier transform infrared spectroscopy, UV–Vis spectroscopy, transmission electron microscopy, X-ray diffraction analysis, energy-dispersive X-ray spectroscopy and thermal behaviors. Stable crystalline AgNPs with average particle sizes of 20.0 nm and 8.0 nm were fabricated from aqueous extract of Quao Binh Chau and Che Vang, respectively. The phytochemicals from the extracts involved in reduction and stabilization of AgNPs are identified …",17,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=vJYe5lkAAAAJ&citation_for_view=vJYe5lkAAAAJ:p2g8aNsByqUC,,1
129,"Multi-task learning for smile detection, emotion recognition and gender classification","Facial expression analysis plays a key role in analyzing emotions and human behaviors. Smile detection, emotion recognition and gender classification are special tasks in facial expression analysis with various potential applications. In this paper, we propose an effective architecture of Convolutional Neural Network (CNN) which can jointly learn representations for three tasks: smile detection, emotion recognition and gender classification. In addition, this model can be trained from multiple sources of data with different kinds of task-specific class labels. The extensive experiments show that our model achieves superior accuracy over recent state-of-the-art techniques in all of three tasks on popular benchmarks. We also show that the joint learning helps the tasks with less data considerably benefit from other tasks with richer data.",22,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sv9ysfUAAAAJ&citation_for_view=sv9ysfUAAAAJ:UeHWp8X0CEIC,,1
131,Graphics processing unit optimizations for the dynamics of the HIRLAM weather forecast model,"Programmable graphics processing units (GPUs) nowadays offer tremendous computational resources for diverse applications. In this paper, we present the implementation of the dynamics routine of the HIRLAM weather forecast model on the NVIDIA GTX 480. The original Fortran code has been converted manually to C and CUDA. Empirically, it is determined what the optimal number of grid points per thread is, and what the best thread and block structures are. A significant amount of the elapsed time consists of transferring data between CPU and GPU. To reduce the impact of these transfer costs, we overlap calculation and transfer of data using multiple CUDA streams. We developed an algorithm that enables our code generator CTADEL to generate automatically the optimal CUDA streams program. Experiments are performed to find out if the applicability of GPUs is useful for Numerical Weather Prediction, in …",22,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sv9ysfUAAAAJ&citation_for_view=sv9ysfUAAAAJ:u5HHmVD_uO8C,,1
133,Efficient maximum matching algorithms for trapezoid graphs,"Trapezoid graphs are intersection graphs of trapezoids between two horizontal lines. Many NP-hard problems can be solved in polynomial time if they are restricted on trapezoid graphs. A matching in a graph is a set of pairwise disjoint edges, and a maximum matching is a matching of maximum size. In this paper, we first propose an O (n (log n) 3) algorithm for finding a maximum matching in trapezoid graphs, then improve the complexity to O (n (log n) 2). Finally, we generalize this algorithm to a larger graph class, namely k-trapezoid graphs. To the best of our knowledge, these are the first efficient maximum matching algorithms for trapezoid graphs.",22,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sv9ysfUAAAAJ&citation_for_view=sv9ysfUAAAAJ:Y0pCki6q_DkC,,1
134,Real-space and plane-wave hybrid method for electronic structure calculations for two-dimensional materials,"We propose a computational approach to combining the plane-wave method and the real-space treatment to describe the periodic variation in the material plane and the decay of wave functions from the material surfaces. The proposed approach is natural for two-dimensional material systems and thus may circumvent some intrinsic limitations involving the artificial replication of material layers in traditional supercell methods. In particular, we show that the proposed method is easy to implement and, especially, computationally effective since low-cost computational algorithms, such as iterative and recursive techniques, can be used to treat matrices with block tridiagonal structure. Using this approach we show first-principles features that supplement the current knowledge of some fundamental issues in bilayer graphene systems, including the coupling between the two graphene layers, the preservation of the σ …",22,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sv9ysfUAAAAJ&citation_for_view=sv9ysfUAAAAJ:W7OEmFMy1HYC,,1
135,Overlapping Communications With Calculations.,"The communication overhead in a parallel implementation of a numerical weather prediction model may possibly partly be hidden behind calculation time. We tried this in a proof-of-concept study with the HIRLAM Eulerian explicit grid point dynamics. We found that savings in total execution time depend on the available hardware. If the network is fast, the additional calculation costs are not compensated by the reduction in communication overhead. But if the interconnection network is not very fast, or on a Grid computer, we are able to almost completely hide communication time. We anticipate several problems before our results can be implemented in an operational configuration of HIRLAM, not the least of them being the code complexity, in particular in a semi-Lagrangian scheme.",22,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sv9ysfUAAAAJ&citation_for_view=sv9ysfUAAAAJ:zYLM7Y9cAGgC,,1
138,Theoretical Considerations on the Optimal Performance of Sub-100 Nanometer Top-Gated Graphene Field-Effect Transistors,"The operation and performance of top-gated sub-100 nanometer graphene channel field-effect transistors were investigated. The device model is designed for graphene with a narrow energy band gap epitaxially grown on the SiC substrate. The issue of graphene-metallic lead coupling is appropriately taken into account. By assuming the graphene-metal physisorption contact, a self-consistent calculation reproduces two regions of high carrier density at the ends of the graphene channels underneath the metallic leads according to the charge transferred effect between the metallic lead surface and graphene. The charge carrier densities in these source and drain regions, however, are not pinned, but vary with respect to the drain and gate voltages. It is shown that, in general, the graphene channel supports the ambipolar characteristics for all device samples, but for the samples with the channels shorter than 40 …",22,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sv9ysfUAAAAJ&citation_for_view=sv9ysfUAAAAJ:ufrVoPGSRksC,,1
139,A router-aided hierarchical P2P traffic localization based on variable additional delay insertion,"Most peer-to-peer (P2P) systems build their own overlay networks for implementing peer selection strategies without taking into account the locality on the underlay network. As a result, a large quantity of traffic crossing internet service providers (ISPs) or autonomous systems (ASes) is generated on the Internet. Controlling the P2P traffic is therefore becoming a big challenge for the ISPs. To control the cost of the cross-ISP/AS traffic, ISPs often throttle and/or even block P2P applications in their networks. In this paper, we propose a router-aided approach for localizing the P2P traffic hierarchically; it features the insertion of additional delay into each P2P packet based on geographical location of its destination. Compared to the existing approaches that solve the problem on the application layer, our proposed method does not require dedicated servers, cooperation between ISPs and P2P users, or modification of existing P2P application software. Therefore, the proposal can be easily utilized by all types of P2P applications. Experiments on P2P streaming applications indicate that our hierarchical traffic localization method not only reduces significantly the inter-domain traffic but also maintains a good performance of P2P applications.",23,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=a6MKQHoAAAAJ&citation_for_view=a6MKQHoAAAAJ:zYLM7Y9cAGgC,,1
140,Correlating objective factors with video quality experienced by end users on P2PTV,"To succeed in providing services, the quality of services should meet users’ satisfaction. This is a motivation to study the relationship between the service quality and the real perceived quality of users, which is commonly referred to as the quality of experience (QoE). However, most of existing QoE studies that focus on video-on-demand or IPTV services analyze only the influence of network behaviors to video quality. This paper focuses on P2P video streaming services, which are becoming a significant portion of Internet traffic, and pays attention to the change of users’ perception with the adjustment of objective factors as well as network behaviors. We propose to use mean opinion score and peak signal to noise ratio methods as QoE evaluations to consider the effect of the chunk loss ratio, the group-of-picture size, and the chunk size. The experimental results provide a convincing reference to build the complete relationship between objective factors and QoE. We believe that this assessment will contribute to study a new service quality evaluation mechanism based on users’ satisfaction in the future.",23,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=a6MKQHoAAAAJ&citation_for_view=a6MKQHoAAAAJ:Y0pCki6q_DkC,,1
141,QoE-driven bandwidth allocation method based on user characteristics,"Network providers are nowadays facing with a big question in allocating network resources due to the constantly increasing of Internet services. While the network resource is not unlimited, users always desire the best quality of experience (QoE) with the huge information exchange. Therefore, finding a justice of network resource allocation based on the user experience is mandatory. In previous studies, network resources were allocated to all users by using a specific utility function without considering the user characteristics. In fact, the network resource consumptions are very different among individual users and directly depend on users' behavior. For instance, the network demands of relax users are usually lower than those of busy users. Thus, allocating the same amount of resources to all users might not meet their expectations. In this paper, we propose two bandwidth allocation methods by classifying users …",23,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=a6MKQHoAAAAJ&citation_for_view=a6MKQHoAAAAJ:u5HHmVD_uO8C,,1
142,A hierarchical P2P traffic localization method with bandwidth limitation,"Recently, most peer-to-peer (P2P) applications ignore traffic costs at internet service providers (ISPs), and thus generate a large amount of traffic crossing ISPs or autonomous systems (ASes) on the Internet. To reduce the cross-ISP/AS traffic, the existing approaches introduce network-aware strategies in which a lot of modifications of P2P systems are required. In this paper, we propose a router-aided method for localizing P2P traffic hierarchically with multiple levels. By intentionally limiting the bandwidth of each connection path between peers based on geographical location of the peers' destinations, the traffic can be localized. Compared to the existing locality-enhancing approaches, our proposed method does not require dedicated servers, cooperation between ISPs and users, or any modification of existing P2P application software. Therefore, the proposal can be easily utilized by all types of P2P applications …",23,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=a6MKQHoAAAAJ&citation_for_view=a6MKQHoAAAAJ:WF5omc3nYNoC,,1
143,An evaluation of precise point positioning using QZSS LEX signal in Vietnam,"Recently, Japan Aerospace Exploration Agency (JAXA) provides Quasi-Zenith Satellite System (QZSS), a regional satellite system covering East Asia and Oceania region. The objective of QZSS is to improve the availability as well as the performance of GNSS system. Besides navigation signals, QZSS also transmits L-band Experimental signal (LEX) for augmentation corrections including orbit and clock information that support Precise Point Positioning (PPP). In this paper, a method for decoding LEX signal is first presented. Next, the availability of the LEX signal and the quality of the broadcasted correction messages for PPP solutions are evaluated in Vietnam. The results of PPP using LEX signal have been compared to those using augmentation corrections from International GNSS service (IGS) in both static and kinematic positioning modes. Experimental results conducted in Hanoi, Vietnam show the feasibility …",23,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=a6MKQHoAAAAJ&citation_for_view=a6MKQHoAAAAJ:_FxGoFyzp5QC,,1
144,Improving retake detection by adding motion feature,"Retake detection is useful for many applications of video summarization. It is a challenging task since different takes of the same scene are usually of different lengths; or have been recorded under different environment conditions. A general approach to solve this problem is to decompose the input video sequence into sub-sequences and then group these sub-sequences into clusters. By combining with temporal information, the clustering result is used to find take and scene boundaries. One of the most difficult steps in this approach is to cluster sub-sequences. Most of previous approaches only use one keyframe for representing one sub-sequence and extract features such as color and texture from this keyframe for clustering. We propose another approach to improve the performance by combining the motion feature extracted from each sub-sequence and the features extracted from each representing …",23,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=a6MKQHoAAAAJ&citation_for_view=a6MKQHoAAAAJ:9yKSN-GCB0IC,,1
145,P2ptv traffic localization by deep packet inspection,"Recently, peer-to-peer has been becoming a promising architecture for developing video streaming services because it can reduce the bottlenecks and the load at the server side. However, P2P systems implement their own routing protocols on overlay networks, which are largely independent of the Internet routing. As a result, the traffic generated by P2P systems is immense and unpredictable. Controlling the P2P traffic is therefore becoming a big challenge for internet service providers (ISPs). Considering the P2P traffic localization is one of the most efficient solutions to optimize the traffic. The existing approaches, however, require several modifications of the application software. In this paper, we propose a novel traffic localization method for P2P streaming application (P2PTV) by deep packet inspection. Based on the peer list and the geographical location of the listed peers, we propose two mechanisms for …",23,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=a6MKQHoAAAAJ&citation_for_view=a6MKQHoAAAAJ:ufrVoPGSRksC,,1
146,P2P traffic localization by forcing packet loss,"Recently, peer-to-peer (P2P) traffic is increasing rapidly in volume day by day. One of the main causes is that most of P2P applications including file sharing and streaming applications often form overlay networks for exchanging data that are oblivious to the underlay network topology. As a result, they generate a large amount of inter-domain traffic causing higher cost for Internet service providers (ISPs). This raises a problem of traffic localization. To optimize the cross-ISP/AS traffic, existing approaches focus on solving the problem on the application layer where each P2P application must be equipped with an additional protocol to obtain underlay network information from an “Oracle” server or an additional locality-aware procedure to estimate location by itself. Therefore some modifications of application software are required for these approaches. In this paper, we propose a novel solution for addressing the …",23,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=a6MKQHoAAAAJ&citation_for_view=a6MKQHoAAAAJ:qjMakFHDy7sC,,1
152,A Router-aided P2P Trafic Localization Method with Bandwidth Limitation,"Recently, peer-to-peer (P2P) systems generate a large amount of unwanted cross-domain traffic on the Internet due to a lack of knowledge about physical network topology. The unwanted cross-domain traffic is especially costly for the internet service providers (ISPs). To reduce the cross-ISP/AS (autonomous system) traffic, the existing approaches introduce network-aware strategies in which a lot of modifications of P2P systems are required. In par-ticular, each P2P application must be modified to integrate with a locality-aware procedure and/or a communication protocol to obtain the topological information from an “oracle” server. In this paper, we propose two schemes for localizing P2P traffic without any peer reaction: (1) fixed-length bandwidth limitation scheme, (2) hierarchical bandwidth limitation scheme. By intentionally limiting the bandwidth of each connection path between peers based on geographical location of the peers destinations, the traffic can be localized with single level for the first scheme and with multiple levels for the second scheme. Compared to the existing locality-enhancing approaches,our two schemes require neither dedicated servers, nor cooperation between ISPs and users, nor any modification of existing P2P application software. Therefore, we believe that all types of P2P applications can easily utilize our proposals. Experiments on P2P streaming applications indicate that the fixed-length bandwidth limitation scheme successfully realizes P2P traffic localization. Moreover, the hierarchical bandwidth limitation scheme not only significantly reduces the cross AS/ISP traffic but also maintains a good performance of P2P …",23,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=a6MKQHoAAAAJ&citation_for_view=a6MKQHoAAAAJ:eQOLeE2rZwMC,,1
156,A Peer List Modification Method for P2PTV Traffic Localization,"Recently, peer-to-peer (P2P) traffic has been increasing rapidly as a result of tremendous growing of video streaming services. Thus, P2P video streaming applications (P2PTV) such as PPStream [1], SopCast [2], and PPTV [3] have been getting more and more popular. In P2P systems, the transmission load is shifted from the server side to the network side, and therefore, the bottlenecks can be reduced. However, controlling the traffic generated by P2P systems becomes more challenging for internet service providers (ISPs) as well as research community. The main cause is that most of P2P applications are bandwidth hungry; they form overlay networks and establish connections based on not the network stability, but the resource availability. The overlay networks are therefore oblivious to the underlay network topology and ISP link cost due to a lack of knowledge about physical network information. As a result, P2P …",23,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=a6MKQHoAAAAJ&citation_for_view=a6MKQHoAAAAJ:UeHWp8X0CEIC,,1
157,On the Relationship between Chunk Loss Ratio and Quality of Experience on P2PTV,"Recently, the world has been experiencing the rapid expansion of peer-to-peer (P2P) communication technology and their applications. In the applications such as Napster, Gnutella, BitTorrent (P2P file sharing) and PPStream, Zattoo, PPTV (P2P video streaming applications or P2P television (P2PTV))[1], there are many advantages that make these applications rapidly developing. However, it is said that no one can guarantee reliable streaming since each user can become both a client and a server. Therefore, how to manage and control the quality in P2PTV is one of the leading considerations in P2P applications.",23,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=a6MKQHoAAAAJ&citation_for_view=a6MKQHoAAAAJ:u-x6o8ySG0sC,,1
159,Link prediction in co-authorship networks based on hybrid content similarity metric,"Link prediction in online social networks is used to determine new interactions among its members which are likely to occur in the future. Link prediction in the co-authorship network has been regarded as one of the main targets in link prediction researches so far. Researchers have focused on analyzing and proposing solutions to give efficient recommendation for authors who can work together in a science project. In order to give precise prediction of links between two ubiquitous authors in a co-authorship network, it is preferable to design a similarity metric between them and then utilizing it to determine the most possible co-author(s). However, the relevant researches did not regard the integration of paper’s content in the metric itself. This is important when considering the collaboration between scientists since it is possible that authors having same research interests are more likely to have a joint paper …",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:zYLM7Y9cAGgC,,1
160,Generating discourse structures for written text,"This paper presents a system for automatically generating discourse structures from written text. The system is divided into two levels: sentence-level and text-level. The sentence-level discourse parser uses syntactic information and cue phrases to segment sentences into elementary discourse units and to generate discourse structures of sentences. At the text-level, constraints about textual adjacency and textual organization are integrated in a beam search in order to generate best discourse structures. The experiments were done with documents from the RST Discourse Treebank. It shows promising results in a reasonable search space compared to the discourse trees generated by human analysts.",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:9ZlFYXVOiuMC,,1
161,An approach to abstractive text summarization,"Abstractive summarization is the technique of generating a summary of a text from its main ideas, not by copying verbatim most salient sentences from text. This is an important and challenge task in natural language processing. In this paper, we propose an approach to abstractive text summarization based on discourse rules, syntactic constraints, and word graph. Discourse rules and syntactic constraints are used in the process of generating sentences from keywords. Word graph is used in the sentence combination process to represent word relations in the text and to combine several sentences into one. Experimental results show that our approach is promising in solving the abstractive summarization task.",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:hqOjcs7Dif8C,,1
162,A study to improve the efficiency of a discourse parsing system,"This paper presents a study of the implementation of a discourse parsing system, where only significant features are considered. Rhetorical relations are recognized based on three types of cue phrases (the normal cue phrases, Noun-Phrase cues and Verb-Phrase cues), and different textual coherence devices. The parsing algorithm and its rule set are developed in order to create a system with high accuracy and low complexity. The data used in this system are taken from the RST Discourse Treebank of the Linguistic Data Consortium (LDC).",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:QIV2ME_5wuYC,,1
163,Natural language interface construction using semantic grammars,"This paper is a study on constructing a natural language interface to relational databases, which accepts natural language questions as inputs and generates textual responses. The question is translated into a SQL query using a semantic grammar and then, a database management system is left to find the result table with its own specialized optimization and planning techniques. The textual responses are generated from the result table based on another semantic grammar and the query type. Experimental results show that this approach can analyze a wide range of questions with high accuracy and produce reasonable textual responses.",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:qxL8FJ1GzNcC,,1
164,Automated discourse segmentation by syntactic information and cue phrases,This paper presents an approach to automatic segmentation of English written text into Elementary Discourse Units (EDUs) 1 using syntactic information and cue phrases. The system takes documents with syntactic information as the input and generates EDUs as well as their nucleus/satellite roles. The experiment shows that this approach can give promising result in comparison with existing research in discourse segmentation.,26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:UeHWp8X0CEIC,,1
165,Automatic feature selection for named entity recognition using genetic algorithm,"This paper presents a feature selection approach for named entity recognition using genetic algorithm. Different aspects of genetic algorithm including computational time and criteria for evaluating an individual (ie, size of the feature subset and the classifier's accuracy) are analyzed in order to optimize its learning process. Two machine learning algorithms, k-Nearest Neighbor and Conditional Random Fields, are used to calculate the accuracy of the named entity recognition system. To evaluate the effectiveness of our genetic algorithm, feature subsets returning by our proposed genetic algorithm are compared to feature subsets returning by a hill climbing algorithm and a backward one. Experimental results show that feature subsets obtained by our genetic algorithm is much smaller than the original feature set without losing of predictive accuracy. Furthermore, these feature subsets result in higher classifier's …",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:MXK_kJrjxJIC,,1
166,Combining proper name-coreference with conditional random fields for semi-supervised named entity recognition in Vietnamese text,"Named entity recognition (NER) is the process of seeking to locate atomic elements in text into predefined categories such as the names of persons, organizations and locations. Most existing NER systems are based on supervised learning. This method often requires a large amount of labelled training data, which is very time-consuming to build. To solve this problem, we introduce a semi-supervised learning method for recognizing named entities in Vietnamese text by combining proper name coreference, named-ambiguity heuristics with a powerful sequential learning model, Conditional Random Fields. Our approach inherits the idea of Liao and Veeramachaneni [6] and expands it by using proper name coreference. Starting by training the model using a small data set that is annotated manually, the learning model extracts high confident named entities and finds low confident ones by using proper name …",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:5nxA0vEk-isC,,1
169,Named entity recognition in Vietnamese text using label propagation,"This paper presents our named entity recognition system for Vietnamese text using labeled propagation. In here we propose: (i) a method of choosing noun phrases as the named entity candidates; (ii) a method to measure the word similarity; and (iii) a method of decreasing the effect of high frequency labels in labeled documents. Experimental results show that our labeled propagate method achieves higher accuracy than the old one [12]. In addition, when the number of the labeled data is small, its accuracy is higher than when using conditional random fields.",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:UebtZRa9Y70C,,1
170,Extracting phrases in Vietnamese document for summary generation,"This paper describes an approach to Vietnamese text summarization, concentrated on the discourse structure of the text. Based on characteristics of Vietnamese, we propose rules for segmenting text into elementary discourse units (edus) and for recognizing discourse relations between textual spans. The score of an edu is computed based on the discourse tree. The edus with highest scores are chosen to put in the summary. Experiments show that this method can give promising results.",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:3fE2CSJIrl8C,,1
171,Optimizing genetic algorithm in feature selection for named entity recognition,"This paper proposes some strategies to reduce the running time of genetic algorithms used in a feature selection task for the problem of named entity recognition. They include:(i) reduction of population size during the evolution process of the genetic algorithm;(ii) parallelization of the fitness computation; and (iii) use of progressive sampling for calculating the optimal sample size of the training data. Maximum Entropy algorithm is then used, as a test classifier, to compute the accuracy of the named entity recognition system with the reduced feature sets identified by the genetic algorithm. Experimental results show that our improved genetic algorithm run three time faster than the standard genetic algorithm, while the accuracy of the named entity recognition system (using Maximum Entropy) on the induced feature subset does not decrease. In addition, the feature subset induced by our improved genetic algorithm is …",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:WF5omc3nYNoC,,1
172,Relation extraction in Vietnamese text using conditional random fields,"Relation extraction is the task of finding semantic relations between entities from text. This paper presents our approach to relation extraction for Vietnamese text using Conditional Random Field. The features used in the system are words, part-of-speech tag, entity type, type of other entities in the sentence, entity’s index and contextual information. In order to evaluate the effect of the contextual information to the system performance, different window sizes have been tested in our experiments. It shown that the system performance is affected by the window size, but it is not directly proportional to the F-score of the system. Our future work includes: (i) testing the system with a larger corpus in order to get a more accurate evaluation of the system; (ii) investigating other features used in the CRF algorithm to increase the system performance; and (iii) researching methods to extract relations outside the sentence’s …",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:Y0pCki6q_DkC,,1
173,Name entity recognition using inductive logic programming,"Named entity recognition (NER) is the process of seeking to locate atomic elements in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, and percentages. It is useful in applying NER to other natural language tasks such as question-answering, text summarization, building semantic web, etc.",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:KlAtU1dfN6UC,,1
175,Using cohesive devices to recognize rhetorical relations in text,"This paper investigates factors that can be used in discourse analysis, specifically, cohesive devices. The paper shows that cohesive devices such as cue phrases can provide information about the linkages inside a text. We propose three types of cue phrases (the ordinary cue phrases, noun-phrase cues, and verb-phrase cues). An algorithm to compute rhetorical relations between two elementary discourse units is also presented.",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:9yKSN-GCB0IC,,1
176,Semi-supervised learning for relation extraction in vietnamese text,"Relation extraction (RE) is the task of finding semantic relations between entities from text. As the supervised learning method requires a large amount of labeled training data, the semi-supervised learning method is the topics of interest. This paper presents a semi-supervised learning approach to relation extraction for Vietnamese text using bootstrapping. As the accuracy of syntactic parsing in Vietnamese text is still not high, we used Shallow Linguistic Kernel (SLK) which combines global kernel and local kernel to present sentences. The differences between our SLK and Giuliano et al.'s SLK [5] are: our global kernel not only use bags of words but also use part of speech, another entities type, a dictionary of compound verbs; The window size of right kernel of our local context starts from the beginning of the sentence to the word immediately before the second entity, the window size of left kernel start from the word …",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:8k81kl-MbHgC,,1
181,Efficient syntactic parsing with beam search,"Implementing a Vietnamese syntactic parser is a difficult task due to the complexity of Vietnamese language. Most existing Vietnamese syntactic parsers are limited by types of sentences they can analyze. This paper introduces a syntactic parser that can analyze a larger range of Vietnamese sentences in a reasonable time. The proposed parser uses a probabilistic Head-Driven Phrase Structure Grammar that can control syntactic and semantic constraints of words in order to produce accurate results. The system uses a parsing algorithm that combines an improved version of the CYK algorithm and the idea of beam search in order to reduce the search space. Our experimental results achieved precision and recall of 82% and 74%, respectively.",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:kNdYIx-mwKoC,,1
182,A frame-based approach to text generation,"This paper is a study on constructing a natural language interface to database, concentrating on generating textual answers. TGEN, a system that generates textual answer from query result tables is presented. The TGEN architecture guarantees its portability across domains. A combination of a frame-based approach and natural language generation techniques in the TGEN provides text fluency and text flexibility. The implementation result shows that this approach is feasible while a deep NLG approach is still far to be reached.",26,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wwioq6sAAAAJ&citation_for_view=Wwioq6sAAAAJ:0EnyYjriUFMC,,1
187,Anomaly Detection with Multinomial Logistic Regression and Naïve Bayesian.,"Intrusion Detection by automated means is gaining widespread interest due to the serious impact of Intrusions on computer system or network. Several techniques have been introduced in an effort to minimize up to some extent the risk associated with Intrusion attack. In this paper, we have used two novel Machine Learning techniques including Multinomial Logistic Regression and Naïve Bayesian in building Anomaly-based Intrusion Detection System (IDS). Also, we create our own dataset based on four attack scenarios including TCP flood, ICMP flood, UDP flood and Scan port. Then, we will test the system’s ability of detecting anomaly-based intrusion activities using these two methods. Furthermore we will make the comparison of classification performance between the Multinomial Logistic Regression and Naïve Bayesian.",1,https://doi.org/10.1007/978-94-007-6738-6_139,1,
189,DGA Botnet Detection Using Supervised Learning Methods.,"Modern botnets are based on Domain Generation Algorithms (DGAs) to build a resilient communication between bots and Command and Control (C&C) server. The basic aim is to avoid blacklisting and evade the Intrusion Protection Systems (IPS). Given the prevalence of this mechanism, numerous solutions have been developed in the literature. In particular, supervised learning has received an increased interest as it is able to operate on the raw domains and is amenable to real-time applications. Hidden Markov Model, C4.5 decision tree, Extreme Learning Machine, Long Short-Term Memory networks have become the state of the art in DGA botnet detection. There also exist several advanced supervised learning methods, namely Support Vector Machine (SVM), Recurrent SVM, CNN+LSTM and Bidirectional LSTM, which have not been suitably appropriated in such domain. This paper presents a first attempt to thoroughly investigate all the above methods, evaluate them on the real-world collected DGA dataset involving 38 classes with 168,900 samples, and should provide a valuable reference point for future research in this field.",1,https://doi.org/10.1145/3155133.3155166,1,
190,A novel SDN controller based on Ontology and Global Optimization for heterogeneous IoT architecture.,"Today, our modern living world is covered by ubiquitous systems that offer the ability to assess, understand and handle the environmental indicators of our urban environment as well as our delicate ecology and natural resources. The sudden increase of such systems creates the notion of Internet of Things (IoT). However, the variety of components and wide-area deployments in IoT create a disadvantage point: the heterogeneity problem. Such system has multiple heterogeneous wireless communication solutions with multiple access technologies such as bluetooth, wifi, zigbee, cellular, MANET, etc. Concretely, the effectiveness is revealed by a variety of access technologies that are working on a common core network with a common policy for every type of access network. The challenge is how to manage this heterogeneous network in a dynamic context with an open and distributed infrastructure. One of the most efficiency solutions for this issue is the Software-Defined Network approach (SDN). This paper proposed a new SDN-based architecture with a centralized controller that has a capacity of self-observing and adapting. The SDN controller has the ability to incorporate and support user requests to classify flow scheduling over task-level. Besides, the paper creates an Ontology for analyzing user's request and based on the Lagrange relaxation theory for a heuristic routing algorithm. The experimental works showed that the proposed solution yielded impressive and good results.",1,https://doi.org/10.1145/3155133.3155143,1,
192,Predicting the performance of multi-agent based simulation with Ethernet switched clusters.,"Predicting exactly the performance for distributed simulation of multi-agent based systems has considerable significance. Some investigations were realized but they usually omitted issue of communication costs between computational nodes. In this paper, we address to the communication cost models and present a model for evaluating the performance of multi-agent based simulation on Ethernet switch based networks of workstations. We validate our model by an application benchmark on random Boolean network. The experimental results demonstrate that our model performs well when the conflicts among agents are negligible.",2,https://doi.org/10.1109/HPCSIM.2009.5192825,1,
193,Reliable Multicast Protocol in Distributed Simulation for Multi-agent Systems.,"Using computational clusters to simulate multi-agent systems has attracted a considerable amount of interest in recent years because of its ability to perform investigations about large complex systems. Many factors such as the load balancing among the nodes, the communication latencies or the synchronizing the logical processes, however, can impact on its performance. This paper focuses on the minimization of communication costs by proposing using reliable multicast mechanism for reducing communication latency problem. This promises to give an improvement in its performance.",2,https://doi.org/10.1109/RIVF.2009.5174657,1,
194,An Efficient Parallel Method for Optimizing Concurrent Operations on Social Networks.,"This paper presents our approach to optimize the performance of both reading and writing concurrent operations on large-scale social networks. Here, we focus on the directed and unweighted relationships among members in a social network. It can then be illustrated as a directed, unweighted graph. Moreover, determining the relationship between any two members is similar to finding the shortest path between two vertices. With such a large-scale dynamic social network, we face the problem of having concurrent operations from adding or removing edges dynamically while one may ask to determine the relationship between two members. To solve this issue, we propose an efficient parallel method based on (i) utilizing an appropriate data structure, (ii) parallelizing the updating actions and (iii) improving the performance of query processing by both reducing the searching space and computing in multi-threaded parallel. Our method was validated by the datasets from SigMod Contest 2016 and SNAP DataSet Collections with fine positive experimental results compared to other solutions.",2,https://doi.org/10.1007/978-3-319-90287-6_10,1,
195,Optimizing the shortest path query on large-scale dynamic directed graph.,"This paper presents our approach in order to optimize the shortest path query on a large-scale directed, dynamic graph such as a social network. For this kind of graph, edges can be dynamically added or removed while a user asks to determine the shortest path between two vertices. To solve this problem, we propose a strategic solution based on (i) an appropriate data structure, (ii) the optimized update actions (insertions and deletions) and (iii) by improving the performance of query processing by both reducing the searching space and computing in multithreaded parallel. Thus, graph is globally organized by the adjacent lists in order to improve the cache hit ratio and the update action performance. The reduction of searching space is based on the way of calculating the potential enqueued vertices. Cilkplus is chosen to parallelize the consecutive queries. Our strategy was validated by the datasets from SigMod Contest 2016 and SNAP DataSet Collection with the good experimental results.",2,https://doi.org/10.1145/3006299.3006321,1,
196,Energy-balanced and fault-tolerant clustering routing protocol for event driven WSNs.,"In recent years, wireless sensor networks (WSNs) are increasingly developed in various areas of applications. The most important feature of WSN is that the sensor nodes are small size, limited processing and low power. Recent research has been focused on developing routing protocol for WSNs with thorough design addressing these features such as simplicity, energy efficiency and energy balance. Those approaches include various hierarchical clustering routing protocols such as LEACH, TEEN, PEGASIS, HEED and their variants. Our research investigates a novel routing protocol in a confined, less solved area of hierarchical clustering routing protocols: hierarchical clustering routing driven by events. We propose a cluster forming algorithm involving multiple rotational cluster heads (CHs) with different number of turns acting as the cluster head. We then investigate a routing mechanism for these multiple CHs to send data to the base station (BS) using common optimal route. Furthermore, in order to increase fault tolerance of WSNs, in our routing protocol, a sensor node needs satisfying required energy to become or remain as a CH. Simulation results in Omnet++ show that our protocol is better improved in energy efficiency, network lifetime, and overall network energy balance than existing protocols of the same type: OEDSR, ARPEES and HPEQ.",4,https://doi.org/10.1145/2350716.2350740,1,
197,Multipath routing for cluster-based and event-based protocols in wireless sensor networks.,"Wireless Sensor Network (WSN) that consists of a large number of small sensing self-powered nodes is a promising approach for a variety of applications. Possible applications of WSN are of interest to the most diverse fields: environmental monitoring, warfare, child education, surveillance, micro-surgery and agriculture are only few examples. Due to limited power of sensor nodes that are distributed in a large geographical area, the WSN requires robust but simple, scalable, energy-efficient and also self-configurable routing algorithms. From these characteristics, WSN routing protocol can coarsely be classified in two main classes: flat and hierarchical approaches. Hierarchical solution (also called cluster-based) separates nodes in clusters and the communication within clusters is possible only through the selected leaders or Cluster Head-CH. In the reality, it is not always necessary to divide all the network to clusters because one event can only appear in a small part. In this case, routing protocol needs to form only one cluster with one CH for minimizing the amount of data sending to the base station (BS). Some such routing approaches (called cluster-based and event-based) are proposed as ARPEES [1], OEDSR [2], HPEQ[3]... but they all create only one optimal route from CH to the BS and this route can be broken when the sensor node's power is negligible. In this paper, we propose two multipath solutions called Multiparth ARPEES (MARPEES) and Energy Aware Mesh Routing Protocol (EMRP) for ensuring reliable and fault-tolerance data transmission while balancing node's energy consumption in the network and minimizing the control overhead. The simulation results show that our solutions achieve better performance than other cluster-based and event-based routing protocols.",4,https://doi.org/10.1145/2350716.2350743,1,
198,A family system based evolutionary algorithm for obstacle-evasion minimal exposure path problem in Internet of Things.,"Barrier coverage in wireless sensor networks (WSNs) is a well-known model for military security applications in IoTs, in which sensors are deployed to detect every movement over the predefined border. The fundamental sub-problem of barrier coverage in WSNs is the minimal exposure path (MEP) problem. The MEP refers to the worst-case coverage path where an intruder can move through the sensing field with the lowest capability to be detected. Knowledge about MEP is useful for network designers to identify the worst coverage in WSNs. Most prior research focused on this problem with the assumption that the WSN has an ideal deployment environment without obstacles, causing existing gaps between theoretical and practical WSNs systems. To overcome this drawback, we investigate a systematic and generic MEP problem under real-world environment networks by presenting obstacles called Obstacle-Evasion-MEP (hereinafter OE-MEP). We propose an algorithm to create several types of arbitrary-shaped obstacles inside the deployment area of WSNs. The OE-MEP problem is an NP-Hard with high dimension, non-differentiation, non-linearity, and constraints. Based upon its characteristics, we then devise an elite algorithm namely Family System based Evolutionary Algorithm (FEA) with our newly-proposed concepts of Family System, tailored to efficiently solve the OE-MEP. We also build an extension to a custom-made simulation environment to integrate a variety of network topologies as well as obstacles. Experimental results on numerous instances indicate that the proposed algorithm is suitable for the converted OE-MEP problem and performs better in solution accuracy than existing approaches.",5,https://doi.org/10.1016/j.eswa.2022.116943,1,
199,Multi-UAV Assisted Data Gathering in WSN: A MILP Approach For Optimizing Network Lifetime.,"In this paper, we study the problem of gathering data from large-scale wireless sensor networks using multiple unmanned air vehicles (UAVs) to gather data at designated rendezvouses, where the goal is to maximize the network lifetime. Previous proposals often consider a practical approach where the problem of determining a data gathering scheme is decomposed into 2 sub-problems: i) partitioning the networks into clusters for determining the rendezvouses as these obtained cluster heads; and ii) determining the paths for a set of a given number of UAVs to come gathering data at these rendezvouses which have been harvesting data within each local clusters, respectively. We try to deal with this as a whole optimization problem, expecting a significant increase in computation complexity which would bring new challenge in creating practical solutions for largescale WSNs. We introduce two alternatives mixed-integer linear programming (MILP) formulations and we show that our best model could solve the problem instances optimally with up to 50 sensor nodes in less than 30 minutes. Next, we propose a heuristic idea to reduce the number of variables in implementing the 3-index model to effectively handle larger-scale networks with size in hundreds. The experiment results show that our heuristic approach significantly prolongs the network lifetime compared to existing most efficient proposals.
In this paper, we study the problem of gathering data from large-scale wireless sensor networks using multiple unmanned air vehicles (UAVs) to gather data at designated rendezvouses, where the goal is to maximize the network lifetime. Previous proposals often consider a practical approach where the problem of determining a data gathering scheme is decomposed into 2 sub-problems: i) partitioning the networks into clusters for determining the rendezvouses as these obtained cluster heads; and ii) determining the paths for a set of a given number of UAVs to come gathering data at these rendezvouses which have been harvesting data within each local clusters, respectively. We try to deal with this as a whole optimization problem, expecting a significant increase in computation complexity which would bring new challenge in creating practical solutions for largescale WSNs. We introduce two alternatives mixed-integer linear programming (MILP) formulations and we show that our best model could solve the problem instances optimally with up to 50 sensor nodes in less than 30 minutes. Next, we propose a heuristic idea to reduce the number of variables in implementing the 3-index model to effectively handle larger-scale networks with size in hundreds. The experiment results show that our heuristic approach significantly prolongs the network lifetime compared to existing most efficient proposals.
",5,https://doi.org/10.1109/RIVF51545.2021.9642131,1,
200,Improved Compact Routing Schemes for Random Interconnects.,"Random topology has been an increasingly favorable approach for designing interconnection networks, as it can provide a combination of low latency and incremental network growth that could not be provided by the traditional rigid topologies. However, the common shortest-path routing in a random interconnect poses a scalability problem, for it requires global network info to make routing decisions and so, the routing table size (RTS) can be very large. Therefore, this manuscript would aim to revisit the well-known research area of landmark-based compact routing and to improve the universal routing schemes for the specific case of random interconnects. It would propose new landmark-based compact routing schemes, using 2 heuristic techniques to select landmarks that are evenly spaced, which would reduce the RTS in the well-known Thorup and Zwick's scheme by up to 18% and produce a shorter average path length.",5,https://doi.org/10.4018/IJDST.2020070105,1,
201,Efficient Landmark-Based Compact Routing for Random Interconnection Topologies.,"We revisit the well-known research topic of landmark-based compact routing in order to support designing interconnection topologies that can meet new demanding requirements and issues brought up by advances in areas such as massively parallel computing or large-scale data centers. Particularly, it is necessary to reduce the routing table size (RTS) as much as possible while maintaining other performance factors in a good state. Based on a new landmark selection technique, we propose our Geographic Landmark-based Compact Routing scheme which can reduce the RTS from the well-known Thorup and Zwick's scheme by roughly 15% for a network of 1024 nodes and roughly 45% for a network of at least 100K nodes, and which also produces shorter avcrago path length.",5,https://doi.org/10.1109/RIVF.2019.8713674,1,
202,Fast Detection and Mitigation to DDoS Web Attack Based on Access Frequency.,"We have been investigating methods for establishing an effective, immediate defense mechanism against the DDoS attacks on Web applications via hacker botnets, in which this defense mechanism can be immediately active without preparation time, e.g. for training data, usually asked for in existing proposals. In this study, we propose a new mechanism, including new data structures and algorithms, that allow the detection and filtering of large amounts of attack packets (Web request) based on monitoring and capturing the suspect groups of source IPs that can be sending packets at similar patterns, i.e. with very high and similar frequencies. The proposed algorithm places great emphasis on reducing storage space and processing time so it is promising to be effective in real-time attack response.",5,https://doi.org/10.1109/RIVF.2019.8713762,1,
203,Balanced landmark-based graph partitioning with application in navigating with limited resources.,"We study the problem of graph partitioning balanced on weight vertex where we aim to maximize the average weight of the obtained subgraphs. Our proposed approach is to partition the graph by a graph Voronoi diagram where the Voronoi nodes, considered as landmarks, are carefully selected in some special manner for achieving the mentioned targets. We also introduce a direct application for this problem, namely navigating with limited resources, where one would like to produce an inexpensive navigation device that is although short of battery and of limited memory but still is capable to help showing directions in a long hiking journey in the wilderness. This is achieved by using a specialized map (for navigating) created based on our proposed graph partitioning. We introduce two different heuristic techniques in selecting the landmarks in such a mentioned desired manner. Initial experiments with our proposed algorithms show encouraging results where we can successfully construct a 0.5-balanced partition for all tested graphs.",5,https://doi.org/10.1145/3368926.3369722,1,
204,Distributed Shortcut Networks: Low-Latency Low-Degree Non-Random Topologies Targeting the Diameter and Cable Length Trade-Off.,"Low communication latency becomes a main concern in highly parallel computers and supercomputers that reach millions of processing cores. Random network topologies are better suited to achieve low average shortest path length and low diameter in terms of the hop counts between nodes. However, random topologies lead to two problems: (1) increased aggregate cable length on a machine room floor that would become dominant for communication latency in next-generation custom supercomputers, and (2) high routing complexity that typically requires a routing table at each node (e.g., topology-agnostic deadlock-free routing). In this context, we first propose low-degree non-random topologies that exploit the small-world effect, which has been well modeled by some random network models. Our main idea is to carefully design a set of various-length shortcuts that keep the diameter small while maintaining a short cable length for economical passive electric cables. We also propose custom routing that uses the regularity of the various-length shortcuts. Our experimental graph analyses show that our proposed topology has low diameter and low average shortest path length, which are considerably better than those of the counterpart 3-D torus and are near to those of a random topology with the same average degree. The proposed topology has average cable length drastically shorter than that of the counterpart random topology, which leads to low cost of interconnection networks. Our custom routing takes non-minimal paths to provide lower zero-load latency than the minimal custom routings on different counterpart topologies. Our discrete-event simulation results using SimGrid show that our proposed topology is suitable for applications that have irregular communication patterns or non-nearest neighbor collective communication patterns.",5,https://doi.org/10.1109/TPDS.2016.2613043,1,
205,An Efficient Compact Routing Scheme for Interconnection Topologies based on the Random Model.,"The field of interconnection network has recently observed a resurgent interest that are fueled by modern ICT technologies wherein traditional topologies have been found less desirable. The recent approach using the random network model has been considered a good answer to new challenges but also has a limitation in routing scalability, i.e. the routing table size (RTS) can become very large which is not affordable by market commodity switches. We propose to design new routing schemes especially designed for the random network model that can improve this situation. As a major contribution, we propose a new compact routing scheme named by CORRA which exploits the random model properties that help to achieve near optimum route length while keeps the routing table size small enough. In general, our new routing algorithm can achieve average route length only a bit longer than in using the shortest path routing. The RTS achieved by CORRA is very small as expected, e.g. 520 for network size 8192, which is small enough to allow network size even up to millions of nodes.",5,https://doi.org/10.1145/3155133.3155186,1,
206,FDDA: A Framework For Fast Detecting Source Attack In Web Application DDoS Attack.,"Anomaly detection techniques usually require a training phase in order to capture the normality traits (typical features) of a given system which is considered as in a state free of attack. The efficiency of detecting abnormal signals mainly depends on processing time and the quality of featured data learned from the training phase. In this research, we propose a framework for fast detecting DDoS attack (FDDA) which can improve the speed and efficiency in detecting an on-going DDoS attack to web application, and hence greatly reduce the training time.

Our main idea is based on using an on-line scanning process where our detection algorithm looks for certain traits of DDoS attack using some specially designed techniques, which if occur then the algorithm rapidly builds a blacklist of requests and suspicious IPs that will be used in the filtering process in the next stage. The mentioned blacklist can be gradually updated during the whole process. The techniques for detecting traits of attack are developed based on some special criteria, which have been observed from the natural behaviors of the request sent from the attacking botnet. We believe that, even aware of these techniques, smart hackers will still find it very hard to escape from making these traits of behaviors.",5,https://doi.org/10.1145/3155133.3155173,1,
207,Layout-Conscious Expandable Topology for Low-Degree Interconnection Networks.,"System expandability becomes a major concern for highly parallel computers and data centers, because their number of nodes gradually increases year by year. In this context we propose a low-degree topology and its floor layout in which a cabinet or node set can be newly inserted by connecting short cables to a single existing cabinet. Our graph analysis shows that the proposed topology has low diameter, low average shortest path length and short average cable length comparable to existing topologies with the same degree. When incrementally adding nodes and cabinets to the proposed topology, its diameter and average shortest path length increase modestly. Our discrete-event simulation results show that the proposed topology provides a comparable performance to 2-D Torus for some parallel applications. The network cost and power consumption of DSN-F modestly increase when compared to the counterpart non-random topologies.",5,https://doi.org/10.1587/transinf.2015EDP7214,1,
208,A Distributed Algorithm For Monitoring An Expanding Hole In Wireless Sensor Networks.,"Holes in sensor networks are regions that have no operating nodes and that may occur due to several reasons, including cases caused by natural obstacles or disaster suffered areas. Determining the location and shape of holes can help to monitor these disasters (such as volcano, tsunami, etc.) or help to make smart, early routing decisions for circumventing a hole. There are many hole determination algorithms proposed in the literature, however, these only consider the networks with static holes i.e. with stable boundary nodes. Moreover, most of these are designed in a centralized manner which is not suitable to the unstable situation of networks with an expanding hole. In this paper, we propose an algorithmic scheme not only for determining the initial shape but also for monitoring and quickly reporting about the area of a hole gradually expanding. Our algorithms are designed in a distributed manner and our initial simulation results show that our protocol is lightweight and feasible with monitoring sensor networks with an expanding hole. ",5,http://www.informatica.si/index.php/informatica/article/view/1279,1,
209,An Interconnection Network Exploiting Trade-Off between Routing Table Size and Path Length.,"Various parallel applications require a low-latency interconnection network to achieve high performance and high scalability. Recently proposed random network topologies achieve low latency, but they require each switch to have a large number of routing table entries, e.g., larger than N for an N-node network, for implementing a minimal routing. In this study, we propose the use of a random network topology with our new routing scheme so that the required routing table size becomes small, e.g., 528 for 8192 nodes, at each switch. Our main finding is that our routing algorithm cannot always follow the minimal paths, but its average path length is still short when compared to that of existing network topologies.",5,https://doi.org/10.1109/CANDAR.2016.0119,1,
210,A Time and Energy Efficient Protocol for Locating Coverage Holes in WSNs.,"There are two main requirements in dealing with coverage holes in wireless sensor networks (WSNs): locating the hole boundary and finding the locations to deploy new sensors for hole patching. The current protocols on finding the patching locations always require re-running the protocols from scratch many times. This constraint causes the time complexity and energy overhead to increase proportionally to the hole size. In this paper, we propose a lightweight protocol to determine coverage holes in wireless sensor network. Our protocol does not only can determine the exact hole boundary but also approximate the boundary by a simpler shape which can help to speed up the patching location finding process. The simulation experiments show that our protocol can reduce more than 56% of time complexity and save more than 46% of energy overhead in comparison with existing protocols.",5,https://doi.org/10.1109/LCN.2016.37,1,
211,A Distributed Protocol for Detecting and Updating Hole Boundary in Wireless Sensor Networks.,"Holes in sensor network are regions without operating nodes which may occur due to several reasons, including cases caused by natural obstacles or disaster suffered areas. Determining the location and shape of holes can help to monitor these disaster events (such as volcano, tsunami, etc.) or help to make smart, early routing decisions for circumventing a hole. There are many hole determination algorithms have been proposed in the literature. However, all of these algorithms only consider the networks with static holes. Moreover, most of these protocols are conducted in a centralized manner which is not suitable with the resource constraint of sensor nodes. In this paper, we consider the networks with dynamic holes and propose a distributed algorithm which can determine and update the hole boundary very quickly. We also conduct simulation to evaluate performance of proposed protocol.",5,https://doi.org/10.1145/2833258.2833297,1,
212,Robust and Efficient Custom Routing for Interconnection Networks with Distributed Shortcuts.,"We aim at creating a robust and efficient custom routing mechanism for Distributed Shortcut Networks (Nguyen et al., 2013), which address new challenging issues posed by recently advanced studies in the areas of massively parallel computing and large-scale data centers. We follow the design principles of Distributed Shortcut Networks (DSN), which construct non-random topologies with the creation of long-range shortcuts inspired by observations in small-world networks. However, we focus on designing a powerful custom routing mechanism which smartly exploits some precious properties of the topology. As a result, our new DSN-a network with a carefully refined routing logic performs significantly better than the basic DSN in term of communication latency while provides strengths in fault-tolerance as well as load-balance. These help the network become robust against link failures or burst of traffic demand while topology-agnostic deadlock-free routing (e.g. the famous up*/down* algorithm) suffers a lot.
",5,https://doi.org/10.4018/ijdst.2014100104,1,
213,Hole Approximation-Dissemination Scheme for Bounded-Stretch Routing in Sensor Networks.,"The routing stretch in a sensor network with a large hole can be controlled if the network nodes have certain information about the hole location and shape, possibly by using some mechanism for hole boundary approximation and information dissemination. However, a finer approximation (for smaller stretch) leads to higher cost of dissemination, which could be unaffordable for the sensors. We provide an insightful analysis to this fundamental trade-off of the hole approximation problem, using a geometric model. Based on this we discuss an efficient approximation-dissemination scheme using a natural, heuristic approximation technique. For reasonable conditions (network is dense enough apart from the hole), we assure that the routing stretch is under a predefined threshold > 1 while the dissemination cost is also under tight control. Our initial experiment results also confirm that our approach is significantly more efficient and economical, compared to existing proposals.",5,https://doi.org/10.1109/DCOSS.2014.17,1,
214,GameGuard: a windows-based software architecture for protecting online games against hackers.,"We propose a software architecture and an implementation framework for protecting Windows-based online games against hackers. Our network model is between 3 parties, the game producer, the game distributor and the game player (client), where we aim to support the distributor to fight against cheater clients with minimum cooperation from the game producer. Our protection system is centered on our specifically designed module, GameGuard, which accomplishes the main protection duties while simultaneously, interacts with the GameClient program so that if a serious hack attempt is detected, both will terminate. Our two layers of protection, in hiding game modules and detecting hacks, integrated with the mechanism of auto-updating/reporting with the distributor server provide a general, powerful yet flexible approach in building hack protection products. We support the game distributors with a general protection framework which is not dependent of specific games and also only need minimum cooperation from the distant game producers.

Our approach is powerful enough that we would defeat most kinds of attacks which are based on hooking Windows APIs, including ones from memory, graphic or hardware attack categories. Our GameGuard system is also robust against attacks directed at the protection modules: we achieve authenticity of our protection modules based on the authenticity of the GameClient assured by the producer. Based on our framework, we have implemented a hack protection system which is being extensively experimented at VTC, the Vietnam Multimedia Corporation, with very encouraging results.",5,https://doi.org/10.1145/1852611.1852643,1,
215,Integrating open data and generating travel itinerary in semantic-aware tourist information system.,"The growth of online data and services on the Web make it become more and more emerging as an indispensable tool of traveling for the tourist industry. It is not denied that various approaches bring benefits for visitors in supporting them of searching tourist attractions, such us interested places for the visit, eating or staying. However, like a coin has two sides, too much information would be the difficulty for people when planning their journeys. Generally, tourists usually have problems when finding a satisfied accommodation without a reference to nearby restaurants, sights or event locations. In addition, travelers suffer from the information overload when they look for information about potential destinations, events and related services. Providing the relevant and up-to-date information for the tourists with different personal interests is still a challenging task for the tourist guide information systems.

This paper presents a semantic web approach for developing STAAR (Semantic Tourist informAtion Access and Recommending) which is a system that addresses the above mentioned issues. Specifically, it is described how an ontology is designed to represent the travel related information, and to support the integrating data from the open repositories. Relying on this ontology, we propose an algorithm for generating travel itinerary on the Web which is relevant to both criterions of the itinerary length and user interest.",6,https://doi.org/10.1145/2095536.2095573,1,
217,Entropy correlation-based clustering method for representative data aggregation in wireless sensor networks.,"One of the popular data aggregation method in wireless sensor network (WSN) is collecting only local representative data based on correlation of sample data. To recognise the local representative nodes, it is necessary to determine the correlation regions. However, recent correlation models are distance based that is not general and need to be determined beforehand or complicated with high computing cost. Thus, in this paper, a novel entropy correlation model is proposed based on joint entropy approximation. Using the proposed model, an entropy correlation-based clustering method is presented and the selection of representative data that satisfying the desired distortion is proposed. The algorithm is validated with practical data.",7,https://doi.org/10.1504/IJSNET.2018.096476,1,
218,Entropy Correlation and Its Impacts on Data Aggregation in a Wireless Sensor Network.,"A correlation characteristic has significant potential advantages for the development of efficient communication protocols in wireless sensor networks (WSNs). To exploit the correlation in WSNs, the correlation model is required. However, most of the present correlation models are linear and distance-dependent. This paper proposes a general distance-independent entropy correlation model based on the relation between joint entropy and the number of members in a group. This relation is estimated using entropy of individual members and entropy correlation coefficients of member pairs. The proposed model is then applied to evaluate two data aggregation schemes in WSNs including data compression and representative schemes. In the data compression scheme, some main routing strategies are compared and evaluated to find the most appropriate strategy. In the representative scheme, with the desired distortion requirement, a method to calculate the number of representative nodes and the selection of these nodes are proposed. The practical validations showed the effectiveness of the proposed correlation model and data reduction schemes. ",7,https://doi.org/10.3390/s18093118,1,
220,Correlation-based clustering in wireless sensor network for energy saving protocol.,"Our research concentrates on the energy efficiency in Wireless Sensor Network. One approach is based on the characteristics of environment -- the correlation among sensed data of nodes in a region. The sensor nodes are clustered into highly correlated regions (HCRs) to take advantage of correlation between sensor nodes in order to save energy. However, the determination of HCR is very complex in calculation, thus causes difficulty in implementation. This paper proposes a correlation-based approach that evaluates the correlation between two data sets using a simple calculation and that guarantees the accuracy in correlated evaluation between data. This correlation-based method is proposed to cluster sensor nodes into HCRs. Because of highly correlated characteristics among sensed data of nodes in the same HCRs, some high correlated-nodes would be inactive for energy saving. Simulation results show that the network lifetime of proposed system is 1.75 times longer than that of the conventional protocol.",7,https://doi.org/10.1145/2542050.2542082,1,
221,User Scheduling for Precoded Satellite Systems with Individual Quality of Service Constraints.,"Multibeam high throughput satellite (MB-HTS) systems will play a key role in delivering broadband services to a large number of users with diverse Quality of Service (QoS) requirements. This paper focuses on MB-HTS where the same spectrum is re-used by all user links and, in particular, we propose a novel user scheduling design capable to provide guarantees in terms of individual QoS requirements while maximizing the system throughput. This is achieved by precoding to mitigate mutual interference. The combinatorial optimization structure requires an extremely high cost to obtain the global optimum even with a reduced number of users. We, therefore, propose a heuristic algorithm yielding a good local solution and tolerable computational complexity, applicable for large-scale networks. Numerical results demonstrate the effectiveness of our proposed algorithm on scheduling many users with better sum throughput than the other benchmarks. Besides, the QoS requirements for all scheduled users are guaranteed.",8,https://doi.org/10.1109/PIMRC50174.2021.9569282,1,
222,User Scheduling and Power Allocation for Precoded Multi-Beam High Throughput Satellite Systems with Individual Quality of Service Constraints.,"For extensive coverage areas, multi-beam high throughput satellite (MB-HTS) communication is a promising technology that plays a crucial role in delivering broadband services to many users with diverse Quality of Service (QoS) requirements. This paper focuses on MB-HTS systems where all beams reuse the same spectrum. In particular, we propose a novel user scheduling and power allocation design capable of providing guarantees in terms of the individual QoS requirements while maximizing the system throughput under a limited power budget. Precoding is employed in the forward link to mitigate mutual interference at the users in multiple-access scenarios over different coherence time intervals. The combinatorial optimization structure from user scheduling requires an extremely high cost to obtain the global optimum even when a reduced number of users fit into a time slot. Therefore, we propose a heuristic algorithm yielding good trade-off between performance and computational complexity, applicable to a static operation framework of geostationary (GEO) satellite networks. Although the power allocation optimization is signomial programming, non-convex on a standard form, the solution can be lower bounded by the global optimum of a geometric program with a hidden convex structure. A local solution to the joint user scheduling and power allocation problem is consequently obtained by a successive optimization approach. Numerical results demonstrate the effectiveness of our algorithms on large-scale systems by providing better QoS satisfaction combined with outstanding overall system throughput",8,https://arxiv.org/abs/2110.02525,1,
223,A Novel Residual Frequency Estimation Method for GNSS Receivers.,"In Global Navigation Satellite System (GNSS) receivers, residual frequency estimation methods are traditionally applied in the synchronization block to reduce the transient time from acquisition to tracking, or they are used within the frequency estimator to improve its accuracy in open-loop architectures. There are several disadvantages in the current estimation methods, including sensitivity to noise and wide search space size. This paper proposes a new residual frequency estimation method depending on differential processing. Although the complexity of the proposed method is higher than the one of traditional methods, it can lead to more accurate estimates, without increasing the size of the search space. ",8,https://doi.org/10.3390/s18010119,1,
224,Situational Awareness: Mapping Interference Sources in Real-Time Using a Smartphone App.,"In the past years, many techniques have been researched and developed to detect and identify the interference sources of Global Navigation Satellite System (GNSS) signals. In this paper, we utilize a simple and portable application to map interference sources in real-time. The results are promising and show the potential of the crowdsourcing for monitoring and mapping GNSS interference distribution. ",8,https://doi.org/10.3390/s18124130,1,
225,Combined GPS L1C/A and L2C signal acquisition architectures leveraging differential combination.,"The GPS modernization program offers a new civil signal on the L2 band, which, together with the legacy L1 C/A signal, spurs the development of dual-frequency receivers for civil applications. This paper proposes a complete combined acquisition architecture dedicated to these receivers. The architecture fully utilizes the capacity of both signal channels as well as the advantages of differential signal processing. High detection sensitivity, finer parameter estimation, built-in bit synchronization mechanism, and acquisition-to-tracking transient time reduction are the advantages of the proposed architecture with respect to others in the literature.",8,https://doi.org/10.1109/TAES.2014.110044,1,
226,Recent results in receiving and decoding signals from the Beidou system.,"Since December 27, 2012, the Beidou Navigation Satellite System officially started to operate. This event is a great opportunity for researchers in South East of Asia to receive and analyze the Beidou signals. After the official statement, the researchers at NAVIS centre monitored the broadcasted signal by using NAVISOFT- our Software Radio Receiver. This paper shows the analysis on the navigation message that was broadcasted by the Beidou satellites on the B1I bandwidth. In general, we were able to observe a valid ephemeris data on visible satellites. The successful PVT computation by using combinations of GEO and MEO/IGSO in static condition through code-phase measurements is indicated in this paper.",8,https://doi.org/10.1109/ICL-GNSS.2013.6577255,1,
227,A robust acquisition architecture for GPS safety-of-life L5 Signal.,"The new GPS L5 signal with advanced characteristics in the signal structure is dedicated for safety-of-life applications that have strict requirements on accuracy, availability and integrity. To cope with this new signal, GPS receivers need to be upgraded with new signal processing blocks. This paper presents a robust acquisition architecture suitable for the L5 signal. The architecture can be considered as a combination of the Cross Ambiguity Function based; the energy-based; and the multiple channel acquisition methods.",8,https://doi.org/10.1109/ICSPCS.2013.6723918,1,
228,Significance of Cell-Correlation Phenomenon in GNSS Matched Filter Acquisition Engines.,"Modern Global Navigation Satellite Systems (GNSS) are going to provide new signals with longer PRN codes and higher chipping rates, which aim to improve the positioning performance with respect to the current GPS. However, these new characteristics also cost GNSS receivers a high computational complexity. Due to their attractive acquisition time performance, matched filter (MF) correlators promise to be a good choice for GNSS signal acquisition engines. Existing methods to evaluate GNSS signal acquisition engine performance parameters, viz. detection probabilities and mean acquisition time, assume that the detections among the neighboring test cells are independent. However in a matched filter correlator, depending on the spacing between the test cells, due to the correlation of the local code with the noise component in the received signal at different time instances within a chip period, there can exist strong correlations, which affect these performance parameters. Also, the presence of cell correlations influences the acquisition threshold setting, which is a critical design parameter. This paper provides a detailed analysis of the significance of the cell-correlation phenomenon in MF correlators for the two widely used signal families in GNSS, namely BPSK and BOC, in particular BPSK(1) and BOC(1,1). Justifying the theoretical analysis with Monte Carlo simulations, it is shown that the maximum error in estimating the mean acquisition time without considering the cell-correlation phenomenon is shown to be about 10% for the BPSK(1) and about 12% for the BOC(1,1) signal.",8,https://doi.org/10.1109/TAES.2012.6178061,1,
229,Partial Differential Postcorrelation Processing for GPS L2C Signal Acquisition.,"L2C is the second civilian signal introduced on the modernized block of GPS satellites. The two PRN sequences employed in L2C, named civil moderate (CM) and civil long (CL), have periods of 20 ms and 1.5 s, respectively. Stemming from the fact that using a full code period (CM or CL) for signal acquisition in GPS L2C receivers might not be necessary in normal situations (e.g. outdoor, light indoor, etc.), in this paper, we introduce a partial acquisition architecture using specially-designed matched filters (MFs) in order to relieve the computational complexity of the acquisition stage. The partial correlation loss is compensated by differential postcorrelation techniques. Three techniques, namely conventional differential combination (CDC), generalized differential combination (GDC), and modified generalized differential combination (MGDC), are investigated in terms of detection probabilities and mean acquisition time leading to the selection of MGDC as the most suitable technique for the L2C partial acquisition. By using this technique, a 2-dB sensitivity improvement with respect to the conventional noncoherent combination and a 94.5% reduction in mean acquisition time in comparison with the full code acquisition are shown for 1-ms partial correlation",8,https://doi.org/10.1109/TAES.2012.6178062,1,
230,A novel method for estimating residual carrier frequency offset in GPS L2C receivers.,"In a GPS receiver, the residual carrier frequency offset, which is the difference between the frequency estimated by the signal acquisition stage and the received one, cannot be neglected. This residual causes the receiver sensitivity loss and also the longer transient time from acquisition to tracking in the synchronization process. This paper proposes a novel estimation method for estimating this residual in GPS receivers working with the new L2C signal. The method utilizes the nature of advance differential signal processing, which is proved to be suitable with L2C signal acquisition, to produce reliable estimations with smaller bias and standard deviation values in comparison with the conventional method in literature.",8,https://doi.org/10.1145/2069216.2069229,1,
231,Deflation-based power iteration clustering.,"Spectral clustering (SC) is currently one of the most popular clustering techniques because of its advantages over conventional approaches such as K-means and hierarchical clustering. However, SC requires the use of computing eigenvectors, making it time consuming. To overcome this limitation, Lin and Cohen proposed the power iteration clustering (PIC) technique (Lin and Cohen in Proceedings of the 27th International Conference on Machine Learning, pp. 655–662, 2010), which is a simple and fast version of SC. Instead of finding the eigenvectors, PIC finds only one pseudo-eigenvector, which is a linear combination of the eigenvectors in linear time. However, in certain critical situations, using only one pseudo-eigenvector is not enough for clustering because of the inter-class collision problem. In this paper, we propose a novel method based on the deflation technique to compute multiple orthogonal pseudo-eigenvectors (orthogonality is used to avoid redundancy). Our method is more accurate than PIC but has the same computational complexity. Experiments on synthetic and real datasets demonstrate the improvement of our approach.",9,https://doi.org/10.1007/s10489-012-0418-0,1,
232,Comprehensive Context Recognizer Based on Multimodal Sensors in a Smartphone.,"Recent developments in smartphones have increased the processing capabilities and equipped these devices with a number of built-in multimodal sensors, including accelerometers, gyroscopes, GPS interfaces, Wi-Fi access, and proximity sensors. Despite the fact that numerous studies have investigated the development of user-context aware applications using smartphones, these applications are currently only able to recognize simple contexts using a single type of sensor. Therefore, in this work, we introduce a comprehensive approach for context aware applications that utilizes the multimodal sensors in smartphones. The proposed system is not only able to recognize different kinds of contexts with high accuracy, but it is also able to optimize the power consumption since power-hungry sensors can be activated or deactivated at appropriate times. Additionally, the system is able to recognize activities wherever the smartphone is on a human’s body, even when the user is using the phone to make a phone call, manipulate applications, play games, or listen to music. Furthermore, we also present a novel feature selection algorithm for the accelerometer classification module. The proposed feature selection algorithm helps select good features and eliminates bad features, thereby improving the overall accuracy of the accelerometer classifier. Experimental results show that the proposed system can classify eight activities with an accuracy of 92.43%",9,https://doi.org/10.3390/s120912588,1,
233,GPARS: a general-purpose activity recognition system.,"The fundamental problem of the existing Activity Recognition (AR) systems is that these are not general-purpose. An AR system trained in an environment would only be applicable to that environment. Such a system would not be able to recognize the new activities of interest. In this paper we propose a General-Purpose Activity Recognition System (GPARS) using simple and ubiquitous sensors. It would be applicable to almost any environment and would have the ability to handle growing amounts of activities and sensors in a graceful manner (Scalable). Given a set of activities to monitor, object names (with embedded sensors) and their corresponding locations, the GPARS first mines activity knowledge from the web, and then uses them as the basis of AR. The novelty of our system, compared to the existing general-purpose systems, lies in: (1) it uses more robust activity models, (2) it significantly reduces the mining time. We have tested our system with three real world datasets. It is observed that the accuracy of activity recognition using our system is more than 80%. Our proposed mechanism yields significant improvement (more than 30%) in comparison with its counterpart.",9,https://doi.org/10.1007/s10489-010-0217-4,1,
234,Towards Smart Homes Using Low Level Sensory Data.,"Ubiquitous Life Care (u-Life care) is receiving attention because it provides high quality and low cost care services. To provide spontaneous and robust healthcare services, knowledge of a patient’s real-time daily life activities is required. Context information with real-time daily life activities can help to provide better services and to improve healthcare delivery. The performance and accuracy of existing life care systems is not reliable, even with a limited number of services. This paper presents a Human Activity Recognition Engine (HARE) that monitors human health as well as activities using heterogeneous sensor technology and processes these activities intelligently on a Cloud platform for providing improved care at low cost. We focus on activity recognition using video-based, wearable sensor-based, and location-based activity recognition engines and then use intelligent processing to analyze the context of the activities performed. The experimental results of all the components showed good accuracy against existing techniques. The system is deployed on Cloud for Alzheimer’s disease patients (as a case study) with four activity recognition engines to identify low level activity from the raw data captured by sensors. These are then manipulated using ontology to infer higher level activities and make decisions about a patient’s activity using patient profile information and customized rules. ",9,https://doi.org/10.3390/s111211581,1,
235,A Fast Implementation of Semi-Markov Conditional Random Fields.,"Recently, Conditional Random Fields (CRF) model has been used and proved to be a good model for sequential modeling. It, however, lacks the capability of duration modeling. Therefore, some researchers introduced semi Markov Conditional Random Fields (semi-CRF) to take into account the duration distribution and showed some improvements. Nevertheless, the training algorithms for semi-CRF require quite a high complexity making semi-CRF impractical in some large-scale problems. Therefore, in this work we propose a fast implementation of the training algorithm in order to reduce the complexity required by semi-CRF. Our theoretical analysis as well as experiments’ result show a noticeable improvement in computation time, which is about ten times less than that of the original algorithm.",9,https://doi.org/10.1007/978-3-642-27183-0_9,1,
236,Emotional speech classification using hidden conditional random fields.,"Although there have been a great number of papers in the area of emotional speech recognition, most of them contribute to the feature extraction phase. Regarding classification algorithm, hidden Markov model (HMM) is still the most commonly used method. Whereas HMM was pointed out to be less accurate than its discriminative counterpart, the hidden conditional random fields (HCRF) model, for example in phone classification or gesture recognition. Therefore in this study, we investigate the use of the HCRF model in emotional speech classification problem. In our experiments, we extracted Mel-frequency cepstral coefficients (MFCC) features from the well-known Berlin emotional speech dataset (EMO) and eNTERFACE 2005 dataset. After that, we used the 10-fold cross validation rule to train, evaluate and compare our result with that of HMM. The experiments show that HCRF achieves significant improvement (p-value ≤ 0.05) in classification accuracy. In addition, we speed up the training phase of the model by caching the gradient computation. Therefore our computation time is much less than that of the existing methods.",9,https://doi.org/10.1145/2069216.2069246,1,
237,Secured WSN-integrated cloud computing for u-Life Care.,"This paper presents a Secured Wireless Sensor Network-integrated Cloud computing for u-Life Care (SC3). SC3 monitors human health, activities, and shares information among doctors, care-givers, clinics, and pharmacies in the Cloud, so that users can have better care with low cost. SC3 incorporates various technologies with novel ideas including; sensor networks, Cloud computing security, and activities recognition.",9,https://doi.org/10.1109/CCNC.2010.5421618,1,
238,An Improved Maximum Relevance and Minimum Redundancy Feature Selection Algorithm Based on Normalized Mutual Information.,"We present in this paper a comprehensive analysis of the mutual information based feature selection algorithms. We point out the limitations of some recent work in this area then propose an improvement to overcome the weak points. The experiment results confirm that we achieve a better feature sets compared with the two recent developed algorithms, which are Maximum Relevance and Minimum Redundancy (mRMR) and Normalized Mutual Information Feature Selection (NMIFS), in terms of the classification accuracy.",9,https://doi.org/10.1109/SAINT.2010.50,1,
240,Design and implementing Big Data system for cardiovascular data.,"Thus far, medical data have been used to serve the need of people's healthcare. In some countries, in recent years, a lot of hospitals have altered the conventional paper medical records into electronic health records. The data in these records grow continuously in real time, which generates a large number of medical data available for physicians, researchers, and patients in need. Systems of electronic health records share a common feature that they are all constituted from open sources for Big Data with distributed structure in order to collect, store, exploit, and use medical data to track down, prevent, treat human's diseases, and even forecast dangerous epidemics.",10,https://doi.org/10.1002/cpe.5068,1,
241,Magnetoelectric Vortex Magnetic Field Sensors Based on the Metglas/PZT Laminates.,"This paper describes the route, from simulations toward experiments, for optimizing the magnetoelectric (ME) geometries for vortex magnetic field sensors. The research is performed on the base of the Metglas/Piezoelectric (PZT) laminates in both open and closed magnetic circuit (OMC and CMC) geometries with different widths (W), lengths (L), and diameters (D). Among these geometries, the CMC laminates demonstrate advantages not only in their magnetic flux distribution, but also in their sensitivity and in their independence of the position of the vortex center. In addition, the ME voltage signal is found to be enhanced by increasing the magnetostrictive volume fraction. Optimal issues are incorporated to realize a CMC-based ME double sandwich current sensor in the ring shape with D × W = 6 mm × 1.5 mm and four layers of Metglas. At the resonant frequency of 174.4 kHz, this sensor exhibits the record sensitivity of 5.426 V/A as compared to variety of devices such as the CMC ME sensor family, fluxgate, magnetoresistive, and Hall-effect-based devices. It opens a potential to commercialize a new generation of ME-based current and (or) vortex magnetic sensors. ",10,https://doi.org/10.3390/s20102810,1,
242,Aligning Multi Sequences on GPUs.,"Implementing Multi Sequence Alignment (MSA) problem using the method of progressive alignment is not feasible on common computing systems; it takes several hours or even days for aligning thousands of sequences if we use sequential versions of the most popular MSA algorithm - Clustal. In this paper, we present our parallel algorithm called CUDAClustal, a MSA parallel program. We have paralleled the first stage of the algorithm Clustal and achieved a significant speedup when compared to the sequential program running on a computer of Pentium 4 3.0 GHz processor. Our tests were performed on one GPU Geforce GTX 295 and they gave a great computing performance: the running time of CUDAClustal is smaller approximately 30 times than Clustal for the first stage. This shows the large benefit of GPU for solving the MSA problem and its high applicability in bioinformatics.",10,https://doi.org/10.1007/978-3-642-36642-0_30,1,
243,Password recovery for encrypted ZIP archives using GPUs.,"Protecting data by passwords in documents such as DOC, PDF or RAR, ZIP archives has been demonstrated to be weak under dictionary attacks. Time for recovering the passwords of such documents mainly depends on two factors: the size of the password search space and the computing power of the underline system. In this paper, we present an approach using modern multi-core graphic processing units (GPUs) as computing devices for finding lost passwords of ZIP archives. The combination of GPU's extremely high computing power and the state-of-the-art password structure analysis methods would bring us a feasible solution for recovering ZIP file password. We first apply password generation rules[9] in generating a reasonable password space, and then use GPUs for exhaustively verifying every password in the space. The experimental results have shown that the password verification speed increases about from 48 to 170 times (depends on the number of GPUs) compared to sequential execution on the Intel Core 2 Quad Q8400 2.66 Ghz. These results have demonstrated the potential applicability of GPUs in this cryptanalysis field.",10,https://doi.org/10.1145/1852611.1852617,1,
244,Plan Model - An Activity Theory Based Approach.,"The Activity Theory, initiated by three researchers of the cultural historical school of Russian psychology, L. S. Vygotsky, A. N. Leont'ev and A. R. Luria, in the period from 1920s to 1930s, aims to explain the development of human psychology when we do purposeful activities. Recently, the theory has again drawn much attention in some fields of information technology such as human-computer interaction, Computer Supported Collaborative Work (CSCW). However, the theory still seems to lack of a suitable formalism. This causes some real problems related to the clarity of key concepts and representation of activities. Those issues have limited the development of the theory. Therefore, our research aims to resolve them by presenting a formalism for the Activity Theory which is called plan model.",10,https://doi.org/10.1145/2833258.2833287,1,
245,"Translation from BPMN to BPEL, current techniques and limitations.","Recently, the transformation between the two classes of workflow languages: graph-oriented and block-oriented with two typical workflow languages BPMN and BPEL, has gained much attention in research community, which has resulted in many interesting techniques and methods. This paper aims to review main current techniques in translation from graph-oriented workflow languages to block-oriented ones in general, and from BPMN to BPEL specifically, that helps to clarify major existing achievements and limitations of these techniques. Some feasible solutions for resolving these limitations are also suggested.",10,https://doi.org/10.1145/2676585.2676602,1,
246,Auto-Updating Portable Application Model of Multi-Cloud Marketplace Through Bidirectional Transformations System.,"At present, the cloud marketplace becomes more and more widely used for delivering cloud applications to consumers. The diversity of IaaS and PaaS services from many cloud providers gives customers many choices that benefit them the most. If a customer is not satisfied with his existing cloud resource service (IaaS or PaaS), he is going to stop using the service in use and consider other cloud service providers. And he also wants his cloud software bought on the market to be hosted on new cloud platforms. However, changing cloud resource services for multi-cloud application is not trivial. In this paper, we propose an approach that uses a Composable Application Model (CAM) to construct the topology of a multi-cloud application in a Blueprint. Thereby, all the changes of cloud platform services are reflected to the Blueprint. In this way, the cloud application is managed. Thus, cloud application operation is guaranteed after one or several of its software components are re-deployed on new cloud platform services and re-established application interconnections so that the operation of the cloud application is as an initial state. For updating the Blueprint, we built a bidirectional transformation system where the core is a bidirectional transformation program. We show how the Blueprint which is described by TOSCA-based specification is automatically correctly auto-updated.",10,https://doi.org/10.3233/FAIA190035,1,
247,Parallel PageRank computation using GPUs.,"Fast & efficient computing of web rank scores is a necessary issue of search engines today. Because of the enormous size of data and the dynamic nature of World Wide Web, this computation is generally executed on large web graphs (to billions webpages) and requires refreshing quite often, so it becomes a challenging task. In this paper, we propose an efficient method for computing PageRank score -- a Google ranking method based on analyzing the link structure of the Web on graphics processing units (GPUs). We have employed a slightly modification of a storage data format called binary 'link structure file' which inspirited from [2] for storing the web graph data. We then divided the PageRank calculating phases into parallel operations for exploiting the computing power of the graphics cards. Our program was written in CUDA language to experiment on a system equipped two double NVIDIA GeForce GTX 295 graphics cards, using two real datasets which were crawled from Vietnamese sites containing 7 million pages, 132 million links and 15 million pages, 200 million links, respectively. The experimental results showed that the computation speed increase from 10 to 20 times when compared to a CPU Intel Q8400 at 2.67 GHz based version, on both datasets. Our method can also scale up well for larger web graphs.",10,https://doi.org/10.1145/2350716.2350751,1,
248,Compiling ML polymorphism with explicit layout bitmap.,"Most of the current implementations of functional languages adopt so-called ""tagged data representations"" to support tracing garbage collection. The representations impose a burden of data conversion on the runtime performance and the interoperability between ML and other languages. In this paper, we present a type-directed compilation method for ML polymorphism that supports natural representations of integers and other atomic data.This is achieved by compiling ML so that each runtime object (a heap block or a stack frame) has a ""bitmap"" that describes the pointer positions in the block. Since a polymorphic function may produce runtime objects of different types, the compiler needs to compute appropriate bitmaps for each instantiation of the function. This would require us to insert extra lambda abstractions and applications to pass the bits required in bitmap calculations. This compilation process should be done for both stack frames and heap-allocated objects including functions' closures and their environment records. We solve these problems by combining the type-directed compilation method with typed closure conversion, and type-preserving A-normalizationThe resulting compilation process is shown to be sound with respect to an untyped operational semantics with bitmap-inspecting garbage collection. The proposed compilation method has been implemented for the full Standard ML Language, demonstrating its practical feasibility.",10,https://doi.org/10.1145/1140335.1140364,1,
249,Improvement of K-nearest Neighbors (KNN) Algorithm for Network Intrusion Detection Using Shannon-Entropy.,"Non-parametric Nearest Neighbor is an algorithm seeking for the closest data points based on the Euclidean Norm (the standard distance between two data points in a multidimensional space). The classical K-nearest Neighbor (KNN) algorithm applies this theory to find K data points in a vicinity of the considering data, then uses majority voting to label its category. This paper proposes a modification to the original KNN to improve its accuracy by changing that Euclidean Norm based on Shannon-Entropy theory in the context of Network Intrusion Detecton System. Shannon-Entropy calculates the importance of features based on the labels of those data points, then the distance between data points would be re-calculated through the new weights found for these features. Therefore, it is possible to find the more suitable K data points nearby. NSL - KDD dataset is used in this paper to evaluate the performance of the proposed model. A comparison is drawn between the results of the classic KNN, related work on its improvement and the proposed algorithm as well as novel deep learning approaches to evaluate its effectivenes in different scenarios. Results reveal that the proposed algorithm shows good performance on NSL - KDD data set. Specifically, an accuracy up to 99.73% detecting DoS attacks is obtained, 5.46% higher than the original KNN, and 1.15% higher than the related work of M-KNN. Recalculating the Euclidean-Norm distance retains the contribution of the features with low importance to the data classification, while assuring that features with higher importance will have a higher impact. Thus, the proposal does not raise any concern for losing information, and even achieves high efficiency in the classification of features and data classification.",11,https://doi.org/10.12720/jcm.16.8.347-354,1,
250,BKIDset - A New Intrusion Detection Dataset To Mitigate The Class Imbalance Problem.,"Applying machine learning techniques to Intrusion Detection System (IDS) is one efficient method to detect existing and new types of network attacks with minimal human supervision and great accuracy. The classification performance of such systems depend heavily on the dataset they are trained on. Most currently deployed datasets suffer from a high imbalance between normal network traffic and anomaly network traffic, which limits the system’s ability to detect minority classes with high accuracy. In this paper, we propose a new model for generating and collecting anomaly network traffic, which is used in conjunction with CICIDS2017 [1] dataset to create a new dataset namely BKIDset, which mitigates the class imbalance problem encountered in CICIDS2017 dataset and introduces additional types of denial-of-service attacks. The paper also proposes different machine learning models trained on BKIDset, that achieve a high accuracy score of 99%.",11,https://doi.org/10.1109/ACOMP53746.2021.00021,1,
251,Enhanced Group-Based Key Management Scheme for Wireless Sensor Networks using Deployment Knowledge.,"Key establishment plays a central role in authentication and encryption in wireless sensor networks, especially when they are mainly deployed in hostile environments. Because of the strict constraints in power, processing and storage, designing an efficient key establishment protocol is not a trivial task. Compared with public key cryptography, symmetric key cryptographic with key predistribution mechanism is more suitable for large-scale wireless sensor networks. Most of previous solutions have some issues on performance and security capabilities. In this paper, we propose a novel key predistribution model using pre-deployment knowledge and random values in pairwise key generation to take advantage in terms of network connectivity, memory cost, energy for transmission and strong resilience against node capture attacks.",11,https://doi.org/10.1109/CCNC.2009.4784870,1,
252,Minimizing the Intrusion Detection Modules in Wireless Sensor Networks.,"Intrusion detection is an efficient method against attacks in computer network and system. Most intrusion detection system for sensor networks and ad hoc networks are based on the cooperative of intrusion detection agents and suffered from a computation and memory usage overload. In this paper, we address this problem by proposing a lightweight algorithm to select and activate the intrusion detection modules on particular sensor nodes. We also apply over-hearing mechanism to reduce the redundant sending alert packets thus enhance network lifetimes.",11,https://doi.org/10.1109/ICCSA.2008.9,1,
254,Optimal Selection and Activation of Intrusion Detection Agents for Wireless Sensor Networks.,"Intrusion detection is an efficient method against attacks in computer network and system. Most intrusion detection system for sensor networks and ad hoc networks are based on the cooperative of intrusion detection agents and suffered from a computation and memory usage overload. In this paper, we address this problem by proposing two algorithms to optimal select and activate the intrusion detection agents for sensor networks.",11,https://doi.org/10.1109/FGCN.2007.175,1,
256,RainPredRNN: A New Approach for Precipitation Nowcasting with Weather Radar Echo Images Based on Deep Learning.,"Precipitation nowcasting is one of the main tasks of weather forecasting that aims to predict rainfall events accurately, even in low-rainfall regions. It has been observed that few studies have been devoted to predicting future radar echo images in a reasonable time using the deep learning approach. In this paper, we propose a novel approach, RainPredRNN, which is the combination of the UNet segmentation model and the PredRNN_v2 deep learning model for precipitation nowcasting with weather radar echo images. By leveraging the abilities of the contracting-expansive path of the UNet model, the number of calculated operations of the RainPredRNN model is significantly reduced. This result consequently offers the benefit of reducing the processing time of the overall model while maintaining reasonable errors in the predicted images. In order to validate the proposed model, we performed experiments on real reflectivity fields collected from the Phadin weather radar station, located at Dien Bien province in Vietnam. Some credible quality metrics, such as the mean absolute error (MAE), the structural similarity index measure (SSIM), and the critical success index (CSI), were used for analyzing the performance of the model. It has been certified that the proposed model has produced improved performance, about 0.43, 0.95, and 0.94 of MAE, SSIM, and CSI, respectively, with only 30% of training time compared to the other methods",12,https://doi.org/10.3390/axioms11030107,1,
257,Checking and correcting the source code of web pages for accessibility,"This paper aims to propose and experiment a framework for checking and correcting websites for accessibility. Existing tools usually check the WCAG-conformance of HTML client pages (that contain the static elements to be displayed through a browser to end users at a moment). Consequently, web developers have to do tedious works of identifying which parts of server source pages (i.e. server-side source codes that generate HTML client pages) cause non-conformant client elements. Unlike these tools, our framework allows directly reporting and suggesting solutions for the elements in the server source pages. The proposed method composes of four steps. First, the HTML client page and the server source page are parsed. Second, the elements of HTML client page that are non-conformant with the WCAG success criteria are identified and reported. Third, a mapping between the HTML client page and the …",27,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=cXezJ8AAAAAJ&citation_for_view=cXezJ8AAAAAJ:qjMakFHDy7sC,,1
258,A New Complex Fuzzy Inference System With Fuzzy Knowledge Graph and Extensions in Decision Making.,"Context and Background:Complex fuzzy theory has a strong practical implication in many real-world applications. Complex Fuzzy Inference System (CFIS) is a powerful technique to overcome the challenges of uncertain, periodic data. However, a question is raised for CFIS: How can we deduce and predict the result in case there is little knowledge about data information and rule base? This is significance because many real applications do not have enough knowledge of rule base for inference so that the performance of systems may be low. Thus, it is necessary to have an approximate reasoning method to represent and derive final results. Motivation: Recently, the Mamdani Complex Fuzzy Inference System (M-CFIS) has been proposed with a specific inference mechanism according to the Mamdani type. A new improvement so-called the Mamdani Complex Fuzzy Inference System with Rule Reduction (M-CFIS-R) has been designed to utilize granular computing with complex similarity measures to reduce the rule base so as to gain better performance in decision-making problems. However in M-CFIS-R, testing data are checked by matching with each rule in the rule base, which leads to a high cost of computational time. Besides, if the testing data contain records that are not inferred by the rule base, the output cannot be generated. This happens in real commerce systems in which the rule base is small at the time of creation and needs to feed with new rules. Methodology: In order to handle those issues, this article first time proposes the Fuzzy Knowledge Graph to represent the rule base in terms of linguistic labels and their relationships according to the rule set. An adjacent matrix of Fuzzy Knowledge Graph is generated for inference. When a record in the Testing dataset is given, it would be fuzzified and labelled. Each component in the record is checked with the Fuzzy Knowledge Graph by the inference mechanism in approximate reasoning called Fast Inference Search Algorithm. Then, we derive .",12,https://doi.org/10.1109/ACCESS.2020.3021097,1,
259,A monotonic optimization approach for solving strictly quasiconvex multiobjective programming problems.,"In this article, we use a monotonic optimization approach to propose an outcome-space outer approximation by copolyblocks for solving strictly quasiconvex multiobjective programming problems which include many classes of captivating problems, for example when the criterion functions are nonlinear fractional. After the algorithm is terminated, with any given tolerance, an approximation of the weakly efficient solution set is obtained containing the whole weakly efficient solution set of the problem. The algorithm is proved to be convergent and it is suitable to be implemented in parallel using convex programming tools. Some computational experiments are reported to show the accuracy and efficiency of the algorithm.",12,https://doi.org/10.3233/JIFS-179690,1,
260,Predictive reliability and validity of hospital cost analysis with dynamic neural network and genetic algorithm.,"Hospital cost analysis (HCA) becomes a key topic and forefront of politics, social welfare and medical discourse. HCA includes a wide range of expenses; yet the foremost attention relates to the money expense in which hospital managers would like to draw a figure of incomes in the past and future. Based on the HCA results, they can develop many plans for improving hospital’s service quality and investing in potential healthcare services in order to deliver better services with lower costs. Machine learning methods are often opted for prediction in HCA. In this paper, we propose a new method for HCA that uses genetic algorithm (GA) and artificial neural network (ANN). Operators of GA are used to boost up calculation to get optimal weights in the forward propagation of ANN. Experiments on a real database of Hanoi Medical University Hospital (HMUH) including calculus of kidney and ureter inpatients show that the new method achieves better accuracy than the relevant ones including linear regression, K-nearest neighbors (KNN), ANN and deep learning. The mean squared error of the proposed model gets the lowest value (0.00360), compared to those of deep learning, KNN and linear regression which are 0.00901, 0.01205 and 0.01718 respectively.",12,https://doi.org/10.1007/s00521-020-04876-w,1,
261,Some Fuzzy Inference Processes in Picture Fuzzy Systems.,"Dealing with uncertain and linguistic information has been always a big problem in the areas of computational intelligence and artificial intelligence. Fuzzy inference mechanism is one of the common approaches to handling uncertain and linguistic information. The conventional fuzzy inference rule is expressed in the form of ""if-then"" rule: ""IF X is A THEN Y is B"", where A and B represent fuzzy sets in universes U, V. Since 2013, Cuong and Kreinovich [7] proposed Picture Fuzzy Sets (PFS), the interest in FPS continues to grow and the research area continues to expand, especially picture fuzzy logic and picture fuzzy inference. PFS is a direct expansion of FS, it enhances the flexibility and efficiency of FS. This paper has presented some new classes of operators in the implication and generalize the Inference of Compositional Rule on the Picture Fuzzy Logic (PFL) setting.",12,https://doi.org/10.1109/KSE.2019.8919454,1,
262,A New Assessment of Cluster Tendency Ensemble approach for Data Clustering.,"The ensemble is an universal machine learning method that is based on the divide-and-conquer principle. The ensemble aims to improve performance of system in terms of processing speed and quality. The assessment of cluster tendency is a method determining whether a considering data-set contains meaningful clusters. Recently, a silhouette-based assessment of cluster tendency method (SACT) has been proposed to simultaneously determine the appropriate number of data clusters and the prototypes. The advantages of SACT are accuracy and less the parameter, while there are limitations in data size and processing speed. In this paper, we proposed an improved SACT method for data clustering. We call eSACT algorithm. Experiments were conducted on synthetic data-sets and color image images. The proposed algorithm exhibited high performance, reliability and accuracy compared to previous proposed algorithms in the assessment of cluster tendency.",12,https://doi.org/10.1145/3287921.3287927,1,
263,Picture inference system: a new fuzzy inference system on picture fuzzy set.,"In this paper, we propose a novel fuzzy inference system on picture fuzzy set called picture inference system (PIS) to enhance inference performance of the traditional fuzzy inference system. In PIS, the positive, neutral and negative degrees of the picture fuzzy set are computed using the membership graph that is the combination of three Gaussian functions with a common center and different widths expressing a visual view of degrees. Then, the positive and negative defuzzification values, synthesized from three degrees of the picture fuzzy set, are used to generate crisp outputs. Learning in PIS including training centers, widths, scales and defuzzification parameters is also discussed. The system is adapted for all architectures such as the Mamdani, the Sugeno and the Tsukamoto fuzzy inferences. Experimental results on benchmark UCI Machine Learning Repository datasets and an example in control theory - the Lorenz system are examined to verify the advantages of PIS.",12,https://doi.org/10.1007/s10489-016-0856-1,1,
264,Dental Diagnosis from X-Ray Images using Fuzzy Rule-Based Systems.,"In practical dentistry, dentists use their experience to examine dental X-ray images and to derive symptoms from patients for concluding possible diseases. This method is based solely on the own dentists' experience. Dental diagnosis from X-Ray images is proposed to support for dentists in their decision making. This paper presents an application of consultant system for dental diagnosis from X-Ray images based on fuzzy rule. Fuzzy rule was applied in many applications and has important role in computational intelligence, data mining, machine learning, etc. Based on a dental X-ray image dataset, we use Fuzzy C-Means to classify them into clusters and construct the rule set. Fuzzy Inference System is then used to evaluate the rules by three validity indices. These rules accompanied with symptoms from patients help dentists in diagnosing dental diseases. This method is implemented and experimentally validated on the real dataset of Hanoi Medical University Hospital, Vietnam against the related algorithms.",12,https://doi.org/10.4018/IJFSA.2017010101,1,
265,A Novel Multiple Fuzzy Clustering Method Based on Internal Clustering Validation Measures with Gradient Descent.,"In this paper, we propose a novel multiple fuzzy clustering method based on internal clustering validation measures with gradient descent. Firstly, some single fuzzy clustering algorithms such as Fuzzy C-Means, Kernel Fuzzy C-Means and Gustafson–Kessel are used to construct similarity matrixes for each partition. Secondly, those similarity matrixes are aggregated into a final one by means of the direct sum of weighted vectors where the values of weights are determined by internal clustering validation measures. Finally, final membership matrix is calculated by minimizing the sum of square errors through the gradient descent method. The proposed approach has been validated in terms of clustering quality on UCI Machine Learning Repository datasets. Experimental results show that the proposed approach’s performance is better than those of other ensemble and standalone methods.",12,https://doi.org/10.1007/s40815-015-0117-1,1,
266,Some Fuzzy Logic Operators for Picture Fuzzy Sets.,"A generalization of the Zadeh's fuzzy sets and Antanassov's intuitionistic fuzzy - ""Picture fuzzy set"" have defined in 2013. This concept is particularly effective in decision making problems, such as voting, solutions choice, financial forecasting, estimating risks in business, etc. This promotes vigorously the construction of the picture fuzzy theory: 1) picture fuzzy operators on picture fuzzy sets, 2) picture fuzzy relations, 3) picture fuzzy soft sets. In this paper, we have investigated main fuzzy logic operators: negations, conjunctions, disjunctions and implications on picture fuzzy sets. As combined with picture fuzzy operators by De Morgan triples, we have also constructed main operations for fuzzy inference processes in picture fuzzy systems.",12,https://doi.org/10.1109/KSE.2015.20,1,
267,Some Extensions of Membership Graphs for Picture Inference Systems.,"In this paper, we have extended the fuzzy inference system (FIS) on picture fuzzy sets in order to get a new system called picture inference system (PIS). PIS is designed based on the membership graph together with the general picture inference scheme. A membership graph is the combination of three lines giving a visual view of the positive, neutral and negative degrees, which reflect the underlying principle of picture fuzzy sets. Some extensions of the membership graph including the triangle, trapezoid, Gaussian and Bell graphs are investigated. The findings of the paper suggest the most efficient membership graph(s) that can be used in the picture inference system.",12,https://doi.org/10.1109/KSE.2015.33,1,
268,Towards Automatic Construction of News Directory Systems.,"Currently news sites and news index sites basically provide streams of news with simple classifications or keyword based searching with publication data. When we try to search for news involving a number of potential keywords or unknown keywords, then the task becomes manually tedious or almost impossible. We present automatic methods for constructing news directory systems which contain collections of news index information with flat or hierarchical classification structures. This directory structure enables us to reach the news articles without knowing the keywords exactly. We implemented and evaluated one sample news directory system.",12,http://www.booksonline.iospress.nl/Content/View.aspx?piid=7936,1,
269,Towards Automatic Detection of Potentially Important International Events/Phenomena from News Articles at Mostly Domestic News Sites.,"Unlike the relative success of general Web search engines detecting important Web pages, it is not yet easy to detect potentially important events/phenomena (other than obvious major newspaper top headlines) happening outside the country from observations of news articles on the Web. Current news index sites or news provider sites such as Google News, BBC, CNN, and Reuters, try to present streams of latest news. We present a new approach to automatic detection of potentially important events/phenomena happening outside the country based on the monitoring of a number of different mostly domestic news sites.",12,http://www.booksonline.iospress.nl/Content/View.aspx?piid=5313,1,
270,Monitoring agriculture areas with satellite images and deep learning.,"Agriculture applications rely on accurate land monitoring, especially paddy areas, for timely food security control and support actions. However, traditional monitoring requires field works or surveys performed by experts, which is costly, slow, and sparse. Agriculture monitoring systems are looking for sustainable land use monitoring solutions, starting with remote sensing on satellite data for cheap and timely paddy mapping. The aim of this study is to develop an autonomous and intelligent system built on top of imagery data streams, which is available from low-Earth orbiting satellites, to differentiate crop areas from non-crop areas. However, such agriculture mapping framework poses unique challenges for satellite image processing, including the seasonal nature of crop, the complexity of spectral channels, and adversarial conditions such as cloud and solar radiance. In this paper, we propose a novel multi-temporal high-spatial resolution classification method with an advanced spatio-temporal–spectral deep neural network to locate paddy fields at the pixel level for a whole year long and for each temporal instance. Our method is built and tested on the case study of Landsat 8 data due to its high spatial resolution. Empirical evaluations on real imagery datasets of different landscapes from 2016 to 2018 show the superior of our mapping model against the baselines with over 0.93 F1-score, the importance of each model design, the robustness against seasonal effects, and the visual mapping results.

",14,https://doi.org/10.1016/j.asoc.2020.106565,1,
271,Monitoring Rice Crop with Dense Segmentation on Satellite Images: A case study in Vietnamese Mekong Delta.,"Rice is the major crop in Vietnam with the crucial roles in national food security and export. While the demand of this crop is increasing overtime, the land use for this purpose is restricted due to various factors such as urban expansion and seawater intrusion. Therefore, a sustainable and effective rice paddy area monitoring is highly desirable, as the traditional methods are costly, inaccurate and requires heavy human-related workload. In this paper, we propose a novel rice monitoring framework using the satellite image, thanks to its high quality and free-to-access. Our framework first preprocesses the raw satellite image to alleviate the effect of adversarial factors such as geometric discrepancy and solar radiation. Then we design a deep learning model inspiring from u-net architecture to exploit simultaneously the spatial, spectral and temporal nature of satellite images. The empirical result conducted over data collected in Vietnamese Mekong Delta shows that our framework outperforms others state-of-the-art baselines and achieves the accuracy of 92.7% without the need of any handcraft feature engineering or expert knowledge.",14,https://doi.org/10.1109/KSE50997.2020.9287368,1,
272,An Ensemble System with Random Projection and Dynamic Ensemble Selection.,"In this paper, we propose using dynamic ensemble selection (DES) method on ensemble generated based on random projection. We first construct the homogeneous ensemble in which a set of base classifier is obtained by a learning algorithm on different training schemes generated by projecting the original training set to lower dimensional down spaces. We then develop a DES method on those base classifiers so that a subset of base classifiers is selected to predict label for each test sample. Here competence of a classifier is evaluated based on its prediction results on the test sample’s k - nearest neighbors obtaining from the projected data of validation set. Our proposed method, therefore, gains the benefits not only from the random projection in dimensionality reduction and diverse training schemes generation but also from DES method in choosing an appropriate subset of base classifiers for each test sample. The experiments conducted on some datasets selected from four different sources indicate that our framework is better than many state-of-the-art DES methods concerning to classification accuracy.",14,https://doi.org/10.1007/978-3-319-75417-8_54,1,
280,Defining and modelling service-based coordinated systems,This paper introduces MEO – a model for securing service-based coordinated systems. The model uses constraints for expressing the application logic of a coordinated system and its required security strategies. Coordination activities are the key concepts used for controlling the execution of participating services. Constraints are specified as pre and post conditions of these coordination activities.,27,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=cXezJ8AAAAAJ&citation_for_view=cXezJ8AAAAAJ:UeHWp8X0CEIC,,1
281,Towards a secure service coordination,"This paper presents an approach for building secure service-based coordinated systems. Secure coordination is considered at two levels: abstraction (i.e., specification) and execution (i.e., run level). At the abstraction level, we define a general model enabling to specify coordination and its related non functional properties (such as security). The idea is to use constraints for expressing the application logic of a coordinated system and its required security strategies. Coordination activities are the key concepts used for controlling the execution of participating services. Constraints are specified as pre and post conditions of these coordination activities. At the execution level, we propose an architecture which implements strategies to verify constraints and manage the secure execution of coordination. We propose also an instantiating vade-mecum to configure execution level components according to a …",27,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=cXezJ8AAAAAJ&citation_for_view=cXezJ8AAAAAJ:u-x6o8ySG0sC,,1
289,Human behavior analysis by means of multimodal context mining,"There is sufficient evidence proving the impact that negative lifestyle choices have on people’s health and wellness. Changing unhealthy behaviours requires raising people’s self-awareness and also providing healthcare experts with a thorough and continuous description of the user’s conduct. Several monitoring techniques have been proposed in the past to track users’ behaviour; however, these approaches are either subjective and prone to misreporting, such as questionnaires, or only focus on a specific component of context, such as activity counters. This work presents an innovative multimodal context mining framework to inspect and infer human behaviour in a more holistic fashion. The proposed approach extends beyond the state-of-the-art, since it not only explores a sole type of context, but also combines diverse levels of context in an integral manner. Namely, low-level contexts, including activities, emotions and locations, are identified from heterogeneous sensory data through machine learning techniques. Low-level contexts are combined using ontological mechanisms to derive a more abstract representation of the user’s context, here referred to as high-level context. An initial implementation of the proposed framework supporting real-time context identification is also presented. The developed system is evaluated for various realistic scenarios making use of a novel multimodal context open dataset and data on-the-go, demonstrating prominent context-aware capabilities at both low and high levels.",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:ufrVoPGSRksC,,1
290,An innovative platform for person-centric health and wellness support,"Modern digital technologies are paving the path to a revolutionary new concept of health and wellness care. Nowadays, many new solutions are being released and put at the reach of most consumers for promoting their health and wellness self-management. However, most of these applications are of very limited use, arguable accuracy and scarce interoperability with other similar systems. Accordingly, frameworks that may orchestrate, and intelligently leverage, all the data, information and knowledge generated through these systems are particularly required. This work introduces Mining Minds, an innovative framework that builds on some of the most prominent modern digital technologies, such as Big Data, Cloud Computing, and Internet of Things, to enable the provision of personalized healthcare and wellness support. This paper aims at describing the efficient and rational combination and …",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:WF5omc3nYNoC,,1
291,Interactive activity recognition using pose-based spatio–temporal relation features and four-level Pachinko Allocation Model,"In this paper, we go beyond the problem of recognizing video-based human interactive activities. We propose a novel approach that permits to deeply understand complex person-person activities based on the knowledge coming from human pose analysis. The joint coordinates of interactive objects are first located by an efficient human pose estimation algorithm. The relation features consisting of the intra and inter-person features of joint distance and angle, are suggested to use for describing the relationships between body components of the individual persons and the interacting two participants in the spatio-temporal dimension. These features are then provided to the codebook construction process, in which two types of codeword are generated corresponding to distance and angle features. In order to explain the relationships between poses, a flexible hierarchical topic model constructed by four layers is …",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:u-x6o8ySG0sC,,1
292,A sleep monitoring application for u-lifecare using accelerometer sensor of smartphone,"Ubiquitous lifecare (u-lifecare) is regarded as a seamless technology that can provide services to the patients as well as facilitate the healthy people to maintain an active lifestyle. In this paper, we develop a sleep monitoring application to assists the healthy people for managing their sleep. It provides an unobtrusive and proactive way for the self-management. We utilize the embedded accelerometer sensor of the smartphone as a client node to collect the sleeping data logs. Our proposed model is server-driven approach and process the data over the server machine. We classify the body movements and compute the useful sleep analytics. It facilitates the users to keep the record of daily sleep and assists to change their unhealthy sleeping habits that are identified by our computed sleep analytics such as bed time, wake up, fell asleep, body movements, frequent body movements at different stages of the …",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:_FxGoFyzp5QC,,1
293,Mining human behavior for health promotion,"The monitoring of human lifestyles has gained much attention in the recent years. This work presents a novel approach to combine multiple context-awareness technologies for the automatic analysis of people's conduct in a comprehensive and holistic manner. Activity recognition, emotion recognition, location detection, and social analysis techniques are integrated with ontological mechanisms as part of a framework to identify human behavior. Key architectural components, methods and evidences are described in this paper to illustrate the interest of the proposed approach.",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:LkGwnXOMwfcC,,1
294,Adaptive hierarchical emotion recognition from speech signal for human-robot communication,"Emotional speech recognition is an interesting application that is able to recognize different emotional states from speech signal. In Human-Robot Interaction (HRI), emotion recognition is being applied on intelligent robots so that they can understand emotional states of user and interact in a more human-like manner. However, it is not easy to apply emotion recognition algorithms in real applications due to the dependence on many factors. In this paper, we introduce hierarchical approaches that generate the binary classification tree automatically and exploit multiple classifiers to recognize different emotions. And then we propose a framework that recognizes emotions from speech signal with a higher accuracy and efficiency in comparison with other algorithms such as Hidden Markov Model (HMM) or Support Vector Machine (SVM). The method automatically creates a binary classification tree and optimizes the …",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:YsMSGLbcyi4C,,1
295,PAM-based flexible generative topic model for 3D interactive activity recognition,"Interactive activity recognition from the RGB videos still remains a challenge, therefore some existing approaches paid the attention to RGB-Depth video process to avoid problems relating to mutual occlusion and redundant human pose and to improve accuracy of skeleton extraction. From the single action to complex interaction activity, it is necessary an efficient model to describe the relationship of body components between multi-human objects. In this research, the authors proposed a hierarchical model based on the Pachinko Allocation Model for interaction recognition. Concretely, the joint features comprising joint distant and joint motion are calculated from the skeleton position and then support to topic modeling. The probabilistic models describing the flexible relationship between features - poselets - activities are generated by this model. Finally, the Binary Tree of Support Vector Machine is applied for …",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:IjCSPb-OGe4C,,1
297,RL-TSCH: A Reinforcement Learning Algorithm for Radio Scheduling in TSCH 802.15.4e.,"In order to apply Internet of Things technologies into real world, the low-power low-cost protocol stack CoAP/UDP/IPv6/RPL/6LoWPAN/802.15.4-802.15.4e is a good solution and appropriate to low-power low-cost applications. The standard IEEE 802.15.4e utilizes scheduled-based multiple access approach in this, at the beginning of each time slot, the nodes will always turn on the radio for a specified period of time to check if packets are sent or not and the process is repeated in the next time slot (Minimal Scheduling Algorithm). Thus, it leads to unnecessary waste of energy if for a while the node does not transmit or send a packet. To overcome this disadvantage, we propose in this paper a reinforcement learning solution, called RL-TSCH for scheduling nodes (controlling the node's radio on or off) of TSCH 802.15.4e based on Markov Decision Process associated with Q-Learning algorithm. We evaluate the performance of our RL-TSCH algorithm and compare with Minimal Scheduling Algorithm in simulation and real testbed with Contiki operating system. The initial result shows that, our mechanism achieves similar packet delivery rate (96%) while saving power consumption up to 30% compared to Minimal Scheduling.",15,https://doi.org/10.1109/ICTC46691.2019.8939833,1,
298,esVHO: Energy Saving Vertical Handover Extension for Local SDN in Non-Interconnected Environment.,"Wireless technologies that offer high data rate are generally energy-consuming ones while low-energy technologies commonly provide low data rate. Both kinds of technologies have been integrated in a single mobile device for different services. Therefore, if the service does not always require high data rate, the low energy technology, i.e., Bluetooth, can be used instead of the energy-consuming one, i.e., Wi-Fi, for saving energy. It is obvious that energy savings are maximized by turning the unused technology off. However, when active sessions of ongoing services migrate between different technologies, the network-layer connectivity must be maintained, or a vertical handover (VHO) between different networks is required. Moreover, when the networks are not interconnected, the VHO must be fully controlled by the device itself. The device typically navigates traffic through the firmware of the wireless network interface cards (WNIC) using their drivers, which are dependent on the vendors. To control the traffic navigation between WNICs without any modification of the WNICs' drivers, Software-Defined Networking (SDN) can be applied locally on the mobile device, the so called local SDN. In the local SDN architecture, a local SDN controller (SDNC) is used to control a virtual OpenFlow switch, which turns WNICs into its switch ports. Although the SDNC can navigate the traffic, it lacks the global view of the network topology. Hence, to correctly navigate traffic in a VHO process, an extended SDN controller (extSDNC) was proposed in a previous work. With the extSDNC, the SDNC can perform VHO based on a link layer trigger but with a significant packet loss rate. Therefore, in this paper, a framework named esVHO is proposed that executes VHO at the network layer to reduce the packet loss rate and reduce energy consumption. Experiments on VHO performance prove that esVHO can reduce the packet loss rate considerably. Moreover, the results of an energy saving experiment show that esVHO performs high energy saving up to 4.89 times compared to the others.",15,https://doi.org/10.1587/transcom.2016NNP0007,1,
299,Investigating the Performance of Link Aggregation on OpenFlow Switches.,"In this paper, we extensively explore the operation of Link Aggregation (LA) on OpenFlow switches in comparison to the LA in conventional switches. The comparison of two LA implementations has been conducted in a real testbed under the UDP and TCP traffic loads. The testbed includes Pica8 P-3925 switches, which support two modes: an OpenFlow switch (i.e., using Open vSwitch) and a conventional switch (i.e., using the operating system called XorPlus). The evaluation results show that two LA implementations achieve similar performance in improving throughput. However, the XorPlus implementation provides a better resilience than the other. Specifically, the LA implementation on XorPlus spends less than 1.49538 seconds to switch the TCP traffic on the faulty link to the other links of a Link Aggregation Group (LAG) while the switchover time is four times longer on the Open vSwitch. In the case of UDP traffic, the maximum switchover time on the Open vSwitch is twice the one on XorPlus.",15,https://doi.org/10.1007/978-3-319-13326-3_19,1,
301,A bi-level optimized charging algorithm for energy depletion avoidance in wireless rechargeable sensor networks.,"In Wireless Rechargeable Sensor Networks (WRSNs), charging scheme optimization is one of the most critical issues, which plays an essential role in deciding the sensors’ lifetime. An effective charging scheme should simultaneously consider both the charging path and the charging time. Existing works, however, mainly focus on determining the optimal charging path and adopt the full charging strategy. The full charging approach may increase the sensors’ charging delay and eventually lead to sensor energy depletion. This paper studies how to optimize the charging path and the charging time at the same time to avoid energy depletion in WRSNs. We first formulate the investigated problem with a Mixed-Integer Linear Programming model. We then leverage the bi-level optimization approach and represent the targeted problem with two levels: the charging path optimization at the upper level and the charging time optimization at the lower level. A combination of Genetic Algorithm and Greedy method is proposed to determine the optimal charging path. Besides, to reduce the computational complexity of charging time identification level, we propose a Particle Swarm Optimization (PSO) algorithm to optimize the charging time of the best charging path in each evolutionary generation. The experimental validation on various network scenarios demonstrates our proposed charging scheme’s superiority over the existing algorithms.",17,https://doi.org/10.1007/s10489-021-02775-8,1,
302,Multi-objective teaching-learning evolutionary algorithm for enhancing sensor network coverage and lifetime.,"Coverage plays a vital role in the performance and proper functioning of wireless sensor networks. However, ensuring a network’s coverage is met numerous challenges due to sensors having limited sensing range, communication range, and energy. Many coverage problems are NP-hard, one of which is the network coverage with lifetime problem (CTLP). As such, a number of meta-heuristic algorithms have been proposed to solve CTLP in practical scenarios. This paper proposes an approach for CTLP based on the teaching–learning based optimization algorithm (TLBO), which is often employed to address continuous optimization problems. Specifically, a discrete version of multi-objective improved teaching–learning based optimization algorithm (MO-ITLBO) called HTLBO is proposed, employing genetic operators inspired by evolutionary computing methods. Experimental results are extensively compared to those obtained from previous approaches, namely MO-ITLBO, fast elitist non-dominated sorting genetic algorithm (NSGA-II), multi-objective differential evolution (MODE), and multi-objective evolutionary algorithm based on decomposition (MOEA/D). The evaluation shows significant improvements in different metrics, including spacing, hypervolume, non-dominated solutions, and coverage.",17,https://doi.org/10.1016/j.engappai.2021.104554,1,
303,A family system based evolutionary algorithm for obstacle-evasion minimal exposure path problem in Internet of Things.,"Barrier coverage in wireless sensor networks (WSNs) is a well-known model for military security applications in IoTs, in which sensors are deployed to detect every movement over the predefined border. The fundamental sub-problem of barrier coverage in WSNs is the minimal exposure path (MEP) problem. The MEP refers to the worst-case coverage path where an intruder can move through the sensing field with the lowest capability to be detected. Knowledge about MEP is useful for network designers to identify the worst coverage in WSNs. Most prior research focused on this problem with the assumption that the WSN has an ideal deployment environment without obstacles, causing existing gaps between theoretical and practical WSNs systems. To overcome this drawback, we investigate a systematic and generic MEP problem under real-world environment networks by presenting obstacles called Obstacle-Evasion-MEP (hereinafter OE-MEP). We propose an algorithm to create several types of arbitrary-shaped obstacles inside the deployment area of WSNs. The OE-MEP problem is an NP-Hard with high dimension, non-differentiation, non-linearity, and constraints. Based upon its characteristics, we then devise an elite algorithm namely Family System based Evolutionary Algorithm (FEA) with our newly-proposed concepts of Family System, tailored to efficiently solve the OE-MEP. We also build an extension to a custom-made simulation environment to integrate a variety of network topologies as well as obstacles. Experimental results on numerous instances indicate that the proposed algorithm is suitable for the converted OE-MEP problem and performs better in solution accuracy than existing approaches.",17,https://doi.org/10.1016/j.eswa.2022.116943,1,
304,Two levels approach based on multifactorial optimization to solve the clustered shortest path tree problem.,"The Clustered Shortest-Path Tree Problem (CluSPT) has a great meaning in theoretical research as well as a wide range of applications in everyday life, especially in the field of network optimization. Being able to solve the CluSPT will path the way for improving practical systems such as agricultural irrigation and product distribution. Multifactorial Evolutionary Algorithm (MFEA) is a variant of Evolutionary Algorithm (EA) aiming at simultaneously solving multiple factorial tasks which can be diverse in types, dimensionalities and search spaces. The applications of MFEA have yet to be fully exploited, but the realm has recently attracted much interest from the research community, especially those who are working on evolutionary multitasking. Considering these characteristics, this paper describes a new approach using the MFEA for solving the CluSPT. The MFEA has two tasks: the first is solving the CluSPT problem and the second is solving a new problem which is decomposed from the CluSPT problem. The goal of the first task is finding the fittest solution as possible for the CluSPT while the goal of the second task is determining the best tree (w.r.t. cost minimization) which envelops all vertices of the problem. This paper also introduces mutation, crossover and population initialization operators for a proposed MFEA to solve the CluSPT. Each of these operators deals with individuals in two phases, one of which ensures that the resulted individual is a spanning tree and the other phase ensures that each of its clustered sub-graphs is also a spanning tree. As the MFEA had to deal with these two tasks, a decoding method was also created to allow communication between the tasks. To assess the effectiveness of the proposed algorithm and methods, the authors implemented them on both Euclidean and Non-Euclidean instances. Experiment results show that the proposed MFEA outperformed the existing MFEA algorithms on two-thirds instances with average improvement value of 47%. Besides, this paper analysis the convergence trends of each task in generations and the affectation of the number of vertices in the largest cluster on the obtained results by using both the Spearman’s rank correlation coefficient and scatter graphs.",17,https://doi.org/10.1007/s12065-020-00501-w,1,
306,A hybrid multifactorial evolutionary algorithm and firefly algorithm for the clustered minimum routing cost tree problem.,"The clustered minimum routing cost tree (CluMRCT) problem is a recent problem with a wide range of real-life applications, especially in designing computer networks with peer-to-peer architecture. Many multifactorial evolutionary algorithms have been proposed to solve multiple CluMRCT problems simultaneously. However, these algorithms only function effectively on complete graphs with small-and-medium sizes. Moreover, the blindness and randomness in the transfer of genetic materials cause a reduction in the exploitation ability and make these algorithms ineffective to solve low-similarity tasks. This paper proposes a hybrid multitasking algorithm named multifactorial firefly algorithm, which integrates the firefly algorithm’s strong exploitation ability to enhance the self-evolution of each task when facing low-similarity tasks while improving inter-task knowledge transfers by delivering higher-quality solutions. Also, the proposed algorithm is equipped with new encoding and decoding to focus more on potential search areas on both complete and sparse graphs. The experiments and Wilcoxon signed-rank tests were conducted on various instances to verify our proposal with several state-of-the-art methods. The results portrayed that the proposed encoding scheme helped multitasking algorithms improve solution quality by 32% on average. Besides, the statistical test values proved the superiority of the proposed hybrid algorithm in terms of solution quality and convergence trend.",17,https://doi.org/10.1016/j.knosys.2022.108225,1,
308,Adaptive Knowledge Transfer in Multifactorial Evolutionary Algorithm for the Clustered Minimum Routing Cost Problem.,"Having tremendous promising impact on society, the Internet of Things refers to the connection of millions of worldwide devices to communicate and exchange data, which requires to aggregate similar devices into clusters for maximizing the network efficiency. Minimum Routing Cost Clustered Tree Problem (CluMRCT) is a lately investigated topic that has significant application towards enhancing the interconnectivity among various devices. Belonging to NP-Hard class, the CluMRCT problem can be solved effectively by a meta-heuristic approach such as Multifactorial Evolutionary Algorithm (MFEA), which can facilitate to find better solutions for multiple problems simultaneously. Recently, an improved framework (called MFEA-II) has been introduced to overcome the weakness of algorithm performance governed by the degree of underlying inter-task synergies in the previous version. Therefore, this paper proposes to apply MFEA-II for solving multiple CluMRCT problems concurrently with the population representation under a probabilistic distribution model adept at online learning. In the proposed algorithm, evolutionary operators are applied in two levels: the first level is to construct a spanning tree in each cluster, while the second one builds a spanning tree for connecting all clusters. To reduce consuming resources, this paper also introduces a new method to calculate the cost of CluMRCT solution in linear time complexity. Numerous types of test instances are implemented to demonstrate the effectiveness of the proposed algorithm when its performance surpassed other state-of-the-art algorithms in most of the datasets.",17,https://doi.org/10.1016/j.asoc.2021.107253,1,
309,A decomposition-based multi-objective optimization approach for balancing the energy consumption of wireless sensor networks.,"Wireless sensor networks consist of many sensor nodes with limited resources and computing capability. Thus, managing energy consumption to prolong network lifetime is a critical issue. Several approaches have been proposed to extend the network lifetime, one of which involves deploying relay nodes to transfer data from sensors to the base station. However, the limited number of relay nodes is a challenge that often goes overlooked. This paper examines the problem of optimizing the network lifetime and the number of relay nodes in three-dimensional terrains. A novel algorithm called MOEA/D-LS is proposed with the aim of obtaining a better tradeoff between two objectives. The algorithm is a hybridization between multiobjective evolutionary algorithm based on decomposition, and a special local search to optimize the former’s subproblems. Simulation results on 3D datasets show that the proposed algorithm has a significantly better performance compared with existing algorithms on all measured metrics.",17,https://doi.org/10.1016/j.asoc.2021.107365,1,
310,A bi-level encoding scheme for the clustered shortest-path tree problem in multifactorial optimization.,"The Clustered Shortest-Path Tree Problem (CluSPT) plays an important role in various types of optimization problems in real-life. Recently, some Multifactorial Evolutionary Algorithms (MFEAs) have been introduced to deal with the CluSPT, but these researches still have some shortcomings, such as evolution operators only perform on complete graphs and huge resource consumption for finding the solution on large search spaces. To overcome these limitations, this paper describes an MFEA-based approach to solve the CluSPT. The proposed algorithm utilizes Dijkstra’s algorithm to construct the spanning trees in clusters while using evolutionary operators for building the spanning tree connecting clusters. This approach takes advantage of both exact and approximate algorithms, so it enables the algorithm to function efficiently on complete and sparse graphs alike. Furthermore, evolutionary operators such as individual encoding and decoding methods are also designed with great consideration regarding performance and memory usage. We have included proof of the repairing method’s efficacy in ensuring all solutions are valid. We have conducted tests on various types of Euclidean instances to assess the effectiveness of the proposed algorithm and methods. Experiment results point out the effectiveness of the proposed algorithm existing heuristic algorithms in most of the test cases. The impact of the proposed MFEA was analyzed, and a possible influential factor that may be useful for further study was also pointed out.",17,https://doi.org/10.1016/j.engappai.2021.104187,1,
311,PoisonGAN: Generative Poisoning Attacks Against Federated Learning in Edge Computing Systems.,"Edge computing is a key-enabling technology that meets continuously increasing requirements for the intelligent Internet-of-Things (IoT) applications. To cope with the increasing privacy leakages of machine learning while benefiting from unbalanced data distributions, federated learning has been wildly adopted as a novel intelligent edge computing framework with a localized training mechanism. However, recent studies found that the federated learning framework exhibits inherent vulnerabilities on active attacks, and poisoning attack is one of the most powerful and secluded attacks where the functionalities of the global model could be damaged through attacker's well-crafted local updates. In this article, we give a comprehensive exploration of the poisoning attack mechanisms in the context of federated learning. We first present a poison data generation method, named Data_Gen, based on the generative adversarial networks (GANs). This method mainly relies upon the iteratively updated global model parameters to regenerate samples of interested victims. Second, we further propose a novel generative poisoning attack model, named PoisonGAN, against the federated learning framework. This model utilizes the designed Data_Gen method to efficiently reduce the attack assumptions and make attacks feasible in practice. We finally evaluate our data generation and attack models by implementing two types of typical poisoning attack strategies, label flipping and backdoor, on a federated learning prototype. The experimental results demonstrate that these two attack models are effective in federated learning.",17,https://doi.org/10.1109/JIOT.2020.3023126,1,
312,Evolutionary algorithm and multifactorial evolutionary algorithm on clustered shortest-path tree problem.,"In literature, Clustered Shortest-Path Tree Problem (CluSPT) is an NP-hard problem. Previous studies focus on approximation algorithms which search for an optimal solution in relatively large space. Thus, these algorithms consume a large amount of computational resources while the quality of obtained results is lower than expected. In order to enhance the performance of the search process, this paper proposes two different approaches which are inspired by two perspectives of analyzing the CluSPT. The first approach intuition is to narrow down the search space by reducing the original graph into a multi-graph with fewer nodes while maintaining the ability to find the optimal solution. The problem is then solved by a proposed evolutionary algorithm. This approach performs well on those datasets having small number of edges between clusters. However, the increase in the size of the datasets would cause the excessive redundant edges in multi-graph that pressurize searching for potential solutions. The second approach overcomes this limitation by breaking down the multi-graph into a set of simple graphs. Every graph in this set is corresponding to a mutually exclusive search space. From this point of view, the problem could be modeled into a bi-level optimization problem in which the search space includes two nested search spaces. Accordingly, the Nested Local Search Evolutionary Algorithm (N-LSEA) is introduced to search for the optimal solution of glscluspt, the upper level uses a simple Local Search algorithm while the lower level uses the Genetic Algorithm. Due to the neighboring characteristics of the local search step in the upper level, the lower level reduced graphs share the common traits among each others. Thus, the Multi-tasking Local Search Evolutionary Algorithm (MLSEA) is proposed to take advantages of these underlying commonalities by exploiting the implicit transfer across similar tasks of multi-tasking schemes. The improvement in experimental results over N-LSEA via this multi-tasking scheme inspires the future works to apply M-LSEA in graph-based problems, especially for those could be modeled into bi-level optimization.",17,https://doi.org/10.1016/j.ins.2020.10.024,1,
313,Multifactorial evolutionary optimization to maximize lifetime of wireless sensor network.,"Prolonging network lifetime is a crucial issue for wireless sensor networks, as sensor nodes operate on limited amounts of battery energy, and replacing or recharging nodes is still quite challenging. One approach is using relay nodes to alleviate sensors’ energy usage when transmitting data. In this work, we tackle the issues of relay node assignment for wireless single-hop sensor and multi-hop sensor networks in three-dimensional terrains. Traditionally, researchers have focused on solving relay node selection for either single-hop or multi-hop networks, one at a time. We propose MFRSEA, a multifactorial evolutionary algorithm utilizing a network random key representation, a constraint-aware fitness function, and a novel crossover operator in order to optimize for both network types simultaneously. Experimental results show that our method outperforms the baseline in several key metrics.",17,https://doi.org/10.1016/j.ins.2021.06.056,1,
314,Parameter adaptation in multifactorial evolutionary algorithm for many-task optimization.,"The advent of multifactorial optimization (MFO) has made a wind of change in intelligence computation in general and specifically in evolutionary computing. Based on the implicit parallelism of population-based search, MFO optimizes different problems simultaneously and entirely. However, the randomness of knowledge transfers raises the question of how to diminish harmful interactions among tasks for more effective transfers. In recent years, many proposals have been devised to handle this paradigm and improve existing algorithms. Notwithstanding the diversity in their concept, there are few efforts to solve many-task optimization (MaTO) that contains beyond three tasks. In light of this reason, this paper proposes two algorithms named SA-MFEA and LSA-MFEA for MaTO. Instead of utilizing fixed parameters, SA-MFEA and LSA-MFEA adapt the probability of random mating parameter to reduce negative transfers based on the historical memory of successful rmp. Besides, LSA-MFEA is capable of enhancing the exploitation by linear population size reduction. To examine the efficiency of the two proposed algorithms, experiments on various many-task benchmark problems and comparison with several state-of-the-art algorithms have been conducted. The results demonstrated that SA-MFEA and LSA-MFEA are competitive in terms of quality of solutions, convergence trend, and computation time.",17,https://doi.org/10.1007/s12293-021-00347-4,1,
315,Multi-objective Sparrow Search Optimization for Task Scheduling in Fog-Cloud-Blockchain Systems.,"In this paper, we first design a system that integrates a blockchain network into a fog-cloud environment to improve the transparency as well as avoid the attack from suspicious parties. We identify task scheduling as the most prominent and crucial issue in such a system and model it as a multi-objective optimization problem. To solve this problem, we propose a multi-objective version of the Sparrow Search Algorithm (SSA), called MO-SSA. Our experimental studies compare MO-SSA with other well-known optimization algorithms (NSGA-II, NSGA-III, and MO-ALO). Results show that MO-SSA archives better results than most of the baseline algorithms on several performance metrics.",17,https://doi.org/10.1109/SCC53864.2021.00065,1,
316,A Two-level Genetic Algorithm for Inter-domain Path Computation under Node-defined Domain Uniqueness Constraints.,"Recent years have witnessed an increment in the number of network components communicating through many network scenarios such as multi-layer and multi-domain, and it may result in a negative impact on resource utilization. An urgent requirement arises for routing the packets most efficiently and economically in large multi-domain networks. In tackling this complicated area, we consider the Inter-Domain Path Computation problem under Node-defined Domain Uniqueness Constraint (IDPC-NDU), which intends to find the minimum routing cost path between two nodes that traverses every domain at most once. Owing to the NP-Hard property of the IDPC-NDU, applying metaheuristic algorithms to solve this problem usually proves more efficient. In like manner, this paper proposes a Two-level Genetic Algorithm (PGA), where the first level determines the order of the visited domains, and the second level finds the shortest path between the two given nodes. Furthermore, to facilitate the finding process, a method to minimize the search space and a new chromosome encoding that would reduce the chromosome length to the number of domains are integrated into this proposed algorithm. To evaluate the efficiency of the proposal, experiments on various instances were conducted. The results demonstrated that PGA outperforms other algorithms and gives results no more than twice the optimal values.",17,https://doi.org/10.1109/CEC45853.2021.9504728,1,
317,Effective Partial Charging Scheme For Minimizing The Energy Depletion And Charging Cost In Wireless Rechargeable Sensor Networks.,"Wireless Rechargeable Sensor Network has emerged as a potential solution for the constrained energy problem in sensor networks in recent years. The charging process has been employed to prolong the sensor's lifetime. An effective charging algorithm requires simultaneously optimize the charging path and charging time at each charging location under the Mobile Charger's limited energy. The existing methods, however, are generally lacking in the literature. Moreover, most works are based on the assumption that Mobile Charger has sufficient or infinite energy to visit and charge all sensors within each charging cycle. This constraint leads to prolonging the waiting charging time of energy-hurry sensors and unnecessary visiting of energy-sufficient sensors. In this paper, we aim at minimizing energy depletion and the charging cost of Mobile Charger in Wireless Rechargeable Sensor Networks without the mentioned limitations above. We first mathematically formulate the investigated problem as mixed integer and linear programming. We propose a novel partial charging scheme based on fuzzy logic and genetic algorithms to determine which sensors should be charged in each cycle and optimize both charging paths and charging time simultaneously. A range of experimental simulations is conducted to demonstrate the effectiveness of our charging scheme. The simulation results show our proposed algorithm's effectiveness compared to the existing works concerning various performance metrics.",17,https://doi.org/10.1109/CEC45853.2021.9504948,1,
318,A Multifactorial Evolutionary Algorithm For Minimum Energy Cost Data Aggregation Tree In Wireless Sensor Networks.,"In wireless sensor networks, the majority of data transmitted by sensor nodes is repeated over and over, and performing processes on them in many cases leads to increased power consumption and reduced network lifetime. Data aggregation is one of the techniques in reducing redundancy and improving energy efficiency; it also increases the lifespan of wireless sensor networks. In this paper, we address the issues of constructing the data aggregation tree that minimizes the total energy cost of data transmissions for two types of networks: without relay nodes and using relay nodes. Traditionally, evolutionary algorithms focus on constructing data aggregation trees for either without relay node networks or using relay nodes networks. Therefore, we propose Potential individuals based Multi-factorial Evolutionary Algorithm (P-MFEA) to solve both issues simultaneously. The proposed scheme shows improved performance in terms of energy consumption.",17,https://doi.org/10.1109/CEC45853.2021.9504807,1,
319,Multi-Armed Bandits for Many-Task Evolutionary Optimization.,"Inspired by the ability of human multitasking, there is a growing body of literature in the computational intelligence community dedicated to solving multiple problems concurrently. One of the research areas that have been receiving much attention in this topic is evolutionary multitasking, which is able to solve multiple complicated optimization problems together, yielding a better result than solving them in isolation. However, researches on evolutionary multitasking mostly focus on solving a small number of problems together. In an attempt to improve evolutionary multitasking, we propose Many-Task Multi-Armed Bandit Evolutionary Algorithm (Ma 2 BEA), including a new structure for the evolutionary multitasking algorithm. It also adopts a well-proven result of Multi-Armed Bandit (MAB) as the method to control the adaptive knowledge exchange between different tasks. In particular, the action of selecting which task to perform inter-task crossover is learned and decided by the designed MAB agent. We verify that Ma 2 BEA correctly learned the underlying relationship between tasks using the simple 10-task benchmarks. Besides, Ma 2 BEA is compared with other evolutionary many-tasking algorithms that have recently been proposed using the Single-Objective Many-task benchmark from the WCCI 2020 Competition on Evolutionary Multi-task Optimization. Empirical results show that Ma 2 BEA is competitive in terms of high solution quality and reasonable execution time.",17,https://doi.org/10.1109/CEC45853.2021.9504691,1,
320,A Multi-task Approach For Maximum Survival Ratio Problem In Large-Scale Wireless Rechargeable Sensor Networks.,"With the breakthrough of electromagnetic power transfer technology, wireless charging has emerged as a hopeful solution for the energy provisioning problem in wireless sensor networks. One of the prominent issues that affect the potential exploitation of this technology is the charging scheduling problem. However, existing works on this topic either focus mainly on using a single mobile charger for the whole network or suffer from several common limitations such as enforcing the chargers to visit all sensors or applying the rigid full-charging scheme. Moreover, they rarely delve into maximizing the survival nodes ratio, which impacts directly on the multi-hop communication of the network. This paper addresses the charging scheduling for multiple mobile chargers without the above limitations. We first formulate a maximum survival ratio problem and prove its NP-hardness. A charging scheme that exploits the advantages of the multifactorial evolutionary algorithm is then proposed to optimize the charging paths of all chargers simultaneously. We finally evaluate the efficacy of the proposed algorithm through extensive simulations. The experimental results demonstrate that our scheduling scheme provides promising outcomes in terms of survival ratio and the traveling energy of chargers.",17,https://doi.org/10.1109/CEC45853.2021.9504930,1,
321,GCRINT: Network Traffic Imputation Using Graph Convolutional Recurrent Neural Network.,"Missing values appear in most multivariate time series, especially in the monitored network traffic data due to high measurement cost and unavoidable loss. In the networking fields, missing data prevents advanced analysis and downgrades downstream applications such as traffic engineering and anomaly detection. Despite the great potential, existing imputation approaches based on tensor decomposition and deep learning techniques have shown limitations in addressing missing values of traffic data due to its dynamic behavior. In this paper, we propose Graph Convolutional Recurrent Neural Network for Imputing Network Traffic (GCRINT), a combination between Recurrent Neural Network (RNN) and Graph Convolutional Neural Network, for filling the missing values of network traffic data. We use a bidirectional Long Short-Term Memory network and Graph Neural Network to efficiently learn the spatial-temporal correlations in partially observed data. We conducted extensive experiments to evaluate our model by using two different datasets and various missing scenarios. The experiment results show that GCRINT achieves significantly low imputation errors and reduces the error by 35% compared to the state-of-the-art methods. GCRINT also helps to obtain a stable performance in the traffic engineering problem.",17,https://doi.org/10.1109/ICC42927.2021.9500687,1,
322,Fuzzy Deep Q-learning Task Offloading in Delay Constrained Vehicular Fog Computing.,"In the age of the ever-growing number of tasks being generated from IoT devices, one of the most crucial problems with enhancing the Quality of Service in multi-access computing is the system's limited resources. To this end, Vehicular Fog Computing (VFC) has emerged as a potential solution that utilizes the idle resources of vehicles to reduce the load imposed on the edge servers. In this paper, we leverage the advantages of both deep reinforcement learning and Fuzzy logic to propose Fuzzy Deep Q-learning base Offloading scheme (FDQO), a real-time offloading scheme in delay constrained VFC. Our objective is to maximize the Quality of Experiences (QoE), which indicates how the task meets its delay constraint. The experiment results show that our proposed approach significantly outperforms the existing algorithms. Specifically, FDQO improves the average QoE by 37.72% compared to using only Deep Q-learning, 7.47% compared to using only Fuzzy logic, and 19% compared to the ∊ -greedy strategy for multi-armed bandits.",17,https://doi.org/10.1109/IJCNN52387.2021.9533615,1,
323,Multi-time-step Segment Routing based Traffic Engineering Leveraging Traffic Prediction.,"Based on the concept of source routing, Segment Routing (SR) allows the source or ingress node to inject a sequence of segment labels into the packet header and specify the routing path. Due to the routing flexibility, SR has been widely used to solve traffic engineering problems such as minimizing the maximum link utilization. However, most of the prior works only solve the problem in a single snapshot without considering network traffic dynamics, resulting in frequent traffic reroute. To cope with this problem, we focus on solving the segment routing based traffic engineering problem by taking into account the future traffic changes. We formulate the multi-time-step segment routing problem and leverage traffic prediction to extend the length of routing cycles. Due to the large search space of multi-time-step segment routing problem, we further propose a heuristic algorithm for incrementally recomputing the segment routing paths in sub-second. Through extensive experiments on real backbone network traffic datasets, we show that our proposal can achieve a near-optimal performance in term of maximum link utilization while significantly reducing the number of routing changes.",17,https://ieeexplore.ieee.org/document/9463952,1,
325,Efficient meta-heuristic approaches in solving minimal exposure path problem for heterogeneous wireless multimedia sensor networks in internet of things.,"One of the well-known methods for evaluating Heterogeneous wireless multimedia sensor networks (HWMSNs) in Internet of Things have drawn attention of the research community because this type of networks possesses great advantages of both coverage and performance. One of the most fundamental issues in HWMSNs is the barrier coverage problem which evaluates the surveillance capability of the network systems, especially those designed for security purposes. Among multiple approaches to solve this issue, finding the minimal exposure path (MEP), which corresponds to the worst-case coverage of the network is the most popular and efficient way. However, the MEP problem in HWMSNs (hereinafter heterogeneous multimedia MEP or HM-MEP) is specifically complex and challenging with the unique features of the HWMSNs. Thus, the problem is then converted into numerical functional extreme with high dimension, non-differential and non-linearity. Adapting to these features, two efficient meta-heuristic algorithms, Hybrid Evolutionary Algorithm (HEA) and Gravitation Particle Swarm Optimization (GPSO) are proposed for solving the problem. The HEA is a hybrid evolutionary algorithm in combination with local search while the GPSO is a novel particle swarm optimization based on the gravity force theory. Experimental results on extensive instances indicate that the proposed algorithms are suitable for the HM-MEP problem and perform well in term of both solution accuracy and computation time compared to existing approaches.",17,https://doi.org/10.1007/s10489-019-01628-9,1,
326,Exploiting relay nodes for maximizing wireless underground sensor network lifetime.,"A major challenge in wireless underground sensor networks is the signal attenuation originated from multi-environment transmission between underground sensor nodes and the above-ground base station. To overcome this issue, an efficient approach is deploying a set of relay nodes aboveground, thereby reducing transmission loss by shortening transmitting distance. However, this introduces several new challenges, including load balancing and transmission loss minimization. This paper tackles the problem of deploying relay nodes to reduce transmission loss under a load balancing constraint by proposing two approximation algorithms. The first algorithm is inspired by Beam Search, combined with a new selection scheme based on Boltzmann distribution. The second algorithm aims to further improve the solutions obtained by the former by reducing the transmission loss. We observe that we can find an optimal assignment between sensor nodes and a set of the chosen relay in polynomial time by reformulating the part of the problem as a bipartite matching problem with minimum cost. Experimental results indicate that the proposed methods perform better than the other existing ones in most of our test instances while reducing the execution time.",17,https://doi.org/10.1007/s10489-020-01735-y,1,
328,A multifactorial optimization paradigm for linkage tree genetic algorithm.,"Linkage Tree Genetic Algorithm (LTGA) is an effective Evolutionary Algorithm (EA) to solve complex problems using the linkage information between problem variables. LTGA performs well in various kinds of single-task optimization and yields promising results in comparison with the canonical genetic algorithm. However, LTGA is an unsuitable method for dealing with multi-task optimization problems. On the other hand, Multifactorial Optimization (MFO) can simultaneously solve independent optimization problems, which are encoded in a unified representation to take advantage of the process of knowledge transfer. In this paper, we introduce Genetic Algorithm (MF-LTGA) by combining the main features of both LTGA and MFO. MF-LTGA is able to tackle multiple optimization tasks at the same time, each task learns the dependency between problem variables from the shared representation. This knowledge serves to determine the high-quality partial solutions for supporting other tasks in exploring the search space. Moreover, MF-LTGA speeds up convergence because of knowledge transfer of relevant problems. We demonstrate the effectiveness of the proposed algorithm on two benchmark problems: Clustered Shortest-Path Tree Problem and Deceptive Trap Function. In comparison to LTGA and existing methods, MF-LTGA outperforms in quality of the solution or in computation time.",17,https://doi.org/10.1016/j.ins.2020.05.132,1,
329,Towards optimal wireless sensor network lifetime in three dimensional terrains using relay placement metaheuristics.,"In wireless sensor networks, sensor nodes located close to the base station or sink are more susceptible to energy loss, leading to premature disconnection of the network or energy holes. This is due to the fact that all traffic is forwarded towards the sink, increasing the workloads for these closer nodes. One solution for this issue is to shorten the hop distance a sensor’s data has to travel until reaching the sink, by deploying additional relay nodes. This paper considers the problem of optimal relay node placement for maximizing the network lifetime of wireless sensor networks in three-dimensional terrains. We first design a mathematical model of the problem and reformulate it as a mixed-integer programming model to provide a basis for finding lower bound solutions. We divide the problem into two phases and show that the second phase can be solved exactly using an algorithm based on maximum flow and binary search. We then propose a local search algorithm for the first phase, utilizing the exact algorithm to create full solutions. Experimental validation on 3D datasets has been carried out to demonstrate the performance of our algorithms.",17,https://doi.org/10.1016/j.knosys.2020.106407,1,
330,Multifactorial evolutionary algorithm for solving clustered tree problems: competition among Cayley codes.,"The Multifactorial Evolutionary Algorithm (MFEA) has emerged as an effective variant of the evolutionary algorithm. MFEA has been successfully applied to deal with various problems with many different types of solution encodings. Although clustered tree problems play an important role in real life, there haven’t been much research on exploiting the strengths of MFEA to solve these problems. One of the challenges in applying the MFEA is to build specific evolutionary operators of the MFEA algorithm. To exploit the advantages of the Cayley Codes in improving the MFEA’s performance, this paper introduces MFEA with representation scheme based on the Cayley Code to deal with the clustered tree problems. The new evolutionary operators in MFEA have two different levels. The purpose of the first level is to construct a spanning tree which connects to a vertex in each cluster, while the objective of the second one is to determine the spanning tree for each cluster. We focus on evaluating the efficiency of the new MFEA algorithm on known Cayley Codes when solving clustered tree problems. In the aspect of the execution time and the quality of the solutions found, each encoding type of the Cayley Codes is analyzed when performed on both single-task and multi-task to find the solutions of one or two different clustered tree problems respectively. In addition, we also evaluate the effect of those encodings on the convergence speed of the algorithms. Experimental results show the level of effectiveness for each encoding type and prove that the Dandelion Code outperforms the remaining encoding mechanisms when solving clustered tree problems.",17,https://doi.org/10.1007/s12293-020-00309-2,1,
331,An Elite Hybrid Particle Swarm Optimization for Solving Minimal Exposure Path Problem in Mobile Wireless Sensor Networks.,"Mobile wireless sensor networks (MWSNs), a sub-class of wireless sensor networks (WSNs), have recently been a growing concern among the academic community. MWSNs can improve network coverage quality which reflects how well a region of interest is monitored or tracked by sensors. To evaluate the coverage quality of WSNs, we frequently use the minimal exposure path (MEP) in the sensing field as an effective measurement. MEP refers to the worst covered path along which an intruder can go through the sensor network with the lowest possibility of being detected. It is greatly valuable for network designers to recognize the vulnerabilities of WSNs and to make necessary improvements. Most prior studies focused on this problem under a static sensor network, which may suffer from several drawbacks; i.e., failure in sensor position causes coverage holes in the network. This paper investigates the problem of finding the minimal exposure paths in MWSNs (hereinafter MMEP). First, we formulate the MMEP problem. Then the MMEP problem is converted into a numerical functional extreme problem with high dimensionality, non-differentiation and non-linearity. To efficiently cope with these characteristics, we propose HPSO-MMEP algorithm, which is an integration of genetic algorithm into particle swarm optimization. Besides, we also create a variety of custom-made topologies of MWSNs for experimental simulations. The experimental results indicate that HPSO-MMEP is suitable for the converted MMEP problem and performs much better than existing algorithms.",17,https://doi.org/10.3390/s20092586,1,
332,Multifactorial Evolutionary Algorithm for Inter-Domain Path Computation under Domain Uniqueness Constraint.,"Nowadays, connectivity among communication devices in networks has been playing a significant role, especially when the number of devices is increasing dramatically that requires network service providers to have a better architecture of management system. One of the popular approach is to divide those devices inside a network into different domains, in which the problem of minimizing path computation in general or Inter-Domain Path Computation under Domain Uniqueness constraint (IDPC-DU) problem in specific has received much attention from the research community. Since the IDPC-DU is NP-complete, an approximate approach is usually taken to tackle this problem when the dimensionality is high. Although Multifactorial Evolutionary Algorithm (MFEA) has emerged as an effective approximation algorithm to deal with various fields of problems, there are still some difficulties to apply directly MFEA to solve the IDPC-DU problem, i.e. different chromosomes may have different numbers of genes or to construct a feasible solution not violating the problem's constraint. Therefore, to overcome these limitations, MFEA algorithm with a new solution representation based on Priority-based Encoding is introduced. With the new representation of the solution, a chromosome consists of two parts: the first part encodes the priority of the vertex while the second part encodes information of edges in the solution. Besides, the paper also proposed a corresponding decoding method as well as novel crossover and mutation operators. Those evolutionary operators always produce valid solutions. For examining the efficiency of the proposed MFEA, experiments on a wide range of test sets of instances were implemented and the results pointed out the effectiveness of the proposed algorithm. Finally, the characteristics of the proposed algorithm are also indicated and carefully analyzed.",17,https://doi.org/10.1109/CEC48606.2020.9185701,1,
333,Optimizing Charging Locations and Charging Time for Energy Depletion Avoidance in Wireless Rechargeable Sensor Networks.,"In recent years, Wireless Rechargeable Sensor Networks, which exploit wireless energy transfer technologies to address the energy constraint problem in traditional Wireless Sensor Networks, has emerged as a promising solution. There are two important factors that affect the performance of a charging process: charging path and charging time. In the literature, many studies have been done to propose efficient charging algorithms. However, most of the existing works focus only on optimizing the charging path. In this paper, we are the first one to jointly take into account both the charging path and charging time. Specifically, we aim at determining the optimal charging path and the charging time at each charging location to minimize the number of dead nodes. We first mathematically formulate the problem under mixed integer and linear programming. Then, we propose a periodic charging scheme, which is based on the Greedy and Genetic algorithm approaches. The experiment results show that our proposed the algorithm reduces significantly the number of dead nodes compared to a relevant benchmark.",17,https://doi.org/10.1109/CEC48606.2020.9185750,1,
334,A Reinforcement Learning Algorithm for Resource Provisioning in Mobile Edge Computing Network.,"Mobile edge computing (MEC) is a model that allows integration of computing power into telecommunications networks, to improve communication and data processing efficiency. In general, providing power to ensure the computing power of edge servers in the MEC network is very important. In many cases, ensuring continuous power supply to the system is not possible because servers are deployed in hard-to-reach areas such as outlying areas, forests, islands, etc. This is when renewable energy prevails as a viable source of power for ensuring stable operation. This paper addresses resource provisioning in the MEC network using renewable energy. We formulate the problem as a Markov Decision Problem and introduce a new approach to optimize this problem in terms of energy and time costs by using a reinforcement learning technique. Our simulation validates the efficacy of our algorithm, which results in a cost three times better than the other methods.",17,https://doi.org/10.1109/IJCNN48605.2020.9206947,1,
335,Minimal Relay Node Placement for Ensuring Network Connectivity in Mobile Wireless Sensor Networks.,"Connectivity is one of the most challenging issues in Wireless Sensor Network (WSN). Connectivity problems in WSN seek to guarantee a satisfactory communication capability where all mobile sensors can connect to a base station via relay nodes in all data gathering events. In this paper, we focus on minimizing the number of relay nodes while ensuring connectivity in Mobile Wireless Sensor Networks. We propose an improved heuristic algorithm named Clustered Steiner Tree Heuristic (CSTH) to solve this problem in two phases. The first phase is Node Anchoring, which utilizes a greedy approach to find anchor points among clusters of mobile sensors. The second phase is called Steiner Relay Placement, in which a Steiner tree-based heuristic is used to minimize the number of relay nodes while maintaining connectivity in each cluster. Experiments were performed to compare CSTH with previous state-of-the-art heuristics for the problem. Results show that our algorithm can significantly improve the number of required relay nodes as well as computation time.",17,https://doi.org/10.1109/NCA51143.2020.9306727,1,
336,Genetic Algorithm-based Periodic Charging Scheme for Energy Depletion Avoidance in WRSNs.,"Thanks to the advancements in wireless power transfer technologies, a new paradigm of wireless sensor network (WSNs) called wireless rechargeable sensor networks (WRSNs) has recently emerged. For a WRSN, designing an efficient charging schedule is a challenging issue due to the inherent constraints of WSNs. Although there have been many efforts to optimize the charging schedule, the existing works suffer from several critical problems. Firstly, they rarely tackle the dead node minimization problem, which is the ultimate objective of wireless charging. Secondly, most of the existing works assume impractical conditions, which include the unlimited battery capacity of the charger, and a fully charging scheme at the sensors. In this paper, aiming at minimizing the number of dead nodes, we propose a novel charging scheme based on the genetic algorithm. Our scheme works when the mobile charger has only limited capacity, and the sensors are charged partially at each charging round. The experiment results show that our proposed algorithm reduces the number of dead nodes significantly compared to other existing studies.",17,https://doi.org/10.1109/WCNC45663.2020.9120800,1,
337,New mechanism of combination crossover operators in genetic algorithm for solving the traveling salesman problem.,"Traveling salesman problem (TSP) is a well-known in computing field. There are many researches to improve the genetic algorithm for solving TSP. In this paper, we propose two new crossover operators and new mechanism of combination crossover operators in genetic algorithm for solving TSP. We experimented on TSP instances from TSP-Lib and compared the results of proposed algorithm with genetic algorithm (GA), which used MSCX. Experimental results show that, our proposed algorithm is better than the GA using MSCX on the min, mean cost values.",17,https://arxiv.org/abs/2001.11590,1,
338,A Multifactorial Optimization Paradigm for Linkage Tree Genetic Algorithm.,"Linkage Tree Genetic Algorithm (LTGA) is an effective Evolutionary Algorithm (EA) to solve complex problems using the linkage information between problem variables. LTGA performs well in various kinds of single-task optimization and yields promising results in comparison with the canonical genetic algorithm. However, LTGA is an unsuitable method for dealing with multi-task optimization problems. On the other hand, Multifactorial Optimization (MFO) can simultaneously solve independent optimization problems, which are encoded in a unified representation to take advantage of the process of knowledge transfer. In this paper, we introduce Multifactorial Linkage Tree Genetic Algorithm (MF-LTGA) by combining the main features of both LTGA and MFO. MF-LTGA is able to tackle multiple optimization tasks at the same time, each task learns the dependency between problem variables from the shared representation. This knowledge serves to determine the high-quality partial solutions for supporting other tasks in exploring the search space. Moreover, MF-LTGA speeds up convergence because of knowledge transfer of relevant problems. We demonstrate the effectiveness of the proposed algorithm on two benchmark problems: Clustered Shortest-Path Tree Problem and Deceptive Trap Function. In comparison to LTGA and existing methods, MF-LTGA outperforms in quality of the solution or in computation time.",17,https://arxiv.org/abs/2005.03090,1,
339,A Heuristic Based on Randomized Greedy Algorithms for the Clustered Shortest-Path Tree Problem.,"Randomized Greedy Algorithms (RGAs) are interesting approaches to solve problems whose structures are not well understood as well as problems in combinatorial optimization which incorporate the random processes and the greedy algorithms. This paper introduces a new algorithm that combines the major features of RGAs and Shortest Path Tree Algorithm (SPTA) to deal with the Clustered Shortest-Path Tree Problem (CluSPT). In our algorithm, SPTA is used to determine the shortest path tree in each cluster while the combination between characteristics of the RGAs and search strategy of SPTA is used to constructed the edges connecting clusters. To evaluate the performance of the proposed algorithm, Euclidean benchmarks are selected. The experimental investigations show the strengths of the proposed algorithm in comparison with some existing algorithms. We also analyze the influence of the parameters on the performance of the algorithm.",17,https://arxiv.org/abs/2005.04095,1,
340,Evolutionary Algorithm and Multifactorial Evolutionary Algorithm on Clustered Shortest-Path Tree problem.,"In literature, Clustered Shortest-Path Tree Problem (CluSPT) is an NP-hard problem. Previous studies often search for an optimal solution in relatively large space. To enhance the performance of the search process, two approaches are proposed: the first approach seeks for solutions as a set of edges. From the original graph, we generate a new graph whose vertex set's cardinality is much smaller than that of the original one. Consequently, an effective Evolutionary Algorithm (EA) is proposed for solving CluSPT. The second approach looks for vertex-based solutions. The search space of the CluSPT is transformed into 2 nested search spaces (NSS). With every candidate in the high-level optimization, the search engine in the lower level will find a corresponding candidate to combine with it to create the best solution for CluSPT. Accordingly, Nested Local Search EA (N-LSEA) is introduced to search for the optimal solution on the NSS. When solving this model in lower level by N-LSEA, variety of similar tasks are handled. Thus, Multifactorial Evolutionary Algorithm applied in order to enhance the implicit genetic transfer across these optimizations. Proposed algorithms are conducted on a series of datasets and the obtained results demonstrate superior efficiency in comparison to previous scientific works.",17,https://arxiv.org/abs/2010.09309,1,
341,Efficient approximation approaches to minimal exposure path problem in probabilistic coverage model for wireless sensor networks.,"A well-known method for evaluating the coverage quality of Wireless Sensor Networks (WSNs) is using exposure as a measure, especially in barrier coverage problems. Among all studies related to exposure, discussions regarding the Minimal Exposure Path (MEP) problem have dominated research in recent years. The problem aims to find a path on which an intruder can penetrate through the sensing field with the lowest probability of being detected. This path along with its exposure value enables network infrastructure designers to identify the worst-case coverage of the WSN and make necessary improvements. Most prior research worked on the MEP problem under the assumption that there are no environmental factors such as vibration, temperature, etc., which causes errors in practical WSN systems. To overcome this drawback, we first formulate the MEP problem based on Probabilistic Coverage Model with noise (hereinafter PM-based-MEP) and introduce a new definition of the exposure metric for this model. The PM-based-MEP is then converted into a numerically functional extreme with high dimension, non-differentially and non-linearity. Adapting to these characteristics, we propose two approximation methods, GB-MEP and GA-MEP, for solving the converted problem. GB-MEP is based on the traditional grid-based method which is fine-tuned by several tweaks, and GA-MEP is formed by the genetic algorithm with a featured individual representation and an effective combination of genetic operators. Experimental results on numerous instances indicate that the proposed algorithms are suitable for the converted PM-based-MEP problem and perform well regarding both solution accuracy and computation time compared with existing approaches.",17,https://doi.org/10.1016/j.asoc.2018.12.022,1,
342,A hybrid clustering and evolutionary approach for wireless underground sensor network lifetime maximization.,"Wireless Underground Sensor Networks (WUSNs) have attracted significant interest in recent years because of their applications in various fields. The major difference between WUSNs and terrestrial wireless sensor networks is that their signals travel through multiple layers: soil, air and a medium interface. As communications in heterogeneous channels result in more transmission loss, a solution is to deploy relay nodes to relay traffic from sensors to base stations/sinks. However, this poses several new challenges, including load balancing and transmission loss minimization in heterogeneous environments. This paper considers the problem of deploying relay nodes to prolong network lifetime under load balancing constraints. This problem can be formalized using a Mixed Integer Linear Programming model as a basis to achieve lower bound solutions. We show that the problem is NP-hard as it can be reduced to the Set Cover Problem. Two novel methods are proposed. The first is a hybridisation of a clustering heuristic and an exact algorithm using a maximum flow with min-max cost formulation. The second is an evolutionary approach to further improve our initial results. Experimental validation on a large set of benchmarks indicates that the proposed methods perform better than the existing methods.",17,https://doi.org/10.1016/j.ins.2019.07.060,1,
343,Node placement for connected target coverage in wireless sensor networks with dynamic sinks.,"Target coverage and connectivity are two fundamental and critical issues in wireless sensor networks. The former is for providing sufficient monitoring quality where all points of interest in the network are covered by sensor nodes. The latter is for guaranteeing satisfactory communicating capability where all sensor nodes can connect to at least one sink via nodes (i.e., sensor nodes and relay nodes). Though considerable efforts have been devoted to optimize the placement of sensor nodes and relay nodes under connected target coverage constraint (i.e., guaranteeing both target coverage and connectivity), all of the existing works in this area address only networks with one static sink. In the meanwhile, although there are many works in literature considered the networks with dynamic sinks, none of them studies how to optimize the location of sensor and relay nodes. In this article, we focus on wireless sensor networks with dynamic sinks, which consist of multiple sinks and the sinks’ positions may change periodically, and study how to place a minimum number of nodes for connected target coverage. Specifically, we decompose the problem into two sub-problems. The first one, named as target coverage problem, is to place sensor nodes for covering all targets. The second one, named as network connectivity problem, is for placing relay nodes to connect sensor nodes to the sinks. We first formulate the target coverage problem under an integer linear programming model and present an exact algorithm to determine the optimal solution. We then propose a constant-approximation algorithm based on partitioning and shifting scheme. For the network connectivity problem, we first prove its NP-hardness and then propose two approximation algorithms. The first one exploiting the minimum group Steiner tree to minimize the placed relay nodes, while the second one is a time-efficient algorithm based on clustering and spanning tree approaches. The experiment results show the superiority of our proposed algorithms in terms of both the number of the required nodes and the time complexity.",17,https://doi.org/10.1016/j.pmcj.2019.101070,1,
344,A multi-objective multi-factorial evolutionary algorithm with reference-point-based approach.,"In recent years, multi-task optimization is one of the emerging topics among evolutionary computation researchers. Multi-Factorial Evolutionary Algorithm (MFEA) is developed based on that individuals, from various cultures, exchange their underlying similarities to improve the convergence characteristic. However, in terms of Multi-Objective Multi-Factorial Optimization (MOMFO), current algorithms employing nondominated front ranking and crowding distance still meet difficulties when the number of objective functions arises. In this paper, we propose a Muli-Objective Multi-Factorial Evolutionary Algorithm (MO-MFEA) with reference-point-based approach to improve the multitasking framework. Rather than using crowding distance to compute individual ranking in the context of MOMFO, we employ a set of reference points to determine the diversity of current population. On the other hand, we improve the guided method that automatically adapt the Random Mating Probability (RMP) in order to exploit shared knowledge among high similar task. Further improvement on genetic operators with JADE crossover and NSLS. The conducted experiments demonstrate our approach performs better than the baseline results.",17,https://doi.org/10.1109/CEC.2019.8790034,1,
345,Prolong the Network Lifetime of Wireless Underground Sensor Networks by Optimal Relay Node Placement.,"Wireless Underground Sensor Networks (WUSNs) have received attention in the past years because of their popularity and cost-effectiveness when they are used in many practical fields such as military applications, environmental applications, and home applications. In WUSNs, sensors are deployed with limited power, once their power is out of, the sensors are ineffectual. The extension of the network’s lifetime is a critical issue in WUSNs, making it a topic of much interest in research. Several approaches have been proposed to keep the sensor nodes active, one of which is deploying relay nodes above ground to transfer data from sensor nodes to the base station. However, this method has faced issues, such as balancing the load of relay nodes and the increased transmission loss between relay nodes and sensor nodes. This paper addresses this concern and proposes two heuristics named Beam Genitor Search and Connection Swap for the relay node placement problem to guarantee load balance among relay nodes and maximize network lifetime. Our experiments show that the proposed methods result in significantly better quality solutions (longer network lifetime) for the problem when compared to the existing methods",17,https://doi.org/10.1007/978-3-030-16692-2_30,1,
346,Joint Transaction Transmission and Channel Selection in Cognitive Radio Based Blockchain Networks: A Deep Reinforcement Learning Approach.,"To ensure that the data aggregation, data storage, and data processing are all performed in a decentralized but trusted manner, we propose to use the blockchain with the mining pool to support IoT services based on cognitive radio networks. As such, the secondary user can send its sensing data, i.e., transactions, to the mining pools. After being verified by miners, the transactions are added to the blocks. However, under the dynamics of the primary channel and the uncertainty of the mempool state of the mining pool, it is challenging for the secondary user to determine an optimal transaction transmission policy. In this paper, we propose to use the deep reinforcement learning algorithm to derive an optimal transaction transmission policy for the secondary user. Specifically, we adopt a Double Deep-Q Network (DDQN) that allows the secondary user to learn the optimal policy. The simulation results clearly show that the proposed deep reinforcement learning algorithm outperforms the conventional Q-learning scheme in terms of reward and learning speed.",17,https://doi.org/10.1109/ICASSP.2019.8683228,1,
347,GAN-DP: Generative Adversarial Net Driven Differentially Privacy-Preserving Big Data Publishing.,"Increasing massive volume of data are generated every single second in this big data era. With big data from multiple sources, adversaries continuously mine private information for potential benefits. Motivated by this, we propose a generative adversarial net (GAN) driven noise generation method under the framework of differential privacy. We add one more perceptron, which is a specifically devised differential privacy identifier. After the generator produces the noise, the discriminator and the proposed identifier game with each other to derive the Nash Equilibrium. Extensive experimental results demonstrate the proposed model meets differential privacy constraints and upgrade data utility simultaneously.",17,https://doi.org/10.1109/ICC.2019.8761070,1,
348,Cow estrus detection with low-frequency accelerometer sensor by unsupervised learning.,"In recent years, Internet of Things (IoT) and Machine Learning (ML) has been applied successfully in agriculture. These technologies increase productivity as well as reduce labor significantly. In this paper, we focus on improving the autonomous cow estrus detection system in terms of energy consumption and precision. In previous detection pipelines, an accelerometer is mounted to the neck of cows to capture motion data with high frequency, followed by the ML algorithm to check the data and determine whether it is in estrus or not. Instead, we configured the accelerometer to sample with low frequency for minimizing its energy consumption. However, low-sampling rate as input of ML pipeline leads to an undesirable higher false alarm rate. To solve this problems, we designed a pipeline of unsupervised learning with a new heuristic post-processing algorithm. The proposed post-processing algorithm is a backtracking algorithm that incorporates the timing constraint of the period obtained by agriculture knowledge. With the constraint, the post-processing algorithm facilitates a significantly higher precision than simple adaptive threshold techniques in previous studies on a simulated dataset. Finally, the overall result of the pipeline with the proposed algorithm is visualized on real-world data captured on the farm in our agriculture department.",17,https://doi.org/10.1145/3368926.3369683,1,
349,New Approach for Solving The Clustered Shortest-Path Tree Problem Based on Reducing The Search Space of Evolutionary Algorithm.,"Along with the development of manufacture and services, the problem of distribution network optimization has been growing in importance, thus receiving much attention from the research community. One of the most recently introduced network optimization problems is the Clustered Shortest-Path Tree Problem (CluSTP). Since the problem is NP-Hard, recent approaches often prefer to use approximation algorithms to solve it, several of which used Evolutionary Algorithms (EAs) and have been proven to be effective. However, most of the prior studies directly applied EAs to the whole CluSTP problem, which leads to a great amount of resource consumption, especially when the problem size is large. To overcome these limitations, this paper suggests a method for reducing the search space of the EAs applied to CluSTP by decomposing the original problem into two sub-problems, the solution to one of which is found by an EAs and that to the other is found by another method. The goal of the first sub-problem is to determine a spanning tree which connects among the clusters, while the goal of the second sub-problem is to determine the best spanning tree for each cluster. In addition, this paper proposes a new EAs, which can be applied to solve the first sub-problem and suggests using the Dijkstra's algorithm to solve the second sub-problem. The proposed approach is comprehensively experimented and compared with existing methods. Experimental results prove that our method is more efficient and more importantly, it can obtain results which are close to the optimal results.",17,http://arxiv.org/abs/1908.07060,1,
351,Improved Cuckoo Search and Chaotic Flower Pollination optimization algorithm for maximizing area coverage in Wireless Sensor Networks.,"The popularity of Wireless Sensor Networks (WSNs) is rapidly growing due to its wide-ranged applications such as industrial diagnostics, environment monitoring or surveillance. High-quality construction of WSNs is increasingly demanding due to the ubiquity of WSNs. The current work is focused on improving one of the most crucial criteria that appear to exert an enormous impact on the WSNs performance, namely the area coverage. The proposed model is involved with sensor nodes deployment which maximizes the area coverage. This problem is proved to be NP-hard. Although such algorithms to handle this problem with fairly acceptable solutions had been introduced, most of them still heavily suffer from several issues including the large computation time and solution instability. Hence, the existing work proposed ways to overcome such difficulties by proposing two nature-based algorithms, namely Improved Cuckoo Search (ICS) and Chaotic Flower Pollination algorithm (CFPA). By adopting the concept of calculating the adaptability and a well-designed local search in previous studies, those two algorithms are able to improve their performance. The experimental results on 15 instances established a huge enhancement in terms of computation time, solution quality and stability.",17,https://doi.org/10.1007/s00521-016-2823-5,1,
352,Node placement for target coverage and network connectivity in WSNs with multiple sinks.,"Target coverage and connectivity are two main challenging and important issues in wireless sensor networks. The former is for providing sufficient monitoring quality where all points of interest in the network are covered by sensor nodes and the latter is for guaranteeing satisfactory communicating capability where all sensors can connect to at least one sink via relay nodes. In this paper, we focus on minimizing the number of nodes (i.e., sensor nodes and relay nodes) to provide target coverage and connectivity in wireless sensor networks with multiple sinks. We formulate the problem as two sub-problems. The first one (named as TC) is for placing sensor nodes to cover all targets and the second one (named as NC) is for placing relay nodes to connect sensor nodes to the sinks. We then propose a heuristic algorithm for the TC problem that exploits clustering technique. We also propose two heuristic algorithms for the NC problem that base on greedy approach and spanning tree. The experiment results show that our protocols can significantly reduce the number of required nodes in comparison with existing protocols.",17,https://doi.org/10.1109/CCNC.2018.8319207,1,
353,Effective Multifactorial Evolutionary Algorithm for Solving the Cluster Shortest Path Tree Problem.,"Arising from the need of all time for optimization of irrigation systems, distribution network and cable network, the Cluster Shortest Path Tree Problem (CSTP) has been attracting a lot of attention and interest from the research community. For such an NP-Hard problem with a great dimensionality, the approximation approach is usually taken. Evolutionary Algorithms, based on biological evolution, has been proved to be effective in finding approximate solutions to problems of various fields. The multifactorial evolutionary algorithm (MFEA) is one of the most recently exploited realms of EAs and its performance in solving optimization problems has been very promising. The main difference between the MFEA and the traditional Genetic Algorithm (GA) is that the former can solve multiple tasks at the same time and take advantage of implicit genetic transfer in a multitasking problem, while the latter solves one problem and exploit one search space at a time. Considering these characteristics, this paper proposes a MFEA for CSTP tasks, together with novel genetic operators: population initialization, crossover, and mutation operators. Furthermore, a novel decoding scheme for deriving factorial solutions from the unified representation in the MFEA, which is the key factor to the performance of any variant of the MFEA, is also introduced in this paper. For examining the efficiency of the proposed techniques, experiments on a wide range of diverse sets of instances were implemented and the results showed that the proposed algorithms outperformed an existing heuristic algorithm for most of the testing cases. In the experimental results section, we also pointed out which cases allowed for a good performance of the proposed algorithm.",17,https://doi.org/10.1109/CEC.2018.8477912,1,
354,A Guided Differential Evolutionary Multi-Tasking with Powell Search Method for Solving Multi-Objective Continuous Optimization.,"Recent years, the field of Multi-Objective Optimization (MOO) has attracted remarkable consideration among evolutionary computation researchers. Evolutionary multitasking paradigm within the domain of MOO has been proposed and demonstrated on some benchmark test functions that indicates potential applications in real world problems. The concept of evolutionary multi-tasking is founded on the fact that individuals from various cultures may share their underlying similarities, thereby facilitating improved convergence characteristics. However, the designate algorithm for MOO multi-tasking is originated from pure genetic search that means it does not imply any advanced local refinement method which also improves the rate of convergence. Memetic algorithms, which is known as a synergy of evolutionary with separate individual learning or local improvement procedures for problem search, offers converging to high-quality solutions more efficiently than their conventional evolutionary counterparts. Accordingly, in this paper, to excel MOO multi-tasking paradigm performance, we propose an algorithm which is based on the idea of Multi-Factorial Evolutionary Algorithm (MFEA) employing Guided differential evolutionary and Powell local search. The accomplished experimental results point out using memetic techniques does an impressive enhancement on Multi-objective continuous optimization.",17,https://doi.org/10.1109/CEC.2018.8477860,1,
355,Cow estrus detection via Discrete Wavelet Transformation and Unsupervised Clustering.,"Estrus is a special periods in the life cycle of female cows. Within this period, they have much more chance to become pregnant. Successfully detecting this period increase the milk and meat productivity of the whole farm. Recently, a potential approach is unsupervised learning on motion data of the cows, similar to human activity recognition based on motion. In particular, an accelerometer is attached to the neck of the cows to measure their acceleration, then the unsupervised algorithm group the measured acceleration time-series. Recent study adopted bag-of-feature and Discrete Fourier Transform for feature extraction, yet it may not reflect the nature of motion data. Thus, we proposed a method based on Discrete Wavelet Transform to get the multi-resolution feature, Dynamic Time Wraping as clustering distance and Iterative-K-Means as clustering algorithm, to better match with the characteristic of cowsâĂŹ movement. The proposed methods demonstrated higher score on human activity recognition dataset with ground truth and more reliable prediction on cow motion dataset.",17,https://doi.org/10.1145/3287921.3287973,1,
356,Formal Verification of ALICA Multi-agent Plans Using Model Checking.,"In multi-agent systems (MAS), plans consisting of sequences of actions are used to accomplish the team task. A critical issue for this approach is avoiding problems such as deadlocks and safety violations. Our recent work addresses that matter by verifying plans composed in a language called ALICA (A Language for Interactive Cooperative Agents) that controls the agents' behavior. The investigation is conducted by creating a translation tool that implements an algorithm for translating ALICA plans into the format used by the real-time model checker UPPAAL. We tested our concept using several cases, and the result is promising to get further insight on multi-agent model checking.",17,https://doi.org/10.1145/3287921.3287947,1,
357,DECOM: A framework to support evolution of IoT services.,"In the heterogeneous and dynamic Internet of Things (IoT), applications and services are frequently subject to change for various reasons such as maintaining their functionality, reliability, availability, and performance. Detecting and communicating these changes are still performed manually by responsible developers and administrators. Such a mechanism will not be adequate anymore in the future of large-scale IoT environments. Therefore, we present a comprehensive framework named DECOM for automatic detection and communication of service changes. Here, we assume that capabilities and interfaces of IoT devices are described and provided through REST services. To be able to detect syntactic as well as semantic changes, we transform an extended version of the interface description into a logic program and apply a sequence of analysis steps to detect changes. The feasibility and applicability of the framework are demonstrated in an IoT application scenario.",17,https://doi.org/10.1145/3287921.3287979,1,
358,An Evolutionary Algorithm for Solving Task Scheduling Problem in Cloud-Fog Computing Environment.,"Recently, IoT (Internet of Things) has grown steadily, which generates a tremendous amount of data and puts pressure on the cloud computing infrastructures. Fog computing architecture is proposed to be the next generation of the cloud computing to meet the requirements of the IoT network. One of the big challenges of fog computing is resource management and operating function, as task scheduling, which guarantees a high-performance and cost-effective service. We propose TCaS - an evolutionary algorithm to deal with Bag-of-Tasks application in cloud-fog computing environment. By addressing the tasks in this distributed system, our proposed approach aimed at achieving the optimal tradeoff between the execution time and operating costs. We verify our proposal by extensive simulation with various size of data set, and the experimental results demonstrate that our scheduling algorithm outperforms 38.6% Bee Life Algorithm (BLA) in time-cost tradeoff, especially, performs much better than BLA in execution time, simultaneously, satisfies user's requirement.",17,https://doi.org/10.1145/3287921.3287984,1,
359,Online load balancing for Network Functions Virtualization.,"Network Functions Virtualization (NFV) aims to support service providers to deploy various services in a more agile and cost-effective way. However, the softwarization and cloudification of network functions can result in severe congestion and low network performance. In this paper, we propose a solution to address this issue. We analyze and solve the online load balancing problem using multipath routing in NFV to optimize network performance in response to the dynamic changes of user demands. In particular, we first formulate the optimization problem of load balancing as a mixed integer linear program for achieving the optimal solution. We then develop the ORBIT algorithm that solves the online load balancing problem. The performance guarantee of ORBIT is analytically proved in comparison with the optimal offline solution. The experiment results on real-world datasets show that ORBIT performs very well for distributing traffic of each service demand across multipaths without knowledge of future demands, especially under high-load conditions.",17,https://doi.org/10.1109/ICC.2017.7996511,1,
360,Genetic algorithm for solving minimal exposure path in mobile sensor networks.,"The Minimal Exposure Path (MEP) is a fundamental factor to measure quality of service in Wireless Sensor Networks (WSNs). The MEP not only improves the performance of WSNs but also guides the action of agents in the sensing field. Up to now, MEP has been focusing on solving problems in stationary sensor networks. MEP with both high dimensionality and non-linear property in stationary WSNs is NP-hard problem. This paper formulated a problem of finding the MEP in Mobile Sensor Networks (MSNs), in which sensors' mobility plays an important role in the execution of application, and then proposed a genetic algorithm to solve it. Our methodology is not only to calculate upper and lower bounds of exposure value, but also to determine the MEP trajectory. The proposed algorithm was experimented on 30 random instances with various numbers of mobile sensors and different velocities of intruder which are proportional to the velocity of sensors. We also compared the minimal exposure value obtained from MSNs model with the one obtained from static sensor network model. Experimental results showed that our genetic algorithm is stable, convergent and effective when applying in both mobile and static models of WSNs.",17,https://doi.org/10.1109/SSCI.2017.8285402,1,
362,A survivable design of last mile communication networks using multi-objective genetic algorithms.,"In this paper, we are interested in the survivable network design problem (SNDP) for last mile communication networks called (L-SNDP). Given a connected, weighted, undirected graph G=(V,E); a set of infrastructure nodes and a set of customers C including two customer types where customers in the subset C1 require a single connection (type-1) and customers in the subset C2 need to be redundantly connected (type-2). The aim is to seek a sub-graph of G with the smallest weight in which all customers are connected to infrastructure nodes and the connections are protected against failures. This is a NP-hard problem and it has been solved only with the objective of minimizing the network cost. In this paper, we introduce a new multi-objective approach to solve L-SNDP called ML-SNDP. These objectives are to minimize the network cost (total cost) and to minimize the maximal amount of sharing links between connections. Results of computational experiments reported show the efficiency of our proposal.",17,https://doi.org/10.1007/s12293-015-0177-7,1,
363,Base Station Location -Aware Optimization Model of the Lifetime of Wireless Sensor Networks.,"Recently, wireless sensor networks (WSNs) have been progressively applied in various fields and areas. However, its limited energy resources is indisputably one of the weakest point that strongly affects the network’s lifetime. A WSN consists of a sensor node set and a base station. The initial energy of each sensor node will be depleted continuously during data transmission to the base station either directly or through intermediate nodes, depending on the distance between sending and receiving nodes. This paper consider determining an optimal base station location such that the energy consumption is kept lowest, maximizing the network’s lifetime and propose a nonlinear programming model for this optimizing problem. Our proposed method for solving this problem is to combine methods mentioned in [1] respectively named the centroid, the smallest total distances, the smallest total squared distances and two greedy methods. Then an improved greedy method using a LP tool provided in Gusek library is presented. Finally, all of the above methods are compared with the optimized solution over 30 randomly created data sets. The experimental results show that a relevant location for the base station is essential.",17,https://doi.org/10.1007/s11036-015-0614-3,1,
364,"Proceedings in Adaptation, Learning and Optimization.","Wireless Sensor Networks (WSNs) have been recently used for various applications. Due to the distributed and (often) unattended nature of the nodes after deployment, the lack of energy and the interruptive process in each sensor are the two major problems of WSN systems. Hence, designing a protocol which not only improves system performance but also lowers sensors’ energy consumption so as to maximize the network lifetime is very much desirable. The network lifetime maximization problem was known to be NP-Hard. This paper addresses the Minimum Energy-Multicasting (MEM) problem in Duty-Cycle Wireless Sensor Networks (DC-WSNs) in which sensors cyclically switch between on/off (wake/sleep) modes. To that end, we propose a local search algorithm and compare its performance with the best algorithm so far called GS-MEM over the four datasets designated for the MEM problem. The experimental results show that our proposed algorithm significantly outperforms GS-MEM in terms of energy cost.",17,https://doi.org/10.1007/978-3-319-49049-6_4,1,
365,Particle Swarm Optimization Algorithms for Maximizing Area Coverage in Wireless Sensor Networks.,"Nowadays, Wireless Sensor Network (WSN) systems appear to be the solution to many industrial applications in a wide range of fields due to their flexibility and scalability. However, limited coverage remains as one major defect of such systems and multiple problems had been proposed investigating practical parameters such as sensing range and obstacles. This paper tackles the area coverage optimization problem with respect to a given number of sensors having different sensing ranges. In order to solve this NP-hard problem, two algorithms named Particle Swarm Optimization (PSO) and Democratic Particle Swarm Optimization (DPSO) algorithms are proposed in this paper. Two proposed algorithms are experimented on 15 instances constructed for this problem and the results are compared to other methods. The experimental results showcase a considerable improvement compared to the existing genetic algorithm in term of execution time and quality of solution.",17,https://doi.org/10.1007/978-3-319-56991-8_65,1,
366,Adaptive multipath routing for network functions virtualization.,"Network Functions Virtualization (NFV) is a recent trend of network transformation that helps service providers offer new and multiple services in a more agile and cost effective way. However, the softwarization and cloudization of network functions can result in high congestion and low network performance for virtual networks. We develop a multipath routing solution for minimizing the maximum link utilization, thus leading to the performance improvement of NFV-based systems, as well as the efficient utilization of network resources. Our proposed algorithm can adapt the link weight system to the dynamic change of service demands for optimizing the distribution of network traffic, with regard to the fundamental NFV characteristics and the Equal-cost Multipath (ECMP) routing feature. The evaluation results show that the proposed solution outperforms a multipath solution with fixed link weight on various performance metrics. Interestingly, we find that the network performance is not significantly improved by increasing the number of paths beyond a threshold.",17,https://doi.org/10.1145/3011077.3011123,1,
367,Label associated dictionary pair learning for face recognition.,"Dictionary learning (DL) has been successfully applied to various pattern classification tasks. Sparse coding has played a vital role in the success of such DL-based models. However, the popular sparsity constraints using l0 or l1-norm often make the training phase time-consuming. Recently, an emerging trend in using l2-norm has shown its advantages in both accuracy and computational speed. However, the supervised approach that exploits label information in the training phase has not been investigated in such l2-norm based methods. In this paper, we propose a novel supervised dictionary learning method that incorporates label information in the objective function. Based on that, we also propose an effective classification schema. Experiments on three popular face recognition datasets show that our method has promising results. Especially, our method has extremely fast speed in test phase, while maintaining competitive accuracy in comparison with other state-of-the-art models",17,https://doi.org/10.1145/3011077.3011105,1,
368,Heuristic algorithm for target coverage with connectivity fault-tolerance problem in wireless sensor networks.,"Wireless sensor networks include a set of sensors being network nodes and a base station. The objective of such systems is to monitor changes which might occur at targets' location. Consequently, assuring the network connectivity is compulsory to maintain a continuous monitoring of given set of targets. However, sensor nodes sometimes stop functioning due to being physically damaged or running out of energy (battery), thus affecting to the connectivity of the system. This paper considers the target coverage with connectivity fault-tolerance problem in wireless sensor networks. The goal of this problem is to come up with a network topology consisting of the least number of sensor nodes that not only offers the greatest target coverage but also maintains the network connectivity even when one random node is defected. This is an NP-Complete combinatorial optimization problem. One heuristic algorithm is proposed to solve the target coverage with connectivity fault-tolerance problem and it is tested on 15 randomly generated instances. Experimental results illustrate a good performance achieved in terms of target coverage, connectivity and fault-tolerance.",17,https://doi.org/10.1109/TAAI.2016.7880178,1,
369,Covering the Target Objects with Mobile Sensors by Using Genetic Algorithm in Wireless Sensor Networks.,"Wireless sensor networks have several applications, such as target detection and tracking, and monitoring battlefields. Coverage is one of the most important performance metrics for wireless sensor networks since it reflects how appropriate an event can be detected and monitored in the sensing field. To achieve optimal coverage, an efficient algorithm should be employed to find the best positions of sensor node deployment. In this paper, an efficient genetic algorithm is proposed to solve the coverage problem of the target objects. The optimal number of sensor nodes starts from fewer randomly deployed nodes and increases gradually in subsequence generations. The performance of the proposed genetic algorithm was evaluated, and the simulation results show that this approach can cover all the target objects as well as minimize the number of additional mobile sensor nodes.",17,https://doi.org/10.17706/jcp.10.5.300-308,1,
370,Heuristic and genetic algorithms for solving survivability problem in the design of last mile communication networks.,"Given a connected, undirected and weighted graph G=(V,E), a set of infrastructure nodes J and a set of customers C include two customer types whereby customers C1 require a single connection (type-1) and customers C2 need to be redundantly connected (type-2). Survivable network design problem (SNDP) seeks a sub-graph of G with the smallest weight in which all customers are connected to infrastructure nodes. SNDP has applications in the design of the last mile of the real-world communication networks. SNDP is NP-hard so heuristic approaches are normally adopted to solve this problem, especially for large-scale networks. This paper proposes a new heuristic algorithm and a new genetic algorithm for solving SNDP. The proposed algorithms are experimented on real-world instances and random instances. Results of computational experiments show that the proposed heuristic algorithm is much more efficient than the other heuristics in running time, and the proposed genetic algorithm is much more efficient than the other heuristics in terms of minimizing the network cost.",17,https://doi.org/10.1007/s00500-014-1429-z,1,
371,Uniform Detection in Social Image Streams.,"Social media mining from Internet has been an emerging research topic. The problem is challenging because of massive data contents from various sources, especially image data from user upload. In recent years, dictionary learning based image classification has been widely studied and gained significant success. In this paper, we propose a framework for automatic detection of interested uniforms in image streams from social networks. The systems is composed of a powerful feature extraction module based on dense SIFT feature and a state-of-the-art discriminative dictionary learning approach. Beside that, a parallel implementation of feature extraction is deployed to make the system work real time. An extensive set of experiments has been conducted on four real-life datasets. The experimental results show that we can obtain the detection rate up to 100% on some datasets. We also get real time performance with a speed of image stream of about 40 images per second. The framework can be applied to emerging applications such as uniform detection, automated image tagging, content base image retrieval or online advertisement based on image content.",17,https://doi.org/10.1109/KSE.2015.63,1,
372,A Study on Non-sparse Dictionary Learning for Pattern Classification.,"Dictionary learning (DL) approach has been successfully applied to many pattern classification problems. Sparse property has played an important role in the success of DL-based classification models. However, the sparsity constraints make the learning problem expensive. Recently, there has been an emerged trend in relaxing the sparsity constraints by using L2-norm constraint. The new approach has shown its advantages in both accuracy and classification time. However, the relationship between the quality of the data and the dictionary learning issues that affect the performance of the system has not been investigated. In this paper, we present a comparative study on non-sparse coding dictionary learning for pattern classification. We then propose a dictionary learning model with a non-sparsity constraint on representation coefficients using L2-norm. Our experimental results on three popular benchmark datasets for image classification show that our proposed model can outperform state-of-the-art models and be a promising approach for dictionary learning based classification.",17,https://doi.org/10.1109/KSE.2015.66,1,
373,An Improved Genetic Algorithm for Maximizing Area Coverage in Wireless Sensor Networks.,"In recent years, Wireless Sensor Networks (WSNs) have proved their power, but they also faced many practical challenges. One of such challenges is covering issue. This paper considers the maximum coverage deployment problem in WSNs. Particularly, with the given number of sensors having various sensing ranges, how to deploy these sensors in a specified domain so that their coverage on which is maximum. This is a NP-hard problem. We propose a new genetic algorithm with some improvements compared to an existing genetic one. These improvements include the definition of a new concept -- the overlapping -- for the fitness function, using a heuristic technique to initialize population and a dynamic mutation. Our algorithm is experimented on 15 instances constructed for this problem. The experimental results show that our proposed algorithm is effective in all terms of the computational complexity, quality of solutions and stability.",17,https://doi.org/10.1145/2833258.2833299,1,
374,An Efficient Framework for Pixel-wise Building Segmentation from Aerial Images.,Detection of buildings in aerial images is an important and challenging task in computer vision and aerial image interpretation. This paper presents an efficient approach that combines Random forest (RF) and a fully connected conditional random field (CRF) on various features for the detection and segmentation of buildings at pixel level. RF allows one to learn extremely fast on big aerial image data. The unary potentials given by RF are then combined in a fully connected conditional random field model for pixel-wise classification. The use of high dimensional Gaussian filter for pairwise potentials makes the inference tractable while obtaining high classification accuracy. Experiments have been conducted on a challenging aerial image dataset from a recent ISPRS Semantic Labeling Contest [9]. We obtained state-of-the-art accuracy with a reasonable computation time.,17,https://doi.org/10.1145/2833258.2833272,1,
375,An effective combination of Genetic Algorithms and the Variable Neighborhood Search for solving Travelling Salesman Problem.,"We investigate a mechanism of combining Genetic Algorithms (GA) and the Variable Neighborhood Search (VNS) for solving Traveling Salesman Problem (TSP). This combination aims to take advantage of GA's strength in exploring solution space and VNS's powerful exploitation in that space. In order to improve the solution quality, we propose a VNS with four neighborhood structures. We experimented on TSP instances from TSP-Lib and compared the results of the proposed algorithm with a hybrid Genetic Algorithm and Variable Neighborhood Search. Experimental results show that the proposed algorithm is better than once in terms of both the solution quality and running times in majority of data instances.",17,https://doi.org/10.1109/TAAI.2015.7407102,1,
376,The new crossover operators and a novel combination of crossover operators for solving Linear Ordering Problem.,"The Linear Ordering Problem (LOP) is a well-known NP-Hard problem and has many applications in various fields. In this paper, we propose two new crossover operators and a novel combination crossover operators in Genetic Algorithm (GA) for solving LOP. We experimented on LOP instances from XLOLIB and compared the results of the proposed algorithm with GA using other crossover operators. Experimental results show that, our proposed algorithm is better than the ones on the maximum, mean cost values.",17,https://doi.org/10.1109/TAAI.2015.7407081,1,
377,A multi-objective approach for solving the survivable network design problem with simultaneous unicast and anycast flows.,"In this paper, we consider the survivable network design problem for simultaneous unicast and anycast flow requests. We assume that the network is modeled by a connected and undirected graph. This problem aims at finding a set of connections with a minimized network cost in order to protect the network against any single failure. The cost is computed using the all capacities modular cost (ACMC) model and a set of flow demands. We name it as ACMC-based survivable network design problem (A-SNDP). It is proved that the problem is NP-hard. We introduce a multi-objective approach to solve A-SNDP. The objectives are to minimize the network cost (NCost) and the network failure (NFail). Extensive simulation results on instances of Polska, Germany and Atlanta networks showed the efficiency of the multi-objective approach for solving A-SNDP.",17,https://doi.org/10.1016/j.asoc.2014.06.001,1,
378,All capacities modular cost survivable network design problem using genetic algorithm with completely connection encoding.,"We study the survivable network design problem (SNDP) for simultaneous unicast and anycast flows in networks where the link cost follows All Capacities Modular Cost (ACMC) model. Given a network modeled by a connected, undirected graph and a set of flow demands, this problem aims at finding a set of connections with a minimized network cost in order to protect the network against any single failure. This paper proposes a new Genetic Algorithm with an efficient encoding to solve the SNDP in networks with ACMC model (A-SNDP). Our encoding scheme is simple and allows large search space. Extensive simulation results on real large topology instances show that the proposed algorithm is much more efficient than the Tabu Search and other conventional Genetic Algorithm in terms of minimizing the network cost.",17,https://doi.org/10.1186/s13673-014-0013-y,1,
379,Reordering dimensions for Radial Visualization of multidimensional data - A Genetic Algorithms approach.,"In this paper, we propose a Genetic Algorithm (GA) for solving the problem of dimensional ordering in Radial Visualization (Radviz) systems. The Radviz is a non-linear projection of high-dimensional data set onto two dimensional space. The order of dimension anchors in the Radviz system is crucial for the visualization quality. We conducted experiments on five common data sets and compare the quality of solutions found by GA and those found by the other well-known methods. The experimental results show that the solutions found by GA for these tested data sets are very competitive having better cost values than almost all solutions found by other methods. This suggests that GA could be a good approach to solve the problem of dimensional ordering in Radviz.
",17,https://doi.org/10.1109/CEC.2014.6900619,1,
380,New Mechanism of Combination Crossover Operators in Genetic Algorithm for Solving the Traveling Salesman Problem.,"The Linear Ordering Problem (LOP) is a well-known NP-Hard problem and has many applications in various fields. In this paper, we propose two new crossover operators and a novel combination crossover operators in Genetic Algorithm (GA) for solving LOP. We experimented on LOP instances from XLOLIB and compared the results of the proposed algorithm with GA using other crossover operators. Experimental results show that, our proposed algorithm is better than the ones on the maximum, mean cost values.",17,https://doi.org/10.1007/978-3-319-11680-8_29,1,
381,Genetic algorithm for solving Survivable Network Design Problem with Extending-Cycle-Based Protected Working Capacity Envelope.,"The study of protection and prevention against failures is an important problem in communication networks due to the growth of capacity demands for data transmission, especially in the Internet traffic. In this paper, we consider the Survivable Network Design Problem (SNDP) with Protected Working Capacity Envelope (PWCE) called capacity SNDP (ca-SNDP) in which a network topology is modeled by a weighted and undirected graph. The weight of each edge is maximum capacity bandwidth of this edge. A working network (Wn) and a backup network (Bn) will be constructed from this topology. Design survivable network with maximum total bandwith in Wn is objective of ca-SNDP. This paper proposes Extending Cycle (EC) model and genetic algorithm for solving ca-SNDP. We experimented and evaluated the effectiveness of proposed aprroach for solving survivable network design problem",17,https://doi.org/10.1109/NaBIC.2014.6921887,1,
382,Improving semantic texton forests with a Markov random field for image segmentation.,"Semantic image segmentation is a major and challenging problem in computer vision, which has been widely researched over decades. Recent approaches attempt to exploit contextual information at different levels to improve the segmentation results. In this paper, we propose a new approach for combining semantic texton forests (STFs) and Markov random fields (MRFs) for improving segmentation. STFs allow fast computing of texton codebooks for powerful low-level image feature description. MRFs, with the most effective algorithm in message passing for training, will smooth out the segmentation results of STFs using pairwise coherent information between neighboring pixels. We evaluate the performance of the proposed method on two well-known benchmark datasets including the 21-class MSRC dataset and the VOC 2007 dataset. The experimental results show that our method impressively improved the segmentation results of STFs. Especially, our method successfully recognizes many challenging image regions that STFs failed to do.",17,https://doi.org/10.1145/2676585.2676621,1,
383,Heuristic Algorithm for Virtual Network Mapping Problem.,"Nowadays, resource allocation for virtual networks (VNs) is brought as an imperative problem. For the characteristics of virtual networks, multiple virtual networks with different topo can co-exist on a shared infrastructure. A difficult point of problem is how to use the resource effectively and to satisfy the requirement of virtual network request. This problem is NP-hard. In this paper, we introduce a heuristic algorithm to solve this problem. To simulate a virtual network mapping problem in real world, we using two input data: an infrastructure network which is modeled by a connected graph and a set of virtual network request graphs with each graph contains their constraints, time and duration. The main purposes are to maximize the revenue and to minimize the cost when allocate the virtual networks to a substrate network. The experimental results are reported to show the efficiency of propose algorithm comparing to the Enhanced Greedy Node Mapping (EGNM) algorithm.",17,https://doi.org/10.1007/978-3-319-13326-3_5,1,
384,Improved Genetic Algorithm for Solving Optimal Communication Spanning Tree Problem.,"Optimal Communication Spanning Tree (OCST) is a well-known NP-hard problem on the graph that seeks for the spanning tree with the lowest cost. The tree cost depends on the communication volume between each pair of nodes. This paper proposed an improved Genetic Algorithm combining with Ahujia and Murty’s Tree Improvement Procedure. The proposed algorithm was experimented on known benchmark tests which used in many papers related to OCST problem, and random instances from 200 to 500 vertexes. The experimental results show that the proposed algorithm is better than the heuristic and out-performance the most recent evolutionary algorithm approaches.",17,https://doi.org/10.1007/978-3-642-37502-6_49,1,
385,Genetic Algorithm for Solving Survivable Network Design with Simultaneous Unicast and Anycast Flows.,"We consider the survivable network design problem for simultaneous unicast and anycast flow requests. In this problem, a network is modeled by a connected, weighted and undirected graph with link cost follows All Capacities Modular Cost (ACMC) model. Given a set of flow demand, this problem aims at finding a set of connection with minimized network cost to protect the network against any single failure. This problem is proved to be NP-hard. In this paper, we propose a new Genetic Algorithm for solving the ACMC Survivable Network Design Problem (A-SNDP). Extensive simulation results on Polska, Germany and Atlanta network instances show that the proposed algorithm is much more efficient than the Tabu Search and other baseline algorithms such as FBB1 and FBB2 in terms of minimizing the network cost.",17,https://doi.org/10.1007/978-3-642-37502-6_144  ,1,
386,Real-coded Genetic Algorithm for solving Multi-Area Economic Dispatch problem.,"We consider the Multi-Area Economic Dispatch problem (MAEDP) in deregulated power system environment for practical multi-area cases with tie line constraints. Our objective is to generate allocation to the power generators in such a manner that the total fuel cost is minimized while all operating constraints are satisfied. This problem is NP-hard. In this paper, we propose Real-coded Genetic Algorithm (RCGA) to solve MAEDP. The experimental results are reported to show the efficiency of proposed algorithms compared to Particle Swarm Optimization with Time-Varying Acceleration Coefficients (PSO-TVAC)",17,https://doi.org/10.1109/CIES.2013.6611735,1,
387,Using contour information for image segmentation.,"This paper proposes an algorithm for image segmentation that improves the graph-based segmentation algorithm by exploiting contour information. The graph-based image segmentation [9] is a fast and efficient method of generating a set of segments from an image. However, its drawback is neglecting the contour information of pixels. Contour can provide significant cues to facilitate the efficient segmentation. We propose an improved weight function that incorporates contour feature into the dissimilarity measure of pixels. We performed experiments on the Berkeley image dataset. Our proposed approach attains significant performance. The experimental results show that our proposed approach is comparable to or even outperforms some state-of-the-art algorithms. In term of global consistency error, our method gives better result while other measures including Probabilistic Rand Index, Variation of Information, and Boundary Displacement Error are close to the best result given by state-of-the-art algorithms.",17,https://doi.org/10.1109/SOCPAR.2013.7054139,1,
388,Genetic algorithm in boosting for object class image segmentation.,"We describe how a task in computer vision can be effectively resolved by employing Genetic Algorithm. This paper focuses on the problem of semantic segmentation of digital images. We propose to use an improved genetic algorithm for the learning parameters of weak classifiers in a boosting learning set up. We propose a new encoding and genetic operators in accordance with this problem. Beside that, we employed multiple image features such as texture-layout, location, color and HoG for improving the accuracy of the system. Experiments are conducted extensively on MSRC, a widely used benchmark image datasets. The experimental results demonstrate that the performance of our system is comparable to, or even outperforms the state-of-the-art algorithms in semantic segmentation.",17,https://doi.org/10.1109/SOCPAR.2013.7054143,1,
389,Recombination Operators in Genetic Algorithm - Based Crawler: Study and Experimental Appraisal.,A focused crawler traverses the web selecting out relevant pages according to a predefined topic. While browsing the internet it is difficult to identify relevant pages and predict which links lead to high quality pages. This paper proposes a topical crawler for Vietnamese web pages using greedy heuristic and genetic algorithms. Our crawler based on genetic algorithms uses different recombination operators in the genetic algorithms to improve the crawling performance. We tested our algorithms on Vietnamese newspaper VnExpress websites. Experimental results show the efficiency and the viability of our approach.,17,https://doi.org/10.1007/978-3-642-34300-1_23,1,
390,Heuristic Algorithms for Solving Survivability Problem in the Design of Last Mile Communication Networks.,"Given a connected, weighted, undirected graph G = (V, E), a set infrastructure nodes and a set customers C includes two customer types where by customers C1 require a single connection (type-1) and customers C2 need to be redundantly connected (type-2). Survivable Network Design Problem (SNDP) seeks sub-graph of G with smallest weight in which all customers are connected to infrastructure nodes. This problem is NP-hard and has application in the design of the last mile of the real-world communication networks. This paper proposes a new heuristic algorithm for solving SNDP. Results of computational experiments are reported to show the efficiency of proposed algorithm.",17,https://doi.org/10.1007/978-3-642-28490-8_54,1,
391,Improving Image Segmentation Using Genetic Algorithm.,"This paper presents a new approach to the problem of semantic segmentation of digital images. We aim to improve the performance of some state-of-the-art approaches for the task. We exploit a new version of texton feature [28], which can encode image texture and object layout for learning a robust classifier. We propose to use a genetic algorithm for the learning parameters of weak classifiers in a boosting learning set up. We conducted extensive experiments on benchmark image datasets and compared the segmentation results with current proposed systems. The experimental results show that the performance of our system is comparable to, or even outperforms, those state-of-the-art algorithms. This is a promising approach as in this empirical study we used only texture-layout filter responses as feature and a basic setting of genetic algorithm. The framework is simple and can be extended and improved for many learning problems.",17,https://doi.org/10.1109/ICMLA.2012.134,1,
392,Genetic Algorithm for Solving the Master Thesis Timetabling Problem with Multiple Objectives.,"Master thesis defense scheduling problem is a real-world practical problem that arises from the Vietnamese Universities. In this paper, we give the formulation of the problem based on realistic requirements. We then show that the considered problem is NP-hard and propose a genetic algorithm for solving it. We experiment the proposed algorithm on the real problem instances taken from Hanoi University of Science and Technology. Experimental results show the feasibility of proposed algorithm.",17,https://doi.org/10.1109/TAAI.2012.50,1,
393,New hybrid genetic algorithm for solving optimal communication spanning tree problem.,"Optimal Communication Spanning Tree (OCST) is a well-known NP-hard problem on the graph that seeks for the spanning tree with the lowest cost. The tree cost depends on the demand and distance between each pair of nodes. This paper presents a Hybrid Genetic Algorithm (HGA) combining the basic GA with the ideas of the Particle Swarm Optimization (PSO) algorithm. In HGA, each individual exploits information of its own experience to search through the solution space with genetic operator. The experiment results show that our HGA outperforms the previous GAs with faster convergence and better solution.",17,https://doi.org/10.1145/1982185.1982421,1,
394,New Multi-parent Recombination in Genetic Algorithm for Solving Bounded Diameter Minimum Spanning Tree Problem.,"Given a connected, weighted, undirected graph G=(V, E) and a bound D, bounded diameter minimum spanning tree problem (BDMST) seeks spanning tree on G with smallest weight in which no path between two vertices contains more than D edges. This problem is NP-hard for 4 les D les |V| - 1. This paper proposes three new multi-parent recombination operators using different methods to choose parents in genetic algorithm for solving bounded diameter minimum spanning tree problem. Results of computational experiments are reported to show the efficiency of proposed algorithms.",17,https://doi.org/10.1109/ACIIDS.2009.89,1,
395,A new hybrid Genetic Algorithm for solving the Bounded Diameter Minimum Spanning Tree problem.,"In this paper, a new hybrid genetic algorithm - known as HGA - is proposed for solving the Bounded Diameter Minimum Spanning Tree (BDMST) problem. We experiment with HGA on two sets of benchmark problem instances, both Euclidean and Non-Euclidean. On the Euclidean problem instances, HGA is shown to outperform the previous best two Genetic Algorithms (GAs) reported in the BDMST literature, while on the Non-Euclidean problem instance, HGA performs comparably with these two GAs.",17,https://doi.org/10.1109/CEC.2008.4631221,1,
396,New Particle Swarm Optimization Algorithm for Solving Degree Constrained Minimum Spanning Tree Problem.,"Given a connected, weighted, undirected graph G=(V, E) and a bound d. The Degree-Constrained Minimum Spanning Tree problem (DCMST or d-MST) seeks the spanning tree with smallest weight in which no vertex have degree more than d. This problem is NP-hard with d≥2. This paper proposes a new Particle Swarm Optimization algorithm for solving the d-MST problem. The proposed algorithm uses some new methods for selecting vector of particles. Results of computational experiments are reported to show the efficiency of the algorithm.",17,https://doi.org/10.1007/978-3-540-89197-0_110,1,
397,NextLab: A new hybrid testbed and development platform for Software-defined Networking.,"Software-defined Network (SDN) is considered as a revolutionary paradigm shift of computer networks in bringing programmable networks and control-data plane separation. Over the last decade, SDN has received a remarkable attention from the networking society. However, much of the previous research is based on Mininet, Maxinet, One and DOT to simulate the SDN. These kinds of testing environment are obviously not able to achieve a proper outcome when compared to the realistic network. In this paper, we propose a novel hybrid SDN testbed, called Nextlab that combines both the virtual and physical devices in order to provide a more accurate simulation. Such testbed is also equipped with a user-friendly GUI to save user's time in configurating the network topology. The experimental works show that the proposed Nextlab gives a better performance result compared to other traditional simulators, such as Mininet.",21,https://doi.org/10.1145/3368926.3369696,1,
398,Traffic behavior recognition using the pachinko allocation model,"CCTV-based behavior recognition systems have gained considerable attention in recent years in the transportation surveillance domain for identifying unusual patterns, such as traffic jams, accidents, dangerous driving and other abnormal behaviors. In this paper, a novel approach for traffic behavior modeling is presented for video-based road surveillance. The proposed system combines the pachinko allocation model (PAM) and support vector machine (SVM) for a hierarchical representation and identification of traffic behavior. A background subtraction technique using Gaussian mixture models (GMMs) and an object tracking mechanism based on Kalman filters are utilized to firstly construct the object trajectories. Then, the sparse features comprising the locations and directions of the moving objects are modeled by PAM into traffic topics, namely activities and behaviors. As a key innovation, PAM captures not only the correlation among the activities, but also among the behaviors based on the arbitrary directed acyclic graph (DAG). The SVM classifier is then utilized on top to train and recognize the traffic activity and behavior. The proposed model shows more flexibility and greater expressive power than the commonly-used latent Dirichlet allocation (LDA) approach, leading to a higher recognition accuracy in the behavior classification.",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:Tyk-4Ss8FVUC,,1
399,Semantic Querying of News Articles With Natural Language Questions.,"The heterogeneity and the increasing amount of the news published on the web create challenges in accessing them. In the authors' previous studies, they introduced a semantic web-based sports news aggregation system called BKSport, which manages to generate metadata for every news item. Providing an intuitive and expressive way to retrieve information and exploiting the advantages of semantic search technique is within their consideration. In this paper, they propose a method to transform natural language questions into SPARQL queries, which could be applied to existing semantic data. This method is mainly based on the following tasks: the construction of a semantic model representing a question, detection of ontology vocabularies and knowledge base elements in question, and their mapping to generate a query. Experiments are performed on a set of questions belonging to various categories, and the results show that the proposed method provides high precision.",24,https://doi.org/10.4018/jitr.2021070103,1,
400,An Uncertainty-Aware Encoder for Aspect Detection.,"Aspect detection is a fundamental task in opinion mining. Previous works use seed words either as priors of topic models, as anchors to guide the learning of aspects, or as features of aspect classifiers. This paper presents a novel weakly-supervised method to exploit seed words for aspect detection based on an encoder architecture. The encoder maps segments and aspects into a low-dimensional embedding space. The goal is approximating similarity between segments and aspects in the embedding space and their ground-truth similarity generated from seed words. An objective function is proposed to capture the uncertainty of ground-truth similarity. Our method outperforms previous works on several benchmarks in various domains.",24,https://doi.org/10.18653/v1/2021.findings-emnlp.69,1,
401,A Novel Method for Recognizing Vietnamese Voice Commands on Smartphones with Support Vector Machine and Convolutional Neural Networks.,"This paper will present a new method of identifying Vietnamese voice commands using Google speech recognition (GSR) service results. The problem is that the percentage of correct identifications of Vietnamese voice commands in the Google system is not high. We propose a supervised machine-learning approach to address cases in which Google incorrectly identifies voice commands. First, we build a voice command dataset that includes hypotheses of GSR for each corresponding voice command. Next, we propose a correction system using support vector machine (SVM) and convolutional neural network (CNN) models. The results show that the correction system reduces errors in recognizing Vietnamese voice commands from 35.06% to 7.08% using the SVM model and 5.15% using the CNN model.",24,https://doi.org/10.1155/2020/2312908,1,
402,A Novel Method to Predict Type for DBpedia Entity.,"Based on extracting information from Wikipedia, DBpedia is a large scale knowledge base and makes this one available using Semantic Web and Linked Data principles. Thanks to crowd-sourcing, it currently covers multiples domains in multilingualism. Knowledge is obtained from different Wikipedia editions by effort of contributors around the world. Their goal is to manually generate mappings Wikipedia templates into DBpedia ontology classes (types). However, this cause makes the type inconsistency for an entity among different languages. As a result, the quality of data in DBpedia can be affected. In this paper, we present the statement of type consistency for an entity in multilingualism. As a solution for this problem, we propose a method to predict the entity type based on a novel conformity measure. We also evaluate our method based on database extracted from aggregating multilingual resources and compare it with human perception in predicting type for an entity. The experimental result shows that our method can suggest informative types and outperforms the baselines.",24,https://doi.org/10.1007/978-3-319-76081-0_11,1,
403,Sport News Semantic Search with Natural Language Questions.,"An increasingly huge amount of sport news published from a number of heterogeneous sources on the Web brings challenges to the traditional searching method using keywords. Providing an expressive way to retrieve news items and exploiting the advantages of semantic search technique in the development of Web-based sports news aggregation system is within our consideration. This paper presents a method to translate natural language questions into queries in SPARQL, the standard query language recommended by W3C for semantic data. Our contribution consists mainly of the construction of a semantic model representing a question, the detection of ontology vocabularies and knowledge base elements in question, and their mapping to generate a query. We evaluate the method based on a set of natural language questions and the results show that the proposed method achieves good performance with respect to precision.",24,https://doi.org/10.1007/978-3-030-05873-9_6,1,
404,Hot Topic Detection on Newspaper.,"Online newspaper nowadays is gradually replacing the traditional one and the variety of articles on newspaper motivated the need for capturing hot topics to give Internet users a shortcut to the hot news. A hot topic always reflects the people's concern in real life and has big impact not only on community but also in business. In this paper, we proposed a novel topic detection approach by applying Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) on Vector Space Model (VSM) to solve the challenge in noisy data and Pearson product-moment correlation coefficient (PMCC) on high ranking keywords to identify topics behind keywords. The proposed approach is evaluated over a dataset of ten thousand of articles and the experimental results are competitive in term of precision with other state-of-the-art methods.",24,https://doi.org/10.1145/3287921.3287965,1,
405,Automatic Alignment between Wikipedia Attributes and DBpedia Properties.,"DBpedia plays a central role in Linked Open Data (LOD), due to the
large and growing number of resources linked to it. Currently, this project extracts information from Wikipedia to represent in RDF triples. The extraction
procedure required to manually map Wikipedia infobox attributes into the
DBpedia properties. However, the number attributes are so large for all Wikipedia editions in different languages. This task therefore is time-consuming
and labor intensive. We propose a novel method to mapping automatically basing on instance-based approach enhanced by using label translation. Experiments on Vietnamese Wikipedia confirm the significant improvement when applying our method.",24,http://ceur-ws.org/Vol-2000/jist2017pd_paper3.pdf,1,
406,Semantic-Based Recommendation Method for Sport News Aggregation System.,"News on the Internet today plays an important role in helping people access daily information around the world. News aggregators are websites that collect and provide content from different sources in one location for easy viewing. However, the increasing number of news on the Internet makes it difficult for readers when they desire to access news they are concerned. One solution to this issue is based on employing recommender systems. In this research, we propose a novel method for news recommendation based on a combination of semantic similarity with content similarity between news and implement it as a feature of semantic-based news aggregators BKSport. Experimental results have shown that, a combination of both kind of similarity measures will result in better recommendation than when using either measure separately.",24,https://doi.org/10.1007/978-3-319-49944-4_3,1,
407,Type Prediction for Entities in DBpedia by Aggregating Multilingual Resources.,"The entity type is considered as very important in DBpedia. Since
this information is inconsistently described in different languages, it is difficult
to recognize the most suitable type of an entity. We propose a method to predict
the entity type based on a novel conformity measure. We combine the
consideration of the specific-level and the majority voting. The experiment
result shows that our method can suggest informative types and outperforms the
baselines. ",24,http://ceur-ws.org/Vol-1690/paper80.pdf,1,
408,A novel approach for automatic extraction of semantic data about football transfer in sport news.,"he purpose of this paper is to propose an automatic method to generate semantic annotations of football transfer in the news. The current automatic news integration systems on the Web are constantly faced with the challenge of diversity, heterogeneity of sources. The approaches for information representation and storage based on syntax have some certain limitations in news searching, sorting, organizing and linking it appropriately. The models of semantic representation are promising to be the key to solving these problems.",24,https://doi.org/10.1108/IJPCC-03-2015-0018,1,
409,DGA Botnet detection using Collaborative Filtering and Density-based Clustering.,"In recent years, the botnet phenomenon is one of the most dangerous threat to Internet security, which supports a wide range of criminal activities, including distributed denial of service (DDoS) attacks, click fraud, phishing, malware distribution, spam emails, etc. An increasing number of botnets use Domain Generation Algorithms (DGAs) to avoid detection and exclusion by the traditional methods. By dynamically and frequently generating a large number of random domain names for candidate command and control (C&C) server, botnet can be still survive even when a C&C server domain is identified and taken down. This paper presents a novel method to detect DGA botnets using Collaborative Filtering and Density-Based Clustering. We propose a combination of clustering and classification algorithm that relies on the similarity in characteristic distribution of domain names to remove noise and group similar domains. Collaborative Filtering (CF) technique is applied to find out bots in each botnet, help finding out offline malwares infected-machine. We implemented our prototype system, carried out the analysis of a huge amount of DNS traffic log of Viettel Group and obtain positive results.",24,https://doi.org/10.1145/2833258.2833310,1,
410,Empowering Exploratory Search on Linked Movie Open Data with Semantic Technologies.,"Nowadays, Linked Open Data (LOD) has grown rapidly to become large open datasets defined by RDF standards. Thanks to development of Data Web, the information on LOD is increasingly deeper, larger and easier to link in multi-domains, constituting the Linked Open Data cloud. Recently end-users applications using linked data sources as background knowledge appeared [15]. Thus, the exploitation of information on LOD effectively brings tremendous values as well as challenges. Meanwhile, Exploratory Search (ES) describes information-seeking processes that are opportunistic, iterative, and multi-tactical [7]. Furthermore, systems based on ES capitalize on new technological capabilities and interface paradigms that facilitate an increased level of interaction with information. In this paper, we present a method on searching and recommending information to empower exploratory search with semantic technologies. Our aim is to use algorithms with incorporation of structured semantics in search to give users the best related-semantic results and enhance users' interactions. We have exploited the data within LinkedMDB1 to support users in finding some information in the movie domain.",24,https://doi.org/10.1145/2833258.2833283,1,
411,Toward a platform for building and exploiting semantic annotation of photo taken with smart phone.,"The rapid development of mobile computing, social network and smart phones have led to a surge in photograph data. Efficient exploiting, searching and reusing this data have become a problem gaining increasing concerns. However, current systems are still encountering many challenges from the differences in perceiving the meaning of a photograph between human and machines. Among many different approaches, semantic web has recently been proven to be robust enough to overcome these challenges. In this paper, we propose the semantic web approach to develop a system which reduces above differences by accommodating an additional layer that annotates metadata with the original photo to describe important and useful information for the user in both content and context semi automatically. We use a mobile application that connects with a semantic web application. With above solution, the interaction of user of the photograph system becomes simple, friendly and quickly more than ever.",24,https://doi.org/10.1145/2684200.2684325,1,
412,Applying skip-gram word estimation and SVM-based classification for opinion mining Vietnamese food places text reviews.,"In this paper, a framework for mining unstructured documents in the form of Vietnamese text comments about locations is proposed. In the first step, the evaluation of users in the form of text comments will be extracted to produce a set of sentences in Vietnamese standard grammars by web analytic processing. Through the second step, using Skip-gram based model, the similarity between each phrase in sentences will be detected. The core of this research focuses on contributing an approach for word representation that reflects Vietnamese semantic information contexts for analyzing as an input of a machine learning based classification, SVM. The application of this research is applied to the STAAR project's opinion mining system.",24,https://doi.org/10.1145/2676585.2676606,1,
413,The VHO Project: A Semantic Solution for Vietnamese History Search System.,"This paper presents the process of building a semantic search application in the field of the Vietnamese history. In particular, the design of ontology in the field of history will come up with three main issues: the historical ontology construction with Vietnamese history books or documents as an input, managing change entities and the storage method of time entities in historical ontology. The experimental result of this article is the VHO web-based application which allows users to ask semantic questions about historical objects.",24,https://doi.org/10.1007/978-3-642-36546-1_32,1,
414,A Method for the Generation of Semantic Annotation from Sport News Using Ontology Based Patterns.,"In the framework of this research, we focus on describing the algorithm to collect, process information in natural language to turn it into semantic annotations based on ontology, patterns and the knowledge ground in sport. This paper will concentrate to solve the problem of enhancing the named entities recognition by developing ontology and enriching the knowledge base for the recognition engine. Besides this, it is at the same time solving the problem of semantic recognition in texts using ontology-based patterns, harnessing the power of ontology in identifying synonyms, aliases.",24,https://doi.org/10.3233/978-1-61499-254-7-364,1,
415,Enhance Matching Web Service Security Policies with Semantic.,"Web service security policy is a way to add some security restrictions to a web service. Matching web service security policies is hard and important to integrate web services effectively. However, the lack of semantics in WS-SecurityPolicy (WS-SP) hampers the effectiveness of matching the compatibility between security policies of different web services. To enhance matching web service security policies, we propose a semantic approach for specifying and matching the security policies. The approach uses the transformation of WS-SP into an OWL-DL ontology, the definition of a set of semantic relations that can exist between the provider and requestor security concepts, and the algorithm to determine the match level of the provider and requestor security policies. We show how these relations and the matching algorithm lead to more correct and more flexible matching of security policies.",24,https://doi.org/10.1007/978-3-319-02741-8_19,1,
416,Semantic approach to travel information search and itinerary recommendation.,"The growth of online data and services on the Web have have led to the Web become an indispensable tool for the tourist industry. It is not denied that various approaches bring benefits for visitors, in supporting their searching for tourist attractions, such as interesting places for the visit, eating or staying. However, like a coin has two sides, too much information would present a difficulty for people when planning their journeys. Generally, tourists usually have problems when trying to find satisfactory accommodation if references to nearby restaurants, sights or event locations are lacking. In addition, travelers suffer from the information overload when they look for information about potential destinations, events and related services. Providing relevant and up‐to‐date information for the tourists with different personal interests is still a challenging task for the tourist guide information systems. The purpose of this paper is to propose a semantic approach for searching tourist information and generating travel itinerary.",24,https://doi.org/10.1108/17440081211258169,1,
417,Towards Efficient Sport Data Integration through Semantic Annotation.,"In news genre, the sport domain is one of great interest to audiences on many occasions. The explosion of Internet these days leads to many obstacles for users in searching information due to enormous amount of data collected from multiple media streams. In this case, Semantic Web is believed to bring a solution for information integration. In this paper, we introduce a novel approach for an integration system of sports information using semantic web. Semantic annotation of sport news on the web is a way to bring success to our system. We propose an algorithm to generate semantic annotations of sport news that relies on the ontology. Named entities recognition is improved by integrating the KIM information extraction system with this ontology, which is dedicated for sport domain. The concepts and relations of ontology detected in the text are combined with these entities to capture the semantic. Initial experiments on soccer news demonstrated promising results.",24,https://doi.org/10.1109/KSE.2012.21,1,
418,Integrating open data and generating travel itinerary in semantic-aware tourist information system.,"The growth of online data and services on the Web make it become more and more emerging as an indispensable tool of traveling for the tourist industry. It is not denied that various approaches bring benefits for visitors in supporting them of searching tourist attractions, such us interested places for the visit, eating or staying. However, like a coin has two sides, too much information would be the difficulty for people when planning their journeys. Generally, tourists usually have problems when finding a satisfied accommodation without a reference to nearby restaurants, sights or event locations. In addition, travelers suffer from the information overload when they look for information about potential destinations, events and related services. Providing the relevant and up-to-date information for the tourists with different personal interests is still a challenging task for the tourist guide information systems.

This paper presents a semantic web approach for developing STAAR (Semantic Tourist informAtion Access and Recommending) which is a system that addresses the above mentioned issues. Specifically, it is described how an ontology is designed to represent the travel related information, and to support the integrating data from the open repositories. Relying on this ontology, we propose an algorithm for generating travel itinerary on the Web which is relevant to both criterions of the itinerary length and user interest.",24,https://doi.org/10.1145/2095536.2095573,1,
419,An Ontology Based Approach to Data Representation and Information Search in Smart Tourist Guide System.,"Tourism is travel for leisure and recreational purposes. However, travelers suffer from information overload when they use the Internet to look for information about potential destinations, events and related services. Providing the relevant information to tourists with different personal interests is still a challenging task for tourist guide information systems. This paper presents an ontology based approach for developing STAAR (Semantic Tourist information Access and Recommending), a system that addresses this problem. STAAR help tourist search information by providing a various semantic search feature in a mobile phone application. In addition, we propose an algorithm for recommending travel route relevant to both criterions: itinerary length and user interest.",24,https://doi.org/10.1109/KSE.2011.33,1,
420,Improving travel information access with semantic search application on mobile environment.,"In last decade, we witness a significant increase in performance of mobile devices. In parallel, the growth of online data and services available on the Web make it become more and more emerging as a gold mine for the tourist guide application. With the inherent mobility and the widely supported Internet connectivity, tourist guide applications on smart phone are currently potential substitutes or supplements to travel book. However, it is a nightmare for tourists to select what are really interesting in a sheer volume of information while mobile devices have limited interactivity due to the inconvenient keyboard and small screen size regarding to computer. Therefore, the goal of our research is to develop a smart tool helping tourists find relevant travel information with minimum effort. This tool, implemented on Android platform, is the main component of STAAR (Semantic Tourist informAtion Access and Recommending), a system that supports request from heterogeneous environments. For the purpose, we describe our ontology formalized in RDFS/OWL to describe travel related information and to support integrating data from Linked data source. Through ontology manipulating web services, our Android application generates dynamic ontology-based user interfaces, allowing tourist to express their need in semantic queries with different levels of complexity, and then get access to relevant information.",24,https://doi.org/10.1145/2095697.2095716,1,
423,A semantic web approach for building technology-monitoring system.,"The information explosion especially on the World Wide Web has made it become the most immense information source and a gold mine for corporations. In the field of technology monitoring, it is very important to be able to get relevant information from heterogeneous sources on the Web. The arrival of Semantic Web technologies promises intelligent retrieval and access to information through the use of semantic annotations based on relevant ontologies. In a scenario of technology monitoring, the automatic generation of semantic annotations of Web document has a significant role. In this article, after describing a new approach based on Semantic Web technologies for building a technology monitoring system, we present an ontology-based algorithm for automatic search and annotation of Web documents. This algorithm will be encapsulated by agents in a multi agent technology monitoring system.",24,https://doi.org/10.1145/1088622.1088659,1,
425,Integrating External Sources in a Corporate Semantic Web Managed by a Multi-agent System.,"We first describe a multi-agent system managing a corporate memory in the form of a corporate semantic web. We then focus on a newly introduced society of agents in charge of wrapping external HTML documents that are relevant to the activities of the organization, by extracting semantic Web annotations using tailored XSLT templates.",24,https://doi.org/10.1007/978-3-540-24612-1_18,1,
426,A general solution supporting real-time and remote electrocardiogram diagnostic based on embedded and mobile technology.,"Studying and applying computer science in supporting cardiovascular disease diagnostic have had many achievements in the world. In Vietnam, related studies, especially about Electrocardiogram- ECG (or EKG) are limited to theory researches and disconnected products, and have no complete solution to apply in healthcare centers, whereas foreign solutions are very expensive. In addition, having no up to date facilities, rural hospitals and healthcare centers in Vietnam cannot meet all the needs of patients and they have to move to big cities for treatment, while the diagnosis can be performed remotely with the advances in technology. Inspired from the actual needs and the growth of technology, we have proposed a general solution to manufacture ECG devices that has compact size, and their accuracy is equivalent with imported ones. We also develop software integrated with ECG devices that support users (patients and doctors) quickly and conveniently with smart-phone. We hope that our solution will bring more efficiency to healthcare centers in Vietnam, especially the doctor in large cities can support remote treatment for patients in rural hospitals. Our main modules are developed and tested separately with 77.50% of accuracy for automatic diagnostic module.",24,https://doi.org/10.1145/2350716.2350744,1,
427,Enhance Java Software Development with Knowledge Acquisition and Management Tools.,"The Semantic Web can be a very promising platform for developing knowledge management systems. It has been applied in many domains, especially in Software Engineering. The main benefit is high improvement in the precision by searching for knowledge, as well as the possibility to retrieve a composition of knowledge sources which are relevant for the software development process. However, the problem is how to represent knowledge in the machine-understandable form, so that relevant knowledge can be found by machine agents. This paper will present a framework that improves Java software development process with our knowledge acquisition and management tools.",24,https://doi.org/10.1109/KSE.2011.19,1,
429,A new method using knowledge reasoning techniques for improving robot performance in coverage path planning.,"representation of the environment in which they are operating, ideally in both a human and machine-readable formalism. An important element in this process lies in Path Planning (PP) with obstacle avoidance in dynamic environments (including cleaning and monitoring in robotics) to identify optimal coverage paths. The study in this paper presents a new approach which combines knowledge reasoning techniques with Breadth First Search to find the optimal path for a cleaning robot in a dynamic environment. This approach is used to apply knowledge inference with conventional coverage PP algorithms to enable robot control and avoid obstacles with optimal coverage PP. The experimental results show that using the proposed approach a robot avoids fixed and mobile obstacles, optimal PP reducing both computational cost and time. When compared to other current approaches, the proposed approach with high-coverage rate and low-repetition rate in coverage performs better than the conventional robot algorithms.",25,https://doi.org/10.1504/IJCAT.2019.099503,1,
430,Modeling Uncertainties in DC-DC Converters with MATLAB® and PLECS®.,"Modeling is the process of formulating a mathematical description of the system. A model, no matter how detailed, is never a completely accurate representation of a real physical system. A mathematical model is always just an approximation of the true, physical reality of the system dynamics.

Uncertainty refers to the differences or errors between model and real systems and whatever methodology is used to present these errors will be called an uncertainty model. Successful robust control-system design would depend on, to a certain extent, an appropriate description of the perturbation considered.

Modeling the uncertainties in the switch mode DC-DC converters is an important step in designing robust controllers. This book studies different techniques which can be used to extract the uncertain model of DC-DC converters. Once the uncertain model is extracted, robust control techniques such as 𝑯∞ and μ synthesis can be used to design the robust controller.

The book is composed of two case studies. The first one is a buck converter and the second one is a Zeta converter. MATLAB® programming is used extensively throughout the book. Some sections use PLECS® as well.

This book is intended to be guide for both academicians and practicing engineers.

Table of Contents: Acknowledgments / Modeling Uncertainties for a Buck Converter / Modeling Uncertainties for a Zeta Converter / Authors' Biographies",25,https://doi.org/10.2200/S00875ED1V01Y201809EEL006,1,
431,Entropy Correlation and Its Impacts on Data Aggregation in a Wireless Sensor Network.,"A correlation characteristic has significant potential advantages for the development of efficient communication protocols in wireless sensor networks (WSNs). To exploit the correlation in WSNs, the correlation model is required. However, most of the present correlation models are linear and distance-dependent. This paper proposes a general distance-independent entropy correlation model based on the relation between joint entropy and the number of members in a group. This relation is estimated using entropy of individual members and entropy correlation coefficients of member pairs. The proposed model is then applied to evaluate two data aggregation schemes in WSNs including data compression and representative schemes. In the data compression scheme, some main routing strategies are compared and evaluated to find the most appropriate strategy. In the representative scheme, with the desired distortion requirement, a method to calculate the number of representative nodes and the selection of these nodes are proposed. The practical validations showed the effectiveness of the proposed correlation model and data reduction schemes.",25,https://doi.org/10.3390/s18093118,1,
432,Building of commonsense knowledge for home robotic services: 'bring something' and 'tidy things up'.,"Sharing commonsense knowledge would be one of the most important keys for interaction between human and service robot. However, the complication of human commonsense makes extracting appropriate information for service robot difficult. This paper presents our approach to build the basic-level knowledge network (BKN) as a commonsense and how it is used for robotic services. In this paper, we focused on two services. One is ‘bring something’ and the other is ‘tidy things up’. We found that for ‘bring something’, relationship between object-activity is important, and for ‘tidy things up’ service, knowledge about relationship between object-place is more important. To perform these services at the practical level, many relative knowledge should be prepared. Moreover, we would like to introduce how BKN is built and show the demonstration result in which how BKN is useful for actual services in real environment.",25,https://doi.org/10.1504/IJMA.2014.059770,1,
433,Commonsense knowledge extraction for Tidy-up robotic service in domestic environments.,"Commonsense is one of the keys to enable human-robot communication in daily life scenarios. It is very difficult for a robot to do tasks ordered by a human without having some basic knowledge to understand the human's commands. This paper proposes a method to automatically build commonsense knowledge for the “Tidy-up” service, in which a robot is asked to take objects such as books, cups, dishes on a table to appropriate places automatically. We defined three object classes that are necessary for the service, namely “Washable”-objects that need to be washed, “Reusable”-objects that need to be stored for reuse, and “Trashable”-objects that need to be disposed of. For each object, multiple attributes were extracted from both the ConceptNet knowledge base and the Google search engine, and fed to classifiers to classify the object into the appropriate class. To evaluate the proposed method, output from classifiers were compared with the result from actual human. The result showed that the proposed approach is efficient in classifying objects and in providing object type as commonsense knowledge, hence, helping the robots to understand human intention and to provide intuitive service.",25,https://doi.org/10.1109/ARSO.2013.6705507,1,
434,"Specification of Daily-Life Objects Places for ""Tidy-Up"" Robotic Service.","In this paper, we focus on the “Tidy-up” robotic service which needs extremely complex and enormous kinds of knowledge about objects, environments, places, and user’s status. Especially, we have tried to find the way for specifying of appropriate places of daily-life objects for the “Tidy-up” robotic service. As the first phase, we categorized daily-life objects into three categories: Trashable, Reusable, and Washable. In the specification of an appropriate place, there are two methods: object category based and individual object trajectory based. For the first method, object state machine has been provided for each object category. In the second method, the object trajectories in term of places have been collected. Based on these historical trajectories, we could recognize the trajectory pattern and use it to provide and appropriate place for individual object.",25,https://doi.org/10.1007/978-3-642-39476-8_123,1,
435,User-oriented RT Service Proposal System in Kukanchi.,"In this research, we develop user-oriented RT service proposal system in Kukanchi. Kukanchi is interactive Human-Space Design and Intelligence. By carrying out data mining of user information, service history, sensor data, and object data, this system finds out those relationships and gives weight to each of them. As a result of experiment, this system came to be able to propose “Tsuide Service” (Additional RT service) that each user expects.",25,https://doi.org/10.1016/j.procs.2013.09.120,1,
436,Evaluation of commonsense knowledge for intuitive robotic service.,"Human commonsense is required to improve quality of robotic application. However, to acquire the necessary knowledge, robot needs to evaluate the appropriateness of the data it has collected. This paper presents an evaluation method, by combining the weighting mechanism in commonsense databases with a set of weighting factors. The method was verified on our Basic-level Knowledge Network. We conducted questionnaire to collect a commonsense data set and estimate weighting factors. Result showed that, the proposed method was able to build Robot Technology (RT) Ontology for a smart “Bring something” robotic service. More importantly, it allowed robot to learn new knowledge when necessary. An intuitive human-robot interface application was developed as an example base on our approach.",25,https://doi.org/10.1109/ICRA.2012.6225332,1,
437,Ambiguous command understanding with commonsense.,"This paper proposes our approach to disambiguate human pointing gesture by using commonsense knowledge. Pointing gesture is a natural way to specify the object that a user needs in the “Bring something” service. However, error in gesture recognition makes robot difficult to select the correct candidate object when there are multiple objects located closely together. To solve this problem, Robot Technology (RT) Ontology was used to make assumption about human activity and necessary objects for the activity. By combining this commonsense reasoning with the spatial constrain from human's pointing direction, robot is able to find appropriate object. Experiment showed the successful rate of object selection was improved, and therefore robotic service becomes more intuitive.",25,https://doi.org/10.1109/ROMAN.2012.6343808,1,
438,Automatic Building Robot Technology Ontology Based on Basic-Level Knowledge.,"When robot comes to our daily life, sharing knowledge is a key factor to realize the symbiosis between human and robot. In previous research, Robot Technology (RT) ontology was proposed as a knowledge base to help robot understands human’s intention in daily activities. However, the method to build that ontology was not discussed. This paper presents our approach to build RT ontology based on basic-level knowledge. We proposed a new structure for RT ontology with Where, What, and How layers based on 4W1H. As for input data, we used educational books and MIT’s ConceptNet. Our method is able to build RT ontology automatically by extracting objects and human activities from these data sources. We also implemented a weighting mechanism for the new ontology. Result shows that our method achieved better accuracy than conventional approach using Internet data.",25,https://doi.org/10.20965/jrm.2011.p0515,1,
439,Building basic level knowledge network.,"In order to realize a human-like intuitive communication between human and robot, a robot should understand the human's basic level knowledge which is usually hidden in communication. However, as human's basic level knowledge has been unconsciously built during our development, it is very hard to build the knowledge network as the computational system. In this paper, we propose the approach to build the basic level knowledge network through different types of corpus including websites, books, and other available common sense knowledge bases. As for the main part, we build our Basic level Knowledge Network system semantic elements, namely concepts and connections. Each connection is weighted by a set of weights which represents the commonness of the semantic element. We implemented our system onto iPad to realize a human-robot interface device for the scenario of “Bring something”.",25,https://doi.org/10.1109/RIISS.2011.5945774,1,
440,Architecture of Kukanchi Middleware.,"There are some efforts for providing middleware for building robot system. However, these approaches are not adequate for service robot. In this paper, we propose Kukanchi Middleware Architecture. Our research focuses on building Kukanchi System which supports interactions between human, robots and space. The proposed architecture provides APIs for robots, sensors, and user interfaces as well as functions to convert user's command to robot command. We intend and expect to simplify Kukanchi System development by applying Kukanchi Middleware.",25,https://doi.org/10.1109/URAI.2011.6145944,1,
441,Development and evolution of RT ontology for automatic service generation system in Kukanchi.,"This paper presents a novel approach for RT Ontology development, including ontology learning and evolution mechanism. In service robotics systems, understanding the relationship between everyday objects and user intention is the key feature to provide suitable services according to context. RT Ontology has shown to be an efficient technique to represent this relationship. In the proposed method, text corpus grabbed from search engines and lightweight natural language processing techniques were used for term extraction and enabling RT Ontology automatic creation. On the other hand, ontology evolution mechanism is introduced. With these learning and evolution capabilities, the presented RT Ontology model may adapt dynamically to the changes of environment and human activities. This will help to improve the robustness of current RT service generation systems, while reduce much of required labor work for ontology development. Experiments were conducted to show the effectiveness of proposed method.",25,https://doi.org/10.1109/IROS.2010.5652618,1,
442,Enhancing extractive summarization using non-negative matrix factorization with semantic aspects and sentence features.,"The main task in extractive text summarization is to evaluate the important of sentences in a document. This paper aims at improving the quality of an unsupervised summarization method, i.e. non-negative matrix factorization, by using sentence features and considering semantically related words using word embeddings (i.e. word2vec) in sentence scoring. The experiments were carried out with different scenario using the DUC 2007 dataset. Experimental results showed that when NMF was combined three types of sentence features (i.e., surface, content, and relevant features) and word2vec, the system got best performance with 42.34% for Rouge-1 and 10.77% for Rouge-2, increasing 0.67% Rouge-1 and 0.78% Rouge-2 in compared with only NMF.

",26,https://doi.org/10.1145/3155133.3155188,1,
443,"Comparison of Machine Learning Methods for Estimating Mangrove Above-Ground Biomass Using Multiple Source Remote Sensing Data in the Red River Delta Biosphere Reserve, Vietnam.","This study proposes a hybrid intelligence approach based on an extreme gradient boosting regression and genetic algorithm, namely, the XGBR-GA model, incorporating Sentinel-2, Sentinel-1, and ALOS-2 PALSAR-2 data to estimate the mangrove above-ground biomass (AGB), including small and shrub mangrove patches in the Red River Delta biosphere reserve across the northern coast of Vietnam. We used the novel extreme gradient boosting decision tree (XGBR) technique together with genetic algorithm (GA) optimization for feature selection to construct and verify a mangrove AGB model using data from a field survey of 105 sampling plots conducted in November and December of 2018 and incorporated the dual polarimetric (HH and HV) data of the ALOS-2 PALSAR-2 L-band and the Sentinel-2 multispectral data combined with Sentinel-1 (C-band VV and VH) data. We employed the root-mean-square error (RMSE) and coefficient of determination (R2) to evaluate the performance of the proposed model. The capability of the XGBR-GA model was assessed via a comparison with other machine-learning (ML) techniques, i.e., the CatBoost regression (CBR), gradient boosted regression tree (GBRT), support vector regression (SVR), and random forest regression (RFR) models. The XGBR-GA model yielded a promising result (R2 = 0.683, RMSE = 25.08 Mg·ha−1) and outperformed the four other ML models. The XGBR-GA model retrieved a mangrove AGB ranging from 17 Mg·ha−1 to 142 Mg·ha−1 (with an average of 72.47 Mg·ha−1). Therefore, multisource optical and synthetic aperture radar (SAR) combined with the XGBR-GA model can be used to estimate the mangrove AGB in North Vietnam. The effectiveness of the proposed method needs to be further tested and compared to other mangrove ecosystems in the tropics",27,https://doi.org/10.3390/rs12081334,1,
444,Some Improvements of Using the NSGA-II Algorithm for the Problem of Resource Allocation and Scheduling and Its Applying to Inventory Management Strategies.,"Vendor-managed inventory (VMI) is an approach to prevent undesired stocking inventories and hence can lead to a cost reduction of the whole supply chain. One of the main objectives of this approach is to optimize the inventory buffer as safety stock and to optimize the scheduling of inventory and delivery. Such optimization could be considered as a problem of the project's resource scheduling and allocation. In this paper, we present some experimentations for solving this problem by implementing two different algorithms: (i) the Nondominated Sorting Genetic Algorithm (NSGA-II), and (ii) the multi-objective optimization algorithm provided by the MOEA framework. Based on the experimented results, we propose some improvements in using NSGA-II to define an optimized VMI strategy. Such a strategy is implemented and demonstrated through the data collected from a real VMI project.",27,https://doi.org/10.1109/KSE.2019.8919492,1,
445,Browser Extension-based Crowdsourcing Model for Website Monitoring.,"Websites play an increasingly important role. They are not only a good marketing resource but also a power business tool. To make websites reliable and always available for customers, a large number of website monitoring services have been developed to periodically check the website uptime and performance. These services typically have a very limited number of checkpoints (i.e., geographical locations where we send monitoring requests to check the websites) and internet service providers (ISP) used by these checkpoints; it is because deploying checkpoints with low usage frequency would waste them a huge amount of money. Thus they cannot detect that a website is reachable from a city and an ISP but is unreachable from the others. To address this issue, this paper presents a crowdsourcing-based approach that makes use of browser extensions as checkpoints to monitor websites. Our main contributions include: (i) a website monitoring approach that combines the concepts of crowdsourcing and browser extensions; (ii) a batch processing technique for handling monitoring requests; (iii) the architecture of a conceptual crowdsourcing-based system for website monitoring that can coordinate a large number of crowd members; and (iv) various techniques to enhance the quality of monitoring.",27,https://doi.org/10.1145/3368926.3369706,1,
446,A Bayesian Critical Path Method for Managing Common Risks in Software Project Scheduling.,"Although project managers nowadays can use a range of tools and techniques to develop, monitor and control project schedules, the task of creating project schedules is often very difficult since it has to deal with planning against uncertainty. Popular techniques for project scheduling based on the assumption that projects are carried out as planned or scheduled -- which hardly happens. This paper takes the advantage of Bayesian Networks in modeling uncertainty and incorporates them in Critical Path Method - one of the most popular means of monitoring project scheduling. The paper also examines common risk factors in project scheduling and proposes a model of 19 common risk factors. A tool was also built, and experiments were carried out to validate the model.",27,https://doi.org/10.1145/3287921.3287962,1,
447,Checking and Correcting the Source Code of Web Pages for Accessibility.,"This paper aims to propose and experiment a framework for checking and correcting websites for accessibility. Existing tools usually check the WCAG-conformance of HTML client pages (that contain the static elements to be displayed through a browser to end users at a moment). Consequently, web developers have to do tedious works of identifying which parts of server source pages (i.e. server-side source codes that generate HTML client pages) cause non-conformant client elements. Unlike these tools, our framework allows directly reporting and suggesting solutions for the elements in the server source pages. The proposed method composes of four steps. First, the HTML client page and the server source page are parsed. Second, the elements of HTML client page that are non-conformant with the WCAG success criteria are identified and reported. Third, a mapping between the HTML client page and the server source page is established. Fourth, fixes to the server source page are suggested; this implies automatic modification of the code of server source page. Therefore, this framework can be applied to create new accessible websites, or to improve the accessibility of existing websites.",27,https://doi.org/10.1109/rivf.2012.6169872,1,
448,Towards a Secure Service Coordination.,"This paper presents an approach for building secure service-based coordinated systems. Secure coordination is considered at two levels: abstraction (i.e., specification) and execution (i.e., run level). At the abstraction level, we define a general model enabling to specify coordination and its related non functional properties (such as security). The idea is to use constraints for expressing the application logic of a coordinated system and its required security strategies. Coordination activities are the key concepts used for controlling the execution of participating services. Constraints are specified as pre and post conditions of these coordination activities. At the execution level, we propose an architecture which implements strategies to verify constraints and manage the secure execution of coordination. We propose also an instantiating vade-mecum to configure execution level components according to a specific set of constraints.",27,https://doi.org/10.1007/11896548_11,1,
449,Defining and Modelling Service-Based Coordinated Systems.,This paper introduces MEO – a model for securing service-based coordinated systems. The model uses constraints for expressing the application logic of a coordinated system and its required security strategies. Coordination activities are the key concepts used for controlling the execution of participating services. Constraints are specified as pre and post conditions of these coordination activities.,27,https://doi.org/10.1007/11914853_24,1,
450,Describing body-pose feature-poselet-activity relationship using Pachinko Allocation Model,"Understanding video-based activities have remained the challenge regardless of efforts from the image processing and artificial intelligence community. However, the rapid developing of computer vision in 3D area has brought an opportunity for the human pose estimation and so far for the activity recognition. In this research, the authors suggest an impressive approach for understanding daily life activities in the indoor using the skeleton information collected from the Microsoft Kinect device. The approach comprises two significant components as the contribution: the pose-based feature extraction under the spatio-temporal relation and the topic model based learning. For extracting feature, the distance between two articulated points and the angle between horizontal axis and joint vector are measured and normalized on each detected body. A codebook is then constructed using the K-means algorithm to encode …",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:d1gkVwhDpl0C,,1
451,Hierarchical emotion classification using genetic algorithms,"Emotion classification from speech signal is an interesting subject of machine learning applications that can provide the emotional or psychological states from speakers. This implicit information is helpful for machine to understand human behavior in more comprehensive way. Many feature extraction and classification methods have being proposed to find the most accurate and efficient method, but this is still an open question for researchers. In this paper, we propose a novel method to select features and classify emotions in hierarchical way using genetic algorithm and support vector machine classifiers in order to find the most accurate binary classification tree. We show the efficiency and robustness of our method by applying and analyzing on Berlin dataset of emotional speech and the experiment results show that our method achieves high accuracy and efficiency.",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:9yKSN-GCB0IC,,1
452,Clustering based multi-object positioning system,"Acoustic source positioning plays an important role in military tracking unwelcome objects. A system for this application must be capable of dealing with the input recorded convolved mixture signals while minimizing the high communication and computation cost. This paper describes a distributed system for positioning multiple independent moving sources relying on acoustic signals. The sensors pre-process the sensed data to obtain the frequency features before compressing and sending it to the base. At the base, the source positioning are carried out via two clustering stages and an optimization method. Analysis and simulation results show that our system provides high accuracy and needs neither much communication nor complex computation in a distributed manner. It is robust even when there exists high noise with Rayleigh multi-path fading under Doppler effect and when the number of independent …",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:2osOgNQ5qMEC,,1
453,Background subtraction with neighbor-based intensity correction algorithm,"An efficient foreground detection algorithm is presented in this work to be robust against consecutively illuminance changes and noise, and adaptive with dynamic speeds of motion in the background. The scene background is firstly modeled by a novel algorithm, namely Neighbor-based Intensity Correction, which identifies and modifies motion pixels extracted from the difference of the background and the current frame. Concretely the first frame is assumed as an initial background to be updated at each new coming frame based on the mechanism of the standard deviation value comparison. Two pixel windows used for standard deviation calculation are generated surrounding a corresponding motion pixel from the background and the current frame. The steadiness of the current background at the pixel-level is measured by a constantly updating factor to decide the usage of the algorithm or not. In the next stage …",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:Y0pCki6q_DkC,,1
454,PAM-based behavior modelling,"A novel approach for human behavior modelling is represented in this paper based on the Pachinko Allocation Model (PAM) algorithm for the video-based road surveillance. In particular, the authors focus on the behavior analysis and modelling for learning and training as the main distribution of this research. Sparse object features in sequence of frames are modelled into activities and behaviors with full topic correlations to avoid omissions of small activities.",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:zYLM7Y9cAGgC,,1
455,Hierarchical Emotional Speech Recognition using K-means clustering,"Automatic emotion classification from speech signal is an important application that will help to improve the human-computer interaction in more intelligent and emotional ways. Current works are trying to increase the accuracy of classification by using different feature extraction and classification methods. In this paper, a hierarchical method is proposed to classify emotion from a speech signal by generating a classification tree using k-means clustering and support vector machine classifier. The experiment shows that the proposed approach is able to achieve better accuracy and performance than existing works.",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:W7OEmFMy1HYC,,1
456,Music and speech discrimination using smartphone,"Music and speech are two basic audio signals that are important to represent different contexts in reality. When applying music and speech classification using smartphone, the problem of various types of device make the accuracy level is not stable and independent on devices. In this paper, we proposed a method to get useful information from original music or speech signal called division of successive frames that are not affected by the difference of devices. And then we apply in the experiment to extract features for music and speech classification task to get improved accuracy.",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:eQOLeE2rZwMC,,1
457,Emotion recognition from speech using Gammatone auditory filterbank,"An application of Gammatone auditory filterbank for emotion recognition from speech is described in this paper. Gammatone filterbank is a bank of Gammatone filters which are used as a preprocessing stage before applying feature extraction methods to get the most relevant features for emotion recognition from speech. In the feature extraction step, the energy value of output signal of each filter is computed and combined with other of all filters to produce a feature vector for the learning step. A feature vector is estimated in a short time period of input speech signal to take the advantage of dependence on time domain. Finally, in the learning step, Hidden Markov Model (HMM) is used to create a model for each emotion class and recognize a particular input emotional speech. In the experiment, feature extraction based on Gammatone filterbank (GTF) shows the better outcomes in comparison with features based on …",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:qjMakFHDy7sC,,1
458,PAM-based Behavior Modelling,"A novel approach for human behavior modelling is represented in this paper based on the Pachinko Allocation Model (PAM) algorithm for the video-based road surveillance. In particular, the authors focus on the behavior analysis and modelling for learning and training as the main distribution of this research. Sparse object features in sequence of frames are modelled into activities and behaviors with full topic correlations to avoid omissions of small activities.",28,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=T6Q9Nm8AAAAJ&citation_for_view=T6Q9Nm8AAAAJ:UeHWp8X0CEIC,,1
459,Symbol descriptor based on shape context and vector model of information retrieval,"In this paper we present an adaptive method for graphic symbol representation based on shape contexts. The proposed descriptor is invariant under classical geometric transforms (rotation, scale) and based on interest points. To reduce the complexity of matching a symbol to a largeset of candidates we use the popular vector model for information retrieval. In this way, on the set of shape descriptors we build a visual vocabulary where each symbol is retrieved on visual words. Experimental results on complex and occluded symbols show that the approach is very promising.",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:W7OEmFMy1HYC,,1
460,A symbol spotting approach based on the vector model and a visual vocabulary,"This paper addresses the difficult problem of symbol spotting for graphic documents. We propose an approach where each graphic document is indexed as a text document by using the vector model and an inverted file structure. The method relies on a visual vocabulary built from a shape descriptor adapted to the document level and invariant under classical geometric transforms (rotation, scaling and translation). Regions of interest selected with high degree of confidence using a voting strategy are considered as occurrences of a query symbol. Experimental results are promising and show the feasibility of our approach.",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:qjMakFHDy7sC,,1
461,Fully automated multi-label image annotation by convolutional neural network and adaptive thresholding,"This paper presents a fully automated and flexible ConvNet-based classifier for multi-label image annotation. The classifier alleviates hierarchical representation of image from a convolutional neural network, and adaptive thresholding technique on the ranked list of label scores. The method can annotate images with arbitrary number of labels that the classifier finds fit, as opposed to common methods which only assign a fixed number of those. Experiments show state-of-the-art on classification accuracy and competitive annotation performance across 2 intrinsically different data-sets, Corel5K and MSRCv2. Although the proposed method shows some limitation in learning label semantics, empirical study indicates that it was due to the established drawback of univariate loss function, which the classifier optimised, in multi-label classification. It, therefore, opens for number of directions to improve the performance …",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:Y0pCki6q_DkC,,1
462,Multi-view discriminant analysis for dynamic hand gesture recognition,"Although there have been attempts to tackle the problem of hand gesture recognition “in-the-wild”, deployment of such methods in practical applications still face major issues such as view point change, clustered background and low resolution of hand regions. In this paper, we investigate these issues based on a frame-work that is intensively designed in terms of both varying features and multi-view analysis. In the framework, we embed both hand-crafted features and learnt features using Convolutional Neural Network (CNN) for gesture representation at single view. We then employ multi-view discriminant analysis (MvDA) based techniques to build a discriminant common space by jointly learning multiple view-specific linear transforms from multiple views. To evaluate the effectiveness of the proposed frame-work, we construct a new multi-view dataset of twelve gestures. These gestures are captured by …",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:UebtZRa9Y70C,,1
463,Improving multi-view human action recognition with spatial-temporal pooling and view shifting techniques,"This paper presents a solution to improve performance of human action recognition from multiple camera views. For each camera view, we started by investigating a bag of words model that consists of STIP features to capture motion, a random forest for feature quantization and a SVM for action classification as baseline. However, to avoid background effect, we take STIP features only in the moving regions detected by background subtraction technique. Then, as some actions are very similar (interclass similarity), they discriminate against each others by some minor motions of body part (hand or foot) or/and by the order of movement during the action, we adopt a spatial-temporal pooling strategy of STIP features to take this difference into account. Finally we propose a strategy of shifting views in testing phase to deal with difference of camera viewpoints from training phase. The result from each view will be …",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:u5HHmVD_uO8C,,1
464,Localisation de symboles dans les documents graphiques,"Cette thèse s'inscrit dans le domaine de la recherche d'images par le contenu et plus spécifiquement dans celui de l'analyse de documents. Nous abordons le problème complexe de la localisation de symboles dans les documents où les symboles ne sont pas isolés de leur contexte. Bien qu'il existe beaucoup de travaux visant à la définition de bons descripteurs pour la représentation d'un symbole, ces derniers ne peuvent généralement pas être utilisés directement pour localiser des symboles dans les documents car on se heurte au paradoxe suivant : pour reconnaître les symboles il faudrait au préalable segmenter le document et réciproquement pour bien segmenter il faudrait au préalable reconnaître le contenu du document. Dans ce contexte, nous présentons nos contributions pour la localisation de symboles dans les documents graphiques où le problème de la localisation est abordé d'un point de vue différent de la plupart des méthodes existantes dans la littérature. Dans le contexte de l'analyse de documents graphiques, pour le problème de la localisation de symboles, presque toutes les études se focalisent sur l'aspect structurel du document, ce qui nécessite de résoudre plusieurs autres problèmes difficiles qui se situent soit en amont de la chaîne de traitements telle la vectorisation soit en aval telle la détection d'isomorphisme de (sous-) graphes. Cette thèse tente de voir ce problème de localisation sous l'aspect pixelaires qui est très rarement abordé dans les travaux précédents. Ainsi, dans nos travaux, nous avons abordé deux points essentiels pour résoudre ce problème. Le premier concerne le choix d'une représentation des …",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:IjCSPb-OGe4C,,1
465,Spatio-temporal multi-level fusion for human action recognition,"Two-stream convolutional networks have achieved great success for action recognition tasks. In this paper, we propose a spatiotemporal network that integrates the spatial and temporal features at multi-level to model the correlation between spatial and temporal information. Based on TSN model [16] where videos are divided into segments, our model integrates spatio-temporal information at either local or global levels. At local levels, temporal information is transferred to spatial stream in each segment. Considering at a global level, we integrate features of entire action extracted from two streams to obtain the final action representation. Moreover, in order to take into consideration the chronological sequence of the segments, we propose strategies for segment aggregation by using Conv3D and LSTM (Long-short term memory). In the training process, we also applied and evaluated several strategies such as …",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:Se3iqnhoufwC,,1
466,Une méthode de binarisation hiérarchique floue,"Cet article propose une approche hiérarchique pour la binarisation d’images de documents graphiques, basée sur un découpage en arbre quaternaire, dont l’intérêt est de traiter l’image à différents niveaux. Une image est d’abord segmentée à bas niveau grâce à une approche globale de binarisation, puis le résultat de cette première segmentation est localement affiné après un découpage récursif de l’image initiale. Ce découpage hiérarchique permet de combiner les avantages de deux types d’approches duales, globale et locale. Les résultats sont prometteurs: selon des critères d’homogénéité et de contraste fixés, la séparation entre le fond et le contenu du document est meilleure que celle obtenue avec certaines méthodes classiques.",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:d1gkVwhDpl0C,,1
467,A camera-based solution for customer behavior identification,"Nowaday, automatic or semi-automatic sale becomes the competitive advantage of retail or producer. In particular, understanding customer behaviors can help improve any vending system. In this work, we focus on finding a solution supporting automatic sales from shelves, that can detect what customers want. We try to set up a hypothetical system and propose a camera-based solution that helps identify customer activity. First, the layout of shelf is determined. Then, our system detects and tracks customer hands. While tracking, hand status (holding or non-holding) are also verified to see if the customer take something. Finally, based on hand's status changes, the system know if the customer takes or returns a product. Further, with a camera-based solution, we would like the proposed solution can run on a machine with a modest configure. In our solution, we choose tiny-YoloV3 for hand detection and MobileNetV2 …",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:MXK_kJrjxJIC,,1
468,Multi-view human action recognition based on tsn architecture integrated with gru,"Human action recognition is one of the most important problems in computer vision. In this paper, we propose a novel architecture for multi-view human action recognition. The proposal exploits the temporal features and fuses the information from different camera views. Based on the idea of TSN (Temporal Segment Networks) which is working with segments of videos, we recommend aggregating scores from segments by an RNN (Recurrent Neural Network) module to enhance the quality of dynamics features. Furthermore, the proposed architecture is designed to form a multi-branch network with each branch taking responsibility for extracting a view-specific information, and the final feature is formed by combining results from branches lastly. Experiments on two datasets NUMA and MicaHandGesture have proved that the proposed architecture works effectively in different scenarios. Our model has achieved …",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:8k81kl-MbHgC,,1
469,Factors Influencing The Performance of Image Captioning Model: An Evaluation,"Recently, neural network-based methods have shown impressive performances in captioning task. There have been numerous attempts with many proposed architectures to solve this captioning problem. In this paper, we present the evaluation of different alternatives in architecture and optimization algorithms for a neural image captioning model. First, we present the study of a image captioning model that is comprised of two modules--a convolutional neural network which encodes the input image into a fixed-dimensional feature vector and a recurrent neural network to decode that representation into a sequence of words describing the input image. After that, we consider different alternatives regarding architecture and optimization algorithm to train the model. We conduct a set of experiments on standard benchmark datasets to evaluate different aspects of the captioning system using standard evaluation methods …",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:zYLM7Y9cAGgC,,1
470,Une approche de localisation de symboles non-segmentés dans des documents graphiques,"Dans cet article, nous proposons une méthode de localisation de symboles dans des documents graphiques. Les occurrences du symbole dans un document sont détectées grâce à un processus de vote sur des régions candidates. L'approche repose sur un vocabulaire visuel et afin de réduire la complexité d'appariement d'un symbole avec d'autres nous utilisons le modèle vectoriel et une indexation par un fichier inverse. Cette méthode s'appuie sur un descripteur défini à partir du concept de contexte de forme1 adapté aux points d'intérêt. Ce descripteur est invariant à la rotation, à la translation et aux changements d'échelles. Les résultats expérimentaux sur la recherche de symboles isolés et sur la localisation de symboles non-segmentés dans le document sont très prometteurs.",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:Tyk-4Ss8FVUC,,1
471,Proposition d'un descripteur de formes et du modèle vectoriel pour la recherche de symboles,"Dans cet article, nous présentons une méthode alternative pour la recherche de symboles graphiques en adaptant le concept de contexte de forme1 aux points d'intérêt d'un symbole pour le décrire et en utilisant le modèle vectoriel pour le rechercher. Le descripteur que nous avons adapté est invariant à la rotation, à la translation et au changement d'échelle. Un vocabulaire visuel construit sur des descripteurs nous permet de réduire la complexité d'appariement d'un symbole avec un grand ensemble de candidats en utilisant le modèle vectoriel utilisé en recherche d'information. Les résultats expérimentaux sur des symboles complets et incomplets sont très prometteurs.",29,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=B-fSEecAAAAJ&citation_for_view=B-fSEecAAAAJ:u-x6o8ySG0sC,,1
472,Predicting the onset of type 2 diabetes using wide and deep learning with electronic health records,"Diabetes is responsible for considerable morbidity, healthcare utilisation and mortality in both developed and developing countries. Currently, methods of treating diabetes are inadequate and costly so prevention becomes an important step in reducing the burden of diabetes and its complications. Electronic health records (EHRs) for each individual or a population have become important tools in understanding developing trends of diseases. Using EHRs to predict the onset of diabetes could improve the quality and efficiency of medical care. In this paper, we apply a wide and deep learning model that combines the strength of a generalised linear model with various features and a deep feed-forward neural network to improve the prediction of the onset of type 2 diabetes mellitus (T2DM).",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:Tyk-4Ss8FVUC,,1
473,Breast cancer prediction using feature selection and ensemble voting,"Breast cancer is the most common cause of cancer among women worldwide. This paper analyses the performance of supervised and unsupervised models for breast cancer classification. Data from Wisconsin Breast Cancer Dataset is used in this paper. Feature selection is processed through scaling and principal component analysis. Final results indicate that Ensemble Voting approach is ideal as a predictive model for breast cancer. The raw data has 569 cases of breast cancer. The data is split into training and testing sets in the ration 70:30, respectively. The benchmark model is then created using Random Forest method. Various models are trained and tested on the data after Feature Scaling and Principle Component Analysis. Cross-validation is performed which showed that our model is stable. Among all the evaluated models, only four models, i.e., Ensemble - Voting Classifier, Logistics Regression, SVM …",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:ufrVoPGSRksC,,1
474,Multimodal detection of Parkinson disease based on vocal and improved spiral test,"Previous studies have used data related to either voice or spiral drawing to detect Parkinson disease (PD). However, different people experience different symptoms and different levels of severity of PD. This paper proposes a multimodal approach combining voice and image test to enhance the reliability of detecting PD patients. To substantiate this idea, we have evaluated both voice and spiral test data using various machine learning models. The results based on the two types of dataset demonstrate an excellent level of accuracy for PD identification. Pairwise correlation and k-means clustering techniques are used to extract features from the vocal dataset. In this classification problem, the highest accuracy of 95.89% is obtained using an ensemble of 3 classification models. The Pearson's correlation is used to extract features from the image dataset. The best accuracy of 99.6% is achieved using the k-Nearest …",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:YsMSGLbcyi4C,,1
475,"Effect of a comprehensive deep-learning model on the accuracy of chest x-ray interpretation by radiologists: a retrospective, multireader multicase study","Chest x-rays are widely used in clinical practice; however, interpretation can be hindered by human error and a lack of experienced thoracic radiologists. Deep learning has the potential to improve the accuracy of chest x-ray interpretation. We therefore aimed to assess the accuracy of radiologists with and without the assistance of a deep-learning model.",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:MXK_kJrjxJIC,,1
476,Music genre classification using residual attention network,"This paper proposes a new method in music genre classification by using the Residual Attention Network (RAN). Through the integration of attention mechanism and stacking attention modules, RAN has shown efficiency in the field of image processing. Residual blocks and attention modules are the most important considerations in RAN. First, each audio file is converted into a set of spectral images. Next, RAN is used for the classification of these spectral images, where the optimal stochastic gradient descent algorithm is used as the training model. The method is evaluated on the music data set from the Zalo AI “Music Genre Classification” challenge. This data set includes recordings of 10 different musical genres in Vietnam. Data augmentation combined with error analysis were implemented on the validation set. The results obtained had an accuracy of 71.7% on the test set. This highlights the potential of the …",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:W7OEmFMy1HYC,,1
477,Predicting hospital readmission patterns of diabetic patients using ensemble model and cluster analysis,"Diabetes is a chronic illness that affects around 425 million people globally in 2017, and this is predicted to increase to 629 million by the end of 2045. The ability to analyze and predict the readmission patterns of diabetic patients would allow the optimization of hospital resources and assessment of treatment effectiveness. This paper proposes an ensemble model to predict hospital readmission by choosing from a pool of 15 models, made up of variants of Logistic Regression, Decision Trees (DT), Neural Network (NN) and Augmented Naïve Bayes (NB) networks. The final ensemble model was assembled using the five best models, determined based on individual model accuracy and the Jaccard distance between them, to maximize overall accuracy and sensitivity. The final ensemble contained DT (CHAID), Tree Augmented Naïve Bayes network, DT (CHAID with boosting), Neural Network with bagging and DT …",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:eQOLeE2rZwMC,,1
478,Lesion segmentation and automated melanoma detection using deep convolutional neural networks and XGBoost,"Melanoma is a lethal form of skin cancer. It is one of the leading causes of deaths related to skin cancer. Most clinical studies show that early diagnosis of melanoma can improve patient outcomes. Currently, the most effective mean of analyzing lesions involves the use of dermoscopic imaging. However, this process is highly Subjective as it is dependent on the proficiency levels and experience of the specialist. Our research aims at minimizing the level of uncertainty and Subjectivity in human assessment. We propose a deep learning approach to model lesion patterns with the goal of performing automated melanoma detection and lesion segmentation from skin images. We use an ensemble of deep learning models to combine multiple hypothesis into a single decision point. This mimics the real world approach where the specialist will typically consult other specialists to cross reference and double check their …",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:Y0pCki6q_DkC,,1
479,An approach to learn hand movements for robot actions from human demonstrations,"We present an approach to learn and generate movements for robot actions from human demonstrations using Dynamical Movement Primitives (DMPs) framework. The human hand movements are recorded by a motion tracker using a Kinect sensor with a color-marker glove. We segment an observed movement into simple motion units which are called as motion primitives. Then, each motion primitive will be encoded by DMPs models. These DMPs models are used to generate a desired movement by from learning a sample movement with the ability of generalization and adaption to new situation as the change of a desired goal. We extend standard DMPs for multi-dimensional data including the hand 3D position as control signal for movement trajectory, the hand orientation representation as control signal for robot end-effector orientation, and the distance between two fingers as control signal for opening/closing …",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:2osOgNQ5qMEC,,1
480,A proposal of extracting of motion primitives by analyzing tracked data of hand motion from human demonstration,"This paper presents a proposal to achieve motion primitives in the execution of manipulation actions from human demonstration. Human hand motion contains the most important information in the execution of manipulation actions. We design a method by using Kinect sensor to capture hand motion of each demonstrated action with a three-color-marker glove. The hand motion tracking data is calculated with three types: hand 3D position, hand orientation and hand states. Then, these tracked data is segmented to extract the motion primitives which then are used for building robot program that executes the action. We categorize three types of motion primitives including translation, rotation and state changing. In this study, we combine segmentation techniques based on mean square velocity and the change of hand state to extract the primitives of translation and state changing in the execution of action 'pick a cup' …",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:d1gkVwhDpl0C,,1
481,"Firm size, business sector and quality of accounting information systems: Evidence from Vietnam","This paper increases the understanding of the quality of accounting information systems in emerging economies, using data from Vietnam as an example. The quality of accounting information systems is a measure combining system quality and information quality. It is important to figure out what aspects of this measure are critical for business to enhance firm performance. This research investigates the level of accounting information system quality and examines the relationships between system quality and firm size, information quality and firm size, system quality and business sector as well as information quality and business sector, respectively. We employed descriptive statistics to illustrate the quality of accounting information systems and One-Way ANOVA to test four hypotheses. The descriptive statistics results demonstrate the level of system quality and information quality, in general, is not excellent. And there are differences in system quality and information quality in each business sector groups and firm size groups. The test result highlights a relationship between system quality and firm size but there are no links between information quality and firm size, system quality and business sector, and information quality and business sector. In conclusion, the paper extends the literature of the quality of accounting information systems and assists state agencies and executives to have a framework to improve the business performance as well.",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:KlAtU1dfN6UC,,1
482,Extraction of actions and objects from instruction manual for executable robot planning,"Home service robots are expected to execute many tasks for human in daily life. Many tasks are often related to handling or operating a certain home appliance. This research aims to analyze the instructions from the user manuals of some types of home appliances to extract actions and objects which then can be understood by robot and used for building the working robot plan. We propose a processing procedure including grammatical parsing the instruction sentences by using a parser, then extracting the pairs of action and object from parse tree of each instruction sentence. A manual comparison is executed to evaluate the extraction results. The suitable extracted actions will be selected for building executing plans for a robot in the next stage.",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:9yKSN-GCB0IC,,1
483,An optimal machine learning model for breast lesion classification based on random projection algorithm for feature optimization,"The purpose of this study is to develop a machine learning model with the optimal features computed from mammograms to classify suspicious regions as benign and malignant. To this aim, we investigate the benefits of implementing a machine learning approach embedded with a random projection algorithm to generate an optimal feature vector and improve classification performance. A retrospective dataset involving 1,487 cases is used. Among them, 644 cases depict malignant lesions, while the rest 843 cases are benign. The locations of all suspicious regions have been annotated by radiologists before. A computer-aided detection scheme is applied to pre-process the images and compute an initial set of 181 features. Then, three support vector machine (SVM) models are built using the initial feature set and embedded with two feature regeneration methods, namely, principal component analysis and random …",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:kNdYIx-mwKoC,,1
484,Programming everyday task using primitive skills and generative model of movement demonstrated by human,"This manuscript describes a method to program everyday manipulation tasks for the robots from human demonstration by using a movement generating model and primitive skills. The hand movement in the demonstrated task is recorded by using a hand motion tracker with a Kinect camera and a color-marker glove. The recorded movement is segmented to sub-actions and then mapped to primitive skills which can be built independently with the task. To adapt with the change of the goal position, Dynamic Movement Primitives model was applied to generate movement trajectory which follows the demonstrated trajectory. In experiment, we considered the task “dispensing water” from a water thermos pot performed by a robot arm. We implemented DMPs model for the sub-movement such as `approaching object' to confirm the adaptation to new goal of movement trajectory. We proposed a list of necessary common …",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:UeHWp8X0CEIC,,1
485,Extracting actions from instruction manual and testing their execution in a robotic simulation,"Service robots are expected to execute household manipulation tasks on daily basis. These tasks are often related to the operation of home appliances and can be decomposed into specific manipulation actions such as pick up a cup, place a cup, press a button, turn a knob, open or close a cover, etc. This research proposes to use task instructions collected from manuals of household appliances to extract a series of actions that a robot needs to perform in order to accomplish the task. We describe the extraction method of action and object from these task instructions including syntax parsing of sentences and searching the pairs of action and object in each parse tree based on part-of-speech. The extracted actions then will be executed by a robot. We apply a simulation environment with ROS, Gazebo simulator and a virtual robot PR2 for testing their execution. In experiments, we implement the extraction method and evaluate the extracted results of actions and object. In addition, we test the execution of actions pick and place in the proposed simulation environment.",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:u-x6o8ySG0sC,,1
486,"Safety and Immunogenicity of Nanocovax, a SARS-CoV-2 Recombinant Spike Protein Vaccine",Background: Nanocovax is a recombinant severe acute respiratory syndrome coronavirus 2 subunit vaccine composed of full-length prefusion stabilized recombinant SARS-CoV-2 spike glycoproteins (S-2P) and aluminum hydroxide adjuvant.,30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:Zph67rFs4hoC,,1
487,Automated Grading in Diabetic Retinopathy Using Image Processing and Modified EfficientNet,"We present our approach in achieving the Quadratic Weighted Kappa (QWK) score of 0.90 on the retinal image dataset from the APTOS 2019 Blindness Detection Kaggle challenge. We analysed various image preprocessing techniques then classified the images with a modified EfficientNet deep learning model. Our image preprocessing techniques helped to bring out the cell loss to the retina, highlight blood vessels, and centering the retina. We found that subtracting the average local color using a Gaussian mask was the most effective preprocessing technique, improving the QWK score by 0.03. We modified the EfficientNet-B5 network with the Batch Normalization layers replaced with Group Normalization and trained the network using the Rectified Adam (RAdam) optimizer. Group Normalization was found to do better for the batch size of 4 and RAdam trained the network better than Adam. This led to …",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:0EnyYjriUFMC,,1
488,Zika preparedness and response in Viet Nam,"The ZIKV PHEIC allowed Viet Nam to test its newly enhanced response system and identify areas that needed to be modified or expanded. Several lessons were learnt. First, while the GDPM led surveillance efforts at the national level, the mode of implementation was determined at the regional level. This practice led to variations in surveillance strategies in different regions, making it challenging to relate the data. In future responses, it would be useful to create a unified implementation plan for surveillance that could be consistently applied throughout the country. Second, while co-opting an existing surveillance system meant a new surveillance system could be established rapidly, it resulted in the creation of an inadequate response. In this response, the dengue surveillance system was initially used as the base for ZIKV surveillance. The dengue system was focused on inpatient surveillance. This turned out to be …",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:ULOm3_A8WrAC,,1
489,Adaptive learning of hand movement in human demonstration for robot action,"This paper describes a process for adaptive learning of hand movements in human demonstration for manipulation actions by robots using Dynamic Movement Primitives (DMPs) framework. The process includes 1) tracking hand movement from human demonstration, 2) segmenting hand movement, 3) adaptive learning with DMPs framework. We implement a extended DMPs model with a modified formulation for hand movement data observed from human demonstration including hand 3D position, orientation and fingers distance. We evaluate the generated movements by DMPs model which is reproduced without changes or adapted to change of goal of the movement. The adapted movement data is used to control a robot arm by spatial position and orientation of its end-effector with a parallel gripper.",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:u5HHmVD_uO8C,,1
490,Programming Everyday Task by Demonstration using Primitive Skills for a Manipulator,"This manuscript presents a method to program everyday manipulation tasks for a robot by human demonstration using primitive skills. The hand movement in the demonstrated task is recorded then segmented into sub-actions. Each sub-action is mapped to primitives skills of the robot. We proposed a list of necessary skills for a manipulator which are common to use in many everyday tasks. In order to adapt with the change of object's location, we applied Dynamic Movement Primitives model for regenerating movement which follows the demonstrated trajectory. In experiment, we considered the task “dispensing water” from a water thermos pot performed by a robot arm to verify the proposed method.",30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:qjMakFHDy7sC,,1
491,"Effects of a comprehensive brain computed tomography deep-learning model on radiologist detection accuracy: a multireader, multicase study",Background: Non-contrast computed tomography of the brain (NCCTB) is commonly used in clinical practice to detect intracranial pathology but is subject to interpretation errors. Machine learning is capable of augmenting clinical decision making and there is an opportunity to apply deep learning to improve the clinical interpretation of NCCTB scans. This retrospective detection accuracy study assessed the performance changes of radiologists assisted by a deep learning model designed to identify many NCCTB clinical ndings and also compared the standalone performance of the model with that of unassisted radiologists.,30,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w_ucG0MAAAAJ&citation_for_view=w_ucG0MAAAAJ:4TOpqqG69KYC,,1
492,Classification de texte et estimation probabiliste par Machine à Vecteurs de Support,"La classification de documents D en classes pré-déterminées Y est simplement présentée comme le problème d’estimation probabiliste de la probabilité a posteriori P (Y/D). Nous présentons ici une méthode basée sur le modèle de Machine à Vecteurs de Support afin de réaliser cette tâche. Il y a deux approches correspondant à deux niveaux de présentation des corpus: en documents et en phrases, que nous avons analysées dans ce défi.",31,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=TmAK4q4AAAAJ&citation_for_view=TmAK4q4AAAAJ:u5HHmVD_uO8C,,1
494,Finding Dense Components in Large-Scale Network Using Randomized Binary Search Tree,"Given a simple undirected graph G=(V, E), the density of a subgraph on vertex set S is defined as a ratio between the number of edges| E (S)| and the number of vertices| S|, where E (S) is the set of edges induced by vertices in S. Finding the maximum density subgraph has become an intense study in recent years, especially in the social network era. Being based on a greedy algorithm that connects with a suitable graph data structure, we have reduced its time complexity by using a randomized binary search tree, also called treap. We make the complexity analysis in both time and memory requirements, including computational experiments in large scale real networks.",31,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=TmAK4q4AAAAJ&citation_for_view=TmAK4q4AAAAJ:UeHWp8X0CEIC,,1
497,Probabilistic Multi-classifier by SVMs from voting rule to voting features,"Let S={(x1, y1),(x2, y2),...,(xm, ym)} be a set of m training examples. We assume that each example xi is drawn from a domain X∈ Rn and each class yi is an integer from the set Y={1,..., k} with k> 2. The posterior probabilities of multiclass problem is a conditional probability of each class y∈ Y given an instance x",31,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=TmAK4q4AAAAJ&citation_for_view=TmAK4q4AAAAJ:9yKSN-GCB0IC,,1
499,Decentralized ciphertext-policy attribute-based encryption schemes for lightweight devices,"In a traditional attribute-based encryption (ABE) system, there is only one central authority who generates and hence knows the secret keys of all users, this problem is known as the key escrow problem. An ABE scheme which can deal with the aforementioned problem is so-called a decentralized (or multi-authority) ABE scheme. There have been several efforts to propose decentralized ABE schemes in the literature, and these schemes can enjoy some interesting properties such as fine-grained access control and full security. However, all of them suffer from two main weaknesses that are large secret key size and slow decryption time. These weaknesses are obviously not desirable for lightweight device applications such as Internet of Things applications. In this paper, we propose two decentralized ciphertext-policy attribute-based encryption schemes. Our proposed schemes have some nice properties such as …",32,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IVJLVc0AAAAJ&citation_for_view=IVJLVc0AAAAJ:TFP_iSt0sucC,,1
501,Building a X-ray database for mammography on vietnamese patients and automatic detecting ROI using mask-RCNN,This paper describes the method of building a X-ray database for Mammography on Vietnamese patients that we collected at Hanoi Medical University Hospital. This dataset has 4664 images (Dicom) corresponding to 1161 standard patients with uniform distribution according to BIRAD from 0 to 5. This paper also presents the method of detecting Region of Interest (ROI) in mammogram based on Mask R-CNN architecture. The method of determining the ROI for accuracy mAP@0.5 = 0.8109 and the accuracy of classification BIRAD levels is 58.44%.,32,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IVJLVc0AAAAJ&citation_for_view=IVJLVc0AAAAJ:_Qo2XoVZTnwC,,1
502,Decentralized broadcast encryption schemes with constant size ciphertext and fast decryption,"Broadcast encryption (BE) allows a sender to encrypt a message to an arbitrary target set of legitimate users and to prevent non-legitimate users from recovering the broadcast information. BE has numerous practical applications such as satellite geolocation systems, file sharing systems, pay-TV systems, e-Health, social networks, cloud storage systems, etc. This paper presents two new decentralized BE schemes. Decentralization means that there is no single authority responsible for generating secret cryptographic keys for system users. Therefore, the scheme eliminates the concern of having a single point of failure as the central authority could be attacked, become malicious, or become unavailable. Recent attacks have shown that the centralized approach could lead to system malfunctioning or to leaking sensitive information. Another achievement of the proposed BE schemes is their performance characteristics that make them suitable for environments with light-weight clients, such as in Internet-of-Things (IoT) applications. The proposed approach improves the performance over existing decentralized BE schemes by simultaneously achieving constant size ciphertext, constant size secret key and fast decryption.",32,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IVJLVc0AAAAJ&citation_for_view=IVJLVc0AAAAJ:isC4tDSrTZIC,,1
503,Compacting ciphertext in multi-channel broadcast encryption and attribute-based encryption,"Multi-channel broadcast encryption (MCBE) and Attribute-based encryption (ABE) are important primitives used in a broad range of concrete application scenarios such as Pay-TV, e-Health, Cloud Storage, Blockchain and so on. The former allows a sender to efficiently send different messages to arbitrarily chosen sets of users (target sets) at the same time, while the latter supports more sophisticated access control by allowing both the encryption and decryption phases to be based on the user's attributes. Recently, Canard el al. at Theoretical Computer Science'18 proposed a new technique for compacting header size in both MCBE and ABE. However, their technique leads to schemes in secret-key setting which has limited applications in practice. In this paper, we propose a new technique also for compacting header size in both MCBE and ABE, but our technique leads to schemes in public key setting which …",32,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IVJLVc0AAAAJ&citation_for_view=IVJLVc0AAAAJ:maZDTaKrznsC,,1
504,Reduced Languages as ω-Generators,"We consider the following decision problem: “Is a rational ω-language generated by a code ?” Since 1994, the codes admit a in terms of infinite words. We derive from this result the definition of a new class of languages, the reduced languages. A code is a reduced language but the converse does not hold. The idea is to “reduce” easy-to-obtain minimal ω-generators in order to obtain codes as ω-generators.",32,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IVJLVc0AAAAJ&citation_for_view=IVJLVc0AAAAJ:mB3voiENLucC,,1
507,One-relation languages and code generators,"We investigate the open problem to characterize whether the infinite power of a given language is generated by an ω-code. In case the given language is a code (i.e. zero-relation language), the problem was solved. In this work, we solve the problem for the class of one-relation languages.",32,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IVJLVc0AAAAJ&citation_for_view=IVJLVc0AAAAJ:k_IJM867U9cC,,1
508,SET DECIPHERABLE LANGUAGES AND GENERATORS,"We investigate the problem to characterize whether the infinite product of a given language  is generated by an -code. Up to now, this problem is open even if language  is a finite language.",32,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IVJLVc0AAAAJ&citation_for_view=IVJLVc0AAAAJ:r0BpntZqJG4C,,1
509,Mapping Potential Planting Areas for Golden Camellias in North Vietnam,"This study mapped potential areas for planting golden camellias in Cao Bang province, North Vietnam. Natural conditions (elevation above sea level, annual precipitation, and annual air temperature), where 6 golden camellia species (Camellia impressinervis, C. kirinoi, C. megasepala, C. tuyenquangensis, C. hamyenensis, and C. tienii) naturally distribute, were used for mapping. Three map layers (topography, precipitation, and air temperature) were used. Each main condition was classified into 3 levels for planting,“not suitable”,“suitable”, and “very suitable”. The results indicated that 1,970 ha, accounting for 0.3% of the total land area of Cao Bang province, was classified as “very suitable” for planting golden camellias.“Suitable” areas counted for 62.7%, and “not suitable” areas accounted for 37% of the total land area, respectively. Mapping suitable planting areas is the 1st step for the successful planting of any species. It is recommended that golden camellias should be first planted in very suitable areas, after which planting may be extended to some suitable areas.",32,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IVJLVc0AAAAJ&citation_for_view=IVJLVc0AAAAJ:iH-uZ7U-co4C,,1
510,Building a X-ray Database for Mammography on Vietnamese Patients and automatic Detecting ROI Using,This paper describes the method of building a X-ray database for Mammography on Vietnamese patients that we collected at Hanoi Medical University Hospital. This dataset has 4664 images (Dicom) corresponding to 1161 standard,32,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IVJLVc0AAAAJ&citation_for_view=IVJLVc0AAAAJ:bEWYMUwI8FkC,,1
511,On the Performance of a Simple Approximation Algorithm for the Longest Path Problem,"The longest path problem is known to be NP-hard. Moreover, they cannot be approximated within a constant ratio, unless . The best known polynomial time approximation algorithms for this problem essentially find a path of length that is the logarithm of the optimum.",32,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IVJLVc0AAAAJ&citation_for_view=IVJLVc0AAAAJ:j3f4tGmQtD8C,,1
512,Facial expression recognition using deep convolutional neural networks,"Facial expressions convey non-verbal information between humans in face-to-face interactions. Automatic facial expression recognition, which plays a vital role in human-machine interfaces, has attracted increasing attention from researchers since the early nineties. Classical machine learning approaches often require a complex feature extraction process and produce poor results. In this paper, we apply recent advances in deep learning to propose effective deep Convolutional Neural Networks (CNNs) that can accurately interpret semantic information available in faces in an automated manner without hand-designing of features descriptors. We also apply different loss functions and training tricks in order to learn CNNs with a strong classification power. The experimental results show that our proposed networks outperform state-of-the-art methods on the well-known FERC-2013 dataset provided on the Kaggle …",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:4TOpqqG69KYC,,1
513,People and parcels sharing a taxi for Tokyo city,"This paper introduces a practical hybrid transportation model for Tokyo city that allows a passenger and parcels are handled in a same taxi. We inherit the recent model given by Li et al. in 2014 and make it much more realistic by adding some constraints related to the real-life case. We propose the time-dependent model to facilitate formulating constraints. The feasibility and the efficacy of the model are proved by two proposed heuristic algorithms. Especially, we use the real-case experimental data set recorded by Tokyo-Musen Taxi company. The data set includes more than 20,000 requests per day, more than 4,500 served taxis per day and more than 130,000 crossing points on the Tokyo map. The experimental results are analyzed on various factors such as the total benefit, the accumulating distances during the day, the number of used taxis and the number of shared requests.",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:ufrVoPGSRksC,,1
514,Optimization of survivable mesh long-reach hybrid WDM-TDM PONs,"Long-reach hybrid wavelength-division multiplexing (WDM) and time-division multiplexing (TDM) passive optical networks (PONs) allow deploying access networks for remote service areas with thousands of customers. Typically, several long fiber cables are run between the central office (CO) of the service provider and each service area in order to feed the service area with data flows. In the service area, array waveguide gratings (AWGs) multiplex and demultiplex wavelengths; then, splitters split wavelengths in order to serve multiple optical network units. This paper proposes use of a mesh topology in service areas, i.e., AWGs can feed each other. This architecture has two main advantages. First, mesh linkages between AWGs make the network structure more robust with a high possibility of integrating survivable schemes. Second, fewer fibers are required between the CO and service areas, leading to a …",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:u-x6o8ySG0sC,,1
515,ECO-generation for p-generalized Fibonacci and Lucas permutations,"Using the ECO method, we give succession rules for p-generalized Fibonacci and Lucas sequences. We provide sets of pattern-avoiding permutations which are enumerated by the p-generalized Fibonacci and Lucas sequences. Finally we give generating algorithms in Constant Amortized Time for these sets.",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:d1gkVwhDpl0C,,1
516,Simulated annealing for the assembly line balancing problem in the garment industry,"Assembly line balancing (ALB) is the problem of assigning a set of tasks to workstations, such that the precedence relations among the tasks are satisfied to optimize different objectives. ALB is an important task for the garment industry. When the product model is changed, the assembly line must be balanced again. There are huge investigations on ALB including different objectives such as minimizing the number of workstations, minimizing the balance delay and minimizing the cycle time. In this paper, the objective of ALB is to minimize the number of workstations for a given cycle time with respect to some constraints on the order of precedence relations among tasks, on the number of tasks and machine types in each group of tasks. We first use the greedy strategy to find an initial solution, then apply the Simulated Annealing (SA) to find the best solutions possible. The proposed algorithms have been evaluated on …",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:L8Ckcad2t8MC,,1
517,Facial smile detection using convolutional neural networks,"Facial expression analysis plays a key role in analyzing emotions and human behaviors. Smile detection is a special task in facial expression analysis with various potential applications such as photo selection, user experience analysis, smiling payment and patient monitoring. Conventional approaches often extract low-level face descriptors and detect smile based on a strong binary classifier. In this paper, we propose an effective architecture of Convolutional Neural Networks (CNNs) to detect smile in real-time speed with high accuracy. The experimental results show that our proposed network outperforms recent state-of-the-art methods.",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:M3ejUd6NZC8C,,1
518,An ant colony optimization algorithm for solving group steiner problem,"Group Steiner Problem (GSP) is an important generalization of some basic NP-hard problems. Many complex real-world applications require solving the GSP in graphs modeling the topology of the given problem, such as: the design of Very Large Scale Integration (VLSI) circuits, the design of a minimal length irrigation network, and routing problems for wireless sensor networks. We show our design of a new algorithm based on an Ant Colony Optimization model to solve the GSP in general graphs. Our experimental results show that our method strongly outperforms the best other heuristic methods for GSP.",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:Tyk-4Ss8FVUC,,1
519,Exhaustive generation for permutations avoiding (colored) regular sets of patterns,"Despite the fact that the field of pattern avoiding permutations has been skyrocketing over the last two decades, there are very few exhaustive generating algorithms for such classes of permutations. In this paper we introduce the notions of regular and colored regular set of forbidden patterns, which are particular cases of right-justified sets of forbidden patterns. We show the (colored) regularity of several sets of forbidden patterns (some of them involving variable length patterns) and we derive a general framework for the efficient generation of permutations avoiding them. The obtained generating algorithms are based on succession functions, a notion which is a byproduct of the ECO method introduced in the context of enumeration and random generation of combinatorial objects. For some classes of permutations falling under our general framework, the corresponding counting sequences are classical in …",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:_kc_bZDykSQC,,1
520,Neural machine translation between Vietnamese and English: an empirical study,"Machine translation is shifting to an end-to-end approach based on deep neural networks. The state of the art achieves impressive results for popular language pairs such as English-French or English-Chinese. However for English-Vietnamese the shortage of parallel corpora and expensive hyper-parameter search present practical challenges to neural-based approaches. This paper highlights our efforts on improving English-Vietnamese translations in two directions:(1) Building the largest open Vietnamese-English corpus to date, and (2) Extensive experiments with the latest neural models to achieve the highest BLEU scores. Our experiments provide practical examples of effectively employing different neural machine translation models with low-resource language pairs.",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:mVmsd5A6BfQC,,1
521,Towards state-of-the-art English-Vietnamese neural machine translation,"Machine translation is one of the most challenging topics in natural language processing. The common approaches to machine translation base on either statistical or rule-based methods. Rule-based translation analyzes sentence structures, requires extensive lexicons with morphological, syntactic, semantic information, and large sets of manually created rules. Statistics-based translation faces the challenge of collecting bilingual text corpora, which is particularly difficult for low resource language pairs as English-Vietnamese. This research aims at building state-of-the-art English-Vietnamese machine translation. Our contribution includes:(1) an enormous effort in collecting training dataset,(2) adaptation of current neural machine for English-Vietnamese translation,(3) an experimental result suggested the unnecessary of Vietnamese word segmentation as a common pre-processing step. Our model achieves a …",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:5nxA0vEk-isC,,1
522,Efficient maximum matching algorithms for trapezoid graphs,"Trapezoid graphs are intersection graphs of trapezoids between two horizontal lines. Many NP-hard problems can be solved in polynomial time if they are restricted on trapezoid graphs. A matching in a graph is a set of pairwise disjoint edges, and a maximum matching is a matching of maximum size. In this paper, we first propose an O (n (log n) 3) algorithm for finding a maximum matching in trapezoid graphs, then improve the complexity to O (n (log n) 2). Finally, we generalize this algorithm to a larger graph class, namely k-trapezoid graphs. To the best of our knowledge, these are the first efficient maximum matching algorithms for trapezoid graphs.",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:eQOLeE2rZwMC,,1
523,An improvement of the overlap complexity in the spaced seed searching problem between genomic DNAs,"In homology search, finding optimal multiple spaced seeds in genomic DNA sequences is NP-hard but even finding good ones is very difficult. The exponential-time algorithm PatternHunter use optimal spaced seeds to increase both the sensitivity and the speed of homology search. The overlap complexity measure based on the overlaps between hits of a multiple seed are well correlated with sensitivity but is computable in polynomial time. Based on overlap complexity, we have improved polynomial-time algorithms to provide better multiple seeds. Our experimental results shows that these improvements significantly run faster and make better quality of spaced seeds than previous algorithms in almost all test cases.",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:zYLM7Y9cAGgC,,1
524,Permutations weakly avoiding barred patterns and combinatorial bijections to generalized Dyck and Motzkin paths,"We explore the weak avoidance of barred patterns in permutations introduced recently by J.-L. Baril. We give its general properties in comparison with the barred pattern avoidance. By showing explicit bijections, we revisit Dyck paths with no peak at height p, Dyck paths with no u d… d u and Motzkin paths via weakly avoiding permutations in S n (132).",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:UeHWp8X0CEIC,,1
525,An information content based partitioning method for the anatomical ontology matching task,"Anatomy ontology matching has been attracting a lot of interest and attention of researchers, especially, biologists, medics and geneticists. This is a very difficult task due to the huge size of anatomy ontologies. Despite the fact that many ontology matching tools have been proposed so far, most of them achieve good results only for small size ontologies. In a recent survey [22], the authors pointed out that the large scale ontology matching problem still presents a real challenge because it is a time consuming and memory intensive process. According to state of the art works, the authors also state that partitioning large scale ontology is a promising solution to deal with this issue. Therefore, in this paper, we propose a partitioning approach to break up the large matching problem into smaller matching subproblems. At first, we propose a method to semantically split anatomy ontology into groups called clusters. It relies …",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:W7OEmFMy1HYC,,1
526,A time-dependent model with speed windows for share-a-ride problems: A case study for Tokyo transportation,"This paper introduces a new fully time-dependent model of a public transportation system in the urban context that allows sharing a taxi between one passenger and parcels with speed widows consideration. The model contains many real-life case features and is presented by a mathematical formulation. We study both static and dynamic scenarios in comparison to traditional strategies, i.e., the direct delivery model. Moreover, we classify speed windows by different zones and congestion levels during a day in the urban context. Different speed windows induce the dynamic graph model for road networks and make the problem much more difficult to solve. Because of the complex model, the preprocessing steps on data as well as on dynamic graphs are very important. We use a greedy algorithm to initiate the solution and then use some local search techniques to improve the solution quality. The experimental data …",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:UebtZRa9Y70C,,1
527,A practical dynamic share-a-ride problem with speed windows for Tokyo city,"This paper deals with a new time-dependent model of public transportation system in the urban context that allows sharing a taxi between a passenger and parcels with speed widows consideration. We solve a realistic dynamic scenario in which requests are accepted or declined at the time of their calls. We classify vehicle speeds by different time windows during a day. Different speed windows induce the dynamic graph model for road networks and make the problem much more difficult to solve. Because of the complex model, the preprocessing steps on data as well as on dynamic graphs are very important. We use a greedy algorithm to initiate the solution and then use some local search techniques to improve the solution quality. The experimental data set is recorded by Tokyo-Musen Taxi company. The data set includes more than 20,000 requests per day, more than 4,500 served taxis per day and more than …",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:roLk4NBRz8UC,,1
528,A  -Approximation Algorithm for Target Coverage Problem in Mobile Air Quality Monitoring Systems,"So far, air quality monitoring is usually handled by monitoring stations located at fixed locations. However, due to the cost of installation, deployment, and operation, the number of monitoring stations deployed is often tiny; thus, the monitored area is limited. To deal with this problem, in this paper, we consider a mobile air quality monitoring system that relies on sensors mounted on buses to broaden the monitoring area. Specifically, we investigate the optimal buses to place the sensors as well as the optimal monitoring timings to maximize the number of critical regions that are monitored. We mathematically formulate the targeted problem and prove its NP-hardness. Then, we exploit the greedy and dynamic programming approaches to propose a polynomial-time 1/2-approximation algorithm. We use the data of real bus routes in Hanoi, Vietnam, for the experimentation and show that the proposed algorithm …",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:mB3voiENLucC,,1
529,An -Approximation Algorithm for Maximizing Coverage Capability in Mobile Air Quality Monitoring Systems,"In this paper, we focus on broadening the monitoring area of a mobile air quality monitoring system, in which the sensors mounted on buses. In particular, we investigate the optimal buses to place the sensors and the optimal monitoring timings to maximize the number of monitored critical regions. We mathematically formulate the targeted problem. Then, we leverage the greedy approach to propose a polynomial-time [(e-1)/(2e-1)] approximation algorithm. We use the data of real bus routes in Hanoi, Vietnam, for the experimentation and show that the proposed algorithm guarantees an average performance ratio of 63.87%.",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:hFOr9nPyWt4C,,1
530,Cycle time enhancement by simulated annealing for a practical assembly line balancing problem,"In the garment industry, assembly line balancing is one of the most significant tasks. To make a product, a manufacturing technique called assembly line is utilized, where components are assembled and transferred from workstation to workstation until the final assembly is finished. Assembly line should always be as balanced as possible in order to maximize efficiency. Different types of assembly line balancing problems were introduced along with many proposed solutions. In this paper, we focus on an assembly line balancing problem where the upper bound of the number of workers is given, tasks and workers have to be grouped into workstations so that the cycle time is minimized, the total number of workers is minimized and balance efficiency is maximized. With unfixed number of workstations and other various constraints, our problem is claimed to be novel. We propose three different approaches: exhaustive search, simulated annealing and simulated annealing with greedy. Computational results affirmed that our SA algorithm performed extremely good in terms of both accuracy and running time. From these positive outcomes, our algorithms clearly show their applicability potential in practice.",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:4JMBOYKVnBMC,,1
531,Arbres de génération et génération exhaustive,"Les travaux présentés dans cette thèse sont le fruit de recherches menées au sein de l'équipe Algorithmique Combinatoire du LE2I, Université de Bourgogne, France. L'objectif de la thèse est d'explorer systématiquement la technique des arbres de génération dans le contexte de la génération exhaustive d'objets combinatoires. Plus précisément, elle s' appuie sur la méthode d'énumération d'objets combinatoires ECO (Enumerating Combinatorial Objects) proposée par Barcucci et al. 1999. On s' intéresse à la génération exhaustive d'objets combinatoires basée sur les arbres de génération pour ces classes, afin d'engendrer des algorithmes efficaces, dans une représensentation et un ordre naturel. Dans un premier temps, nous présentons une étude pour la génération des mots de Dyck et des classes relatives dans le chemin du réseaux Z2. Une approche unifiée est proposée en imposant une restriction de la méthode ECO et une restriction des règles de croissance afin de générer exhaustivement et efficacement ces mots. Nous présentons ensuite une nouvelle technique, la fonction de succession, qui peut être vue comme un raffinement des règles de succession et peut-être considérée comme leur contrepartie algorithmique. En utilisant cette technique, nous développons des algorithmes de génération pour de larges classes de permutations à motifs exclus. Les dernières parties sont des études approfondies sur des classes connues: classes de Fibonacci et Lucas généralisées et classes de compositions d'entiers. De nouvelles règles de succession sont proposées et de nouvelles classes de permutations à motifs exclus sont construites …",36,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7Bpp8U0AAAAJ&citation_for_view=7Bpp8U0AAAAJ:_FxGoFyzp5QC,,1
533,Generalised Theory on the Effects of Sampling Frequency on GNSS Code Tracking,"Synchronisation of the received Pseudorandom (PRN) code and its locally generated replica is fundamental when estimating user position in Global Navigation Satellite System (GNSS) receivers. It has been observed through experiments that user position accuracy decreases if sampling frequency is an integer multiple of the nominal code rate. This paper provides an accuracy analysis based on the number of samples and the residual code phase of each code chip. The outcomes reveal that the distribution of residual code phases in the code phase range [0, 1/n s ), where n s is the number of samples per code chip, is the root cause of accuracy degradation, rather than the ratio between sampling frequency and nominal code rate. Doppler frequencies, coherent integration periods, front-end filter bandwidths and received Carrier to Noise ratios (C/N 0) also influence receiver accuracy. Also provided are a sampling …",37,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_OY4QEAAAAAJ&citation_for_view=_OY4QEAAAAAJ:qjMakFHDy7sC,,1
534,GNSS receiver implementations to mitigate the effects of commensurate sampling frequencies on DLL code tracking,"The sampling frequency of a digitized intermediate frequency signal has a strong effect on the measurement accuracy of Global Navigation Satellite System (GNSS) receivers. The delay-locked loop tracking error is significant when the sampling frequency is an integer multiple of the code chipping rate, the so-called commensurate sampling frequency, and the number of distinct instantaneous residual code phases is low. This results in distortions of the correlation shape and discriminator functions that lead to a significant accuracy degradation. These effects are most pronounced when the sampling frequency is low. Notwithstanding, it is generally good for receivers to keep the sampling frequency to a minimum owing to the processing load and power consumption. It creates a challenge for existing GNSS signal processing techniques. Random, sine and sawtooth jitters have been found to mitigate these …",37,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_OY4QEAAAAAJ&citation_for_view=_OY4QEAAAAAJ:2osOgNQ5qMEC,,1
535,A novel design of low power consumption GPS positioning solution based on snapshot technique,"Typical Global Positioning System (GPS) receivers, which are increasingly used in many areas, consume too much energy due to heavy processing in a long time. This makes them not useful and sometimes even impractical for applications with high mobility or operating in obstructed regions where the GPS signal is present in short time. To address the problem, we propose a new design for a fast GPS receiver which is able to achieve the result with several milliseconds of logged GNSS data. Thanks to the open policy of GPS navigation messages, the logged data can be post-processed to achieve a good estimate of position. Compared with traditional commercial receivers, the power-consumption of our proposed design is about 23% of a typical GPS receiver.",37,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_OY4QEAAAAAJ&citation_for_view=_OY4QEAAAAAJ:UeHWp8X0CEIC,,1
537,Recent results in receiving and decoding signals from the Beidou system,"Since December 27, 2012, the Beidou Navigation Satellite System officially started to operate. This event is a great opportunity for researchers in South East of Asia to receive and analyze the Beidou signals. After the official statement, the researchers at NAVIS centre monitored the broadcasted signal by using NAVISOFT- our Software Radio Receiver. This paper shows the analysis on the navigation message that was broadcasted by the Beidou satellites on the B1I bandwidth. In general, we were able to observe a valid ephemeris data on visible satellites. The successful PVT computation by using combinations of GEO and MEO/IGSO in static condition through code-phase measurements is indicated in this paper.",37,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_OY4QEAAAAAJ&citation_for_view=_OY4QEAAAAAJ:Tyk-4Ss8FVUC,,1
538,A dynamically configurable decimator for a GNSS baseband receiver,"Modern global navigation satellite system (GNSS) front ends sample the received radio-frequency signal at an intermediate frequency (IF) and, then, downconvert it to baseband in the digital domain. A locally generated spreading code is subsequently correlated with the baseband signal at the sampling frequency. This can mean that the receiver correlates at an unnecessarily high frequency (sometimes more than four times the chipping rate). The power consumption is, thus, high. If the baseband signal is downsampled after appropriate filtering, the correlator can process at a fraction of the sampling frequency. The power dissipation is considerably reduced as a result. Furthermore, by resampling at an integer submultiple of the sampling frequency, not only is downsampling jitter eliminated, but also architectural configurability in the correlator is possible, allowing further reductions in power and resource …",37,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_OY4QEAAAAAJ&citation_for_view=_OY4QEAAAAAJ:IjCSPb-OGe4C,,1
540,Performance Assessment of the New L2C CNAV GPS Signal,"L2C is a relatively new GPS signal which is part of the GPS modernization program. In April 2014, the Air Force began broadcasting a valid L2C CNAV navigation message. When combined with the L1 C/A legacy signal in a dual-frequency receiver, L2C enables interesting features such as improved reliability, PVT solution accuracy and robustness in weak signal environments. Based on L1 C/A signal, our software receiver has been adapted to the L2C signal structure. This paper suggests methods for acquisition, tracking and PVT computation for the L2C signal exploiting the benefits of double frequency receiver.",37,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_OY4QEAAAAAJ&citation_for_view=_OY4QEAAAAAJ:9yKSN-GCB0IC,,1
541,GNSS-Based Solutions for Road Applications in Vietnam,"Navigation Satellite Systems is a cross cutting technology that is used in many fields of modern life. Precise determination of position, velocity and time is particularly important in road applications and related technological solutions, which enable considerable improvements in traffic management and monitoring. In Vietnam, public transport vehicles and lorries must be equipped with black boxes to enable traffic management and control. This requirement and other safety needs, along with the increase of malicious tampering of black boxes to prevent their correct functioning, have motivated the development of some technological solutions and research lines. Some of the achieved results have been already applied in the real Vietnamese environment. ",37,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_OY4QEAAAAAJ&citation_for_view=_OY4QEAAAAAJ:_FxGoFyzp5QC,,1
542,Positioning precision of GPS/Galileo integration in Vietnam,"The design of Galileo has been conceived so as to facilitate its possible joint use with GPS thus contributing to a favorable multi-GNSS environment. Evaluating the performances of such multi-GNSS combination is crucial for end users to assess the quality of obtainable PVT and the easiness of interoperability of the two systems. In this paper, we evaluate the GPS/Galileo integration performances in terms of estimated precision in position determination and in terms of availability of the service. Results obtained from real data are presented and show that there is an improvement of both these criteria when the two systems are jointly used.",37,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_OY4QEAAAAAJ&citation_for_view=_OY4QEAAAAAJ:ufrVoPGSRksC,,1
543,An ultra-low-cost antenna array frontend for GNSS application,"Antenna array frontends play a key role in grabbing GNSS digitalized signal and providing the input data to the signal processing stage. However, the existing antenna array frontends for GNSS application are often bulky and costly. In this paper, a low-cost and compact antenna array frontend is proposed. Besides, a possible solution to estimate the clock drift raised by the internal architecture of the hardware components is also introduced. The performance evaluation of the proposed frontend shows that the frontend is effectively applicable for GNSS applications.",37,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_OY4QEAAAAAJ&citation_for_view=_OY4QEAAAAAJ:Y0pCki6q_DkC,,1
552,A new cache replacement algorithm for last-level caches by exploiting tag-distance correlation of cache lines,"Cache memory plays a crucial role in determining the performance of processors, especially for embedded processors where area and power are tightly constrained. It is necessary to have effective management mechanisms, such as cache replacement policies, because modern embedded processors require not only efficient power consumption but also high performance. Practical cache replacement algorithms have focused on supporting the increasing data needs of processors. The commonly used Least Recently Used (LRU) replacement policy always predicts a near-immediate re-reference interval, hence, applications that exhibit a distant re-reference interval may perform poorly under LRU replacement policy. In addition, recent studies have shown that the performance gap between LRU and theoretical optimal replacement (OPT) is large for highly-associative caches. LRU policy is also susceptible to …",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:d1gkVwhDpl0C,,1
553,A dynamic CTA scheduling scheme for massive parallel computing,"Recent computing devices execute massive parallel data requiring huge computing hardware. To satisfy increasing computing need, GPUs providing powerful computational capability are employed to execute both graphics and general-purpose applications (GPGPUs). In the GPGPU, executing multiple applications together can increase the data parallelism, resulting in high resource utilization. Improving the resource utilization of the GPGPU can increase the GPGPU performance. However, various kinds of applications have different execution time depending on their workload sizes. Therefore, if one application is completed earlier than the other ones, resource underutilization problem may happen because the hardware resource allocated for the early completed application becomes idle. In this work, a CTA-aware dynamic streaming multiprocessors scheduling scheme is proposed for multiple …",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:zYLM7Y9cAGgC,,1
554,Early miss prediction based periodic cache bypassing for high performance GPUs,"The aim of the hierarchical cache memories that are equipped for GPUs is the management of irregular memory access patterns for general purpose workloads. The level-1 data cache (L1D) of the GPU plays an important role for its ability in the provision of high bandwidth and low-latency data accesses. Unfortunately, the GPU L1D may become a performance bottleneck due to facing many performance challenges such as cache contention and resource congestion. These critical issues come from a large number of simultaneous requests from the SIMT cores to the limited-capacity L1D. We observe that many applications have a large number of requests with a very low reuse probability, resulting in the GPU performance degradation. To overcome these challenges, we propose an efficient cache bypassing mechanism that can periodically filter the access stream and make an accurate bypassing decision to …",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:Tyk-4Ss8FVUC,,1
555,Enhancing Matrix Multiplication With a Monolithic 3-D-Based Scratchpad Memory,"Convolutional neural networks (CNNs) are one of the most popular machine learning algorithms. The convolutional layers, which account for the most execution time of CNNs, are implemented with matrix multiplication because the convolution operation performs dot products between filters and local regions of the input. On the other hand, GPUs with thousands of cores were proven to significantly accelerate matrix multiplication, compared to CPUs with a limited number of cores, especially for large matrices. However, the current memory architecture allows only one row access at a time so that multiple accesses are necessary to read the column data of the second matrix, thus slowing down matrix multiplication. In this study, we adopt the monolithic 3-D integration for the GPU scratchpad memory, called monolithic 3-D integration (M3D) scratchpad memory (SPM), to enhance matrix multiplication. The M3D SPM allows …",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:UebtZRa9Y70C,,1
556,A novel warp scheduling scheme considering long-latency operations for high-performance GPUs,"Graphics processing units (GPUs) have become one of the best platforms for exploiting the plentiful thread-level parallelism of applications. However, GPUs continue to underutilize their hardware resources for optimizing the performance of numerous general-purpose applications. One primary reason for this is the inefficiency of existing warp schedulers in hiding long-latency operations such as global loads and stores. This study proposes a long-latency operation-based warp scheduler to improve GPU performance. In the proposed warp scheduler, warps are partitioned into different pools based on the characteristics of instructions that are subsequently executed. Specifically, this warp scheduler uses warps that are likely waiting for long-latency operations for a guiding role. Meanwhile, other warps perform filling roles (i.e., to overlap the latencies caused by the guiding warps). Our experimental results …",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:roLk4NBRz8UC,,1
557,Application characteristics-aware sporadic cache bypassing for high performance GPGPUs,"Modern graphics processing units (GPUs) with massive parallel architecture can boost the performance of both graphics and general-purpose applications. With the support of new programming tools, GPUs have become one of the most attractive platforms in the exploitation of the high thread-level parallelism. In the recent GPUs, hierarchical cache memories have been employed to support irregular memory-access patterns. However, the L1 data cache exhibits a poor efficiency in GPUs, and this is mainly due to the cache contention and the resource congestion. This paper shows that the L1 data cache does not always positively impact applications in terms of the performance; in fact, many applications are even slowed down due to the use of the L1 data cache. In this paper, a novel cache bypassing mechanism (CARB) is proposed to increase the efficiency of the GPU cache management and to improve the GPU …",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:Y0pCki6q_DkC,,1
558,Exploring the relation between monolithic 3D L1 GPU cache capacity and warp scheduling efficiency,"The warp scheduler plays an important role in the GPU for efficient utilization of hardware resources. However, the efficiency of the warp scheduler is often limited by the L1 cache (especially, L1 data cache) capacity; providing large capacity for an L1 cache is challenging due to the increased latency. In this paper, we adopt Monolithic 3D (M3D) technology to design a large capacity L1 cache for GPU performance enhancement, not deteriorating the latency. Our evaluation results show that the M3D L1 cache improves GPU performance by 2.18~2.24× on average, compared to the 2D conventional L1 cache.",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:YsMSGLbcyi4C,,1
559,NTB branch predictor: dynamic branch predictor for high-performance embedded processors,"Branch prediction accuracy becomes more crucial in high-performance embedded processors. The importance of branch prediction in embedded processors continues to grow in the future. Many branch predictors have been proposed to alleviate the performance penalty due to branch mispredictions. However, recent embedded processors still have problems in increasing the branch prediction accuracy. This paper proposes number of taken branch instructions (NTB) branch predictor, a new dynamic branch predictor for high-performance embedded processors. The NTB branch predictor utilizes two-bit saturating counters in the pattern history table based on the information about the number of taken-branches in the global branch history. The proposed NTB branch predictor achieves improved accuracy by making use of longer branch history with no hardware overhead, because hardware resources for …",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:u-x6o8ySG0sC,,1
560,CTA-aware dynamic scheduling scheme for streaming multiprocessors in high-performance GPUs,"GPGPUs can provide powerful computational capability and are employed to execute both graphics and general-purpose applications. Hardware resource utilization is one of the most important factors in determining the GPGPU performance. For GPGPUs, multiple-application execution can increase the data parallelism, resulting in high resource utilization. However, applications have different execution time depending on their workload sizes. Therefore, if one application is completed earlier than the other ones, resource underutilization problem may happen because the hardware resource allocated for the early completed application become idle. In this work, a CTA-aware dynamic streaming multiprocessors scheduling scheme is proposed for multiple-application execution in the GPGPU to efficiently manage hardware resources. Compared to the baseline architecture, the proposed CTA-aware …",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:IjCSPb-OGe4C,,1
561,A Novel Last-Level Cache Replace ment Policy to Improve the Performance of Mobile Systems,"Baseline replacement policy which has proven itself over and over again is LRU policy. This algorithm seeks to replace the lines that were used least recently and it performs well by exploiting temporal locality. This work presents a strategy based on access distribution pattern among the lines in the set during the period of two consecutive LRU replacements to help the standard LRU algorithm in deciding the victim blocks more optimally. Our strategy improves the second-level cache miss rates over the pure LRU algorithm by 15% on average, leading to the performance improvement by 4.71%.",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:u5HHmVD_uO8C,,1
562,A Study on L1 Data Cache Bypassing Methods for High-Performance GPUs,"Graphics Processing Units (GPUs) with massive parallel architecture have been widely used to boost performance of both graphics and general-purpose programs. GPGPUs become one of the most attractive platforms in exploiting plentiful thread-level parallelism. In recent GPUs, cache hierarchies have been employed to deal with applications with irregular memory access patterns. Unfortunately, GPU caches exhibit poor efficiency due to arising many performance challenges such as cache contention and resource congestion caused by large number of active threads in GPUs. Cache bypassing can be a solution to reduce the impact of cache contention and resource congestion. In this paper, we introduce a new cache bypassing technique that is able to make effective bypassing decisions. In particular, the proposed mechanism employs a small memory, which can be accessed before actual cache …",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:W7OEmFMy1HYC,,1
563,Workload Characteristics-based L1 Data Cache Switching-off Mechanism for GPUs,"Modern graphics processing units (GPUs) have become one of the most attractive platforms in exploiting high thread level parallelism with the support of new programming tools such as CUDA and OpenCL. Recent GPUs has applied cache hierarchy to support irregular memory access patterns; however, L1 data cache (L1D) exhibits poor efficiency in the GPU. This paper shows that the L1D does not always positively affect the applications in terms of performance and energy efficiency for the GPU. The performance of the GPU is even harmed by using the L1D for lots of applications. Our proposed technique exploits the characteristics of the currently-executed applications to predict the performance impact of the L1D on the GPU and then decides whether to continuously use the cache for the application or not. Our experimental results show that the proposed technique improves the GPU performance by 9.4% and saves up to 52.1% of the power consumption in the L1D.",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:eQOLeE2rZwMC,,1
566,A New Prefetch Policy for Data Filter Cache in Energy-Aware Embedded Systems,"As process technology scales down, energy consumption in embedded processors becomes a crucial issue. In embedded processors, data cache accounts for a considerable portion of total dynamic energy consumption. In this paper, we propose a novel energy-efficient Prefetch Data Filter cache (PDF-cache) technique that enables filtering cache accesses by using the access pattern to data cache when a loop instruction is executed. In the proposed architecture, the accesses to data cache are partly migrated to the PDF-cache, which has a very small size. According to our experimental results, the proposed cache architecture with PDF-cache can reduce the dynamic energy consumption compared to the baseline about 7.1% on average with little storage overheads.",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:UeHWp8X0CEIC,,1
569,Varying Parameter Configurations on GPU,"Graphics processing units (GPUs) can provide higher throughput than central processing units (CPUs). This leads to their wide use in modern computing systems, especially when state-of-the-art GPUs can execute both graphics and general-purpose applications. However, due to different characteristics of these two application types, it is necessary to have a proper configuration to full utilize computational resources of GPUs. This paper studies various organizations of GPUs and analyzes the impact of parameter selection on the performance of GPUs.",40,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=lDX7uNQAAAAJ&citation_for_view=lDX7uNQAAAAJ:9yKSN-GCB0IC,,1
572,A tabu search for time-dependent multi-zone multi-trip vehicle routing problem with time windows,"We propose a tabu search meta-heuristic for the Time-dependent Multi-zone Multi-trip Vehicle Routing Problem with Time Windows. Two types of neighborhoods, corresponding to the two sets of decisions of the problem, together with a strategy controlling the selection of the neighborhood type for particular phases of the search, provide the means to set up and combine exploration and exploitation capabilities for the search. A diversification strategy, guided by an elite solution set and a frequency-based memory, is also used to drive the search to potentially unexplored good regions and, hopefully, enhance the solution quality. Extensive numerical experiments and comparisons with the literature show that the proposed tabu search yields very high quality solutions, improving those currently published.",43,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XJZBIgsAAAAJ&citation_for_view=XJZBIgsAAAAJ:zYLM7Y9cAGgC,,1
573,Multi-trip pickup and delivery problem with time windows and synchronization,"In this paper, we consider two-tiered city logistics systems accounting for both the inbound and outbound traffic, that have not been taken into account in models and algorithms for vehicle routing research. The problem under study, called the Multi-trip Pickup and Delivery Problem with Time Windows and Synchronization, has two sets of intertwined decisions: the routing decisions which determine the sequence of customers visited by each vehicle route, the scheduling decisions which plan movements of vehicles between facilities within time synchronization restrictions. We propose a tabu search algorithm integrating multiple neighborhoods targeted to the decision sets of the problem. To assess the proposed algorithm, tests have been conducted on the first benchmark instances of the problem which have up to 72 facilities and 7200 customer demands. As no previous results are available in the literature …",43,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XJZBIgsAAAAJ&citation_for_view=XJZBIgsAAAAJ:Y0pCki6q_DkC,,1
574,A hybrid generational genetic algorithm for the periodic vehicle routing problem with time windows,"We propose a new population-based hybrid meta-heuristic for the periodic vehicle routing problem with time windows. This meta-heuristic is a generational genetic algorithm that uses two neighborhood-based meta-heuristics to optimize offspring. Local search methods have previously been proposed to enhance the fitness of offspring generated by crossover operators. In the proposed method, neighborhood-based meta-heuristics are used for their capacity to escape local optima, and deliver optimized and diversified solutions to the population of the next generation. Furthermore, the search performed by the neighborhood-based meta-heuristics repairs most of the constraint violations that naturally occur after the application of the crossover operators. The genetic algorithm we propose introduces two new crossover operators addressing the periodic vehicle routing problem with time windows. The two …",43,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XJZBIgsAAAAJ&citation_for_view=XJZBIgsAAAAJ:9yKSN-GCB0IC,,1
575,Synchronized multi-trip multi-traffic pickup & delivery in city logistics,"The paper introduces the first methodology addressing with a single fleet of vehicles the routing of the three different types of transportation demands encountered in City Logistics, inbound, outbound and intra-city traffic. We propose a tabu search meta-heuristic calling on various neighbourhoods, dynamically selected, to provide an efficient search combining exploration and exploitation capabilities. The result analysis of extensive computational experiments qualify the impact of a number of major problem characteristics and search strategies on the quality of the meta-heuristic, the behaviour of the solutions, and the management of the City Logistics system.",43,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XJZBIgsAAAAJ&citation_for_view=XJZBIgsAAAAJ:qjMakFHDy7sC,,1
576,A hybrid genetic algorithm for the periodic vehicle routing problem with time windows,"We propose a new population-based hybrid meta-heuristic for the periodic vehicle routing problem with time windows. Two neighborhood-based meta-heuristics are used to educate the offspring generated by a new crossover operator to enhance the solution quality. This hybridization provides the means to combine the exploration capabilities of population-based methods and the systematic, sometimes aggressive search capabilities of neighborhood-based methods, as well as their proficiency to explore the infeasible part of the search space to both repair infeasible solutions and tunnel toward, hopefully, improved ones. Extensive numerical experiments and comparisons with all methods proposed in the literature show that the proposed methodology yields very high quality solutions, improving those currently published.",43,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XJZBIgsAAAAJ&citation_for_view=XJZBIgsAAAAJ:IjCSPb-OGe4C,,1
577,Multi-zone multi-trip pickup and delivery problem with time windows and synchronization,"In this paper, we consider two-tier City Logistics systems accounting for both the inbound and outbound traffic that have not been taken into account in models and algorithms for vehicle routing research. The problem under study, called the Multi-zone Multi-trip Pickup and Delivery Problem with Time Windows and Synchronization, has two sets of intertwined decisions: the routing decisions which determine the sequence of customers visited by each vehicle route, the scheduling decisions which plan movements of vehicles between facilities within time synchronization restrictions. We propose a tabu search algorithm integrating multiple neighborhoods targeted to the decision sets of the problem. To assess the proposed algorithm, tests have been conducted on the first benchmark instances of the problem which have up to 72 facilities and 7200 customer demands. As no previous results are available in the literature …",43,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XJZBIgsAAAAJ&citation_for_view=XJZBIgsAAAAJ:u-x6o8ySG0sC,,1
578,A Tabu Search for the Time-Dependent Multi-Zone Multi-Trip Vehicle Routing Problem with Time Windows,"We propose a tabu search meta-heuristic for the Time dependent Multi-zone Multi-trip Vehicle Routing Problem with Time Windows. Two types of neighborhoods, corresponding to two decision sets of the problem, together with a strategy controlling the selection of the neighborhood type for particular phases of the search, provide the means to set up and combine exploration and exploitation capabilities for the search. A diversication strategy, guided by an elite solution set and a frequency-based memory, is also used to drive the search to potentially unexplored good regions and, hopefully, enhance the solution quality. Extensive numerical experiments and comparisons with the literature show that the proposed tabu search yields very high quality solutions, improving those currently published.",43,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XJZBIgsAAAAJ&citation_for_view=XJZBIgsAAAAJ:2osOgNQ5qMEC,,1
579,Synchronized Multi-Trip Multi-Traffic Pickup & Delivery in City Logistics,The paper introduces the first methodology addressing with a single fleet of vehicles the routing of the three different types of transportation demands encountered in,43,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XJZBIgsAAAAJ&citation_for_view=XJZBIgsAAAAJ:u5HHmVD_uO8C,,1
580,Protecting Consensus Seeking NIDS Modules against Multiple Attackers,"This work concerns distributed consensus algorithms and application to a network intrusion detection system (NIDS)[21]. We consider the problem of defending the system against multiple data falsification attacks (Byzantine attacks), a vulnerability of distributed peer-to-peer consensus algorithms that has not been widely addressed in its practicality. We consider both naive (independent) and colluding attackers. We test three defense strategy implementations, two classified as outlier detection methods and one reputation-based method. We have narrowed our attention to outlier and reputation-based methods because they are relatively light computationally speaking. We have left out control theoretic methods which are likely the most effective methods, however their computational cost increase rapidly with the number of attackers. We compare the efficiency of these three implementations for their computational …",43,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XJZBIgsAAAAJ&citation_for_view=XJZBIgsAAAAJ:Tyk-4Ss8FVUC,,1
581,An Adaptive Large Neighborhood Search for Multi-trip Multi-traffic Pickup and Delivery problem with Time Windows and Synchronization,"This paper introduces an adaptive large neighborhood search metaheuristic to solve the multi-trip multi-traffic pickup and delivery problem with time windows and synchronization (MTT-PDTWS). With adaptive destroy and repair operators, it learns over time which destroy and repair operators are the most effective and governs the operator selection biased toward highly effective one. The computational experiments display the impacts of these operators on the solution quality and the performance of the ALNS in comparison to the only existing tabu search methodology addressing the MTT-PDTWS.",43,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XJZBIgsAAAAJ&citation_for_view=XJZBIgsAAAAJ:UeHWp8X0CEIC,,1
583,A real-time control using wireless sensor network for intelligent energy management system in buildings,"Industrial evolution brings major new challenges due to increasing energy demands. This phenomenon encourages the improvement of control methodologies that reduce resource requirements. It has been lately observed that the building sector contributes considerably to final energy demand. For example, electricity used in France by this sector has reached 284 TWh, accounting for 65% of all electricity consumed in 2007 (434 TWh), and this situation continues to increase. Moreover, the link between increased CO2 emissions and the use of energy is also considered, particularly in the building environment. 404 million tones of CO2 gas is emitted in France, and 22.6% originates from this sector. In the light of developments in microelectro-mechanical systems (MEMS), along with progress made in communication and embedded smart sensors, the building sector has a huge potential for mitigating demand. This …",44,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zmtZGk0AAAAJ&citation_for_view=zmtZGk0AAAAJ:9yKSN-GCB0IC,,1
584,Optimization of essential oil yield from Vietnamese green pepper (Piper nigrum) using hydro-distillation method,"In this study, the essential oil of green pepper collected from Vung Tau City, Vietnam was recovered by hydro-distillation technique, giving the yield of approximately 0.75% yield on optimal conditions. Single factor investigation showed that optimal conditions of the extraction process consisted of water and material ratio of 5: 1 mL/g, extraction time of 180 min, flow rate of 2.5 mL/min, size of material of puree, time for soaking of 3 hours. Nineteen components were identified in the obtained Piper nigrum essential oils using GC-MS. The main components of essential oils were Caryophyllene, 3-Carene, β-Pinene, α-Pinene, Sabinene, and Elemene.",44,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zmtZGk0AAAAJ&citation_for_view=zmtZGk0AAAAJ:eQOLeE2rZwMC,,1
585,Optimizatoin of Microwave-assisted extraction and compositional determination of essential oil from leaves of Eucalyptus globulus,"The extraction of essential oil extracted from Eucalyptus globulus leaves in Southern Vietnam by microwave-assisted extraction (MAE) was investigated. The experimental study was conducted to determine the effect of different extracting parameters such as size of material, water to material ratio (1 mg/mL to 5 mg/mL), time (20 min to 80 min) and microwave power (100W to 600W). The essential oil content is determined by Gas Chromatography-Mass Spectrometry GC-MS method. The main ingredients of Eucalyptus globulus essential oils were Eucalyptol (38.771%). The highest essential oil content of 2.65 mg/L was achieved with the ground material, at the ratio of raw materials to water of 1:3 mL/g, extraction time of 60 min, and microwave power of 450W. In conclusion, MAHD method is considered an efficient extraction process, saving time and cost of materials and heating.",44,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zmtZGk0AAAAJ&citation_for_view=zmtZGk0AAAAJ:W7OEmFMy1HYC,,1
586,Application of Response Surface Methodology in the Process of extracting essential oil from the Calamondin (Citrus microcarpa) Peels,"Calamondin (Citrus microcarpa) essential oil is applied in different fields, including medicine, food, and cosmetic. This study aimed to determine theextraction yield of essential oil from Calamondin peels and optimize the extraction process using Response Surface Methodology (RSM). A three-level three-factors Box Behnken design with three variables including extraction temperature (103-137 C), time (95-145 min), and water to the material ratio (1.32-4.68 mL/g) was adopted. The aforementioned factors are identified to exert significant influence on the essential oil efficiency. Adopting a central composite design, optimal processing conditions were determined. The maximum yield of essential oil was 3.2%, achieved at conditions of the ratio of water and raw materials (3.20: 1 mL/g), the temperature of 120.84 C and time of 21.27 min. The results showed good fits with the proposed model for the essential oil …",44,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zmtZGk0AAAAJ&citation_for_view=zmtZGk0AAAAJ:Y0pCki6q_DkC,,1
587,"Développement de méthodes intelligentes pour la gestion énergétique des bâtiments, utilisant des capteurs sans fils","L’utilisation des charges de manière active et intelligente et leur gestion optimale sont parmiles préoccupations majeures des gestionnaires, des fournisseurs, des commercialisateurs et desconsommateurs d’énergie, et constitue l’un de axes privilégies du projet smart-grid.L’objectif de cette thèse est de développer et réaliser un système de pilotage des charges desbâtiments résidentiels ou tertiaires en temps réel en utilisant des réseaux de capteurs sans fil. Lesméthodes de gestion de charge ont été développées afin de minimiser les pics de consommation,maintenir le confort thermique et minimiser le coût global de consommation. Une conception dusystème de gestion des charges en temps réel sans fil a été proposée avec la communicationstandardisée ZigBee. Des démonstrateurs basés sur la régulation adaptative des chauffages et desclimatisations d’une part et sur une méthode de délestage doux pour des cuisinières d’autre part ontété développés. Le prototype du système proposé a été réalisé et testé lors de campagnes demesures sur un appartement expérimental afin de montrer l’intérêt et les performances des méthodesproposées. Le système développé permet d’éviter l’utilisation des modèles prédictifs qui sont trèsdifficiles à identifier et à reconfigurer en cas d’ajout de nouveaux appareils électriques dans lesbâtiments contrôlés.",44,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zmtZGk0AAAAJ&citation_for_view=zmtZGk0AAAAJ:d1gkVwhDpl0C,,1
588,Effects of microwave extraction conditions on polyphenol content and antioxidant activity of pomelo extract (Citrus maxima (Burm.) Merr.),"The albedo, white peel of pomelo Citrus maxima (Burm.) Merr., is by-product of food processing, and has polyphenol and antioxidant activity. However, there are not many studies on polyphenol from pomelo peel in Vietnam by microwave extraction. Microwave-assisted extraction helps more efficient heating, faster energy transfer, increasing or decreasing temperature control, saving solvent and cost to extract active ingredients from plant materials. This research applied microwave extraction to extract polyphenol from pomelo Citrus maxima (Burm.) Merr. The factors that affected the extraction process such as solvent concentration, microwave power, extraction time and material to solid ratio were investigated. The results obtained that the highest polyphenol content (2.46 gGAE/L) and antioxidant activity (1325.85 µmolTE/L) with ethanol concentration of 60%, microwave power of 300W, microwave-assisted time of 2 …",44,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zmtZGk0AAAAJ&citation_for_view=zmtZGk0AAAAJ:LkGwnXOMwfcC,,1
589,Secure eeg-based user authentication system integrated with robust watermarking,"Electroencephalogram (EEG) data has been widely used in health care sector. Recent studies explore the potential of EEG in biometric authentication because of its advantages. However there are security problems in EEG based user authentication system, especially in remote application with unsecured channel that are not investigated thoroughly in previous works. In this paper, a secure user authentication system based on EEG data is integrated with watermarking scheme which applies hybrid Discrete Wavelet Transform-Singular Value Decomposition (DWT-SVD) and Quantization Module Index (QIM). The developed model is able to enhance the security of the system against spoofing, relay and communication attacks while not degrade the EEG-based user authentication performance. The impact of watermarking on the recognition performance of EEG-based user authentication system has also been …",44,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zmtZGk0AAAAJ&citation_for_view=zmtZGk0AAAAJ:Tyk-4Ss8FVUC,,1
590,Chlorophyll content and antioxidant activity from folium sauropi (Sauropus androgynus (L.) Merr) with microwave-assisted extraction,"Folium sauropi is grown commonly in Vietnam, it is a vegetable that contains many bio-active compounds such as chlorophyll, polyphenol, and antioxidant activity. The use of natural pigments having clear origin is a trend in food processing nowadays. The objective of this study was to find suitable conditions for extracting chlorophyll with antioxidant activity (DPPH) in folium sauropi leaves by microwave-assisted extraction. When extracted with acetone of 90, microwave power of 300W, solid to solvent ratio of 1: 30, microwave-assisted extraction time of 120 seconds; chlorophyll content (14.43±0.16 μg/mL), and DPPH radical-scavenging activity (886.64±15.89 μmol/L) obtained highest.",44,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zmtZGk0AAAAJ&citation_for_view=zmtZGk0AAAAJ:_FxGoFyzp5QC,,1
591,A method for improvement the parameter estimation of non-linear regression in growth model to predict project cost at completion,"In this paper, we propose a comparison between existing parameter estimation methods of a nonlinear regression-based growth model to forecast project duration as well as its cost at completion. To analyze and perform some experiments, we used the Gompertz growth model and the dataset comes from a number of similar previous studies. Two other nonlinear models were also applied to compare the results with the Gompertz model in terms of fitness-function score. The performance of the proposed methods is also the key for further studies in fitting the S- curve and predicting the Estimate-to-Complete and the Estimate-at-Completion of certain projects.",44,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zmtZGk0AAAAJ&citation_for_view=zmtZGk0AAAAJ:WF5omc3nYNoC,,1
592,Intelligent energy management in residential buildings with a real-time control & wireless meterrecording system,"Industrial evolution brings major new challenges due to increasing energy demands. This phenomenon encourages the improvement of control methodologies that reduce resource requirements. Lately, it has been observed that the building sector contributes considerably to final energy demand. Consequently, studies have been conducted on the technical systems and solutions applied in this sector to the intelligent management of energy. Our work focuses on the smart control of energy management systems to monitor operation of electrical equipment. This paper starts by addressing the structure of the control system in buildings, which involves the connection of equipment, to enable communication with each other, and the integration of micro-sensors to enable automatic control of meter-recording systems. We then present an application for a heating control. This application uses a new real-time control method that allows peak consumption to be reduced while maintaining thermal comfort. It is based on wireless sensor network (WNS) technology which offers simultaneous measurement, and an interoperable communication network for the control method proposed. This method is tested and the results demonstrate the outstanding quality of its capability to control heating load and to adapt to any problems that may arise (by taking into account changing price, signals from energy provider and distribution system operator, etc).",44,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zmtZGk0AAAAJ&citation_for_view=zmtZGk0AAAAJ:u5HHmVD_uO8C,,1
593,A LSTM based framework for handling multiclass imbalance in DGA botnet detection,"In recent years, botnets have become a major threat on the Internet. Most sophisticated bots use Domain Generation Algorithms (DGA) to pseudo-randomly generate a large number of domains and select a subset in order to communicate with Command and Control (C&C) server. The basic aim is to avoid blacklisting, sinkholing and evade the security systems. Long Short-Term Memory network (LSTM) provides a mean to combat this botnet type. It operates on raw domains and is amenable to immediate applications. LSTM is however prone to multiclass imbalance problem, which becomes even more significant in DGA malware detection. This is due the fact that many DGA classes have a very little support in the training dataset. This paper presents a novel LSTM.MI algorithm to combine both binary and multiclass classification models, where the original LSTM is adapted to be cost-sensitive. The cost items are …",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:YOwf2qJgpHMC,,1
594,QoE-based server selection for content distribution networks,"As current server capacity and network bandwidth become increasingly overloaded by the rapid growth of high quality emerging multimedia services such as mobile online gaming, social networking or IPTV, a critical factor of success of these multimedia services becomes the end-user perception of quality while them using the service. As a result, user-centered approaches that consider quality of experience (QoE) constitute the current design trend for network systems of content providers and network operators. A content distribution network (CDN) that replicates the content from original servers to the replicated servers close to end users is actually an effective solution to improve network quality. We propose a QoE-based server selection algorithm in the context of a CDN architecture. Using realistic characteristics of the server selection process, we formalize our selection model as a sequential decision problem …",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:W7OEmFMy1HYC,,1
595,A novel quic traffic classifier based on convolutional neural networks,"Nowadays, network traffic classification plays an important role in many fields including network management, intrusion detection system, malware detection system, etc. Most of the previous research works concentrate on features extracted in the non-encrypted network traffic. However, these features are not compatible with all kind of traffic characterization. Google's QUIC protocol (Quick UDP Internet Connection protocol) is implemented in many services of Google. Nevertheless, the emergence of this protocol imposes many obstacles for traffic classification due to the reduction of visibility for operators into network traffic, so the port and payload- based traditional methods cannot be applied to identify the QUIC- based services. To address this issue, we proposed a novel technique for traffic classification based on the convolutional neural network which combines the feature extraction and classification phase into …",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:Wp0gIr-vW9MC,,1
596,DGA botnet detection using supervised learning methods,"Modern botnets are based on Domain Generation Algorithms (DGAs) to build a resilient communication between bots and Command and Control (C&C) server. The basic aim is to avoid blacklisting and evade the Intrusion Protection Systems (IPS). Given the prevalence of this mechanism, numerous solutions have been developed in the literature. In particular, supervised learning has received an increased interest as it is able to operate on the raw domains and is amenable to real-time applications. Hidden Markov Model, C4. 5 decision tree, Extreme Learning Machine, Long Short-Term Memory networks have become the state of the art in DGA botnet detection. There also exist several advanced supervised learning methods, namely Support Vector Machine (SVM), Recurrent SVM, CNN+ LSTM and Bidirectional LSTM, which have not been suitably appropriated in such domain. This paper presents a first attempt …",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:4TOpqqG69KYC,,1
597,Detecting attacks on web applications using autoencoder,"Web attacks have become a real threat to the Internet. This paper proposes the use of autoencoder to detect malicious pattern in the HTTP/HTTPS requests. The autoencoder is able to operate on the raw data and thus, does not require the hand-crafted features to be extracted. We evaluate the original autoencoder and its variants and end up with the Regularized Deep Autoencoder, which can achieve an F1-score of 0.9463 on the CSIC 2010 dataset. It also produces a better performance with respect to OWASP Core Rule Set and other one-class methods, reported in the literature. The Regularized Deep Autoencoder is then combined with Modsecurity in order to protect a website in real time. This algorithm proves to be comparable to the original Modsecurity in terms of computation time and is ready to be deployed in practice.",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:dhFuZR0502QC,,1
598,Qoe model driven for network services,"Actually, wealthy network services such as Internet protocol television (IPTV) and Voice over IP (VoIP) are expected to become more pervasive over the Next Generation Network (NGN). In order to serve this purpose, the quality of these services should be evaluated subjectively by users. This is referred to as the quality of experience (QoE). However, there are many issues that may impact user’s perception in many different ways. This paper surveys the QoE models and systems for existing and future network services.",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:u-x6o8ySG0sC,,1
599,Quality of Experience for Multimedia: Application to Content Delivery Network Architecture,"Based on a convergence of network technologies, the Next Generation Network (NGN) is being deployed to carry high quality video and voice data. In fact, the convergence of network technologies has been driven by the converging needs of end-users. The perceived end-to-end quality is one of the main goals required by users that must be guaranteed by the network operators and the Internet Service Providers, through manufacturer equipment. This is referred to as the notion of Quality of Experience (QoE) and is becoming commonly used to represent user perception. The QoE is not a technical metric, but rather a concept consisting of all elements of a user's perception of the network services. The authors of this book focus on the idea of how to integrate the QoE into a control-command chain in order to construct an adaptive network system. More precisely, in the context of Content-Oriented Networks used to redesign the current Internet architecture to accommodate content-oriented applications and services, they aim to describe an end-to-end QoE model applied to a Content Distribution Network architecture. About the Authors Abdelhamid Mellouk is Full Professor at University of Paris-Est C-VdM (UPEC), Networks & Telecommunications (N&T) Department and LiSSi Laboratory, France. Head of several executive national and international positions, he was the founder of the Network Control Research activity at UPEC with extensive international academic and industrial collaborations. His general area of research is in adaptive real-time control for high-speed new generation dynamic wired/wireless networks in order to maintain acceptable …",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:WF5omc3nYNoC,,1
600,QoE content distribution network for cloud architecture,"Cloud Service has become now a real trend in the development of network services. It is a concept of deporting resources and services on remote servers as opposed to traditional hosting on the user workstation. In fact, cloud services require more bandwidth due to high quality media. Content Distribution Network (CDN) with a mechanism of replication of information among multiple servers is a solution to support high request volume and ameliorate network quality. Integrating Cloud concept into Content Distribution Network (CDN) is an effective approach to improve network quality. In this paper, we propose a Content Distribution Network Cloud Architecture (CDNCA). Our approach is based not only on Quality of Service criteria (QoS) (e.g. round trip time, network hops, loss rate, etc.) but also on the Quality of Experience that represents end-users perception and satisfaction. Experimental results show that CDNCA …",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:9yKSN-GCB0IC,,1
601,Perinatal dioxin exposure and neurodevelopment of 2-year-old Vietnamese children in the most contaminated area from Agent Orange in Vietnam,"Bien Hoa airbase is the most contaminated area of dioxin contamination from Agent Orange in Vietnam, but little is known about the neurodevelopmental effects of perinatal dioxin exposure on children living nearby. We recruited 210 mother–newborn resident pairs in 2012 and 78 pairs in 2015 and followed them for 2 years to assess the children's neurodevelopment. As a control group, we used 120 mother–child pairs recruited in 2014 in the Ha Dong district of Ha Noi City, an unexposed area. Perinatal dioxin exposure levels were indicated by levels of 2,3,7,8-tetrachlorodibenzo-p-dioxin (TCDD) and toxic equivalency values of polychlorodibenzodioxins, polychlorodibenzofurans, and nonortho-polychlorinated biphenyls (TEQ-PCDD/Fs/noPCBs) in maternal breast milk. The Bayley Scales of Infant and Toddler Development, Third Edition (Bayley-III) were used to assess neurodevelopment, and scores in each …",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:r0BpntZqJG4C,,1
602,Implementation of vision-based autonomous mobile platform to control by A∗ algorithm,"Aiming at rareness of navigation solutions for vehicle platform in unapplicable-GPS or low quality of GPS environment, this paper investigated an approach of vision-based path planning and navigation by a stereo camera. Firstly, camera is calibrated to get its parameters. In this stage, host PC must deterime locations of all objects in working area. When start point and target point is given out, host based on A* algorithm generates trajectory for mobile platform to prevent obstacles. Later, vehicle is driven to track the path planning. Additionally, host is able to update status and re-construct trajectory to avoid dynamic obstacles. The low-cost hardware architecture of mobile platform is built-in to verify the design. From the experiments, it can be seen clearly that vehicle runs on the optimized trajectory and navigated successfully.",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:aqlVkmm33-oC,,1
603,Institutional investor activism,"While much has been written about institutional investor activism in the corporate governance arena, much less is known about their advocacy activities in the realm of environmental and social issues. In fact, at least six survey articles summarize the voluminous research on corporate governance shareholder activism in the United States (Black 1997; Gillan and Starks 1998, 2007; Karpoff 2001; Romano 2001; Ferri 2011). Notably, most of the research surveyed in these articles appears in mainstream finance journals. In contrast, only Sjostrom (2008) surveys the social responsibility shareholder activism literature; she reports that of 34 studies, only one appears in a finance journal.",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:LkGwnXOMwfcC,,1
604,Design of green agriculture system using internet of things and image processing techniques,"Recent years, farming issue is one of the important problems and need to be considered. To handle this issue, it is so essential that we need figure out solutions to assure the growth of crops effectively. In this paper, we propose the new solution to plant automatically and monitor remotely. Firstly, the model of plantation is described in depth. Then, CPU plays a central role to handle the irrigation and aftercare the whole system. It is implemented several sensors and actuators to manipulate the environmental farm. The system parameters are updated in cloud frequently. Furthermore, all of plantations is supervised by digital cameras. The host PC will analyze the image to predict the process of development in agriculture. Therefore, via Internet of Things (IoT) and image processing technology, farmers are able to estimate the best harvesting time and assure the mass production of food.",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:IWHjjKOFINEC,,1
605,Global state-dependent QoE based routing,"For years, wireless network systems have been trying to satisfy end-users and support high quality multimedia applications such as Mobile TV, VoIP, etc. Combining wireless networks with multimedia content distribution needs efficient routing protocols. We develop in this paper a new routing protocol, namely DOQAR (Dynamic Optimized QoE Adaptive Routing), to improve the user perception and optimize the usage of network resources. In our end-to-end model, smartphone users connect to content servers in a wired network across a wireless access network. In order to evaluate the QoE, we use a Multi-Layer Perception-based (MLP) method. Experimental results show a significant performance against other traditional routing protocols.",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:u5HHmVD_uO8C,,1
606,User QoE-based adaptive routing system for future Internet CDN,"The most important tendency of future Internet architectures is maintaining the best Quality of Experience (QoE), which represents the subjective perception of end-users using network services with network functions such as admission control, resource management, routing, traffic control, etc. Among of them, we focus on routing mechanism driven by QoE end-users. Nowadays, most existing routing protocols have encountered NP-complete problem when trying to satisfy multi QoS constraints criteria simultaneously. With the intention for avoiding the classification problem of these multiple criteria reducing the complexity problem for the future Internet, we propose a protocol based on user QoE measurement in routing paradigm to construct an adaptive and evolutionary system. Our approach, namely QQAR (QoE Q-learning based Adaptive Routing), is based on Q-Learning, a Reinforcement Learning algorithm …",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:IjCSPb-OGe4C,,1
607,User to user adaptive routing based on qoe,"Service quality can be defined as “the collective effect of service performances which determine the degree of satisfaction of a user of the service” [1]. In other words, quality is the customer's perception of a delivered service. As larger varieties of services are offered to customers, the impact of network performance on the quality of service will be more complex. It is vital that service engineers identify network-performance issues that impact customer service. They also must quantify revenue lost due to service degradation. The Quality of Experience (QoE) becomes recently the most important tendency to guarantee the quality of network services. QoE represents the subjective perception of end-users using network services with network functions such as admission control, resource management, routing, traffic control, etc. In this paper, our main focus is routing mechanism driven by QoE end-users. With the purpose of …",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:UeHWp8X0CEIC,,1
608,Mabrese: A new server selection method for smart SDN-based CDN architecture,"A content delivery network (CDN) is able to handle high traffic and offer reliable services by geographically bringing the content data to the edge network, where replica servers are installed closer to the end users. The traditional CDN architecture lacks a global view of the whole network and, thus, cannot dynamically and optimally perform the server selection task. In this letter, we propose to integrate a software-defined network (SDN) into the CDN architecture. Our intuitive motivation is to decouple the control plane from the forwarding plane in order to offer flexibility and programmability by using the centralized controller. This letter also develops a novel server selection algorithm that is based on the so-called multi-armed bandit problem. Such an algorithm is proved to optimize the server selection function and provide a good experimental result in terms of both average response time and reward score.",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:hC7cP41nSMkC,,1
609,A new fall detection system on Android smartphone: Application to a SDN-based IoT system,"Falls are one of the very serious problems in the healthcare system for the elderly, often resulting in a rapid decline in functionality and death. Serious consequences of sustaining a fall include broken or fractured bones, superficial cuts and abrasions as well as soft tissue damage. Several solutions are proposed to resolve such problems, however, major difficulties they encounter are cost, comfort and performance. In this paper, we propose a new Android application for fall detection based on theory of machine learning and data mining. Concretely, we used a classification method to detect if it is a fall down or another common events (e.g. sitting, jumping, etc.). The obtained experimental results show that our proposal yielded impressive results compared to related works.",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:_kc_bZDykSQC,,1
610,Experimental design of PC-based servo system,"This paper concerntrates on the analysis, design and control methodology in the machining servo system. The overall architecture of motion system is investigated to realize the drawbacks of traditional scheme. Then, this research proposes the suitable design of controller due to rapid calculation, high precision and friendly graphical user interface. The diagram of Fuzzy self-tuning PID and feedforward is constructed to drive the servo machine. The hardware platform of controller has been accomplised such as schematic, PCB artworks and soldering. Later, the middleware and firmware have been closed to promote the execution between controller and computer. The intelligent algorithm is embedded in board level by TMS320C6727 to overcome the problems, for example the existing nonlinear properties, external disturbances or servo lag phenomena. Based on the theoretical design, several simulation results are …",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:qxL8FJ1GzNcC,,1
611,Quality of experience vs. quality of service: Application for a CDN Architecture,"We are witnessing in recent years a rapid development of interconnecting applications. In addition to those that contributed to the popularity of the early Internet (email, file transfer, etc.), there are now applications that rely on network data more sensitive. They include sound applications (voice, music programs, etc.), image applications (television programs, videoconferencing, video on demand, etc.) and urgent information applications (market orders). However, it is important for the operators and providers not to forget sight of the reason for this new infrastructure: to provide network service that user wants to use. Accomplishing this idea means assuring positive experience of end users. Therefore, service providers are switching the focus from traditional Quality-of-Service (QoS) to user satisfaction, which is the overall success of a network from the user perspective. The perceived end-to-end quality becomes one of …",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:eQOLeE2rZwMC,,1
612,Real-Time state-dependent routing based on user perception,"In order to successfully resolve the network infrastructure's problems the network provider has to improve the service quality. However in traditional ways, maintaining and improving of the service quality are generally determined in terms of quality of service criteria, not in terms of satisfaction and perception to the end-user. The latter is represented by Quality of Experience (QoE) that becomes recently the most important tendency to guarantee the quality of network services. QoE represents the subjective perception of end-users using network services with network functions such as admission control, resource management, routing, traffic control, etc. In this paper, we focus on routing mechanism driven by QoE end-users. Today, NP-complete is one of the most routing algorithm problems when trying to satisfy multi QoS constraints criteria simultaneously. In order to avoid the classification problem of these multiple …",47,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Ed___84AAAAJ&citation_for_view=Ed___84AAAAJ:d1gkVwhDpl0C,,1
613,Distributed hole-bypassing protocol in WSNs with constant stretch and load balancing,"Geographic routing has been widely used in wireless sensor networks because of its simplicity and efficiency resulting from its local and stateless nature. However, when subjected to routing holes (ie, regions without sensor nodes that have communication capability), geographic routing suffers from the so-called local minimum phenomenon, where packets are stopped at the hole boundary. This local minimum phenomenon results in problems of load imbalance (ie, a higher traffic intensity around the hole boundary) and routing path enlargement due to the long hole detour paths. Although several protocols have been proposed to address these issues, the load imbalance problem has not been solved thoroughly, and none of the existing protocols can solve both of these problems. In this article, we propose a distributed hole-bypassing routing protocol named ACOBA (Adaptive forbidden area-based COnstant …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:YsMSGLbcyi4C,,1
614,An energy efficient and load balanced distributed routing scheme for wireless sensor networks with holes,"In this paper we present a new approach to route packets in the presence of routing holes. In our proposal, nodes cooperate to determine the approximate polygon of a specific hole and then exchange information about the approximate polygon. Based on the hole covering parallelogram and the hole view angle of a specific node, packets can be forwarded along an escape route that bends around the hole. We rigorously prove that the Euclidean stretch of an escape route is bounded. Simulation results show that the proposed scheme can save more than 16% of the energy consumption and 7% of the network lifetime in the comparison with existing routing algorithms. The average length of routing paths in our approach is less than 60% of other routing schemes.",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:Tyk-4Ss8FVUC,,1
615,Deep convolutional LSTM network-based traffic matrix prediction with partial information,"Accurate prediction of the future network traffic plays an important role in various network problems (e.g. traffic engineering, capacity planning, quality of service provisioning, etc.). However, the modern network communication is extremely complicated and dynamic, which makes the tasks of modeling and predicting the network behavior very difficult. To this end, a common approach is to apply the traditional time series prediction techniques such as Autoregressive Integrated Moving Average or Linear Regression. Besides that, there are some studies exploiting Deep Learning techniques such as Restricted Boltzmann Machine or Recurrent Neural Network (RNN) to estimate the traffic volume. Although the prediction accuracy largely depends on the amount of historical data, measuring all the network traffic is impossible or impractical due to the monitoring resources constraints as well as the dynamics of temporal …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:8k81kl-MbHgC,,1
616,A performance evaluation of energy efficient schemes for green office networks,"Power consumption in office networks has become a major issue due to its running cost. In order to reduce the power consumption, several approaches have been proposed such as Energy Efficient Ethernet (EEE) and Energy Efficient Wireless aggregation (EEW). In this research, we develop an analytical model and apply the model to evaluate and compare performance of the existing technologies. The parameters of interest include the number and the average throughput of clients. Our analysis results show that EEW is more efficient than EEE with a small number of clients or low average throughput. Motivated by this result, we examine the possibility of conserving more power by introducing a combination of EEE and EEW, called EEEW. EEEW can take advantage of both EEE as well as EEW and should be very strong against the varying conditions of the network. We found that when the average throughput of …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:zYLM7Y9cAGgC,,1
617,TELPAC: A time and energy efficient protocol for locating and patching coverage holes in WSNs,"Guaranteeing sufficient sensing coverage is of prime importance in wireless sensor networks. Unfortunately, due to many reasons such as natural disruptions, adversarial attacks, or energy depletion, the occurrence of coverage holes is unavoidable. In order to assure the quality of service, coverage holes should be patched (i.e. by deploying new sensors) as soon as they appear. The solutions in state-of-the-art protocols still incur time complexity and energy overhead that increase with the size of coverage holes. To avoid that issue, this paper introduces a novel protocol (namely, TELPAC) which efficiently locate the hole boundary and determine the patching locations. The main idea behind TELPAC is to approximate the hole by a polygon whose edges are aligned by a regular triangle lattice. Based on such approximation, the patching locations are then detected by using a regular hexagon tessellation. We …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:0EnyYjriUFMC,,1
618,Node placement for target coverage and network connectivity in WSNs with multiple sinks,"Target coverage and connectivity are two main challenging and important issues in wireless sensor networks. The former is for providing sufficient monitoring quality where all points of interest in the network are covered by sensor nodes and the latter is for guaranteeing satisfactory communicating capability where all sensors can connect to at least one sink via relay nodes. In this paper, we focus on minimizing the number of nodes (i.e., sensor nodes and relay nodes) to provide target coverage and connectivity in wireless sensor networks with multiple sinks. We formulate the problem as two sub-problems. The first one (named as TC) is for placing sensor nodes to cover all targets and the second one (named as NC) is for placing relay nodes to connect sensor nodes to the sinks. We then propose a heuristic algorithm for the TC problem that exploits clustering technique. We also propose two heuristic algorithms for …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:W7OEmFMy1HYC,,1
619,Load balanced and constant stretch routing in the vicinity of holes in WSNs,"Because of its simplicity and scalability, geographic routing is a popular approach in wireless sensor networks, which can achieve a near-optimal routing path in the networks without holes (i.e., regions without working sensors). With the occurrence of holes, however, geographic routing faces the problems of load imbalance and routing path enlargement. In the literature, several proposals have attempted to fix these issues, but the majority of them considers only the cases when both the source and the destination stay fairly far from the holes. Recently, a few work has been proposed to tackle the problem of routing in the vicinity of routing holes. However, none of them addresses the two problems (i.e., load imbalance and routing path enlargement) concurrently, and none of them can solve the problem of load imbalance thoroughly. In this paper, we introduce a novel approach in dealing with routing in the vicinity of …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:Y0pCki6q_DkC,,1
620,Genetic algorithm-based periodic charging scheme for energy depletion avoidance in wrsns,"Thanks to the advancements in wireless power transfer technologies, a new paradigm of wireless sensor network (WSNs) called wireless rechargeable sensor networks (WRSNs) has recently emerged. For a WRSN, designing an efficient charging schedule is a challenging issue due to the inherent constraints of WSNs. Although there have been many efforts to optimize the charging schedule, the existing works suffer from several critical problems. Firstly, they rarely tackle the dead node minimization problem, which is the ultimate objective of wireless charging. Secondly, most of the existing works assume impractical conditions, which include the unlimited battery capacity of the charger, and a fully charging scheme at the sensors. In this paper, aiming at minimizing the number of dead nodes, we propose a novel charging scheme based on the genetic algorithm. Our scheme works when the mobile charger has only …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:M3ejUd6NZC8C,,1
621,Node placement for connected target coverage in wireless sensor networks with dynamic sinks,"Target coverage and connectivity are two fundamental and critical issues in wireless sensor networks. The former is for providing sufficient monitoring quality where all points of interest in the network are covered by sensor nodes. The latter is for guaranteeing satisfactory communicating capability where all sensor nodes can connect to at least one sink via nodes (i.e., sensor nodes and relay nodes). Though considerable efforts have been devoted to optimize the placement of sensor nodes and relay nodes under connected target coverage constraint (i.e., guaranteeing both target coverage and connectivity), all of the existing works in this area address only networks with one static sink. In the meanwhile, although there are many works in literature considered the networks with dynamic sinks, none of them studies how to optimize the location of sensor and relay nodes. In this article, we focus on wireless sensor …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:5nxA0vEk-isC,,1
622,Efficient approximation of routing holes in wireless sensor networks,"Routing holes in wireless sensor networks (WSN) do occur due to several reasons, including cases caused by natural obstacles or disaster suffered areas. Computing the exact boundary of a hole is possible [5], but can be impractical with large holes. In this paper, we consider the problem of how to approximate a hole by a simplistic shape, covering polygon such that if demanded either the approximation error or the memory space needed for reporting this polygon shape can be made as small as desired. We present an efficient algorithmic scheme that can produce cost-effective and flexible solutions that can be customized to fit to specific situations. Based on the basic BoundHole algorithm for computing the exact hole boundary [5], we propose a two-folded approximation technique:(1) covering the hole with a minimal polygon with edges aligned to a given square grid;(2) dynamically trimming-off this polygon …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:d1gkVwhDpl0C,,1
623,A distributed protocol for detecting and updating hole boundary in wireless sensor networks,"Holes in sensor network are regions without operating nodes which may occur due to several reasons, including cases caused by natural obstacles or disaster suffered areas. Determining the location and shape of holes can help to monitor these disaster events (such as volcano, tsunami, etc.) or help to make smart, early routing decisions for circumventing a hole. There are many hole determination algorithms have been proposed in the literature. However, all of these algorithms only consider the networks with static holes. Moreover, most of these protocols are conducted in a centralized manner which is not suitable with the resource constraint of sensor nodes. In this paper, we consider the networks with dynamic holes and propose a distributed algorithm which can determine and update the hole boundary very quickly. We also conduct simulation to evaluate performance of proposed protocol.",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:u5HHmVD_uO8C,,1
624,Load balanced routing with constant stretch for wireless sensor network with holes,"Because of its simplicity and scalability, geographic routing is a popular approach in wireless sensor networks, which can achieve a near-optimal routing path in the networks without of holes. With the occurrence of holes, however, geographic routing faces the problems of hole diffusion and routing path enlargement. Several recent proposals attempt to fix these issues by deploying a special, forbidding area around the hole, which helps to improve the congestion on the hole boundary but still causes significant load imbalancing due to static detour routes bypassing this fixed forbidding area. Also, a significant enlargement on routing path is still possible due to the possibly significant difference between this forbidding area and the hole. Another recent approach can achieve a low route stretch (constant bounded) but still, the load imbalancing due to the holes is a concern. In this paper, we introduce a novel approach …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:9yKSN-GCB0IC,,1
625,Pm2. 5 prediction using genetic algorithm-based feature selection and encoder-decoder model,"The concentration of fine particulate matter (PM2.5), which represents inhalable particles with diameters of 2.5 micrometers and smaller, is a vital air quality index. Such particles can penetrate deep into the human lungs and severely affect human health. This paper studies accurate PM2.5 prediction, which can potentially contribute to reducing or avoiding the negative consequences. Our approach’s novelty is to utilize the genetic algorithm (GA) and an encoder-decoder (E-D) model for PM2.5 prediction. The GA benefits feature selection and remove outliers to enhance the prediction accuracy. The encoder-decoder model with long short-term memory (LSTM), which relaxes the restrictions between the input and output of the model, can be used to effectively predict the PM2.5 concentration. We evaluate the proposed model on air quality datasets from Hanoi and Taiwan. The evaluation results show that our model …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:mVmsd5A6BfQC,,1
626,An on-demand charging for connected target coverage in WRSNs using fuzzy logic and Q-Learning,"In wireless rechargeable sensor networks (WRSNs), a mobile charger (MC) moves around to compensate for sensor nodes’ energy via a wireless medium. In such a context, designing a charging strategy that optimally prolongs the network lifetime is challenging. This work aims to solve the challenges by introducing a novel, on-demand charging algorithm for MC that attempts to maximize the network lifetime, where the term “network lifetime” is defined by the interval from when the network starts till the first target is not monitored by any sensor. The algorithm, named Fuzzy Q-charging, optimizes both the time and location in which the MC performs its charging tasks. Fuzzy Q-charging uses Fuzzy logic to determine the optimal charging-energy amounts for sensors. From that, we propose a method to find the optimal charging time at each charging location. Fuzzy Q-charging leverages Q-learning to determine the next charging location for maximizing the network lifetime. To this end, Q-charging prioritizes the sensor nodes following their roles and selects a suitable charging location where MC provides sufficient power for the prioritized sensors. We have extensively evaluated the effectiveness of Fuzzy Q-charging in comparison to the related works. The evaluation results show that Fuzzy Q-charging outperforms the others. First, Fuzzy Q-charging can guarantee an infinite lifetime in the WSRNs, which have a sufficient large sensor number or a commensurate target number. Second, in other cases, Fuzzy Q-charging can extend the time until the first target is not monitored by 6.8 times on average and 33.9 times in the best case, compared to existing …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:4JMBOYKVnBMC,,1
627,Modeling and minimizing latency in three-tier v2x networks,"Leveraging mobile cloud computing (MCC) and mobile edge computing (MEC) for offloading computational tasks is a promising approach to enabling delay-sensitive applications executing vehicles. Despite MCC and MEC's ability and complementary characteristics, most of the existing works on offloading focus on only either MCC or MEC. In this paper, we study their cooperation in a three-tier offloading model of a V2X network where a vehicle can offload computational tasks to cloud computing and MEC. Specifically, we investigate the optimal offloading probabilities of three offloading paths, including Vehicle-to- Infrastructure, Vehicle-to-Cloud, and Infrastructure-to-Cloud. Our contribution is twofold. First, we derive a mathematical model of task execution latency and a formulation to find an optimal solution for the minimum latency problem. Second, we propose an approximation algorithm based on the genetic …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:ZeXyd9-uunAC,,1
628,Exploiting q-learning in extending the network lifetime of wireless sensor networks with holes,"Geographic routing is one of the most popular routing protocols in wireless sensor networks (WSNs) due to its simplicity and efficiency. However, with the occurrence of holes, geographic routing incurs with the so-called local minimum problem that may lead to a long hole detour path as well as the traffic concentration around the hole boundary. In consequence, the network lifetime is shortened. In this paper, we aim at proposing a lightweight distributed geographic routing protocol, which can prolong the lifetime of WSNs under the hole occurrence. Our main idea is to exploiting Q-learning technique to estimate the distance from a node to the holes. The routing decision is then determined based on the residual energy of the nodes, their estimated distance to the holes, and their distance to the destination. The simulation experiments show that our protocol strongly outperforms state-of-the-art protocols in terms of the …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:3fE2CSJIrl8C,,1
629,Empowering 5g mobile devices with network softwarization,"The fifth generation of mobile wireless networks (5G) will provide an infrastructure with abundant and reliable connectivity for innovative and complicated applications. In 5G, 5G mobile devices, which will have improved computing resources for such applications, play an essential role. However, the network stack of 5G devices may continue to be borrowed from 4G legacy operating systems, thereby degrading user experience. In this paper, we posit that 5G users should have flexibility in utilizing networks and gain more awareness of network selection. To this end, we exploit network softwarization technologies to empower 5G devices. We then devise 5GSoft, a novel softwarized networking stack on each 5G mobile device. 5GSoft includes wireless virtualization to relax the dependence on hardware, thereby enabling sharing and multiple access. The 5GSoft device can concurrently exploit surrounding wireless …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:NMxIlDl6LWMC,,1
630,Energy-efficient routing in the proximity of a complicated hole in wireless sensor networks,"A quest for geographic routing schemes of wireless sensor networks when sensor nodes are deployed in areas with obstacles has resulted in numerous ingenious proposals and techniques. However, there is a lack of solutions for complicated cases wherein the source or the sink nodes are located close to a specific hole, especially in cavern-like regions of large complex-shaped holes. In this paper, we propose a geographic routing scheme to deal with the existence of complicated-shape holes in an effective manner. Our proposed routing scheme achieves routes around holes with the (1+)-stretch. Experimental results show that our routing scheme yields the highest load balancing and the most extended network lifetime compared to other well-known routing algorithms as well.",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:e5wmG9Sq2KIC,,1
631,"Q-learning-based, Optimized On-demand Charging Algorithm in WRSN","This paper introduces a novel charging strategy for wireless rechargeable sensor networks (WRSNs), in which a mobile charger (MC) moves and wirelessly transfers the power to the sensor nodes. The first distinct point of this work is designing the MC's charging algorithm under the consideration of target coverage and connectivity. As a solution, we introduce a novel on-demand charging scheme for WRSNs that optimize the charging time at each MC's charging location. Moreover, we take advantage of the Q-learning technique (i.e., hence named our algorithms Q-charging) to maximize the number of monitored targets. Q-charging can prioritize the sensor nodes, which play a more critical role in the network. Hence, Q-charging can select a suitable charging location aiming to provide sufficient power for the prioritized sensors. We have evaluated our proposal in comparison to the previous works. The evaluation …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:qUcmZB5y_30C,,1
632,Optimizing charging locations and charging time for energy depletion avoidance in wireless rechargeable sensor networks,"In recent years, Wireless Rechargeable Sensor Networks, which exploit wireless energy transfer technologies to address the energy constraint problem in traditional Wireless Sensor Networks, has emerged as a promising solution. There are two important factors that affect the performance of a charging process: charging path and charging time. In the literature, many studies have been done to propose efficient charging algorithms. However, most of the existing works focus only on optimizing the charging path. In this paper, we are the first one to jointly take into account both the charging path and charging time. Specifically, we aim at determining the optimal charging path and the charging time at each charging location to minimize the number of dead nodes. We first mathematically formulate the problem under mixed integer and linear programming. Then, we propose a periodic charging scheme, which is based …",49,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=L_NKoQwAAAAJ&citation_for_view=L_NKoQwAAAAJ:4TOpqqG69KYC,,1
633,Errors in the administration technique of insulin pen devices: a result of insufficient education,"Insulin is a high-alert medication in both inpatient and outpatient settings. Insulin can cause significant harm when administered in error. Despite advancements in insulin pen technology, errors in the administration technique remain an issue. Although various factors can contribute to administration errors, lack of education on how to operate these devices is one of the most common reasons they occur. As such, the mechanical technique used by the patient needs to be continually assessed in order to reinforce education where needed. We describe three unique patient cases that depict incorrect administration techniques when using pen devices and the consequences that could have resulted from these errors. These cases involve the use of a syringe instead of a pen needle, injecting without removing the inner cap, and dialing the pen back down instead of pushing the plunger. Although pen devices are …",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:UxriW0iASnsC,,1
634,Post-sunset rise of equatorial F layer—or upwelling growth?,"According to the so-called upwelling paradigm, development of equatorial plasma bubbles (EPBs) involves (1) appearance of an upwelling (i.e., local uplift with a zonal width of ~ 400 km) in the bottomside of the equatorial F layer, (2) its growth via the F-region interchange instability during the post-sunset rise (PSSR) of the F layer, and (3) launching of EPBs, which starts near the end of PSSR, from within the confines of the upwelling. In this description, the PSSR is presumed to be the primary driver of the paradigm, with upwelling growth dependent on PSSR strength. As constructed, the paradigm describes EPB development when PSSR is strong (i.e., high solar activity), but not when it is weak. We, show, for the first time, that when PSSR is weak (e.g., low solar activity), upwelling growth can still be comparable in strength to what would be considered a strong PSSR, and that this growth drives EPB development. Given that EPBs do not develop outside of upwellings, regardless of solar activity, we are led to conclude, against mainstream thinking, that the controlling driver for EPB development is upwelling growth, not PSSR. For continued progress toward understanding EPB development, a crucial next step is to identify the source mechanism for upwelling growth, especially when PSSR is weak, and to better understand the complexities of the underlying physics.",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:R3hNpaxXUhUC,,1
635,A method for Vietnamese Text Normalization to improve the quality of speech synthesis,"Being necessary for a Text-To-Speech (TTS) system, text-normalization is general a challenging problem, especially for Vietnamese because of the local context. Recent researches in text-normalization in Vietnamese for TTS systems are still at the beginning with very simple sets of ad hoc rules for individual cases in spite of the ambiguity of real text. The purpose of this paper is to take some initial steps towards methodically normalizing input text in Vietnamese for a TTS system. This paper proposes a categorization and a normalization model for Vietnamese text based on related results for other languages. An experimental application is implemented to demonstrate the model, which uses several techniques including letter language model and decision trees for classifying NSWs and both supervised and unsupervised approaches for expanding abbreviations.",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:qjMakFHDy7sC,,1
636,Improvement of mangrove soil carbon stocks estimation in North Vietnam using Sentinel-2 data and machine learning approach,"Quantifying total carbon (TC) stocks in soil across various mangrove ecosystems is key to understanding the global carbon cycle to reduce greenhouse gas emissions. Estimating mangrove TC at a large scale remains challenging due to the difficulty and high cost of soil carbon measurements when the number of samples is high. In the present study, we investigated the capability of Sentinel-2 multispectral data together with a state-of-the-art machine learning (ML) technique, which is a combination of CatBoost regression (CBR) and a genetic algorithm (GA) for feature selection and optimization (the CBR-GA model) to estimate the mangrove soil C stocks across the mangrove ecosystems in North Vietnam. We used the field survey data collected from 177 soil cores. We compared the performance of the proposed model with those of the four ML algorithms, i.e., the extreme gradient boosting regression (XGBR), the …",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:4DMP91E08xMC,,1
637,"Effects of tidal forcing, conductivity gradient, and active seeding on the climatology of equatorial spread F over Kwajalein","Plasma structure in nighttime equatorial F layer, referred to as equatorial spread F (ESF), displays climatology whose seasonal variation depends on longitude. At longitudes where ESF favors equinoxes, times when maxima occur can be predicted in terms of the day of year, when E region sunset is simultaneous in conjugate hemispheres (i.e., “sunset nodes”). Aside from occurrences around equinoxes, there are only three longitudes where ESF also occurs during a solstice; one is the central Pacific region. Here ESF activity is strong during the June solstice, when solar activity is high. To understand this puzzling behavior, ESF climatology over the Kwajalein Atoll was compared with properties of the postsunset rise (PSSR) of the F layer and seeding activity in the troposphere. The key findings are as follows: (1) Maxima in PSSR velocity (VPSSR) are better aligned with equinoxes than with sunset nodes; hence …",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:_Qo2XoVZTnwC,,1
638,HMM-based TTS for Hanoi Vietnamese: issues in design and evaluation,"This paper presents the development and evaluation of an HMM-based TTS system for the modern Hanoi dialect of Northern Vietnamese, a tonal language. A study of specific phonetic and prosodic features of Hanoi Vietnamese is discussed. Consequences on the design of an HMM-based TTS system are derived. Using this knowledge, a TTS system, called VTed, is then developed under the Mary TTS platform. The second part of the paper is devoted to perceptual evaluations of Vietnamese speech synthesis. Three kinds of evaluations are considered necessary for quality assessment of this tonal language. The general MOS assessment, utterance- level intelligibility, and tone-level intelligibility tests are conducted on the VTed system under a   natural speech reference   condition. The results show 1.21 points difference between natural and synthetic speech for the MOS test, a 0.2%    0.9% difference for the utterance-level intelligibility test, 23% on average and    depending on the tone type    from 0% to 37% difference for the tone-level intelligibility test. These results demonstrate the need for more specific works on tonal/prosodic level to improve automatic synthesis of Vietnamese and other tonal languages.",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:u5HHmVD_uO8C,,1
639,Non-uniform unit selection in Vietnamese speech synthesis,"In concatenative-based speech synthesis systems, speech is generated by concatenating acoustic units together, so selection of these units directly impacts the quality of synthetic speech. In our previous Text To Speech (TTS) system [8], speech was synthesized by concatenating acoustic units together. These units were of a single type, such as diphones or half syllables. Thanks to recent improvements in CPU speed and memory capacity, we can now increase the database size and perform more complex searches. In this paper, we develop the method of non-uniform unit selection using many different types of units. We find that the quality of speech is directly related to the size of the units used. This method was applied in different ways in different languages. This paper describes the way of applying this method for Vietnamese TTS to improve the quality of speech synthesis system.",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:2osOgNQ5qMEC,,1
640,"In vitro toxicity, genotoxicity and antigenotoxicity of Nigella sativa extracts from different geographic locations","Nigella sativa or black cumin is used as a spice and as a natural remedy against a great variety of illnesses. However, plants growing at different locations, i.e., coping with different environmental stress conditions, may have different contents of specialized compounds include alkaloids, flavonoids, and terpenoids. Therefore their properties and biological effects may be different.",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:pyW8ca7W8N0C,,1
641,"Evaluation of sperm DNA fragmentation index, Zinc concentration and seminal parameters from infertile men with varicocele","The aim of this study was to investigate the effect of varicocele on DNA fragmentation index (DFI), zinc concentration and seminal parameters in infertile patients. In this prospective study, 179 men with at least 1‐year history of infertility and varicocele were examined for semen quality at Hanoi Medical University Hospital (HMUH), Hanoi, Vietnam. In addition, an inverse correlation between zinc concentration and the degree of sperm DNA fragmentation in patients with clinical varicocele was found. The difference in mean values of sperm DNA fragmentation index in patients with various grades of varicoceles can be neglected, whereas most patients with varicocele of grades II and III had DFI >30%. Varicocele is associated with high levels of DNA damage in spermatozoa and reduced zinc levels that correlate with different grades of disease. Therefore, DNA fragmentation index and zinc concentration can be used as …",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:ZeXyd9-uunAC,,1
642,Four new lignans and IL-2 inhibitors from Magnoliae Flos,"Four new lignans, a furofuran lignan medioresinol B (10) and three tetrahydrofuran lignans kobusinol C (16), 7′-methoxy magnostellin A (21), and mangnostellin D (23), along with 19 known lignans, were isolated from the flower buds of Magnolia biondii PAMP. The structures of the isolates were elucidated using spectroscopic analysis, mainly one-and two-dimensional NMR, high resolution-MS, and circular dichroism techniques as well as Mosher’s esterification method. The anti-allergic effects of the isolated compounds were evaluated by analyzing the inhibition of interleukin-2 (IL-2) expression in Jurkat T-cells. Compounds 11–14 reduced IL-2 expression in a dose-dependent manner.",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:5nxA0vEk-isC,,1
643,BBrowser: Making single-cell data easily accessible,"BioTuring’s BBrowser is a software solution that helps scientists effectively analyze single-cell omics data. It combines big data with big computation and modern data visualization to create a unique platform where scientists can interact and obtain important biological insights from the massive amounts of single-cell data. BBrowser has three main components: a curated single-cell database, a big-data analytics layer, and a data visualization module. BBrowser is available for download at: https://bioturing.com/bbrowser/download.",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:dshw04ExmUIC,,1
645,Tobacco cessation in Vietnam: exploring the role of village health workers,"The purpose of this study was to explore current tobacco use treatment (TUT) practice patterns, and attitudes and beliefs among Village Health Workers (VHWs) about expanding their role to include delivering smoking cessation interventions and the perceived barriers. We conducted a survey of 449 VHWs from 26 communes in Thai Nguyen province, Vietnam. We assessed TUT practice patterns including asking about tobacco use, advising smokers to quit, offering assistance (3As) and attitudes, self-efficacy, and norms related to TUT. Seventy two per cent of VHWs reported asking patients if they use tobacco, 78.6% offered advice to quit, and 41.4% offered cessation assistance to few or more patients in the past month. Self-efficacy was low, with 53.2% agreeing that they did not have the skills to counsel patients about smoking cessation. The most commonly reported barriers to offering TUT were a lack of training …",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:P5F9QuxV20EC,,1
646,Practice environmental cost management accounting: The case of Vietnamese brick production companies,"During the past few years, there has been a growing interest in applications of environmental cost management accounting (ECMA) in different manufacturing industries. ECMA is a useful tool for environmental performance improvement and achieving better financial benefits. Nevertheless, ECMA implementation is in early stages in many countries such as Vietnam. Brick industry is one of the most polluted sectors, which has significant impact on the environment. The purpose of this paper is to provide a clear image of ECMA application in Vietnamese brick manufacturing enterprises. Both qualitative and quantitative research methodologies are used to analyze the participants' attitudes, perceptions and concerns about the potential for applying ECMA. The results indicate that the brick industry adopted low level of ECMA and provide some necessary suggestions to promote the enterprises applying ECMA.",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:hqOjcs7Dif8C,,1
647,Auditing financial statements in insurance enterprises: The case of Vietnam,"This study is conducted to study the implementation of the audit techniques and procedures executed by auditors using risk-based approach. The study also determines factors affecting audit quality of financial statements in Vietnamese insurance enterprises. Quantitative method is used in the study where questionnaires are sent to 250 independent auditors and 186 valid questionnaires are encrypted, declared and entered into SPSS software version 22.0. The results show that in audit techniques, except for document testing and verification technique, there were significant differences in application level of material testing, observation, interview, calculation and analysis techniques between big 4 firms and non-big 4 firms. The analysis also confirms that the most significant difference was between the two audit groups. In audit procedures, while control testing is not performed singly, detailed testing can be performed alone at all three assurance levels and the analysis is performed alone in the assurance level of 1. It is possible to combine them in these proce-dures with the assurance level of 2 and 3. On the other hand, the combination of these three proce-dures is only guaranteed at level 3. The results of the factor analysis and regression model test demonstrate that there were six factors influencing the audit quality of financial statements in insur-ance enterprises; namely audit method, auditors’ quality, awareness of auditors and director board, characteristics of insurance enterprises, legal environment and position of the audit firm. In particular, audit method and auditors’ quality are the strongest factors.",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:0EnyYjriUFMC,,1
648,Adsorptive removal of cationic dyes using hybrid material-based polyelectrolyte modified laterite soil,"Strong polyanion, polystyrene sulfonate (PSS) on the laterite soil to form a new material has been investigated to remove cationic dyes from aqueous solution. The laterite soil after surface modification by PSS was characterized by Fourier transform infrared spectroscopy (FT-IR), Brunauer-Emmett-Teller (BET), scanning electron microscopy (SEM), transmission electron microscopy (TEM), X-ray diffraction (XRD) and zeta potential measurements. The conditions for adsorption of PSS on laterite soil were selected as, contact time 150 min, 50 mM NaCl, pH 4, solid-liquid ratio 5 mg/mL. The laterite soil after PSS adsorption was called polymer modified laterite (PML). The PML was used for study on the removal of the molecular organic dyes, crystal violet (CV) and methylene blue (MB). The effective parameters for removal of both CV and MB using PML were adsorption time 75 min, while the optimum pH values were 8 …",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:_kc_bZDykSQC,,1
649,Adherence to antiplatelet therapy after coronary intervention among patients with myocardial infarction attending Vietnam National Heart Institute,"Adherence to antiplatelet therapy is critical to successful treatment of cardiovascular conditions. However, little has been known about this issue in the context of constrained resources such as in Vietnam. The objective of this study was to examine the adherence to antiplatelet therapy among patients receiving acute myocardial infarction interventions and its associated factors. In a cross-sectional survey design, 175 adult patients revisiting Vietnam National Heart Institute diagnosed with acute myocardial infarction were approached for data collection from October 2014 to June 2015. Adherence to antiplatelet therapy was assessed by asking patients whether they took taking antiplatelet regularly as per medication (do not miss any dose at the specified time) for any type of antiplatelet (aspirin, clopidogrel, ticlopidine...) during the last month before the participants came back to take re-examinations. The results indicated that the adherence to antiplatelet therapy among patients was quite high at 1 month; it begins to decline by 6 months, 12 months, and more than 12 months (less than 1 month was 90.29%; from 1 to 6 months 88.0%, from 6 to 12 months 75.43%, and after 12 months only 46.29% of patients). Multivariable logistic regression was utilized to detect factors associated with the adherence to antiplatelet therapy. It showed that patients with average income per month of $300 or more (OR=2.92, 95% CI=1.24-6.89), distance to the hospital of less than 50km (OR=2.48, 95% CI: 1.12-5.52), taking medicine under doctor’s instructions (OR=3.65; 95% CI=1.13-11.70), and timely re-examination (OR=3.99, 95% CI=1.08-14.73) were more likely to …",50,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7ebuo_9_TsgC&citation_for_view=7ebuo_9_TsgC:UebtZRa9Y70C,,1
653,Hierarchical fusion network for periocular and iris by neural network approximation and sparse autoencoder,"The eye region is one of the most attractive sources for identification and verification due to the representative availability of such biometric modalities as periocular and iris. Many score-level fusion approaches have been proposed to combine these two modalities targeting to improve the robustness. The score-level approaches can be grouped into three categories: transformation-based, classification-based and density-based. Each category has its own benefits, if combined can lead to a robust fusion mechanism. In this paper, we propose a hierarchical fusion network to fuse multiple fusion approaches from transformation-based and classification-based categories into a unified framework for classification. The proposed hierarchical approach relies on the universal approximation theorem for neural networks to approximate each fusion approach as one child neural network and then ensemble them into a …",51,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Kb80d2kAAAAJ&citation_for_view=Kb80d2kAAAAJ:u5HHmVD_uO8C,,1
654,A Two-level Genetic Algorithm for Inter-domain Path Computation under Node-defined Domain Uniqueness Constraints,"Recent years have witnessed an increment in the number of network components communicating through many network scenarios such as multi-layer and multi-domain, and it may result in a negative impact on resource utilization. An urgent requirement arises for routing the packets most efficiently and economically in large multi-domain networks. In tackling this complicated area, we consider the Inter-Domain Path Computation problem under Node-defined Domain Uniqueness Constraint (IDPC-NDU), which intends to find the minimum routing cost path between two nodes that traverses every domain at most once. Owing to the NP-Hard property of the IDPC-NDU, applying metaheuristic algorithms to solve this problem usually proves more efficient. In like manner, this paper proposes a Two-level Genetic Algorithm (PGA), where the first level determines the order of the visited domains, and the second level finds …",51,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Kb80d2kAAAAJ&citation_for_view=Kb80d2kAAAAJ:9yKSN-GCB0IC,,1
655,A two-level strategy based on evolutionary algorithm to solve the inter-domain path computation under node-defined domain uniqueness constraint,"The Inter-Domain Path Computation problem under Node-defined Domain Uniqueness constraint (IDPC-NDU) is a recently investigated topic for finding the effective routing paths on the multi-domain network topology as well as transportation. The objective of the IDPC-NDU is to find the shortest path in the multi-domain directed graph that traverses every domain at most once. Since the IDPC-NDU belongs to NP-Hard class, this paper proposes a novel two-level approach based on an Evolutionary Algorithm (EA) to solve it. The first level aims to determine the sequence of crossed domains using an improved Genetic Algorithm (GA), while the second one aims to locate the minimally costly path between two nodes among the entire domains. Furthermore, we devise an approach to represent a chromosome, which reduces the chromosome length to the number of domains. Experiments on numerous sets of instances …",51,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Kb80d2kAAAAJ&citation_for_view=Kb80d2kAAAAJ:d1gkVwhDpl0C,,1
656,An Efficient Two-Phase Metaheuristic for the Multiple Minimum Back-Walk-Free Latency Problem,"We extend the Multiple Minimum Latency Problem (mMLP) by ignoring the back-walking costs; the resulting problem is called the Multiple Minimum Back-Walk-Free Latency Problem (mMBLP). In this article, we provide a two-phase metaheuristic algorithm for this problem. In a first phase, the Insertion Heuristic (IH) builds an initial solution while the Randomized Variable Neighborhood Search (RVND) combines with the perturbation and Adaptive Memory (AM) techniques to generate numerous neighborhoods in a second phase. This combination prevents the search from local optima. The algorithm is implemented with benchmark dataset. The results indicate that the problems with up to 76 vertices can be found exactly in a short time. Moreover, the algorithm is comparable with the other metaheuristic algorithms in accordance with the solution quality. ",51,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Kb80d2kAAAAJ&citation_for_view=Kb80d2kAAAAJ:u-x6o8ySG0sC,,1
657,A note on the width of sparse random graphs,"In this note, we consider the width of a supercritical random graph according to some commonly studied width measures. We give short, direct proofs of results of Lee, Lee and Oum, and of Perarnau and Serra, on the rank- and tree-width of the random graph  when  for  constant. Our proofs avoid the use, as a black box, of a result of Benjamini, Kozma and Wormald on the expansion properties of the giant component in this regime, and so as a further benefit we obtain explicit bounds on the dependence of these results on . Finally, we also consider the width of the random graph in the weakly supercritical regime, where  and . In this regime, we determine, up to a constant multiplicative factor, the rank- and tree-width of  as a function of  and .",51,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Kb80d2kAAAAJ&citation_for_view=Kb80d2kAAAAJ:IjCSPb-OGe4C,,1
658,Component behaviour and excess of random bipartite graphs near the critical point,"The binomial random bipartite graph  is the random graph formed by taking two partition classes of size  and including each edge between them independently with probability . It is known that this model exhibits a similar phase transition as that of the binomial random graph  as  passes the critical point of . We study the component structure of this model near to the critical point. We show that, as with , for an appropriate range of  there is a unique `giant' component and we determine asymptotically its order and excess. We also give more precise results for the distribution of the number of components of a fixed order in this range of . These results rely on new bounds for the number of bipartite graphs with a fixed number of vertices and edges, which we also derive.",51,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Kb80d2kAAAAJ&citation_for_view=Kb80d2kAAAAJ:UeHWp8X0CEIC,,1
659,Component Behaviour of Random Bipartite Graphs,"We study the component behaviour of the binomial random bipartite graph G(n, n, p) near the critical point. We show that, as is the case in the binomial random graph G(n, p), for an appropriate range of p there is a unique ‘giant’ component of order at least  and determine asymptotically its order and excess. Our proofs rely on good enumerative estimates for the number of bipartite graphs of a fixed order, as well as probabilistic techniques such as the sprinkling method.",51,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Kb80d2kAAAAJ&citation_for_view=Kb80d2kAAAAJ:qjMakFHDy7sC,,1
660,Planarity and genus of sparse random bipartite graphs,"The genus of the binomial random graph  is well understood for a wide range of . Recently, the study of the genus of the random bipartite graph , with partition classes of size  and , was initiated by Mohar and Ying, who showed that when  and  are comparable in size and  is significantly larger than  the genus of the random bipartite graph has a similar behaviour to that of the binomial random graph. In this paper we show that there is a threshold for planarity of the random bipartite graph at  and investigate the genus close to this threshold, extending the results of Mohar and Ying. It turns out that there is qualitatively different behaviour in the case where  and  are comparable, when whp the genus is linear in the number of edges, than in the case where  is asymptotically smaller than , when whp the genus behaves like the genus of a sparse random graph  for an appropriately chosen .",51,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Kb80d2kAAAAJ&citation_for_view=Kb80d2kAAAAJ:2osOgNQ5qMEC,,1
661,Vietnamese diacritics restoration as sequential tagging,"Diacritics restoration is the process of restoring original script from diacritic-free script by correct insertion of diacritics. In this paper, this problem is casted as a sequential tagging task where each term is tagged with its own accents. We did careful evaluations on three domains of Vietnamese: writing language, spoken language and literature using two methods: conditional random fields (CRFs) and support vector machines (SVMs), and achieved promising results. We also investigated two levels of lexical: learning from letters and learning from syllables. Although the former performs poorly than the latter, it shows stable results in all three language domains. Therefore, the letter level approach is more useful when we have to deal with unknown words or when words in a sentence are reordered and repeated to achieve stylistic and artistic effect.",53,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xFvB5kwAAAAJ&citation_for_view=xFvB5kwAAAAJ:2osOgNQ5qMEC,,1
662,Using main content extraction to improve performance of Vietnamese web page classification,"Web page classification is the process of categorizing a web page into one or more classes which have been predetermined. If we remove all HTML tags from a web page, then this process can be considered as a text classification problem. However, this approach does not achieve high precision due to noisy contents, which always exist in regular HTML documents. To address this problem, we propose using a content extraction method to extract the main contents of the web pages and use them for the classification task. Experimental results show that the proposed method significantly improves the precision of the Vietnamese web page classification from 71% to 80%. It also indicates that context features such as the anchor texts of reference links and the contents of tags"" TITLE"" can use as a good summarization for web page contents.",53,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xFvB5kwAAAAJ&citation_for_view=xFvB5kwAAAAJ:9yKSN-GCB0IC,,1
663,An intelligent natural language interface to relational databases,"This paper is a study on constructing a natural language interface to relational databases, which accepts natural language fuzzy questions as inputs and generates answers under the form of tables or short answers. Firstly, the question is parsed using a semantic grammar and then, it is translated into a SQL query using a set of translation rules. Finally, a database management system is left to find the result table with its own specialized optimization and planning techniques. Experimental results show that this approach can analyze a wide range of questions with high accuracy and produce reasonable answers.",53,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xFvB5kwAAAAJ&citation_for_view=xFvB5kwAAAAJ:qjMakFHDy7sC,,1
666,Generating qualified summarization answers using fuzzy concept hierarchies,"In this paper, we introduce a partially automated method to generate qualified answers at multiple abstraction levels for database queries. We examine the issues involving data summarization by Attribute-Oriented Induction (AOI) on large databases using fuzzy concept hierarchies. Because a node may have many abstracts, the fuzzy hierarchies become more complex and vaguer than crisp ones. Therefore, we cannot use exactly the original AOI algorithm with crisp hierarchies, applied for fuzzy hierarchies, to get interesting answers. The main contribution of this paper is that we propose a new approach to refine fuzzy hierarchies and evaluate tuple-terminal conditions to reduce noisy tuples. The foundations of our approach are the generalization hierarchy and a new method to estimate tuple quality. We implemented the algorithm in our knowledge discovery system and the experimental results show that the …",53,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xFvB5kwAAAAJ&citation_for_view=xFvB5kwAAAAJ:d1gkVwhDpl0C,,1
672,A real-time model based support vector machine for emotion recognition through EEG,"Recently, there has been a significant amount of work on the recognition of human emotions. The results of the work can be applied in real applications, for example in market survey or neuro-marketing. This interesting problem requires to recognize naturally human emotions which come from our mind but ignore the external expressions fully controlled by a subject. A popular approach uses key information from electroencephalography (EEG) signals to identify human emotions. In this paper, we proposed an emotion recognition model based on the Russell's circumplex model, Higuchi Fractal Dimension (HFD) algorithm and Support Vector Machine (SVM) as a classifier. Moreover, we also proposed a method to determine an emotion label of a series of EEG signals. Our model includes two main approaches in machine learning step. In a first approach, machine learning was utilized for all EEG signals from …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:u5HHmVD_uO8C,,1
673,An efficient exact algorithm for Minimum Latency Problem,"ABSTRACT The Minimum Latency Problem (MLP) is a class of combinational optimization problems that has many practical applications. In the general case, the MLP is proved to be NP-hard. One of the approaches to solve the problem is using exact algorithms. However, the algorithms which were recently proposed are applied only to the problems with small size, ie, 26 vertices. In this paper, we present a new exact algorithm to solve the MLPs with a larger size. Our algorithm is based on the branch and bound method and it has two new rules that improve the pruning technique. We have evaluated the algorithm on several data sets. The results show that the problems up to 40 vertices can be solved exactly.",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:d1gkVwhDpl0C,,1
674,Improved genetic algorithm for minimum latency problem,"Minimum Latency Problem (MLP), also known as traveling repairman problem, in general case, is proved to be NP--hard. This paper presents several new techniques based on the scheme of the genetic algorithm for solving MLP. The experimental results on the proposed algorithm show that it gives the approximation ratio which is about 1.9 times higher than the lower bound on the optimal solution.",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:u-x6o8ySG0sC,,1
675,A meta-heuristic algorithm combining between Tabu and variable neighborhood search for the minimum latency problem,"Minimum Latency Problem (MLP) is a class of NP-hard combinatorial optimization problems which has many practical applications. In this paper, we investigate the global structure of the MLP solution space to propose a suitable meta-heuristic algorithm for the problem, which combines Tabu search (TS) and Variable Neighborhood Search (VNS). In the proposed algorithm, TS is used to prevent the search from getting trapped into cycles, and guide VNS to escape local optima. In a cooperative way, VNS is employed to generate diverse neighborhoods for TS. We also introduce a novel neighborhoods’ structure for VNS and present a constant time operation for calculating the latency cost of each neighboring solution. Extensive numerical experiments and comparisons with the state of the art meta-heuristic algorithms in the literature show that the proposed algorithm is highly competitive, providing the new best …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:2osOgNQ5qMEC,,1
676,An efficient two-phase metaheuristic algorithm for the time dependent traveling Salesman problem,"The Time Dependent Traveling Salesman Problem (TDTSP) is a class of NP-hard combinatorial optimization problems which has many practical applications. To the best of our knowledge, developing metaheuristic algorithm for the problem has not been studied much before, even though it is a natural and general extension of the Minimum Latency Problem (MLP) or Traveling Salesman Problem (TSP). In this paper, we propose an effective two-phase metaheuristic which combines the Insertion Heuristic (IH), Variable Neighborhood Search (VNS) and the tabu search (TS) to solve the problem. In a construction phase, the IH is used to create an initial solution that is good enough. In an improvement phase, the VNS is employed to generate diverse and various neighborhoods, while the main attribute of tabu search is to prohibit our algorithm from getting trapped into cycles, and to guide the search to escape local …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:LkGwnXOMwfcC,,1
677,A grasp+ vnd algorithm for the multiple traveling repairman problem with distance constraints,"Multiple Traveling Repairmen Problem (MTRP) is a class of NP-hard combinatorial optimization problems. In this paper, an other variant of MTRP, also known as Multiple Traveling Repairmen Problem with Distance Constraint (MTRPD), is introduced. In MTRPD problem, a fleet of vehicles serves a set of customers. Each vehicle which starts from the depot is not allowed to travel any distance longer than a limit and each customer must be visited exactly once. The goal is to find the order of customer visits of all vehicles that minimizes the sum of all vertices’ waiting time. To the best of our knowledge, the problem has not been studied much previously, even though it is a natural and practical extension of the Traveling Repairman Problem or Multiple Traveling Repairmen Problem case. In our work, we propose a metaheuristic algorithm which is mainly based on the principles of Greedy Randomized Adaptive Search Procedure (GRASP) and Variable Neighborhood Descent (VND) to solve the problem. The GRASP is used to build an initial solution which is good enough in a construction phase. In a cooperative way, the VND is employed to generate diverse neighborhoods in an improvement phase, therefore, it can help the search escape from local optimal. Extensive numerical experiments on 321 benchmark instances show that our algorithm can find the optimal solutions with up to 50 vertices in several instances. For larger instances, our algorithm obtains provably near-optimal solutions, even for large instances.",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:WF5omc3nYNoC,,1
678,An effective GRASP+ VND metaheuristic for the k-Minimum Latency Problem,"Minimum Latency Problem (MLP) is a class of NP-hard combinatorial optimization problems which has many practical applications. In this paper, a general variant of MLP, also known as k-MLP is introduced. In k - MLP problem, the cost of objective function becomes the sum of waiting times at sites and k vehicles cover one of k routes. The goal is to find the order of customer visits that minimizes the sum of waiting time. The problem is a natural and practical extension of the k =1 case. In our work, we propose the first meta-heuristic algorithm which is mainly based on the principles of Greedy Randomized Adaptive Search Procedure (GRASP) and Variable Neighborhood Descent (VND) to solve the problem. The GRASP is used to build an initial solution which is good enough in construction phase. In a cooperative way, the VND is employed to generate diverse neighborhoods in improvement phase, therefore, it can …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:Y0pCki6q_DkC,,1
679,A metaheuristic for the delivery man problem with time windows,"The Delivery Man Problem with Time Windows (DMPTW) is an extension of the Delivery Man Problem.  The objective of DMPTW is to minimize the sum of customers’ arrival time while the deliveries are made during a specific time window given by the customers. Another close variant of objective is a travel duration. In the case, the problem minimizes the sum of travel durations between a depot and customer locations. It has many practical applications to network problems, e.g., whenever servers have to accommodate a set of requests to minimize clients’ total (or average) waiting time. To solve medium to large-sized instances, a two-phase metaheuristic algorithm is proposed. A construction stage generates a feasible solution using Neighborhood Descent with Random neighborhood ordering (RVND), and the optimization stage improves the feasible solution with an Iterated Local Search. Moreover, Tabu Search …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:3fE2CSJIrl8C,,1
680,A parallel algorithm combines genetic algorithm and ant colony algorithm for the minimum latency problem,"The Minimum Latency Problem is a class of combinatorial optimization problems which has many practical applications. Recently, several meta-heuristic algorithms were proposed. These algorithms were developed in either a trajectory-based meta-heuristic or sequential approach, which start with a single initial solution and, at each step of the search, the current solution is replaced by another (often the best) solution found in its neighborhood. Since the search space of the problem is combinatorial explosion, these algorithms often explore a subset of the search space, therefore, they can fall into local optima in some cases. In order to overcome the drawbacks of the current algorithms, we propose a population-based algorithm that combines the genetic algorithm (GA) and the ant colony algorithm (ACO) according to island parallel model. In our parallel approach, subpopulations are developed in different ways …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:UeHWp8X0CEIC,,1
681,A hybrid metaheuristic for solving asymmetric distance-constrained vehicle routing problem,"The Asymmetric Distance-Constrained Vehicle Routing Problem (ADVRP) is NP-hard as it is a natural extension of the NP-hard Vehicle Routing Problem. In ADVRP problem, each customer is visited exactly once by a vehicle; every tour starts and ends at a depot; and the traveled distance by each vehicle is not allowed to exceed a predetermined limit. We propose a hybrid metaheuristic algorithm combining the Randomized Variable Neighborhood Search (RVNS) and the Tabu Search (TS) to solve the problem. The combination of multiple neighborhoods and tabu mechanism is used for their capacity to escape local optima while exploring the solution space. Furthermore, the intensification and diversification phases are also included to deliver optimized and diversified solutions. Extensive numerical experiments and comparisons with all the state-of-the-art algorithms show that the proposed method is highly …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:MXK_kJrjxJIC,,1
682,The hybridization of ACO+ GA and RVNS algorithm for solving the time-dependent traveling salesman problem,"The time-dependent traveling salesman problem is a class of combinatorial optimization problems. Naturally, metaheuristic is a suitable approach to solve the problem with large sizes in short computation time. Previously, several metaheuristics have been proposed for solving the problem. These algorithms might have a strong search intensification, and their diversification mechanisms may not be sufficient. Due to the random nature, population-based algorithms improve on the chance of finding a globally. In this paper, we propose a population-based algorithm that combines an ant colony algorithm (ACO), genetic algorithm (GA), and neighborhood descent with random neighborhood ordering (RVND). In the algorithm, the ACO and GA are used to explore the promising solution areas that are yet to refined while the RVND exploits them with the hope of improving a solution. Therefore, our metaheuristic algorithm …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:YOwf2qJgpHMC,,1
683,General Variable Neighborhood Search for the Quote-Travelling Repairman Problem,"The Quota-Travelling Repairman Problem (QTRP) tries to find a tour that minimizes the waiting time while the profit collected by a repairman is not less than a predefined value. The Q-TRP is an extended variant of the Travelling Repairman Problem (TRP). The problem is NP-hard problem; therefore, metaheuristic is a natural approach to provide near-optimal solutions for large instance sizes in a short time. Currently, several algorithms are proposed to solve the TRP. However, the quote constraint does not include, and these algorithms cannot be adapted to the Q-TRP. Therefore, developing an efficient algorithm for the Q-TRP is necessary. In this paper, we suggest a General Variable Neighborhood Search (GVNS) that combines with the perturbation and Adaptive Memory (AM) techniques to prevent the search from local optima. The algorithm is implemented with a benchmark dataset. The results demonstrate that good solutions, even the optimal solutions for the problem with 100 vertices, can be reached in a short time. Moreover, the algorithm is comparable with the other metaheuristic algorithms in accordance with the solution quality.",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:Zph67rFs4hoC,,1
684,Metaheuristic for the Traveling Repairman Problem with Time Window Constraints,"Traveling Repairman Problem (TRP) is a class of NP-hard combinatorial optimization problems which has many practical applications. In this paper, a general variant of TRP, also known as TRPTW is introduced. The TRPTW problem deals with finding a tour in order to serve a set of locations, each one within a specified time window. Obviously, TRPTW is more complex than TRP because it is a generation of TRP. Due to NPhard problem, metaheuristic needs to be developed to provide near-optimal solutions within a short computation time for large instance sizes. However, the main issue of metaheuristics is that they fall into local optima in some cases since the search space of the problem is combinatorial explosion. In order to overcome the drawback, we propose a metaheuristic algorithm which is mainly based on Variable Neighborhood Search (VNS) and Shaking techniques to solve the problem. The aim of …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:_FxGoFyzp5QC,,1
685,A Subgradient Method to Improve Approximation Ratio in the Minimum Latency Problem,"The Minimum Latency Problem (MLP) is a combinatorial optimization problem which has many practical applications. Recently, several approximation algorithms with guaranteed approximation ratio have been proposed to solve the MLP problem. These algorithms start with a set of solutions of the k −MST or k −troll problem, then convert the solutions into Eulerian tours, and finally, concatenate these Eulerian tours to obtain a MLP tour. In this paper, we propose an algorithm based on the principles of the subgradient method. It still uses the set of solutions of the k −MST or k −troll problem as an input, then modifies each solution into a tour with cost smaller than that of Eulerian tour and finally, uses obtained tours to construct a MLP tour. Since the low cost tours are used to build a MLP tour, we can expect the approximation ratio of obtained algorithm will be improved. In order to illustrate this intuition, we …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:zYLM7Y9cAGgC,,1
686,An Efficient Two-Phase Metaheuristic for the Multiple Minimum Back-Walk-Free Latency Problem,"We extend the Multiple Minimum Latency Problem (mMLP) by ignoring the back-walking costs; the resulting problem is called the Multiple Minimum Back-Walk-Free Latency Problem (mMBLP). In this article, we provide a two-phase metaheuristic algorithm for this problem. In a first phase, the Insertion Heuristic (IH) builds an initial solution while the Randomized Variable Neighborhood Search (RVND) combines with the perturbation and Adaptive Memory (AM) techniques to generate numerous neighborhoods in a second phase. This combination prevents the search from local optima. The algorithm is implemented with benchmark dataset. The results indicate that the problems with up to 76 vertices can be found exactly in a short time. Moreover, the algorithm is comparable with the other metaheuristic algorithms in accordance with the solution quality. ",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:5nxA0vEk-isC,,1
687,Using Metaheuristic for Solving the Resource-Constrained Deliveryman Problem,"Deliveryman Problem (DMP) is a class of NP-hard combinatorial optimization problems which has many practical applications. In this paper, a general variant of DMP, also known as Resource-Constrained Deliveryman Problem (RCDMP) is introduced. The RCDMP problem deals with finding a tour with minimum waiting time sum so that it consumes not more than the Rmax unites of the resources, where Rmax is some constant. Due to NP-hard problem, metaheuristic approach needs to be developed to provide near-optimal solutions for large instance sizes at a reasonable amount of time. However, the main issue of metaheuristics is that they fall into local optima. In this work, we propose a metaheuristic algorithm which is mainly based on Randomized Variable Neighborhood Search (RVNS) with using Tabu list to solve the problem. The combination supports our algorithm to escape local optimal. Extensive …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:Se3iqnhoufwC,,1
688,A RVND+ ILS Metaheuristic to Solve the Delivery Man Problem with Time Windows,"The Delivery Man Problem with Time Windows (DMPTW) is an extension of the Delivery Man Problem (DMP). The objective of DMPTW is to minimize the sum of travel durations between a depot and several customer locations while the delivery made in a specific time window given by the customers. It has many practical applications in network, e.g., whenever servers have to accommodate a set of requests so as to minimize clients’ total (or average) waiting time. To solve medium and large size instances, two-phase metaheuristic algorithm is proposed. The first stage constructs a feasible solution using Neighborhood Descent with Random neighborhood ordering (RVND), and the optimization stage improves the feasible solution with an Iterated Local Search (ILS). The proposed algorithm is tested on the DMPTW benchmark instances from the literature. The results indicate that the developed algorithm has …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:roLk4NBRz8UC,,1
689,A hybrid algorithm combining genetic algorithm with ant colony algorithm for the minimum latency problem.,"MinimumLatency Problem is a class of combinatorial optimization problems that have many practical applications. In the general case, the problem is proven to be NP-hard. Therefore, using a meta-heuristic algorithm is a suitable approach for solving this problem. In this paper, we propose a meta-heuristic algorithm which combines Ant Colony (ACO) and Genetic Algorithm (GA). In our algorithm, ACO generates a population for GA. Meanwhile, the genetic information of GA helps ants to create a better population in the next step. In addition, to maintain the diversity of population, our algorithm uses three types of the ants which have different characteristics. We evaluate the algorithm on five benchmark data sets. The experimental results show that our algorithm gives a better solution than the state-of-the-art meta-heuristic algorithms on several instances of datasets.",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:IjCSPb-OGe4C,,1
690,An Experimental Study about Efficiency of the Approximation Algorithms for Minimum Latency Problem,"Minimum Latency Problem is a class of combinational optimization problems which have many practical applications. In general, the problem is proved to be NP-hard and unless P = NP, a polynomial time approximation scheme is unlikely to exist. Therefore, using approximation algorithm is a suitable approach for solving the problem. In recent times, several approximation algorithms were proposed. However, efficiency of the algorithms was only evaluated in a theoretical manner. They did not indicate real approximation ratio, running time in reality and there is a lack of comparison between the algorithms from experiments. In addition, quality of lower bound, which was an important part of the algorithms, was not mentioned. In an approach to fulfill these omissions, we implemented the algorithms on several test data to evaluate and compare real efficiency of them in terms of real ratio, running time and quality of …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:qjMakFHDy7sC,,1
691,Penalty Variable Neighborhood Search for the Bounded Single-Depot Multiple Traveling Repairmen Problem,"Multiple Traveling Repairmen Problem (mTRP) is a class of NP-hard combinatorial optimization problems with many practical applications. In this paper, a general variant of mTRP, also known as the Bounded Single-Depot Multiple Traveling Repairmen Problem (Bounded-mTRP), is introduced. In the Bounded-mTRP problem, a ñeet of identical vehicles is dispatched to serve a set of customers. Each vehicle that starts from the depot is only allowed to visit the number of customers within a predetermined interval, and each customer must be visited exactly once. Such restrictions appear in real-life applications where the purpose is to have a good balance of workloads for the repairmen. The goal is to ñnd the order of customer visits that minimizes the sum of waiting times. In our work, the proposed algorithm is encouraged by the efficiency of the algorithms in [15, 19, 20] that are mainly based on the principles of the …",54,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=XM8BiVwAAAAJ&citation_for_view=XM8BiVwAAAAJ:ULOm3_A8WrAC,,1
692,Bezier curve based path planning for autonomous vehicle in urban environment,"This paper presents a Bézier curve based path planner which enables the anti-collision behavior of an electronic car. The anti-collision system is a fundamental module in the architecture. The path tracking implementation uses pure pursuit algorithm. The anti-collision system based on laser scanner data consists of estimating the trajectories and behavior of surrounding objects, and a Bézier curve based path planner. Experimental results are presented showing the effectiveness of the overall navigation control system.",55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:UeHWp8X0CEIC,,1
693,Human drivers based active-passive model for automated lane change,"Lane change maneuver is a complicated maneuver, and incorrect maneuvering is an important reason for expressway accidents and fatalities. In this scenario, automated lane change has great potential to reduce the number of accidents. Previous research in this area, typically, focuses on the generation of an optimal lane change trajectory, while ignoring the human behavior model. To understand the human lane change behavior model, we carried out experiments on Japanese expressways. By analyzing the human-driver lane change data, we propose a two-segment lane change model that mimics the human-driver. We categorize the driving environment based on the observation grid and propose different lane change behaviors to handle the different scenarios. We develop an intuitive method to select the suitable lane change behavior, for a given scenario, using active (accelerate/decelerate) and passive …",55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:YsMSGLbcyi4C,,1
694,Unified path planner for parking an autonomous vehicle based on RRT,"Maneuvering autonomous vehicles in constrained environments, such as autonomous vehicle parking, is not a trivial task and has received increasing attention from both the academy and industry. However, the traditional methods divide the problem into parallel parking, perpendicular parking, and echelon parking, then different methods are applied for the parking motion planning. In this paper a Rapidly-exploring Random Tree (RRT) based path planner is implemented for autonomous vehicle parking problem, which treats all the situations in a unified manner. As the RRT method sometimes generates some complicated paths, a smoother is also implemented for smoothing generated paths. The proposed algorithm is verified in simulation and generates applicable solutions for the proposed application scenarios.",55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:u-x6o8ySG0sC,,1
695,General behavior and motion model for automated lane change,"Lane change maneuver is a cause for many severe highway accidents and automatic lane change has great potentials to reduce the impact of human error and number of accidents. Previous researches mostly tried to find an optimal trajectory and ignore the behavior model. Presented methods can be applied for simple lane change scenario and generally fail for complicated cases or in the presence of time/distance constraints. Through analysis and inspiring of human driver lane change data, we propose a multi segments lane change model to mimic the human driver for challenging scenarios. We also propose a method to convert behavior/motion selection to a time-based pattern recognition problem. We developed a simulation platform in PreScan and evaluated proposed automatic lane change method for challenging scenarios.",55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:Y0pCki6q_DkC,,1
696,Dynamic and Safe Path Planning Based on Support Vector Machine among Multi Moving Obstacles for Autonomous Vehicles,"We propose a practical local and global path-planning algorithm for an autonomous vehicle or a car-like robot in an unknown semi-structured (or unstructured) environment, where obstacles are detected online by the vehicle's sensors. The algorithm utilizes a probabilistic method based on particle filters to estimate the dynamic obstacles' locations, a support vector machine to provide the critical points and Bezier curves to smooth the generated path. The generated path safely travels through various static and moving obstacles and satisfies the vehicle's movement constraints. The algorithm is implemented and verified on simulation software. Simulation results demonstrate the effectiveness of the proposed method in complicated scenarios that posit the existence of multi moving objects.",55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:u5HHmVD_uO8C,,1
697,Narrow passage path planning using fast marching method and support vector machine,"This paper introduces a novel path planning method under non-holonomic constraint for car-like vehicles, which associates map discovery and heuristic search to attain an optimal resultant path. The map discovery applies fast marching method to investigate the map geometric information. After that, the support vector machine is performed to find obstacle clearance information. This information is then used as a heuristic function which helps greatly reduce the search space. The fast marching is performed again, guided by this function to generate vehicle motions under kinematic constraints. Experimental results have shown that this method is able to generate motions for non-holonomic vehicles. In comparison with related methods, the path generated by proposed method is smoother and stay farther away from the obstacles.",55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:W7OEmFMy1HYC,,1
698,A practical and optimal path planning for autonomous parking using fast marching algorithm and support vector machine,"This paper proposes a novel practical path planning framework for autonomous parking in cluttered environments with narrow passages. The proposed global path planning method is based on an improved Fast Marching algorithm to generate a path while considering the moving forward and backward maneuver. In addition, the Support Vector Machine is utilized to provide the maximum clearance from obstacles considering the vehicle dynamics to provide a safe and feasible path. The algorithm considers the most critical points in the map and the complexity of the algorithm is not affected by the shape of the obstacles. We also propose an autonomous parking scheme for different parking situation. The method is implemented on autonomous vehicle platform and validated in the real environment with narrow passages.",55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:_FxGoFyzp5QC,,1
699,Safe path planning among multi obstacles,"This paper proposed a practical path-planning algorithm for an autonomous vehicle or a car-like robot in an unknown semi-structured (or unstructured) environment, where obstacles are detected online by the vehicle's sensors. The algorithm is based on particle filter, Bézier curves and support vector machine to provide a safe path among various static and moving obstacles and to satisfy the vehicle's curvature constraints. The algorithm has been implemented and verified on the simulation software. Experimental results demonstrate the effectiveness of the proposed method in complicated conditions with existing of multi objects.",55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:9yKSN-GCB0IC,,1
700,Vehicle path planning with maximizing safe margin for driving using Lagrange multipliers,"We propose a path planning method for autonomous vehicle in cluttered environment with narrow passages. Different from traditional methods, we use a learning approach based on RBF kernel SVM to maximize the safety margin for driving. We use the Lagrange multipliers of SVM dual model to find most critical points in map and generate optimized hyperplane for path. The method is implemented on autonomous vehicle for outdoor parking and compared to well-known method in autonomous vehicle literatures. The experiments prove that the method is able to generate smooth and safe path in shorter time compared to other methods.",55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:qjMakFHDy7sC,,1
701,"Real time localization, path planning and motion control for autonomous parking in cluttered environment with narrow passages",This paper propose a novel practical method for autonomous parking in cluttered environment with narrow passages. We present a modified FastSLAM algorithm for environment mapping to reduce the map entropy and increase the localization accuracy for autonomous parking. The proposed path planning method is based on predefined arc paths for real time generation of smooth paths to avoid obstacles. The corresponding control commands are generated to minimize the steering angle control error which executed by the vehicle actuators. The proposed parking method is implemented on an autonomous vehicle platform and evaluated in the different environments with narrow passages.,55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:IjCSPb-OGe4C,,1
702,Optimal motion generation for autonomous vehicle in maze-like environment,"In car-like autonomous vehicle systems, it is an essential task of generating the motion commands according to a given path/strategy. Quite a few theories and techniques have been proposed for the generation of the motion commands in autonomous vehicles, such as pure pursuit, Stanley's nonlinear feedback, chained-form control of kinematic model and the linearized optimal control of dynamic model. Here a non-linear optimization algorithm based on the vehicle's kinematic model and the actuators' model is proposed, which combines the control system dynamic behaviors and gives out the control sequences directly. It starts with modeling the local kinematic behavior and actuators' dynamics. Then online optimization algorithm is applied to the objective function of minimizing the energy cost, execution time and tracking error with some trade-off weights among them. The experiments showed that it worked well …",55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:zYLM7Y9cAGgC,,1
703,Practical global and local path planning algorithm for autonomous vehicles parking,"This paper proposes a novel practical path planning framework for autonomous parking in cluttered environments with narrow passages. The proposed global path planning method is based on an improved Fast Marching algorithm to generate a path with considering the moving forward and backward maneuver. In addition, the Support Vector Machine is utilized to provide the maximum clearance from obstacles considering the vehicle dynamics to provide a safe and feasible path. We also propose a local path planning for obstacle avoidance and tracking global path. The method is implemented on an autonomous vehicle platform and validated in the real environment with narrow passages.",55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:LkGwnXOMwfcC,,1
704,"Real time Mapping, Path Planning and Control for Autonomous Parking",We present an architecture for autonomous parking at complex environment with narrow passages. A modified FastSLAM algorithm is developed to map environment by laser scanner. The proposed mapping method reduces the map entropy and increases the mapping and localization accuracy for safe autonomous parking. We also propose a real time path planning method to avoid static and moving obstacles and generate smooth movement for vehicle. The corresponding control commands for steering and acceleration are generated to minimize the heading and positioning errors during parking. The method is implemented and tested on autonomous vehicle platform for complicated parking in narrow passages. The experiments prove that the proposed method is able to do smooth and safe autonomous parking in real time at dynamic and complex parking environment.,55,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0DrURkEAAAAJ&citation_for_view=0DrURkEAAAAJ:d1gkVwhDpl0C,,1
711,Kendrick: A domain specific language and platform for mathematical epidemiological modelling,"Mathematical modelling of infectious diseases often uses simulation models in order to explore transmission mechanisms and to plan potential management strategies to control the epidemics. However, bridging the gap between the conceptual model of epidemiology and its simulation on computer can lead to some issues related to the lack of expressiveness of implemented models and their reusability and adaptability to new circumstances due to detail computer instructions in General-purpose Programming Languages (GPLs). In this paper, we propose to develop a Domain Specific Language (DSL) for expressively specifying mathematical models of epidemiology and to construct a flexible simulation environment for investigating them. We then use our proposed approach to experiment the measles model in different epidemiological aspects. Our platform is also validated through statistical comparisons …",56,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=aZosWNYAAAAJ&citation_for_view=aZosWNYAAAAJ:2osOgNQ5qMEC,,1
712,Separation of concerns in epidemiological modelling,"Modeling and simulation have been heavily used in epidemiology, for instance to study the transmission of infectious diseases, their pathogenicity and their propagation. A major hindrance to modeling in epidemiology is the mixing of concerns that ought to be separated. The most obvious one is the computer implementation that should not be mixed with domain aspects. But several domain concerns should also be separated from the core epidemiological ones. These include the distribution of the studied populations into spatial regions, age intervals, sexes, species, viral strains... We propose an approach that relies on a mathematical model of the dynamics of a compartment-based population. The separation of domain concerns is provided by expressing each one as a stochastic automaton and combining them with a tensor sum. A DSL, Kendrick, and a tool, support this approach that has been validated on …",56,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=aZosWNYAAAAJ&citation_for_view=aZosWNYAAAAJ:qjMakFHDy7sC,,1
713,Explicit composition constructs in dsls: The case of the epidemiological language kendrick,"Domain Specific Languages (DSL) are designed to be syntactically and semantically easier to use than general-purpose languages performing the same task. This is generally achieved by tailoring programming notions and constructs to the domain tasks at hand. Yet there are examples of domain specific problems that demand elaborate constructs (for eg aspects in Aspect DSLs) which need to be carefully introduced as to avoid complicating the language. We present such an example in Pharo for the epidemiological language KENDRICK. KENDRICK simplifies the programming of epidemiological simulations by decomposing highly-coupled monolithic models into modular concerns. This decomposition though, is based on a mathematical model that introduces non-trivial composition semantics into the language that need to be carefully integrated. In this work, we address this problem by extending KENDRICK's …",56,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=aZosWNYAAAAJ&citation_for_view=aZosWNYAAAAJ:d1gkVwhDpl0C,,1
715,"Separation of Concerns in Extended Epidemiological Compartmental Models--provisional version, see the final version on the conference page when available--","Epidemiological models become more and more complex as new concerns are taken into account (age, sex, spatial heterogeneity, containment or vaccination policies, etc.). This is problematic because these aspects are typically intertwined which makes models difficult to extend, change or reuse. The Kendrick approach has shown promising results to separate epidemiological concerns but is restricted to homogeneous compartmental models. In this paper, we report on an attempt to generalize the Kendrick approach to support some aspects of contact networks, thereby improving the predictive quality of models with significant heterogeneity in the structure of contacts, while keeping the simplicity of compartmental models. This approach has been validated on two different techniques to generalize compartmental models.",56,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=aZosWNYAAAAJ&citation_for_view=aZosWNYAAAAJ:UeHWp8X0CEIC,,1
716,Explicit Composition Constructs in DSLs,"Domain Specific Languages (DSL) are designed to be syntactically and semantically easier to use than generalpurpose languages performing the same task. This is generally achieved by tailoring programming notions and constructs to the domain tasks at hand. Yet there are examples of domain specific problems that demand elaborate constructs (for eg aspects in Aspect DSLs) which need to be carefully introduced as to avoid complicating the language. We present such an example in Pharo for the epidemiological language KENDRICK. KENDRICK simplifies the programming of epidemiological simulations by decomposing highly-coupled monolithic models into modular concerns. This decomposition though, is based on a mathematical model that introduces non-trivial composition semantics into the language that need to be carefully integrated. In this work, we address this problem by extending KENDRICK’s DSL with explicit composition semantics, entities and operators.",56,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=aZosWNYAAAAJ&citation_for_view=aZosWNYAAAAJ:9yKSN-GCB0IC,,1
731,Khảo sát tính ổn định của một số đặc trưng ngữ âm trong nhận dạng người nói,This paper presents the results of studying stability of the fundamental frequency and the first four formant frequencies in speaker recognition. The results were learned from the experiment of 35 people speaking Vietnamese in laboratory condition. Finally the use of these in the world of forensics is examined.,58,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=TLThMMQAAAAJ&citation_for_view=TLThMMQAAAAJ:_FxGoFyzp5QC,,1
734,Xây dựng và khảo sát độ dài từ khóa trong nhận dạng người nói phụ thuộc vào từ khóa tiếng Việt theo mô hình Markov ẩn,A text-dependent speaker identification system for Vietnamese language was proposed. Vietnamese isolated digits 09 and their concatenations were used for speaking text. Well-known Hiden Markov Model (HMM) was conducted for recognition engine. The problem is that how many word of the Vietnamese text so that the system is most effective. Studying the identification rate shows optimum number of words of the text for the system.,58,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=TLThMMQAAAAJ&citation_for_view=TLThMMQAAAAJ:UebtZRa9Y70C,,1
737,Blind source separation for convolutive mixtures,"Le problème de la séparation aveugle de sources est maintenant un problème bien connu, pour lequel de nombreuses méthodes ont été proposées dans le cas de mélanges instantanés, et même dans le cas de mélanges convolutifs de signaux en bande étroite. Dans ce papier, nous présentons des algorithmes pour la séparation de signaux à large bande, dans le cas où les mélanges convolutifs peuvent être modélisés par des filtres à Réponse Impulsionnelle Finie (RIF). Sous la condition de bruit blanc d'ordre 4, nous montrons dans ce papier que l'estimation des coefficients des filtres de separation peut être fondée sur l'annulation de cumulants croisés d'ordre 4 des sorties. Cependant, des algorithmes plus simples n'utilisant que quelques cumulants ou moments croisés d'ordre 4, et des moments d'ordre 2, présentent en pratique la même efficacité. Dans le cas de mélanges simulés, les résultats …",62,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ZbnhBSAAAAAJ&citation_for_view=ZbnhBSAAAAAJ:IjCSPb-OGe4C,,1
738,Systemic endothelial activation occurs in both mild and severe malaria. Correlating dermal microvascular endothelial cell phenotype and soluble cell adhesion molecules with …,"Fatal Plasmodium falciparum malaria is accompanied by systemic endothelial activation. To study endothelial activation directly during malaria and sepsis in vivo, the expression of cell adhesion molecules on dermal microvascular endothelium was examined in skin biopsies and correlated with plasma levels of soluble (circulating) ICAM-1, E-selectin, and VCAM-1 and the cytokine tumor necrosis factor (TNF)-alpha. Skin biopsies were obtained from 61 cases of severe malaria, 42 cases of uncomplicated malaria, 10 cases of severe systemic sepsis, and 17 uninfected controls. Systemic endothelial activation, represented by the up-regulation of inducible cell adhesion molecules (CAMs) on endothelium and increased levels of soluble CAMs (sCAMs), were seen in both severe and uncomplicated malaria and sepsis when compared with uninfected controls. Plasma levels of sICAM-1, sVCAM-1, and sE-selectin …",62,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ZbnhBSAAAAAJ&citation_for_view=ZbnhBSAAAAAJ:qxL8FJ1GzNcC,,1
739,Willingness to pay for social health insurance in central Vietnam,"Background A social health insurance (SHI) program was implemented in Vietnam in 1992. Participation is compulsory for some groups, such as formal-sector workers, and voluntary for other groups. In 2013, 68% of the total population was covered by SHI, with most enrollees from compulsory groups. Enrollment has remained low among persons whose enrollment is voluntary. As a result, households face financial risk due to high out-of-pocket payments for health care. The goal of this study is to identify willingness to pay (WTP) for the SHI scheme among persons whose enrollment is voluntary and to examine factors that influence their choice. Method Three hundred thirty-one (331) uninsured persons from three districts and one city of Thua Thien Hue province were interviewed face to face using a structured questionnaire. Contingent valuation technique was used to assess the WTP among the study participants. Each individual was asked to choose the maximum premium they were willing to pay for a health insurance card per year with three copayment levels of 0%, 10%, and 20%. Seven premium levels were offered ranging from 0 Vietnamese Dong (VND) to 900,000 VND (42.12 USD). The mean WTP of respondents for each scenario was estimated. Multiple linear regression analysis was used to identify factors influencing WTP for SHI. Results The survey found that 73.1%, 72.2%, and 71.6%, respectively for each copayment level, of the respondents would agree to participate in the SHI scheme and are willing to pay an annual premium of 578,926 VND (27.1 USD); 473,222 VND (22.1 USD); and 401,266 VND (18.8 USD) at the copayment …",62,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ZbnhBSAAAAAJ&citation_for_view=ZbnhBSAAAAAJ:mVmsd5A6BfQC,,1
740,Agroforestry: contribution to food security and climate-change adaptation and mitigation in Southeast Asia,"Agroforestry, the practice of using trees on farms, and the integration of forestry and agriculture as part of multifunctional landscapes, can provide multiple economic, social and environmental benefits. In a global comparison, Southeast Asia had the highest biomass carbon stock (per hectare) on agricultural land of all regions, with an increasing trend (60 tonnes of carbon per hectare in 2000 and 65 tonnes per hectare in 2010). Farmers in different parts of the region have been adopting diverse agroforestry systems and practices, increasing the productivity of farmlands, and helping to secure food, income and other basic needs. At the same time, the significant amount of carbon sequestered by trees in agroforestry systems is contributing to climate-change mitigation, while forests and trees are ‘air conditioners’ cooling their surroundings by bringing water back into the atmosphere. Yet, wide-scale agroforestry …",62,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ZbnhBSAAAAAJ&citation_for_view=ZbnhBSAAAAAJ:35N4QoGY0k4C,,1
741,Cost-effectiveness analysis of a screening program for breast cancer in Vietnam,This study aimed to evaluate the cost-effectiveness of a screening program for breast cancer from the health care payers’ perspective.,62,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ZbnhBSAAAAAJ&citation_for_view=ZbnhBSAAAAAJ:4TOpqqG69KYC,,1
742,A novel approach for text detection in images using structural features,"We propose a novel approach for finding text in images by using ridges at several scales. A text string is modelled by a ridge at a coarse scale representing its center line and numerous short ridges at a smaller scale representing the skeletons of characters. Skeleton ridges have to satisfy geometrical and spatial constraints such as the perpendicularity or non-parallelism to the central ridge. In this way, we obtain a hierarchical description of text strings, which can provide direct input to an OCR or a text analysis system. The proposed method does not depend on a particular alphabet, it works with a wide variety in size of characters and does not depend on orientation of text string. The experimental results show a good detection.",62,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ZbnhBSAAAAAJ&citation_for_view=ZbnhBSAAAAAJ:Tyk-4Ss8FVUC,,1
743,A combined finite difference and finite element model for temperature and stress predictions of cast-in-place cap beam on precast columns,"In this study, a combined finite difference and finite element model was developed to predict the temperature development, thermally induced stresses and associated cracking risk in the concrete of a cast-in-place cap beam cast on precast columns of a bridge. The numerical model considers degree of hydration dependent heat rate, Young’s modulus development, strength development and early age tensile and compressive creep behavior. The temperature and stress analyses were performed on two sections of a cast-in-place cap beam (with a cross section of 1.6 m × 2.1 m): one at mid-span of the cap beam and the other on top of the precast column. The results show that the section of the cap beam at the column had high tensile stresses at the mid-sides which exceeded the early age concrete tensile strength when not covered with insulation blankets during construction. Additionally, the use of insulation …",62,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ZbnhBSAAAAAJ&citation_for_view=ZbnhBSAAAAAJ:vV6vV6tmYwMC,,1
744,A multibiometric encryption key algorithm using fuzzy vault to protect private key in BioPKI based security system,"The Public Key Infrastructure (PKI) provides excellent security for e-transactions and other applications. However, in traditional PKI system, Private Key is often stored at central databases or inside distributed smart-cards and protected by password or PIN that may be lost, stolen, forgotten or guessed. Current trend for PKI system is to be based on physiological and behavioral characteristics of people, known as biometrics. But this approach still poses a threat which is the secrecy of template. Moreover, biometric based security system also faces some problems such as: noise in sensed data, intra-class variations, inter-class similarities, non-universality, spoofing attack. In this paper, we propose a Multibiometric Encryption Key Algorithm to overcome the aforementioned issues by using Fuzzy Vault Scheme - a cryptosystem - which secures both the Private Key and multi-biometric templates. We also present the BioPKI …",62,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ZbnhBSAAAAAJ&citation_for_view=ZbnhBSAAAAAJ:u-x6o8ySG0sC,,1
745,An approach to protect private key using fingerprint biometric encryption key in BioPKI based security system,"In traditional Public Key Infrastructure (PKI) system, Private Key could be stored in central database or store distributed in smart-card and delivered to the users. The Private Key is usually protected by passwords that are easily guessed or stolen and thus lead to the collapse of the whole system. Current trend for PKI system is based on physiological and behavioral characteristics of persons, known as biometrics. This approach can increase the security of Private Key because in theory, the biometric features could not be guessed or forged. However, this approach still reveals a gap that is the vulnerability of storage device of Private Key and biometrics data. Malefactors can attack directly to these storage devices and steal user identification information. In this paper, we propose a solution that uses Biometric Encryption Key (BEK) to encrypt Private Key and protect Private Key in a secure way for both of two these kind …",62,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ZbnhBSAAAAAJ&citation_for_view=ZbnhBSAAAAAJ:d1gkVwhDpl0C,,1
749,Inflationary Implication of Gold Price in Vietnam,"The sustained elevated gold price domestically, hovering persistently above the global market price, underscores the peculiar nature of the gold market in Vietnam and the resiliently strong demand for gold in the local market. In particular, the movements in the price of gold seem to lead a symmetrical trend in the headline inflation since the outbreak of the 2007 global financial crisis. The primary objective of this study is therefore to assess possible inflationary consequence of the gold price movements in Vietnam. Past studies demonstrate that if gold could be viewed as a financial asset, shifts in the gold price should be monitored as one of the determining factors of inflation. Yet, hardly any study has assessed potential inflationary implication of gold in Vietnam, especially during the recent years of volatile and double-digit inflation rates.",62,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ZbnhBSAAAAAJ&citation_for_view=ZbnhBSAAAAAJ:_kc_bZDykSQC,,1
750,"Industry regulation, fund characteristics, and the efficiency of Australian private health insurers","We examine the technical and scale efficiency of 30 Australian private health insurers during the period 2010–2017 using data envelopment analysis (DEA). We calculate industry pure technical efficiency of 91.6 percent and scale efficiency of 95.1 percent. We also employ a two‐stage DEA truncated bootstrapped regression to model efficiency on fund and policy factors. The results show that premium restrictions and risk equalisation policies, and fund characteristics like size, but not for‐profit/not‐for‐profit status, market share and leverage, exert significant effects. The findings suggest substantial scope for regulatory reform and structural change in the Australian private health insurance industry as a means of improving its efficiency.",62,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ZbnhBSAAAAAJ&citation_for_view=ZbnhBSAAAAAJ:_Qo2XoVZTnwC,,1
751,Influence of F0 on Vietnamese syllable perception,"Understanding and managing tonal characteristics of Vietnamese language is one of the most difficult aspects in Vietnamese speech processing. However, at present, there is no common agreement about the influence of fundamental frequency (F0) on the perception of Vietnamese syllables. Thus, instead of analyzing the F0 of a limited number of Vietnamese syllables like other methods found in literature, this paper will present a new methodology based on, first the synthesis of arbitrary syllables, and secondly perception tests. This approach permits us to understand and define more precisely the role of F0 in the characterisation of Vietnamese tones.",63,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tjHCioIAAAAJ&citation_for_view=tjHCioIAAAAJ:J-pR_7NvFogC,,1
752,Linear F0 contour model for Vietnamese tones and Vietnamese syllable synthesis with TD-PSOLA,"Understanding and managing tonal characteristics of Vietnamese language is one of the most difficult aspects in Vietnamese speech processing. Our newest results indicate that, the initial consonant of one Vietnamese syllable does not carry information of the tone, the Vietnamese tone has an effect only on the Final part of the syllable. Based on obtained results, this article proposes linear F0 contour models for the generation of Vietnamese tones. These models only describe the F0 evolution of a final part of the Vietnamese syllable, and they are evaluated through perception tests.",63,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tjHCioIAAAAJ&citation_for_view=tjHCioIAAAAJ:umqufdRvDiIC,,1
753,"A novel approach in continuous speech recognition for Vietnamese, an isolating tonal language","This paper proposes a new approach for the integration of the Vietnamese language characteristics into a Large Vocabulary Continuous Speech Recognition System (LVCSR) which was built for some European languages. Firstly, a new module of tone recognition using Hidden Markov model was constructed. Secondly, several methods were applied to transform a text corpus of monosyllabic words into text corpus of polysyllabic words and a statistical language model of polysyllabic words was built by using the new text corpus. Finally, all the knowledge has been included in the LVCSR system so that this system can be adapted for Vietnamese. Experiments are made on the VNSPEECHCORPUS. The results show that the accuracy of Vietnamese recognition system was increased, 46% of relative reduction of the word error rate is obtained by using Vietnamese language characteristics.",63,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tjHCioIAAAAJ&citation_for_view=tjHCioIAAAAJ:bEWYMUwI8FkC,,1
754,"Large vocabulary continuous speech recognition for Vietnamese, an under-resourced language",This paper proposes a method to build a Vietnamese Large Vocabulary Continuous Speech Recognition system (Vietnamese LVCSR system). The difference between Vietnamese and European languages is analyzed and used to adapt a LVCSR system for European languages to Vietnamese. Experiments are implemented on the VNSPEECHCORPUS. The results show that the accuracy of Vietnamese recognition system is increased by using Vietnamese language characteristics.,63,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tjHCioIAAAAJ&citation_for_view=tjHCioIAAAAJ:iH-uZ7U-co4C,,1
755,AUTOMATIC IDENTIFICATION OF VIETNAMESE DIALECTS,"The dialect identification has been under study for many languages over the world nevertheless the research on signal processing for Vietnamese dialects is still limited and there are not many published works. There are many different dialects for Vietnamese. The influence of dialectal features on speech recognition systems is important. If the information about dialects is known during speech recognition process, the performance of recognition systems will be better because the corpus of these systems is normally organized according to different dialects. In our experiments, MFCC coefficients, formants, correspondent bandwidths and the fundamental frequency with its variants are input parameters for GMM. The experiment result for the dialect corpus of Vietnamese shows that the performance of dialectal identification with baseline increases from 58.6% for the case using only MFCC coefficients to 70.8% for the case using MFCC coefficients and the information of fundamental frequency. By combining the formants and their bandwidths with the normalized F0 according to average and standard deviation F0, the best recognition rate is 72.2%.",63,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tjHCioIAAAAJ&citation_for_view=tjHCioIAAAAJ:sSrBHYA8nusC,,1
756,GMM for emotion recognition of Vietnamese,"This paper presents the results of GMM-based recognition for four basic emotions of Vietnamese such as neutral, sadness, anger and happiness. The characteristic parameters of these emotions are extracted from speech signals and divided into different parameter sets for experiments. The experiments are carried out according to speaker-dependent or speaker-independent and content-dependent or content-independent recognitions. The results showed that the recognition scores are rather high with the case for which there is a full combination of parameters as MFCC and its first and second derivatives, fundamental frequency, energy, formants and its correspondent bandwidths, spectral characteristics and F0 variants. In average, the speaker-dependent and content-dependent recognition scrore is 89.21%. Next, the average score is 82.27% for the speaker-dependent and content-independent recognition. For the speaker-independent and content-dependent recognition, the average score is 70.35%. The average score is 66.99% for speaker-independent and content-independent recognition. Information on F0 has significantly increased the score of recognition",63,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tjHCioIAAAAJ&citation_for_view=tjHCioIAAAAJ:dTyEYWd-f8wC,,1
759,"Large vocabulary continuous speech recognition for Vietnamese, a under-resourced language",This paper proposes a method to build a Vietnamese Large Vocabulary Continuous Speech Recognition system (Vietnamese LVCSR system). The difference between Vietnamese and European languages is analyzed and used to adapt a LVCSR system for European languages to Vietnamese. Experiments are implemented on the VNSPEECHCORPUS. The results show that the accuracy of Vietnamese recognition system is increased by using Vietnamese language characteristics.,63,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tjHCioIAAAAJ&citation_for_view=tjHCioIAAAAJ:HtEfBTGE9r8C,,1
760,SOME NEW RESULTS ON AUTOMATIC IDENTIFICATION OF VIETNAMESE FOLK SONGS CHEO AND QUANHO,"Vietnamese folk songs are very rich in genre and content. Identifying Vietnamese folk tunes will contribute to the storage and search for information about these tunes automatically. The paper will present an overview of the classification of music genres that have been performed in Vietnam and abroad. For two types of very popular folk songs of Vietnam such as Cheo and Quan ho, the paper describes the dataset and GMM (Gaussian Mixture Model) to perform the experiments on identifying some of these folk songs. The GMM used for experiment with 4 sets of parameters containing MFCC (Mel Frequency Cepstral Coefficients), energy, first derivative and second derivative of MFCC and energy, tempo, intensity, and fundamental frequency. The results showed that the parameters added to the MFCCs contributed significantly to the improvement of the identification accuracy with the appropriate values of Gaussian component number M. Our experiments also showed that, on average, the length of the excerpts was only 29.63% of the whole song for Cheo and 38.1% of the whole song for Quan ho, the identification rate was only 3.1% and 2.33% less than the whole song for Cheo and Quan ho respectively.",63,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tjHCioIAAAAJ&citation_for_view=tjHCioIAAAAJ:ODE9OILHJdcC,,1
761,AUTOMATIC IDENTIFICATION OF SOME VIETNAMESE FOLK SONGS CHEO AND QUANHO USING CONVOLUTIONAL NEURAL NETWORKS,"We can say that music in general is an indispensable spiritual food in human life. For Vietnamese people, folk music plays a very important role, it has entered the minds of every Vietnamese person right from the moment of birth through lullabies for children. In Vietnam, there are many different types of folk songs that everyone loves, and each has many different melodies. In order to archive and search music works with a very large quantity, including folk songs, it is necessary to automatically classify and identify those works. This paper presents the method of determining the feature parameters and then using the convolution neural network (CNN) to classify and identify some Vietnamese folk tunes as Quanho and Cheo. Our experimental results show that the average highest classification and identification accuracy are 99.92% and 97.67%, respectivel.",63,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tjHCioIAAAAJ&citation_for_view=tjHCioIAAAAJ:OR75R8vi5nAC,,1
762,BUIDING A CARDIOVASCULAR MONITORING AND WARNING SYSTEM,"This paper describes a method to develop a system on the Android platform to monitor and alert the cardiovascular condition of the human body. The system uses heart rate sensor to measure electrocardiographic signals, Arduino Uno R3 circuit to control reception of ECG signals and transmit ECG signals to mobile devices via Bluetooth module HC-05. Our Android application on mobile devices allows users to manage personal information, measure, store and display ECG signals. The application also has the function of performing measurement data statistics in graphical form.",63,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tjHCioIAAAAJ&citation_for_view=tjHCioIAAAAJ:OcBU2YAGkTUC,,1
763,Generative Event Schema Induction with Entity Disambiguation,"This paper presents a generative model to event schema induction. Previous methods in the literature only use head words to represent entities. However, elements other than head words contain useful information. For instance, an armed man is more discriminative than man. Our model takes into account this information and precisely represents it using probabilistic topic distributions. We illustrate that such information plays an important role in parameter estimation. Mostly, it makes topic distributions more coherent and more discriminative. Experimental results on benchmark dataset empirically confirm this enhancement.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:Tyk-4Ss8FVUC,,1
764,Word Sense Disambiguation as a Traveling Salesman Problem,"Word sense disambiguation (WSD) is a difficult problem in Computational Linguistics, mostly because of the use of a fixed sense inventory and the deep level of granularity. This paper formulates WSD as a variant of the traveling salesman problem (TSP) to maximize the overall semantic relatedness of the context to be disambiguated. Ant colony optimization, a robust nature-inspired algorithm, was used in a reinforcement learning manner to solve the formulated TSP. We propose a novel measure based on the Lesk algorithm and Vector Space Model to calculate semantic relatedness. Our approach to WSD is comparable to state-of-the-art knowledge-based and unsupervised methods for benchmark datasets. In addition, we show that the combination of knowledge-based methods is superior to the most frequent sense heuristic and significantly reduces the difference between knowledge-based and …",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:u5HHmVD_uO8C,,1
765,Ranking Multidocument Event Descriptions for Building Thematic Timelines,"This paper tackles the problem of timeline generation from traditional news sources. Our system builds thematic timelines for a general-domain topic defined by a user query. The system selects and ranks events relevant to the input query. Each event is represented by a one-sentence description in the output timeline. We present an inter-cluster ranking algorithm that takes events from multiple clusters as input and that selects the most salient and relevant events. A cluster, in our work, contains all the events happening in a specific date. Our algorithm utilizes the temporal information derived from a large collection of extensively temporal analyzed texts. Such temporal information is combined with textual contents into an event scoring model in order to rank events based on their salience and query-relevance.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:9yKSN-GCB0IC,,1
766,Diacritics Restoration in Vietnamese: Letter based vs. Syllable based Model,"In this paper, we present some approaches to diacritics restoration in Vietnamese, based on letters and syllables. Experiments with language-specified feature selection are conducted to evaluate contribution of different types of feature. Experimental results reveal that combination of Adaboost and C4.5, using letter-based feature set, achieves 94.7% accuracy, which is competitive with other systems for diacritics restoration in Vietnamese. Test data for diacritics restoration task in Vietnamese could be freely collected with simple preprocessing, whereas large test data for many natural language processing tasks in Vietnamese is lack. So, diacritic restoration could be used as an application-driven evaluation framework for lexical disambiguation tasks.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:u-x6o8ySG0sC,,1
767,A Dataset for Open Event Extraction in English,"This article presents a corpus for development and testing of event schema induction systems in English. Schema induction is the task of learning templates with no supervision from unlabeled texts, and to group together entities corresponding to the same role in a template. Most of the previous work on this subject relies on the MUC-4 corpus. We describe the limits of using this corpus (size, non-representativeness, similarity of roles across templates) and propose a new, partially-annotated corpus in English which remedies some of these shortcomings. We make use of Wikinews to select the data inside the category Laws & Justice, and query Google search engine to retrieve different documents on the same events. Only Wikinews documents are manually annotated and can be used for evaluation, while the others can be used for unsupervised learning. We detail the methodology used for building the corpus and evaluate some existing systems on this new data.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:Y0pCki6q_DkC,,1
768,A Vietnamese Language Model based on Recurrent Neural Network,"Language modeling plays a critical role in many natural language processing (NLP) tasks such as text prediction, machine translation and speech recognition. Traditional statistical language models (e.g. n-gram models) can only offer words that have been seen before and can not capture long word context. Neural language model provides a promising solution to surpass this shortcoming of statistical language model. This paper investigates Recurrent Neural Networks (RNNs) language model for Vietnamese, at character and syllable-levels. Experiments were conducted on a large dataset of 24M syllables, constructed from 1,500 movie subtitles. The experimental results show that our RNN-based language models yield reasonable performance on the movie subtitle dataset. Concretely, our models outperform n-gram language models in term of perplexity score.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:W7OEmFMy1HYC,,1
769,Using Wiktionary to Improve Lexical Disambiguation in Multiple Languages,"This paper proposes using linguistic knowledge from Wiktionary to improve lexical disambiguation in multiple languages, focusing on part-of-speech tagging in selected languages with various characteristics including English, Vietnamese, and Korean. Dictionaries and subsumption networks are first automatically extracted from Wiktionary. These linguistic resources are then used to enrich the feature set of training examples. A first-order discriminative model is learned on training data using Hidden Markov-Support Vector Machines. The proposed method is competitive with related contemporary works in the three languages. In English, our tagger achieves 96.37% token accuracy on the Brown corpus, with an error reduction of 2.74% over the baseline.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:2osOgNQ5qMEC,,1
770,Margin Perceptron for Word Sense Disambiguation,"Word Sense Disambiguation (WSD) is an AI-complete problem where senses of words in the documents must be correctly selected from a senses inventory. Support Vector Machines (SVM) method has been successfully applied to supervised WSD. In contrast, perceptron has not been popular in supervised WSD. In this paper, a supervised method combining Margin Perceptron (MP) and Platt's probabilistic output is proposed to solve the word sense ambiguity problem. Experiments were conducted on Senseval-3 English Lexical Sample Task data set. The performance is comparable with systems using SVMs. Our system is in line with the best system participating in Senseval-3, regarding that we only used given training data, and no classifiers combination technique was applied. The advantage of our method is mainly two-fold: Firstly, good achieved performance shows that MP can be applied to problem with …",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:d1gkVwhDpl0C,,1
771,Bktreebank: Building a Vietnamese Dependency Treebank,"Dependency treebank is an important resource in any language. In this paper, we present our work on building BKTreebank, a dependency treebank for Vietnamese. Important points on designing POS tagset, dependency relations, and annotation guidelines are discussed. We describe experiments on POS tagging and dependency parsing on the treebank. Experimental results show that the treebank is a useful resource for Vietnamese language processing.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:YsMSGLbcyi4C,,1
772,Neural Sequence Labeling for Vietnamese POS Tagging and NER,"This paper presents a neural architecture for Vietnamese sequence labeling tasks including part-of-speech (POS) tagging and named entity recognition (NER). We applied the model described in [[1]] that is a combination of bidirectional Long-Short Term Memory and Conditional Random Fields, which rely on two sources of information about words: character-based word representations learned from the supervised corpus and pre-trained word embeddings learned from other unannotated corpora. Experiments on benchmark datasets show that this work achieves state-of-the-art performances on both tasks - 93.52% accuracy for POS tagging and 94.88% F1 for NER. Our sourcecode is available at hear.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:WF5omc3nYNoC,,1
773,Semantic Relatedness for Biomedical Word Sense Disambiguation,This paper presents a graph-based method for all-word word sense disambiguation of biomedical texts using semantic relatedness as edge weight. Semantic relatedness is derived from a term-topic co-occurrence matrix. The sense inventory is generated by the MetaMap program. Word sense disambiguation is performed on a disambiguation graph via a vertex centrality measure. The proposed method achieves competitive performance on a benchmark dataset.,64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:qjMakFHDy7sC,,1
774,Supervised Attention for Answer Selection in Community Question Answering,"Answer selection is an important task in community Question Answering (cQA). In recent years, attention-based neural networks have been extensively studied in various natural language processing problems, including question answering. This paper explores match-LSTM for answer selection in cQA. A lexical gap in cQA is more challenging as questions and answers typical contain multiple sentences, irrelevant information, and noisy expressions. In our investigation, word-by-word attention in the original model does not work well on social question-answer pairs. We propose integrating supervised attention into match-LSTM. Specifically, we leverage lexical semantic from external to guide the learning of attention weights for question-answer pairs. The proposed model learns more meaningful attention that allows performing better than the basic model. Our performance is among the top on SemEval datasets.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:roLk4NBRz8UC,,1
775,Korean-Vietnamese Neural Machine Translation with Named Entity Recognition and Part-of-Speech Tags,"Since deep learning was introduced, a series of achievements has been published in the field of automatic machine translation (MT). However, Korean-Vietnamese MT systems face many challenges because of a lack of data, multiple meanings of individual words, and grammatical diversity that depends on context. Therefore, the quality of Korean-Vietnamese MT systems is still sub-optimal. This paper discusses a method for applying Named Entity Recognition (NER) and Part-of-Speech (POS) tagging to Vietnamese sentences to improve the performance of Korean-Vietnamese MT systems. In terms of implementation, we used a tool to tag NER and POS in Vietnamese sentences. In addition, we had access to a Korean-Vietnamese parallel corpus with more than 450K paired sentences from our previous research paper. The experimental results indicate that tagging NER and POS in Vietnamese sentences can improve the quality of Korean-Vietnamese Neural MT (NMT) in terms of the Bi-Lingual Evaluation Understudy (BLEU) and Translation Error Rate (TER) score. On average, our MT system improved by 1.21 BLEU points or 2.33 TER scores after applying both NER and POS tagging to the Vietnamese corpus. Due to the structural features of language, the MT systems in the Korean to Vietnamese direction always give better BLEU and TER results than translation machines in the reverse direction.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:LkGwnXOMwfcC,,1
778,Utilizing SBERT For Finding Similar Questions in Community Question Answering,"The BERT model was fine-tuned to give state-of-the-art results in sentence-pair regressions. However, this model requires that both questions are fed into the network, which leads to a massive computational overhead. Instead of computing on n pairs of sentences, SBERT was proposed to learn sentence representation by computing on only one query question. This model was proven to work effectively on semantic textual similarity (STS). In this paper, we explore SBERT model for question retrieval in Community Question Answering. Results show that SBERT decreases slightly in performance compared to BERT4ECOMMERCE. However, This model reduces the effort for finding the most similar question from 795 seconds with BERT to about 0.828 seconds with SBERT, while maintaining the accuracy from BERT.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:8k81kl-MbHgC,,1
779,A Study on Seq2seq for Sentence Compression in Vietnamese,"Text summarization is an important yet challenging task in natural language processing. In this paper, we investigate Pointer Generator Networks for sentence compression. Using Vietnamese as a case study, our model could yield sentence summaries with high quality of syntax, factual correctness and completeness. Interestingly, we demonstrate that only a simple filtering technique is required to generate training data of sentence-summary pairs without any human annotation",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:hqOjcs7Dif8C,,1
780,Utilizing Bert for Question Retrieval on Vietnamese E-commerce Sites,"Question retrieval is an important task in question answering. This task is considered to be challenging due to the lexical gap issue, ie, similar questions could be expressed in different words or phrases. Although there are numerous researches conducted on question retrieval task in English, the corresponding problem in Vietnamese hasn’t been studied much. In this investigation, we highlight our efforts on question retrieval in Vietnamese e-commerce sites majorly in two directions:(1) Building a Vietnamese dataset for question retrieval in e-commerce domain.(2) Conducting experiments using recent deep learning techniques including BERT-based classifiers. Our results provide practical examples of effectively employing these models on Vietnamese e-commerce data. Particularly, we demonstrate that a BERT model trained on e-commerce texts yields significant improvement on question retrieval over BERT trained on general-domain texts.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:UebtZRa9Y70C,,1
781,HMMs for Unsupervised Vietnamese Word Segmentation,"Word segmentation is an important problem in natural language processing. Most of previous works on Vietnamese word segmentation are supervised learning. In this paper, we propose an unsupervised method for Vietnamese word segmentation based on Hidden Markov Models. We naturally encode prior linguistic knowledge into model learning. In decoding, we propose an enhancement of Viterbi decoding algorithm with external token ordering statistics from Pointwise Mutual Information. Evaluation on benchmark datasets shows that the proposed method works reasonably well. Sourcecode is available at https://github.com/longbb/word_recognition.",64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:_FxGoFyzp5QC,,1
782,Unsupervised Sentence Embeddings for Answer Summarization in Non-factoid CQA,This paper presents a method for summarizing answers in Community Question Answering. We explore deep Auto-encoder and Long-short-term-memory Auto-encoder for sentence representation. The sentence representations are used to measure similarity in Maximal Marginal Relevance algorithm for extractive summarization. Experimental results on a benchmark dataset show that our unsupervised method achieves state-of-the-art performance while requiring no annotated data.,64,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HvpiYrcAAAAJ&citation_for_view=HvpiYrcAAAAJ:eQOLeE2rZwMC,,1
783,Evolutionary algorithms to optimize task scheduling problem for the IoT based bag-of-tasks application in cloud–fog computing environment,"In recent years, constant developments in Internet of Things (IoT) generate large amounts of data, which put pressure on Cloud computing’s infrastructure. The proposed Fog computing architecture is considered the next generation of Cloud Computing for meeting the requirements posed by the device network of IoT. One of the obstacles of Fog Computing is distribution of computing resources to minimize completion time and operating cost. The following study introduces a new approach to optimize task scheduling problem for Bag-of-Tasks applications in Cloud–Fog environment in terms of execution time and operating costs. The proposed algorithm named TCaS was tested on 11 datasets varying in size. The experimental results show an improvement of 15.11% compared to the Bee Life Algorithm (BLA) and 11.04% compared to Modified Particle Swarm Optimization (MPSO), while achieving balance between completing time and operating cost.",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:eflP2zaiRacC,,1
784,An evolutionary algorithm for solving task scheduling problem in cloud-fog computing environment,"Recently, IoT (Internet of Things) has grown steadily, which generates a tremendous amount of data and puts pressure on the cloud computing infrastructures. Fog computing architecture is proposed to be the next generation of the cloud computing to meet the requirements of the IoT network. One of the big challenges of fog computing is resource management and operating function, as task scheduling, which guarantees a high-performance and cost-effective service. We propose TCaS-an evolutionary algorithm to deal with Bag-of-Tasks application in cloud-fog computing environment. By addressing the tasks in this distributed system, our proposed approach aimed at achieving the optimal tradeoff between the execution time and operating costs. We verify our proposal by extensive simulation with various size of data set, and the experimental results demonstrate that our scheduling algorithm outperforms 38.6 …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:LPZeul_q3PIC,,1
785,A proactive cloud scaling model based on fuzzy time series and SLA awareness,"Cloud computing has emerged as an optimal option for almost all computational problems today. Using cloud services, customers and providers come to terms of usage conditions defined in Service Agreement Layer (SLA), which specifies acceptable Quality of Service (QoS) metric levels. From the view of cloud-based software developers, their application-level SLA must be mapped to provided virtual resource-level SLA. Hence, one of the important challenges in clouds today is to improve QoS of computing resources. In this paper, we focus on developing a comprehensive autoscaling solution for clouds based on forecasting resource consumption in advance and validating prediction-based scaling decisions. Our prediction model takes all advantages of fuzzy approach, genetic algorithm and neural network to process historical monitoring time series data. After that the scaling decisions are validated and …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:vRqMK49ujn8C,,1
786,A multivariate fuzzy time series resource forecast model for clouds using LSTM and data correlation analysis,"Today, almost all clouds only offer auto-scaling functions using resource usage thresholds, which are defined by users. Meanwhile, applying prediction-based auto-scaling functions to clouds still faces a problem of inaccurate forecast during operation in practice even though the functions only deal with univariate monitoring data. Up until now, there are still very few efforts to simultaneously process multiple metrics to predict resource utilization. The motivation for this multivariate processing is that there could be some correlations among metrics and they have to be examined in order to increase the model applicability in fact. In this paper, we built a novel forecast model for cloud proactive auto-scaling systems with combining several mechanisms. For preprocessing data phase, to reduce the fluctuation of monitoring data, we exploit fuzzification technique. We evaluate the correlations between different metrics to …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:geHnlv5EZngC,,1
787,Towards a blockchain-based certificate authentication system in Vietnam,"Anti-forgery information, transaction verification, and smart contract are functionalities of blockchain technology that can change the traditional business processes of IT applications. These functionalities increase the data transparency, and trust of users in the new application models, thus resolving many different social problems today. In this work, we take all the advantages of this technology to build a blockchain-based authentication system (called the Vietnamese Educational Certification blockchain, which stands for VECefblock) to deal with the delimitation of fake certificate issues in Vietnam. In this direction, firstly, we categorize and analyze blockchain research and application trends to make out our contributions in this domain. Our motivating factor is to curb fake certificates in Vietnam by applying the suitability of blockchain technology to the problem domain. This study proposed some blockchain-based application development principles in order to build a step by step VECefblock with the following procedures: designing overall architecture along with business processes, data mapping structure and implementing the decentralized application that can meet the specific Vietnamese requirements. To test system functionalities, we used Hyperledger Fabric as a blockchain platform that is deployed on the Amazon EC2 cloud. Through performance evaluations, we proved the operability of VECefblock in the practical deployment environment. This experiment also shows the feasibility of our proposal, thus promoting the application of blockchain technology to deal with social problems in general as well as certificate management in Vietnam.",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:mvPsJ3kp5DgC,,1
788,A generic development and deployment framework for cloud computing and distributed applications,"Cloud computing have paved the way for advance of IT-based demand services. This technology helps decrease operation costs, solve scalability issue and many more user and provider constraints. However, development and deployment of distributed applications on cloud environment becomes a more and more complex tasks. Cloud users must spend a lot of time to prepare, install and configure their applications on clouds. In addition, after development and deployment, the applications almost cannot move from a cloud to others due to the lack of interoperability between them. To address these problems, we present in this paper a novel development and deployment framework for cloud distributed applications/services. Our approach is based on abstraction and object-oriented programming technique, allowing users to easily and rapidly develop and deploy their services into cloud environment. The approach also enables service migration and interoperability among the clouds.",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:kNdYIx-mwKoC,,1
789,A heuristics approach to mine behavioural data logs in mobile malware detection system,"Nowadays, in the era of Internet of Things when everything is connected via the Internet, the number of mobile devices has risen exponentially up to billions around the world. In line with this increase, the volume of data generated is enormous and has attracted malefactors who do ill deeds to others. For hackers, one of the popular threads to mobile devices is to spread malware. These actions are very difficult to prevent because the application installation and configuration rights are set by owners, who usually have very low knowledge or do not care about the security. In this study, our aim is to improve security in the environment of mobile devices by proposing a novel system to detect malware intrusions automatically. Our solution is based on modelling user behaviours and applying the heuristic analysis approach to mobile logs generated during the device operation process. Although behaviours of individual …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:4fKUyHm3Qg0C,,1
790,Efficient time-series forecasting using neural network and opposition-based coral reefs optimization,"In this paper, a novel algorithm called opposition-based coral reefs optimization (OCRO) is introduced. The algorithm is built as an improvement for coral reefs optimization (CRO) using opposition-based learning (OBL). For efficient modeling as the main part of this work, a novel time series forecasting model called OCRO-multi-layer neural network (MLNN) is proposed to explore hidden relationships in the non-linear time series data. The model thus combines OCRO with MLNN for data processing, which enables reducing the model complexity by faster convergence than the traditional back-propagation algorithm. For validation of the proposed model, three real-world datasets are used, including Internet traffic collected from a private internet service provider (ISP) with distributed centers in 11 European cities, WorldCup 98 contains request numbers to the server in football world cup season in 1998, and Google cluster log dataset gathered from its data center. Through the carried out experiments, we demonstrated that with both univariate and multivariate data, the proposed prediction model gains good performance in accuracy, run time and model stability aspects as compared with other modern learning techniques like recurrent neural network (RNN) and long short-term memory (LSTM). In addition, with used real datasets, we intend to concentrate on applying OCROMLNN to distributed systems in order to enable the proactive resource allocation capability for e-infrastructures (eg clouds services, Internet of Things systems, or blockchain networks).",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:q3oQSFYPqjQC,,1
791,A resource usage prediction system using functional-link and genetic algorithm neural network for multivariate cloud metrics,"Designing prediction-based auto-scaling systems for cloud computing is an attractive topic for scientists today. However, there are many barriers, which must be solved before applying these systems to practice. Some challenges include: improving accuracy for prediction models, finding a simple and effective forecast method instead of complex techniques, and processing multivariate resource metrics at the same time. So far, there are no existing proactive auto-scaling solutions for clouds that have addressed all those challenges. In this paper, we present a novel cloud resource usage prediction system using functional-link neural network (FLNN). We propose an improvement for the FLNN by exploiting genetic algorithm (GA) to train learning model in order to increase forecast effectiveness. To deal with multivariate input data, several mechanisms also are combined together to enable the ability of processing …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:fQNAKQ3IYiAC,,1
792,Enhancing service capability with multiple finite capacity server queues in cloud data centers,"Cloud computing took a step forward in the efficient use of hardware through virtualization technology. And as a result, cloud brings evident benefits for both users and providers. While users can acquire computational resources on-demand elastically, cloud vendors can also utilize maximally the investment costs for data centers infrastructure. In the Internet era, the number of appliances and services migrated to cloud environment increases exponentially. This leads to the expansion of data centers, which become bigger and bigger. Not just that these data centers must have the architecture with a high elasticity in order to serve the huge upsurge of tasks and balance the energy consumption. Although in recent times, many research works have dealt with finite capacity for single job queue in data centers, the multiple finite-capacity queues architecture receives less attention. In reality, the multiple queues …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:D03iK_w7-QYC,,1
793,Iot services for solving critical problems in vietnam: A research landscape and directions,"Critical problems in Vietnam - such as food safety, traffic management, saltwater intrusion, and wastewater management - require novel Internet of Things (IoT) solutions. Here, the authors analyze current research and developments and propose specific IoT services as solutions.",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:f2IySw72cVMC,,1
794,Hybridization of galactic swarm and evolution whale optimization for global search problem,"The works presented in this paper addresses the robust population-based global optimization that is influenced by the simplicity and efficiency principles introduced in two new generation optimization algorithms. Galactic Swarm Optimization is inspired by the motion of stars, galaxies, and superclusters of galaxies under the influence of gravity. It acts well as a global controller of the whole optimization process by employing multiple flexible cycles of exploration and exploitation phases to find new, better solutions. However, the optimization process still suffers poverty in the exploitation phase, which is improved in this work by its hybridization with our evolution version of the Whale Optimization Algorithm. Concretely, the exploitation phase of Galactic Swarm Optimization is replaced by our Evolution Whale Optimization Algorithm to avoid early convergence. The Whale Optimization Algorithm mimics the unusual …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:V3AGJWp-ZtQC,,1
795,Effective computation resilience in high performance and distributed environments,"The work described in this paper aims at effective computation resilience for complex simulations in high performance and distributed environments. Computation resilience is a complicated and delicate area; it deals with many types of simulation cores, many types of data on various input levels and also with many types of end-users, which have different requirements and expectations. Predictions about system and computation behaviors must be done based on deep knowledge about underlying infrastructures, and simulations' mathematical and realization backgrounds. Our conceptual framework is intended to allow independent collaborations between domain experts as end-users and providers of the computational power by taking on all of the deployment troubles arising within a given computing environment. The goal of our work is to provide a generalized approach for effective scalable usage of the computing power and to help domain-experts, so that they could concentrate more intensive on their domain solutions without the need of investing efforts in learning and adapting to the new IT backbone technologies.",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:abG-DnoFyZgC,,1
796,EO-CNN: an enhanced CNN model trained by equilibrium optimization for traffic transportation prediction,"Time-ordered data are widely available in many real-life areas like traffic transportation, economic growth, weather prediction, as well as in monitoring and distributed system workloads and many more. Recently, deep learning models are often applied to solve time-series prediction due to their quality. While deep learning models such as recurrent neural networks are the most well-known in this direction, convolutional neural networks (CNNs) is more known for image processing. However, CNNs are also a strong candidate for sequence modeling as well as time-series forecasting. In general, deep learning models are often trained by backpropagation using an optimization algorithm like gradient descent. In this paper, we design a novel variant for training CNN based on meta-heuristic algorithm Equilibrium Optimization (EO). The proposed model, therefore, is called by EO-CNN is consequently applied to traffic …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:WqliGbK-hY8C,,1
797,Building resource auto-scaler with functional-link neural network and adaptive bacterial foraging optimization,"In this paper, we present a novel intelligent proactive auto-scaling solution for cloud resource provisioning systems. The solution composes of an improvement variant of functional-link neural network and adaptive bacterial foraging optimization with life-cycle and social learning for proactive resource utilization forecasting as a part of our auto-scaler module. We also propose several mechanisms for processing simultaneously different resource metrics for the system. This enables our auto-scaler to explore hidden relationships between various metrics and thus help make more realistic for scaling decisions. In our system, a decision module is developed based on the cloud Service-Level Agreement (SLA) violation evaluation. We use Google trace dataset to evaluate the proposed solution well as the decision module introduced in this work. The gained experiment results demonstrate that our system is …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:eJXPG6dFmWUC,,1
798,A strategy for server management to improve cloud service QoS,"According to [1], cloud computing is one of ICT core areas in the next five years. The computing paradigm also is a research trend that has attracted strongly scientist community. Until now, there are many studies, which have focused on saving energy for servers in cloud systems. During the operation process of cloud data center, idle servers bring about power waste phenomenon. In order to overcome the problem, turning off idle servers is the popular solution applied in most researches. However, the turn on/off processes often affect significantly to quality of service (QoS) of cloud services because the server turning on always takes a long time and thus reduces the capability of quick response of services that run on these servers. In this paper, we present a strategy for effective server management. The strategy is developed based on a novel three-state model for physical servers belonging to cloud data centers …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:yD5IFk8b50cC,,1
799,Challenges and strategies for developing decentralized applications based on blockchain technology,"This paper reveals challenges and strategies of data models, deployment scenarios, business processes, and consensus models when enterprises or individuals want to apply blockchain to their information system in order to leverage advantages of the technology. A blockchain is a digital ledger, which is encrypted and stored on multiple computers in a public or private network and the data stored therein cannot be altered or deleted by a single individual. The changes for existing applications using blockchain bring difficulties for both managers and developers. The decentralized manner is different from centralized ones in aspects of how to store data in blocks, who has reading and writing permission, and how to validate transactions among many parties. We illustrate our proposal by developing an example blockchain application that enables to authenticate Vietnamese certificates at schools and …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:VOx2b1Wkg3QC,,1
800,Multiple peer Chord rings approach for device discovery in iot environment,"Recently, with the advances of information and electrical technologies, Internet of Thing (IoT) has moved from being a far vision to an increasing market reality. IoT promises to connect all physical “things” via a dynamic global network infrastructure and create intelligent behaviors in different contexts without human manipulations. In line with this development, the number of IoT solutions also have augmented exponentially covering many domains from health care, urban management to life quality improvement and so forth. However, separate appearances of these solutions today has led to barriers in enlarging smart environments because they lack a mechanism for integration. It is well-known that in the very near future, IoT requires the capabilities of high scale and dynamic adaptation with linking many new and existing smart contexts together. Hence, the need of having a comprehensive management method for …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:tS2w5q8j5-wC,,1
801,Abstraction layer for development and deployment of cloud services,"In this paper, we will present an abstraction layer for cloud computing, which intends to simplify the manipulation with virtual machines in clouds for easy and controlled development and deployment of cloud services. It also ensures interoperability between different cloud infrastructures and allows developers to create cloud appliances easily via inheritance mechanisms.",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:9yKSN-GCB0IC,,1
802,"B-Box-A Decentralized Storage System Using IPFS, Attributed-based Encryption, and Blockchain","In recent years, centralized storage systems have been extensively adopted by many companies, organizations, and individuals for storing and sharing data. These systems, however, make concerns for users of a single point of failure and the involvement of a centralized entity or third party. Therefore, there is a need for developing decentralized storage systems to overcome the drawbacks of traditional approach. In order to enhance secure and transparent characteristics of decentralized storage systems, in this paper, we present a combination of IPFS (InterPlanetary File System), ABE (Attribute-based Encryption), Multi-Authority ABE (MA-ABE), and Ethereum blockchain. In particular, we facilitate the advantages of IPFS network to store user's data in a distributed manner. Furthermore, we make the use of MA-ABE to encrypt a document, which an user needs to share it among multiple organizations. The hash …",65,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=M_GMPswAAAAJ&citation_for_view=M_GMPswAAAAJ:eq2jaN3J8jMC,,1
804,Point-of-Conflict Prediction for Pedestrian Path-Planning,"A simulation model for pedestrian navigation often requires many problems to be addressed. An example is pedestrian path-planning in the case of a moving obstacle within the environment. To resolve the problem, it is necessary to model the prediction process of the pedestrian agent in order to specify a point-of-conflict area. In this paper, we propose an approach for our pedestrian agent to navigate in this situation. This process is accomplished by predicting a possible point-of-conflict with the obstacle and planning the path accordingly. Our implementation of this approach has demonstrated the capability of the agent to plan a more competent path as well as closer to the thinking process in human cognition.",69,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w9954LYAAAAJ&citation_for_view=w9954LYAAAAJ:IjCSPb-OGe4C,,1
805,A pedestrian path-planning model in accordance with obstacle's danger with reinforcement learning,"Most microscopic pedestrian navigation models use the concept of"" forces"" applied to the pedestrian agents to replicate the navigation environment. While the approach could provide believable results in regular situations, it does not always resemble natural pedestrian navigation behaviour in many typical settings. In our research, we proposed a novel approach using reinforcement learning for simulation of pedestrian agent path planning and collision avoidance problem. The primary focus of this approach is using human perception of the environment and danger awareness of interferences. The implementation of our model has shown that the path planned by the agent shares many similarities with a human pedestrian in several aspects such as following common walking conventions and human behaviours.",69,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w9954LYAAAAJ&citation_for_view=w9954LYAAAAJ:d1gkVwhDpl0C,,1
806,The Impact of Obstacle’s Risk in Pedestrian Agent’s Local Path-Planning,"While the risk from the obstacle could significantly alter the navigation path of a pedestrian, this problem is often disregarded by many studies in pedestrian simulation, or is hindered by a simplistic simulation approach. To address this problem, we proposed a novel simulation model for the local path-planning process of the pedestrian agent, adopting reinforcement learning to replicate the navigation path. We also addressed the problem of assessing the obstacle’s risk by determining its probability of collision with the obstacle, combining with the danger from the obstacle. This process is subsequently incorporated with our prediction model to provide an accurate navigation path similar to the human thinking process. Our proposed model’s implementation demonstrates a more favorable result than other simulation models, especially in the case of the obstacle’s appearance. The pedestrian agent is capable of assessing the risk from the obstacle in different situations and adapting the navigation path correspondingly.",69,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w9954LYAAAAJ&citation_for_view=w9954LYAAAAJ:zYLM7Y9cAGgC,,1
807,A New Context-Based Sentence Embedding and the Semantic Similarity of Sentences,"Calculating semantic similarity between sentences is a difficult task for computers due to the complex structure and syntax of a sentence. Typically, in order to represent a sentence, there are numerous significant characteristics which need to be alternatively considered, for example, ambiguity, words’ order, the context of sentences, etc. Various methods have been proposed to construct a language model for computing the similarity, such as average words embedding or sentence embedding based on auto-encoder architecture. However, these methods usually focus on the sentence and skip the influence of the previous sentences. In the paper, we introduce a novel approach to transform from sentences with context to embedding vectors based on auto-encoder architecture. Experiment results showed that the proposed method could find a better result for estimating similarity sentences in a certain scenario.",69,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w9954LYAAAAJ&citation_for_view=w9954LYAAAAJ:UeHWp8X0CEIC,,1
809,Cognitive prediction of obstacle's movement for reinforcement learning pedestrian interacting model,"Recent studies in pedestrian simulation have been able to construct a highly realistic navigation behaviour in many circumstances. However, when replicating the close interactions between pedestrians, the replicated behaviour is often unnatural and lacks human likeness. One of the possible reasons is that the current models often ignore the cognitive factors in the human thinking process. Another reason is that many models try to approach the problem by optimising certain objectives. On the other hand, in real life, humans do not always take the most optimised decisions, particularly when interacting with other people. To improve the navigation behaviour in this circumstance, we proposed a pedestrian interacting model using reinforcement learning. Additionally, a novel cognitive prediction model, inspired by the predictive system of human cognition, is also incorporated. This helps the pedestrian agent in …",69,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=w9954LYAAAAJ&citation_for_view=w9954LYAAAAJ:Tyk-4Ss8FVUC,,1
810,Towards a blockchain-based certificate authentication system in Vietnam,"Anti-forgery information, transaction verification, and smart contract are functionalities of blockchain technology that can change the traditional business processes of IT applications. These functionalities increase the data transparency, and trust of users in the new application models, thus resolving many different social problems today. In this work, we take all the advantages of this technology to build a blockchain-based authentication system (called the Vietnamese Educational Certification blockchain, which stands for VECefblock) to deal with the delimitation of fake certificate issues in Vietnam. In this direction, firstly, we categorize and analyze blockchain research and application trends to make out our contributions in this domain. Our motivating factor is to curb fake certificates in Vietnam by applying the suitability of blockchain technology to the problem domain. This study proposed some blockchain-based application development principles in order to build a step by step VECefblock with the following procedures: designing overall architecture along with business processes, data mapping structure and implementing the decentralized application that can meet the specific Vietnamese requirements. To test system functionalities, we used Hyperledger Fabric as a blockchain platform that is deployed on the Amazon EC2 cloud. Through performance evaluations, we proved the operability of VECefblock in the practical deployment environment. This experiment also shows the feasibility of our proposal, thus promoting the application of blockchain technology to deal with social problems in general as well as certificate management in Vietnam.",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:ULOm3_A8WrAC,,1
811,Linked widgets: An approach to exploit open government data,"More and more governments publish Open Government Data (OGD) for their citizens, which receives great interest; because they are not only covering many domains, but also are meaningful and trustful. Since the data are not ready to be linked to other sources, some governments have started to follow the Linked Open Data paradigm as a mean to improve data quality. However, exploiting Linked Data is still a challenging task, which requires a profound understanding of underlying datasets, as well as considerable technical expertise. In this paper, we present an approach to remove the barrier between end users and Linked Data generally, and Open Government Linked Data particularly.",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:d1gkVwhDpl0C,,1
812,Open Linked Widgets Mashup Platform,"Since the emergence of the mashup concept on the web around 2005, a large stream of academic research and industrial development resulted in numerous architecture proposals, platforms and editing tools. This strong initial interest in mashup technologies and promising use case demonstrations notwithstanding, however, commercial platforms such as Microsoft Popfly, IBM Mashup Center, and Google Mashup Editor failed to gain widespread adoption by consumers and enterprises and were eventually discontinued. This failure may be attributed to a number of common limitations of these platforms:(i) they are each useful only for a single or a limited number of restricted problems in specific domains;(ii) they are closed, ie, developers cannot contribute and share their widgets;(iii) widgets, which are crucial elements of any mashup platform, are usually not modeled in sufficient semantic detail to support widget search and composition features that facilitate reuse. This paper addresses these limitations by introducing an open mashup platform based on semantic web technologies. We present a novel architecture in which widgets equipped with a semantic, graph-based model can cooperate with each other in a mashup created by end users through simple drag and drop operations. Widgets created freely by independent developers and hosted on arbitrary servers can be discovered and combined easily through our introduced semantic search feature.",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:9yKSN-GCB0IC,,1
813,Distributed mashups: a collaborative approach to data integration," – This paper aims to present a collaborative mashup platform for dynamic integration of heterogeneous data sources. The platform encourages sharing and connects data publishers, integrators, developers and end users. ",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:_FxGoFyzp5QC,,1
814,Towards an Environmental Information System for Semantic Stream Data.,"The future of the earth’s environmental systems will to a major extent be decided in cities as already more than 50% of the population is concentrated there. Pervasively available sensors and the data they generate can help to address pressing environmental challenges in urban areas by making crucial information available to researchers and decision-makers. However, environmental data is at present typically stored in disparate systems and formats, which inhibits reuse and recombination. Furthermore, the large amounts of environmental data that stream in continuously require novel processing approaches. So far, however, research at the intersection of environmental sciences and urban data to overcome these barriers has been scarce. To address these issues, we develop a novel framework using semantic web technologies. We apply ontological concepts and semantic stream processing technologies in order to facilitate combination, comparison, and visualization of heterogeneous data from various sources. The platform for environmental data stream analysis introduced in this paper can inform and support decision-making by non-expert users. We propose and discuss a three-step framework and outline initial results.",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:2osOgNQ5qMEC,,1
815,Challenges and Strategies for Developing Decentralized Applications Based on Blockchain Technology,"This paper reveals challenges and strategies of data models, deployment scenarios, business processes, and consensus models when enterprises or individuals want to apply blockchain to their information system in order to leverage advantages of the technology. A blockchain is a digital ledger, which is encrypted and stored on multiple computers in a public or private network and the data stored therein cannot be altered or deleted by a single individual. The changes for existing applications using blockchain bring difficulties for both managers and developers. The decentralized manner is different from centralized ones in aspects of how to store data in blocks, who has reading and writing permission, and how to validate transactions among many parties. We illustrate our proposal by developing an example blockchain application that enables to authenticate Vietnamese certificates at schools and …",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:Zph67rFs4hoC,,1
816,Widget-based Exploration of Linked Statistical Data Spaces,"Today, public statistical data plays an increasingly important role both in public policy formation and as a facilitator for informed decision-making in the private sector. In line with the increasing adoption of open data policies, the amount of data published by governments and organizations on the web is growing rapidly. To increase the value of such data, the W3C recommends the RDF Data Cube Vocabulary to facilitate the publication of data in a more structured and interlinked manner. Although important first steps toward building a web of statistical Linked Datasets have been made, providing adequate facilities for end users to interactively explore and make use of the published data remains an unresolved challenge. This paper presents a widgetbased approach to deal with this issue. In particular, we introduce a mashup platform that allows users lacking advanced skills and knowledge of Semantic Web technologies to interactively analyze datasets through widget compositions and visualizations. Furthermore, we provide mechanisms for the interconnection of datasets to support sophisticated knowledge extraction.",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:YsMSGLbcyi4C,,1
817,"B-Box-A Decentralized Storage System Using IPFS, Attributed-based Encryption, and Blockchain","In recent years, centralized storage systems have been extensively adopted by many companies, organizations, and individuals for storing and sharing data. These systems, however, make concerns for users of a single point of failure and the involvement of a centralized entity or third party. Therefore, there is a need for developing decentralized storage systems to overcome the drawbacks of traditional approach. In order to enhance secure and transparent characteristics of decentralized storage systems, in this paper, we present a combination of IPFS (InterPlanetary File System), ABE (Attribute-based Encryption), Multi-Authority ABE (MA-ABE), and Ethereum blockchain. In particular, we facilitate the advantages of IPFS network to store user's data in a distributed manner. Furthermore, we make the use of MA-ABE to encrypt a document, which an user needs to share it among multiple organizations. The hash …",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:YOwf2qJgpHMC,,1
818,Linked data processing provenance: towards transparent and reusable linked data integration,"The growth of Linked Data has created a promising environment for data exploration and a growing number of tools allow users to interactively integrate data from various sources. Eliciting the reliability of the results of such ad-hoc integration processes, consistently recreating those results, and identifying changes upon re-execution, however, can be difficult. Automated process provenance trail creation can provide major benefits in this context, because (i) it enables users to trace the contribution of individual sources and processing steps to the final outcome and judge whether the result can be trusted;(ii) it ensures repeatability and raises the trustworthiness of results;(iii) it ideally enables reconstruction of Linked Data integration processes from the provenance information embedded in the final result. In this paper, we present a provenance model that facilitates automatic generation of semantic provenance …",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:3fE2CSJIrl8C,,1
819,A web-based platform for dynamic integration of heterogeneous data,"Whereas the number of open and accessible data sources on the web is growing rapidly, data becomes more heterogeneous and is difficult to use or reuse. Even though many national and international organizations have published their data according to the Linked Data principles, a considerable number of data sources is still available in traditional formats, eg, HTML, XML, CSV, JSON. This results in a challenge of data aggregation and integration. To address this issue, we apply the visual programming paradigm to develop an open web platform. The platform is based on Semantic Web technologies and aims at encouraging and facilitating use of heterogeneous Open Data sources. We define Linked Widgets as user-driven modules which support users in accessing, processing, integrating, and visualizing different kinds of data. By connecting Linked Widgets from different developers, users without …",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:eQOLeE2rZwMC,,1
820,Toward a Framework for Statistical Data Integration.,"A large number of statistical data sets have been published on the web by various organizations in recent years. The resulting abundance creates opportunities for new analyses and insights, but that frequently requires integrating data from multiple sources. Inconsistent formats, access methods, units, and scales hinder data integration and make it a tedious or infeasible task. Standards such as the W3C RDF data cube vocabulary and the content-oriented guidelines of SDMX provide a foundation to tackle these challenges. In this paper, we introduce a framework that semi-automatically performs semantic data integration on statistical raw data sources at query time. We follow existing standards to transform non-semantic data structures to RDF format. Furthermore, we describe each data set with semantic metadata to deal with inconsistent use of terminologies. This metadata provides the foundation for cross-dataset querying through a mediator that rewrites queries appropriately for each source and returns consolidated results.",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:UebtZRa9Y70C,,1
821,A Drag-and-block Approach for Linked Open Data Exploration,"Since its initial definition in 2007, the concept of Linked Open Data (LOD) has gained strong traction in the scientific community. However, mainstream adoption has been limited and the emergence of an envisioned global linked data space is still in its early stages. One possible explanation is the gap between the large amounts of published LOD datasets and the lack of end-user tools to effectively explore them. Because existing applications are tailored towards specific datasets and do not allow for reuse and extension, novice users have so far had limited means to access the rich data sources being published. To address this issue, we introduce a novel approach to support non-expert users in the flexible exploration of LOD. To this end, we define a formal model that makes use of existing links between interconnected datasets. We implement the model in a mashup platform and illustrate its potential by means of use cases combining Open Data and Linked Open Data sources.",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:IjCSPb-OGe4C,,1
822,Towards blockchainizing land valuation certificate management procedures in Vietnam,"In this paper, we concentrate on the process of managing the land valuation certificate using blockchain in Vietnam. To approach this problem, we base on the e-government application framework published by the Ministry of Information and Communication and Ministry of Natural Resources and Environment to develop a blockchain-based component which will be linked to the framework. Our component has three layers corresponding to three main layers of the architecture. After that, we develop a decentralized application serving the procedures of land valuation certificate management. With our application, certificates are stored and can be traced from the blockchain network. Our component hence helps create the transparency of management processes. We also carry out preliminary experiments using the proposed system to prove its feasibility during test operation processes. Our research also paves the …",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:4TOpqqG69KYC,,1
823,Statspace: A unified platform for statistical data exploration,"In recent years, the amount of statistical data available on the web has been growing fast. Numerous organizations and governments publish data sets in a multitude of formats and encodings, using different scales, and providing access through a wide range of mechanisms. Due to such inconsistent publishing practices, integrated analysis of statistical data is challenging. StatSpace tackles this problem through semantic integration and provides uniform access to disparate statistical data. At present, it incorporates more than 1,800 data sets published by a variety of data providers including the World Bank, the European Union, and the European Environment Agency. StatSpace transparently lifts data from raw sources, maps geographical and temporal dimensions, aligns value ranges, and allows users to explore and integrate the previously isolated data sets. This paper introduces the constituent elements …",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:MXK_kJrjxJIC,,1
824,Toward a statistical data integration environment: the role of semantic metadata,"In most government and business organizations alike, statistical data provides the foundation for strategic planning and for the management of operations. In this context, the use of increasingly abundant statistical data available on the web creates new opportunities for interesting applications and facilitates more informed decision-making. For the majority of end users, however, viable means to explore statistical data sets available on the web are still scarce. Gathering and relating statistical data from multiple sources is hence typically a tedious manual process that requires significant technical expertise. Data that is being published with associated semantics, using standards such as the W3C RDF Data Cube Vocabulary, lays the foundation to overcome such limitations. In this paper, we develop a semantic metadata repository that describes each statistical data set and develop mechanisms for the interconnection …",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:roLk4NBRz8UC,,1
825,Integrating streaming data into semantic mashups,"Streaming data, i.e., frequently changing and potentially infinite data flows, are becoming more and more prevalent in today's interconnected world. In a Smart City context, for instance, citizens can access real-time information about a city's weather, pollution, or traffic conditions. However, turning vast real-time data streams into actionable knowledge poses significant challenges. For instance, heterogeneous data from varied sources needs to be normalized and integrated, ideally using a common structured vocabulary. Furthermore, to provide users with both real-time and aggregate views on the data, data stream access requires support for temporal operators. Finally, it is difficult for non-technical users to integrate and leverage streaming data. To tackle these challenges, we propose an extension of the Linked Widgets concept for the streaming data domain. In this paper, we outline a platform prototype for the …",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:0EnyYjriUFMC,,1
826,An Autocomplete Input Box for Semantic Annotation on the Web.,"A large share of websites today allow users to contribute and manage user-generated content. This content is often in textual form and involves names, terms, and keywords that can be ambiguous and difficult to interpret for other users. Semantic annotation can be used to tackle such issues, but this technique has been adopted by only a few websites. This may be attributed to a lack of a standard web input component that allows users to simply and efficiently annotate text. In this paper, we introduce an autocomplete-enabled annotation box that supports users in associating their text with DBpedia resources as they type. This web component can replace existing input fields and does not require particular user skills. Furthermore, it can be used by semantic web developers as a user interface for advanced semantic search and data processing back-ends. Finally, we validate the approach with a preliminary user study.",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:Se3iqnhoufwC,,1
827,Linked widgets platform: Lowering the barrier for open data exploration,"Despite a drastic increase in available Open and Linked Data, unmediated utilization of these data by end users is still relatively uncommon. Applications built on top of Open Data are typically domain-specific and discovering appropriate solutions that fit users’ rapidly shifting needs is a cumbersome process. In line with the Linked Data paradigm, end user tools should be based on openness, foster reusability, and be flexible enough to handle arbitrary data sources. We develop an open platform based on Semantic Web technologies that encourages developers and users to access, process, integrate, and visualize Open Data sources. To help users overcome technological barriers of adoption and get in touch with Open Data, we introduce the concept of Linked Widgets. By connecting Linked Widgets from different developers, users without programming skills can compose and share ad-hoc applications …",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:Tyk-4Ss8FVUC,,1
828,Efficient syntactic parsing with beam search,"Implementing a Vietnamese syntactic parser is a difficult task due to the complexity of Vietnamese language. Most existing Vietnamese syntactic parsers are limited by types of sentences they can analyze. This paper introduces a syntactic parser that can analyze a larger range of Vietnamese sentences in a reasonable time. The proposed parser uses a probabilistic Head-Driven Phrase Structure Grammar that can control syntactic and semantic constraints of words in order to produce accurate results. The system uses a parsing algorithm that combines an improved version of the CYK algorithm and the idea of beam search in order to reduce the search space. Our experimental results achieved precision and recall of 82% and 74%, respectively.",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:u5HHmVD_uO8C,,1
829,Semantic mashup composition from natural language expressions: preliminary results,"Despite an abundance of data available on the web today, satisfying users' complex information needs intelligently by automatically integrating and processing data from various sources remains challenging. In recent years, a large stream of research into mashups as a paradigm of end user development has emerged. These mashups foster combination and reuse of data and services and thereby allow end users to create novel applications. Developing such mashups efficiently and effectively, however, is still difficult for users that lack technical expertise. To address this issue, we extend a mashup platform with automatic mashup composition mechanisms and an agent that assists users in mashup design. To this end, we leverage semantics to simplify the mashup composition process on multiple levels. We associate each widget (ie, mashup component) with a semantic model of inputs and outputs. These …",70,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jRUt_S4AAAAJ&citation_for_view=jRUt_S4AAAAJ:5nxA0vEk-isC,,1
830,Facial expression recognition using deep convolutional neural networks,"Facial expressions convey non-verbal information between humans in face-to-face interactions. Automatic facial expression recognition, which plays a vital role in human-machine interfaces, has attracted increasing attention from researchers since the early nineties. Classical machine learning approaches often require a complex feature extraction process and produce poor results. In this paper, we apply recent advances in deep learning to propose effective deep Convolutional Neural Networks (CNNs) that can accurately interpret semantic information available in faces in an automated manner without hand-designing of features descriptors. We also apply different loss functions and training tricks in order to learn CNNs with a strong classification power. The experimental results show that our proposed networks outperform state-of-the-art methods on the well-known FERC-2013 dataset provided on the Kaggle …",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:LkGwnXOMwfcC,,1
831,An efficient framework for pixel-wise building segmentation from aerial images,Detection of buildings in aerial images is an important and challenging task in computer vision and aerial image interpretation. This paper presents an efficient approach that combines Random forest (RF) and a fully connected conditional random field (CRF) on various features for the detection and segmentation of buildings at pixel level. RF allows one to learn extremely fast on big aerial image data. The unary potentials given by RF are then combined in a fully connected conditional random field model for pixel-wise classification. The use of high dimensional Gaussian filter for pairwise potentials makes the inference tractable while obtaining high classification accuracy. Experiments have been conducted on a challenging aerial image dataset from a recent ISPRS Semantic Labeling Contest [9]. We obtained state-of-the-art accuracy with a reasonable computation time.,72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:u5HHmVD_uO8C,,1
832,Learning from Data Stream Based on Random Projection and Hoeffding Tree Classifier,"In this study, we introduce an ensemble-based approach for online machine learning. Here, instead of working on the original data, several Hoeffding tree classifiers classify and are updated on the lower dimensional projected data generated from originality by random projections. Since random projection is unstable, from one example, many diverse training data can be created to train the set of Hoeffding tree classifiers. The experiments conducted on a number of datasets chosen from different sources demonstrate that the proposed approach performs significantly better than the single Hoeffding tree and some well-known online learning algorithms including additive models and Online Bagging.",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:UebtZRa9Y70C,,1
833,A mixed noise removal method based on total variation,"Due to the technology limits, digital images always include some defects, such as noise. Noise reduces image quality and affects the result of imageprocessing. While in most cases, noise has Gaussian distribution, in biomedical images, noise is usually a combination of Poisson and Gaussian noises. This combination is naturally considered as a superposition of Gaussian noise over Poisson noise. In this paper, we propose a method to remove such a type of mixed noise based on anovel approach: we consider the superposition of noises like a linear combination. We use the idea of the total variation of an image intensity (brightness) function to remove this combination of noises.",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:hqOjcs7Dif8C,,1
834,"Multi-task learning for smile detection, emotion recognition and gender classification","Facial expression analysis plays a key role in analyzing emotions and human behaviors. Smile detection, emotion recognition and gender classification are special tasks in facial expression analysis with various potential applications. In this paper, we propose an effective architecture of Convolutional Neural Network (CNN) which can jointly learn representations for three tasks: smile detection, emotion recognition and gender classification. In addition, this model can be trained from multiple sources of data with different kinds of task-specific class labels. The extensive experiments show that our model achieves superior accuracy over recent state-of-the-art techniques in all of three tasks on popular benchmarks. We also show that the joint learning helps the tasks with less data considerably benefit from other tasks with richer data.",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:_FxGoFyzp5QC,,1
835,A denoising method based on total variation,"Today large amounts of digital images are created by various modern devices such as digital cameras, X-Ray scanners, and so on. Noise reduces image quality and result of the processing. For example, biomedical images are a type of digital images. In these images, there is a combination of two types of noises: Gaussian noise and Poisson noise. In this paper, we propose a method to remove these noises. This method is based on the total variation of an image intensity (brightness) function. We combine two famous denoising models to remove this combination of noises.",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:u-x6o8ySG0sC,,1
836,Discriminative deep feature learning for facial emotion recognition,"Emotion recognition is an important task in facial expression analysis with various potential applications. The goal of this task is to classify facial images into seven classes: disgust, neutral, sad, happy, fear, surprise and angry. In this paper, we propose a discriminative deep feature learning approach with dense convolutional networks (DenseNet) for facial emotion recognition. Particularly, we employ an auxiliary loss, namely center loss, to regulate the training process of neural networks in order to reduce the intra-class variation of the deep features and, hence, to enhance the discriminative power of the learned networks. The experimental results show that our proposed approach achieves superior performance in comparison with other recent state- of-the-art methods on the well-known FERC-2013 dataset.",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:Se3iqnhoufwC,,1
837,Fully residual convolutional neural networks for aerial image segmentation,"Semantic segmentation from aerial imagery is one of the most essential tasks in the field of remote sensing with various potential applications ranging from map creation to intelligence service. One of the most challenging factors of these tasks is the very heterogeneous appearance of artificial objects like buildings, cars and natural entities such as trees, low vegetation in very high-resolution digital images. In this paper, we propose an efficient deep learning approach to aerial image segmentation. Our approach utilizes the architecture of fully convolutional network (FCN) based on the backbone ResNet101 with additional upsampling skip connections. Besides typical color channels, we also use DSM and normalized DSM (nDSM) as the input data of our models. We achieve overall accuracy of 91%, which is in top 4 among 140 submissions from all over the world on the well-known Vaihingen dataset from ISPRS 2D …",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:kNdYIx-mwKoC,,1
838,Facial smile detection using convolutional neural networks,"Facial expression analysis plays a key role in analyzing emotions and human behaviors. Smile detection is a special task in facial expression analysis with various potential applications such as photo selection, user experience analysis, smiling payment and patient monitoring. Conventional approaches often extract low-level face descriptors and detect smile based on a strong binary classifier. In this paper, we propose an effective architecture of Convolutional Neural Networks (CNNs) to detect smile in real-time speed with high accuracy. The experimental results show that our proposed network outperforms recent state-of-the-art methods.",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:roLk4NBRz8UC,,1
839,Facial UV map completion for pose-invariant face recognition: a novel adversarial approach based on coupled attention residual UNets,"Pose-invariant face recognition refers to the problem of identifying or verifying a person by analyzing face images captured from different poses. This problem is challenging due to the large variation of pose, illumination and facial expression. A promising approach to deal with pose variation is to fulfill incomplete UV maps extracted from in-the-wild faces, then attach the completed UV map to a fitted 3D mesh and finally generate different 2D faces of arbitrary poses. The synthesized faces increase the pose variation for training deep face recognition models and reduce the pose discrepancy during the testing phase. In this paper, we propose a novel generative model called Attention ResCUNet-GAN to improve the UV map completion. We enhance the original UV-GAN by using a couple of U-Nets. Particularly, the skip connections within each U-Net are boosted by attention gates. Meanwhile, the features from two U-Nets are fused with trainable scalar weights. The experiments on the popular benchmarks, including Multi-PIE, LFW, CPLWF and CFP datasets, show that the proposed method yields superior performance compared to other existing methods.",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:mVmsd5A6BfQC,,1
840,An adaptive algorithm for restoring image corrupted by mixed noise,"Image denoising is one of the fundamental problems in image processing. Digital images are often contaminated by noise due to the image acquisition process under poor conditions. In this paper, we propose an effective approach to remove mixed Poisson-Gaussian noise in digital images. Particularly, we propose to use a spatially adaptive total variation regularization term in order to enhance the ability of edge preservation. We also propose an instance of the alternating direction algorithm to solve the proposed denoising model as an optimization problem. The experiments on popular natural images demonstrate that our approach achieves superior accuracy than other recent state-of-the-art techniques.",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:ULOm3_A8WrAC,,1
841,Ag-curesnest: A novel method for colon polyp segmentation,"Colorectal cancer is among the most common malignancies and can develop from high-risk colon polyps. Colonoscopy is an effective screening tool to detect and remove polyps, especially in the case of precancerous lesions. However, the missing rate in clinical practice is relatively high due to many factors. The procedure could benefit greatly from using AI models for automatic polyp segmentation, which provide valuable insights for improving colon polyp detection. However, precise segmentation is still challenging due to variations of polyps in size, shape, texture, and color. This paper proposes a novel neural network architecture called AG-CUResNeSt, which enhances Coupled UNets using the robust ResNeSt backbone and attention gates. The network is capable of effectively combining multi-level features to yield accurate polyp segmentation. Experimental results on five popular benchmark datasets show that our proposed method achieves state-of-the-art accuracy compared to existing methods.",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:ZeXyd9-uunAC,,1
842,Multi-view discriminant analysis for dynamic hand gesture recognition,"Although there have been attempts to tackle the problem of hand gesture recognition “in-the-wild”, deployment of such methods in practical applications still face major issues such as view point change, clustered background and low resolution of hand regions. In this paper, we investigate these issues based on a frame-work that is intensively designed in terms of both varying features and multi-view analysis. In the framework, we embed both hand-crafted features and learnt features using Convolutional Neural Network (CNN) for gesture representation at single view. We then employ multi-view discriminant analysis (MvDA) based techniques to build a discriminant common space by jointly learning multiple view-specific linear transforms from multiple views. To evaluate the effectiveness of the proposed frame-work, we construct a new multi-view dataset of twelve gestures. These gestures are captured by …",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:4TOpqqG69KYC,,1
843,YOLOv3-VD: A sparse network for vehicle detection using variational dropout,"Deep neural networks (DNNs) are currently state-of-the-art methods in many important AI tasks. However, DNNs usually contain a lot of parameters that make them prone to overfitting and slow in inference. In this paper, we apply variational dropout to sparsify YOLOv3 network for vehicle detection. We then prune redundant layers and compress the network to reduce the memory size and accelerate the inference speed of the model. Experiments show that we can eliminate up to 91% weights in the original YOLOv3 with a negligible decrease of accuracy",72,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=6-IZoAUAAAAJ&citation_for_view=6-IZoAUAAAAJ:YOwf2qJgpHMC,,1
846,A Copy Detection Method Based on SCAM and PPCHECKER.,"With the widespread use of the Internet and the availability of a huge amount of digital documents online, plagiarism is increasing. This is a serious problem not only in publishing of scientific documents but also in education. Copying is a frequent way used in plagiarism. Documents can be copied completely or some parts. Many document copy detection (DCD) methods have been proposed, however, few of them allow us to detect partial copy with high efficiency and in reasonable time. In this paper, we propose a schema for detecting copies including partial copies. This proposed method is based on SCAM and PPCHECKER methods, that benefits advantages of both methods. Experimental results with high precision demonstrate the effectiveness of the proposed method",29,https://doi.org/10.1145/2833258.2833268,1,
848,Attention-based network for effective action recognition from multi-view video.,"A human action recognition system is affected by many challenges such as background clutter, partial occlusion, lighting, viewpoint, execution rate. Using complementary information from different views can improve view changing and occlusion problems. However, how to effectively integrate the information from multi-view images? In this paper, we propose an effective approach for multi-view human action recognition. The proposition is based on attention mechanism to pass discriminate feature between views. It is designed to form a multi-branch network whose each branch takes responsibility for extracting a view-specific feature. Furthermore, we built a cross-view attention module to enhance action recognition by transferring knowledge between views (branches). Experiments on three datasets show that the proposed solution works effectively in different scenarios. Our models have achieved the best results on two datasets (NUMA and MicaHandGesture) for both cross-subject and cross-view evaluations. On the NUMA dataset, the accuracy of our best models reach to 99.56% and 92.74% in cross-subject and cross-view evaluation scenarios respectively. And on the MicaHandGesture dataset, the accuracy are 99.06%, 91.71% in two scenarios respectively. The obtained results surpass other previous works such as Multi-Branch TSN with GRU [5] (93.81% in cross-subject evaluation, 84.4% in cross-view evaluation on the NUMA) and DA-Net [31] (92.1% for cross-subject evaluation (video-level), and 84.2% for cross-view evaluation on the NUMA dataset). We also obtained very promising results on a large-scale NTU RGB+D dataset.",29,https://doi.org/10.1016/j.procs.2021.08.100,1,
849,"MAP Estimation With Bernoulli Randomness, and Its Application to Text Analysis and Recommender Systems.","MAP estimation plays an important role in many probabilistic models. However, in many cases, the MAP problem is non-convex and intractable. In this work, we propose a novel algorithm, called BOPE, which uses Bernoulli randomness for Online Maximum a Posteriori Estimation. We show that BOPE has a fast convergence rate. In particular, BOPE implicitly employs a prior which plays as regularization. Such a prior is different from the one of the MAP problem and will be vanishing as BOPE does more iterations. This property of BOPE is significant and enables to reduce severe overfitting for probabilistic models in ill-posed cases, including short text, sparse data, and noisy data. We validate the practical efficiency of BOPE in two contexts: text analysis and recommender systems. Both contexts show the superior of BOPE over the baselines.",29,https://doi.org/10.1109/ACCESS.2020.3008534,1,
850,Object Counting Based on Density Using Perspective Transformation.,"This paper presents a framework to count objects in images with perspective issues. The counting framework based on density estimation of the two authors Victor Lempitsky and Andrew Zisserman [1] produces excellent results in counting objects that are severely overlapped. However, the framework does not address the perspective issues, which frequently happen in many situations. It assumes that the subjected objects have the same size in every location of the images. Therefore, we propose a perspective model and integrate it into the existing density estimation framework to reduce the error caused by perspective distortions. In order to generate the perspective model, the heights of some objects are collected in addition to their locations to learn a perspective map. Using the map, we estimate the heights of the objects based on their positions in the images. The framework is adapted to exploit the height information of objects to customize the size of the normalized 2D Gaussian kernel, which is used to generate the ground truth density of images, and the size of the Gaussian window when computing the Scale- Invariant Feature Transform (SIFT) features. The adapted framework is then applied to count the number of motorbikes in Hanoi streets. The results are later compared with the ones produced by the original framework to see the effectiveness of our proposed solution. Our proposed framework performs better in high-density images while maintaining competitive accuracy in low-density images.",29,https://doi.org/10.1109/RIVF.2019.8713681,1,
851,Distributed mashups: a collaborative approach to data integration," – This paper aims to present a collaborative mashup platform for dynamic integration of heterogeneous data sources. The platform encourages sharing and connects data publishers, integrators, developers and end users. ",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:zYLM7Y9cAGgC,,1
852,An automated test data generation method for void pointers and function pointers in C/C++ libraries and embedded projects.,"Automated test data generation for unit testing C/C++ functions using concolic methods is well-known for improving software quality while reducing human testing effort. However, there have been only a few researches related to generating test data for void pointers and function pointers which are commonly used in C/C++ libraries and embedded projects. This paper proposes a concolic-based method named VFP (Void and Function Pointers test data generation) to generate test data for void pointers and function pointers. The key idea of VFP method is to preprocess the source code of the project under test to find all possible types of void pointers and references of function pointers. These types and references are used in the initial test data generating phase of the concolic testing method. VFP method is implemented in VFP verification tool to test on various C/C++ libraries and embedded projects. The experimental results show that VFP significantly improves the coverage of the generated test data in comparison with existing methods.",30,https://doi.org/10.1016/j.infsof.2022.106821,1,
853,Method for Improving Quality of Adversarial Examples.,"To evaluate the robustness of DNNs, most of the adversarial methods such as FGSM, box-constrained L-BFGS, and ATN generate adversarial examples with small Lp-norm. However, these adversarial examples might contain many redundant perturbations. Removing these perturbations increases the quality of adversarial examples. Therefore, this paper proposes a method to improve the quality of adversarial examples by recognizing and then removing such perturbations. The proposed method includes two phases namely the autoencoder training phase and the improvement phase. In the autoencoder training phase, the proposed method trains an autoencoder that learns how to recognize redundant perturbations. In the second phase, the proposed method uses the trained autoencoder in combination with the greedy improvement step to produce more high-quality adversarial examples. The experiments on MNIST and CIFAR-10 have shown that the proposed method could improve the quality of adversarial examples significa",30,https://doi.org/10.5220/0010814400003116,1,
854,Arext: Automatic Regular Expression Testing Tool Based on Generating Strings With Full Coverage.,"This paper introduces a testing tool for regular expressions (regexes) named AREXT. AREXT can automatically extract regexes from C++ source files and visually represent them as DFA graphs. Given a regex, AREXT can generate a set of positive and negative strings with 100% coverage of nodes, edges, and edge pairs. We leverage prior works of synthesizing regexes from natural language to create benchmarks for evaluating AREXT. Some current tools, i.e., EGRET and MUTREX, are also being evaluated and compared. Experiments show that AREXT can outperform EGRET as AREXT can detect more unexpected synthesized regexes in almost all benchmarks. The evaluation results indicate that strings with 100% coverage metrics (generated by AREXT) or strings with maximum mutation score (generated by MUTREX) are not enough to ensure the correctness of regexes under test. Experiments also show that combining AREXT, EGRET, and MUTREX can detect a majority of unwanted synthesized regexes (87–91%).",30,https://doi.org/10.1109/KSE53942.2021.9648604,1,
855,Automated Test Data Generation for Typescript Web Applications.,"This research proposes a Pattern-based Unit Testing method, namely PUT, to generate test data automatically for Typescript web applications. The main idea is to analyze the internal structure of source code to detect patterns of attribute usages. These patterns are then used to generate test data directly. An implemented tool and experimental results are presented to demonstrate the effectiveness of PUT in comparison with the automated random method. It shows that PUT could increase function coverage by 3.57 %-23.81 %, statement coverage by 0.5%-27.41 %, and branch coverage by 3.3%-39.24%. Therefore, PUT could provide potential usefulness for automated unit testing of Typescript web applications in practice.",30,https://doi.org/10.1109/KSE53942.2021.9648782,1,
856,A Counterexample Analysis Method for Assume-Guarantee Verification of Component-Based Software.,"This paper presents a method for counterexamples analysis during the assume-guarantee verification process of component-based software. The method helps the process use up all possibilities to reach a conclusive result when verifying large scale software. For this purpose, we improve the equivalence queries answering algorithm on Teacher side to return all possible counterexamples when processing an assumption candidate. The returned counterexamples are analyzed on Learner side to find all possible observation tables which correspond to better assumption candidates for later learning iterations. These observation tables are stored in a list which contains all possibilities for the verification process to reach a conclusive result. Discussions about the importance, correctness, and complexity of the proposed method are included.",30,https://doi.org/10.1109/KSE53942.2021.9648741,1,
857,An Autoencoder-based Method for Targeted Attack on Deep Neural Network Models.,"This paper presents an autoencoder-based method for a targeted attack on deep neural network models, named AE4DNN. The proposed method aims to improve the existing targeted attacks in terms of their generalization, transferability, and the trade-off between the quality of adversarial examples and the computational cost. The idea of AE4DNN is that an autoencoder model is trained from a balanced subset of the training set. The trained autoencoder model is then used to generate adversarial examples from the remaining subset of the training set, produce adversarial examples from new samples, and attack other DNN models. To demonstrate the effectiveness of AE4DNN, the compared methods are box-constrained L-BFGS, Carlini-Wagner ‖L‖ 2 attack, and AAE. The comprehensive experiment on MNIST has shown that AE4DNN can gain a better transferability, improve generalization, and generate high quality of adversarial examples while requiring a low cost of computation. This initial result demonstrates the potential ability of AE4DNN in practice, which would help to reduce the effort of testing deep neural network models.",30,https://doi.org/10.1109/RIVF51545.2021.9642102,1,
858,A framework for assume-guarantee regression verification of evolving software.,"This paper presents a framework for verifying evolving component-based software using assume-guarantee logic. The goal is to improve CDNF-based assumption generation method by having local weakest assumptions that can be used more effectively when verifying component-based software in the context of software evolution. For this purpose, we improve the technique for responding to membership queries when generating candidate assumptions. This technique is then integrated into a proposed backtracking algorithm to generate local weakest assumptions. These assumptions are effectively used in rechecking the evolving software by reducing time required for assumption regeneration within the proposed framework. The proposed framework can be applied to verify software that is continually evolving. An implemented tool and experimental results are presented to demonstrate the effectiveness and usefulness of the framework.",30,https://doi.org/10.1016/j.scico.2020.102439,1,
859,Practical approach to access the impact of global variables on program parallelism.,"Global variables may have a significant impact on preventing programs from automatic parallelism. This paper introduces a practical approach to measure the effect of global variables on program parallelism. First, we conduct static data dependence analysis among program variables and represent such dependencies by a Variable Dependence Graph. Then, we analyze this graph for measuring and identifying which global variables have a significant impact on program parallelism. To evaluate this approach, we conduct experiments on 20 benchmark programs and an industrial application. The experimental results show that half of the studied programs contain large impact variables which may be the cause of preventing programs from parallel execution.",30,https://doi.org/10.1109/ACOMP50827.2020.00019,1,
860,Generate Test Data from C/C++ Source Code using Weighted CFG and Boundary Values.,"This paper presents two test data automatic generation methods which are based on weighted control flow graph (named WCFT) and boundary values of input parameters (named BVTG). Firstly, WCFT method generates a CFG from a given unit function, updates weight for it, then generates test data from the greatest weight test paths. In the meantime, WCFT can And dead code that can be used for automatic source code errors fix. Secondly, BVTG method generates test data from boundary values of input parameters of the given unit function. The combination of the two generated test data sets from these two methods will improve the error detection ability while maintaining a high code coverage. An implemented tool (named WCFT4Cpp) and experimental results are also presented to show the effectiveness of the two proposed methods in both time required to generate test data and error detection ability.",30,https://doi.org/10.1109/KSE50997.2020.9287629,1,
861,On Locally Minimum and Strongest Assumption Generation Method for Component-Based Software Verification.,"Since software becomes more complex during its life cycle, the verification cost becomes higher, especially for such methods which are using model checking in general and assume-guarantee reasoning in specific. To address the problem of reducing the assume-guarantee verification cost, this paper presents a method to generate locally minimum and strongest assumptions for verification of component-based software. For this purpose, we integrate a variant of membership queries answering technique to an algorithm which considers candidate assumptions that are smaller and stronger first, larger and weaker later. Because the algorithm stops as soon as it reaches a conclusive result, the generated assumptions are the locally minimum and strongest ones. The correctness proof of the proposed algorithm is also included in the paper. An implemented tool, test data, and experimental results are presented and discussed.",30,https://doi.org/10.1587/transinf.2018FOP0004,1,
862,Improvements of Directed Automated Random Testing in Test Data Generation for C++ Projects.,"This paper improves the breadth-first search strategy in directed automated random testing (DART) to generate a fewer number of test data while gaining higher branch coverage, namely Static DART or SDART for short. In addition, the paper extends the test data compilation mechanism in DART, which currently only supports the projects written in C, to generate test data for C++ projects. The main idea of SDART is when it is less likely to increase code coverage with the current path selection strategies, the static test data generation will be applied with the expectation that more branches are covered earlier. Furthermore, in order to extend the test data compilation of DART for C++ context, the paper suggests a general test driver technique for C++ which supports various types of parameters including basic types, arrays, pointers, and derived types. Currently, an experimental tool has been implemented based on the proposal in order to demonstrate its efficacy in practice. The results have shown that SDART achieves higher branch coverage with a fewer number of test data in comparison with that of DART in practice.",30,https://doi.org/10.1142/S0218194019500402,1,
863,On Implementation of the Improved Assume-Guarantee Verification Method for Timed Systems.,"The two-phase assume-guarantee verification method for timed systems using TL algorithm implemented in the learner has been known as a potential method to solve the problem of state space explosion in model checking thanks to its divide and conquer strategy. This paper presents three improvements to the verification method. First, we remove the untimed verification phase from the verification process. This removal reduces the time complexity of the verification process because of the great time complexity of this phase. Second, we introduce a maxbound to the equivalence queries answering algorithm implemented in the teacher which acts as a method for the teacher to return ""don't know"" results to the learner to prevent the verification process from many endless scenarios. Finally, we introduce a technique to analyze the counterexample received from the teacher and another one implemented in the equivalence queries answering algorithm which helps the teacher not return a counterexample that has been returned to the learner. This technique keeps the verification process from running forever in several circumstances. We give primitive experimental results for both two-phase assumption generation method and the improved one with some discussions in the paper",30,https://doi.org/10.1145/3368926.3369659,1,
864,A method for Automated User Interface Testing of Windows-based Applications.,"This paper proposes a method for automated user interface testing of Windows-based applications to increase the accuracy in identifying the target widgets or executing several interactions. The key idea of this method is to generate new test scenarios from widgets and test specification where widgets are extracted during the execution of the application and test specification is generated by combining the interactions of widgets. Furthermore, the paper contributes some techniques to detect hidden widgets which considering as one of the most challenging problems in user interface testing. Currently, a supporting tool has been implemented and tested with several industrial projects. The details of the experimental results will be presented and discussed.",30,https://doi.org/10.1145/3287921.3287939,1,
865,A Method for Automated Test Cases Generation from UML Models with String Constraints.,"This paper proposes an automated test cases generation method from sequence diagrams and class diagrams with string constraints. The method supports UML 2.0 sequence diagrams including twelve combined fragments. An algorithm for generating test scenarios are developed to avoid test paths explosion without having data sharing points of threads in parallel fragments or weak sequencing fragments. Test data are also generated with solving constraints of string variables. We standardize string constraints and equations at the boundary of variables that are input formula of Z3-str solver. Comparing with the current approach of the solver, some preprocessing rules are extended for other operations such as charAt, lastindexOf, trim, startsWith and endsWith. If a result of the Z3-str is SAT, test data of each test scenario are generated to satisfy the constraints with boundary coverage. A tool is implemented to support the proposed method, and some experiments are also presented to illustrate the effectiveness of the tool.",30,https://doi.org/10.1007/978-3-319-56660-3_45,1,
866,A Test Data Generation Method for C/C++ Projects.,"This research proposes an automated test data generation method for C/C++ projects to generate the lower number of test data while gaining higher code coverage in comparison with KLEE, CAUT, PathCrawler, and CREST. In order to do that, the proposed method contributes an algorithm named loop depth first search by combining both static testing and concolic testing together. Besides, the paper also provides an improvement symbolic execution for avoiding the initial test data problem in the concolic testing. Currently, a tool supporting the proposed method has been developed and applied to test on different C/C++ projects in several software companies. The experimental results show the higher coverage with the lower number of test data compared with the existing methods. The experimental results display the effectiveness and practical usefulness of the proposed method for automated test data generation in practice.",30,https://doi.org/10.1145/3155133.3155144,1,
867,A Correlation-aware Negotiation Approach for Service Composition.,"Composing existing services to create new services has been considered as an important activity of developing service-oriented architecture systems. The increment of services that provide the same functionality but different qualities leads to the complexity in finding the best solution for a composite service. In a heterogeneous and dynamic environment, QoS negotiation provides a flexible means for choosing suitable atomic services for service compositions. However, most proposed negotiation approaches assume that services are independent of others in terms of quality. Consequently, these negotiation approaches are not able to handle the correlation factors among services. This paper presents a flexible correlation-aware negotiation approach for service compositions. In our approach, the service correlations are considered as factors affecting the choice of concrete services. The effectiveness of the approach is demonstrated via experiments.",30,https://doi.org/10.1145/2833258.2833264,1,
868,A Method for Automated Test Data Generation from Sequence Diagrams and Object Constraint Language.,"This paper proposes an automated test data generation method from the information embedded in model elements such as Unified Modeling Language (UML) sequence diagrams, class diagrams, and Object Constraint Language (OCL). The method supports UML 2.0 sequence diagrams including eight kinds of combined fragments describing control flow of systems. Comparing with some approaches by using depth first search (DFS) or breadth first search (BFS) algorithms, the proposed method generates all possible test scenarios with the higher error uncover capability. Test data for testing loop fragment is also generated. Therefore, it helps to detect errors in testing loops and the concurrency errors such as safety and liveness property of the systems.",30,https://doi.org/10.1145/2833258.2833294,1,
869,An Efficient Method for Automated Generating Models of Component-Based Software.,This paper proposes an efficient method for automated generating models of component-based software. This method accepts regular expressions that describe behaviors of software components. The proposed method uses the CNNFA algorithm to parse the regular expressions in order to generate corresponding models. This method can generate minimal accurate models of the software components. A tool is implemented and applied for some typical systems to show the efficiency of this method. The obtained experimental results show that this method is faster than existing methods. The generated models play an important role in making such model-based approaches as model checking and model-based testing more feasible in practice.,30,https://doi.org/10.1007/978-3-319-11680-8_40,1,
872,An assume-guarantee model checker for component-based systems.,"This paper introduces an assume-guarantee model checker, named AGMC, for verifying correctness of designs of component-based systems. Given UML 2.0 sequence diagrams that describe behaviors of the system components and a required property, AGMC generates accurate models of the components represented by labeled transition systems (LTSs) automatically. AGMC then model checks that whether the system satisfies the property by using the assume-guarantee verification method. AGMC has been implemented and tested by applying some typical component-based systems. The implemented AGMC is not only useful to verify component-based systems in practice but also has a potential to solve the state space explosion problem in model checking.",30,https://doi.org/10.1109/RIVF.2013.6719860,1,
873,A Method for Generating Models of Black-Box Components.,"The model-based approaches are difficult to be applied in practice due to the lack of formal models describing behaviors of systems. This paper proposes a method for generating accurate models of components in order to solve this issue in the context of the component-based systems. The key idea of this method is to generate minimal deterministic finite automata as the accurate models of the corresponding components. For this purpose, the proposed method first computes a set of traces as a regular language of a given component by executing all possible experiments over the alphabet of the component. This method then constructs a regular expression to represent this set. After that, a minimal deterministic finite automaton as an accurate model of the component is generated by applying the Thompson algorithm and some optimized activities. The generated models are useful for the existing model-based approaches, e.g., model checking and model-based testing in improving quality of component-based software. An implemented tool supporting the method and experimental results are also presented.",30,https://doi.org/10.1109/KSE.2012.15,1,
875,Modular Conformance Testing and Assume-Guarantee Verification for Evolving Component-Based Software.,"This paper proposes a framework for modular verification of evolving component-based software. This framework includes two stages: modular conformance testing for updating inaccurate models of the evolved components and modular verification for evolving component-based software. When a component is evolved after adapting some refinements, the proposed framework focuses on this component and its model in order to update the model and recheck the whole evolved system. The framework also reuses the previous verification results and the previous models of the evolved components to reduce the number of steps required in the model update and modular verification processes. An implementation and some experimental results are presented.",30,https://doi.org/10.1587/transfun.E92.A.2772,1,
876,Detachable Web-Based Learning Framework to Overcome Immature ICT Infrastructure Toward Smart Education.,"Amid the recent disease outbreaks that have been spreading across the world, the education systems in every country have witnessed a dramatic transformation. In particular, the situation has promoted online learning on an unprecedented scale, with classes being held virtually on digital platforms. This vital transformation is a major challenge in developing countries where information infrastructure for remote learning is lacking. Inspired by these observations, this study first investigated the obstacles in the way of the deployment of smart education systems (SESs) in developing countries from a technical perspective. Consequently, a detachable SES framework, named vSmartEdu, is proposed. The framework is based on a hybrid online/offline web-service model, which adopts a service-based architecture (SBA) design concept to develop smart classrooms. In particular, the online mode is activated for a web-based classroom if an Internet service is available. In contrast, the offline-version of the system is available in offline mode in packaged form, and is utilized when the Internet is not available in a local classroom. Finally, a prototype was deployed to collect feedback from learners and educators at various educational levels. The trial implementation and survey results concretely validate the feasibility and advantages of the proposed solution.",34,https://doi.org/10.1109/ACCESS.2021.3062371,1,
877,Night-Time Vehicle Distance Estimation Using Camera Geometry and Deep Learning.,"Vehicle distance estimation has been considered as one of the important topics in traffic video processing. There have been many algorithms proposed to deal with this problem. However, these algorithms all have their own drawbacks such as long processing duration, huge data requirements, extremely computational consumption, and low accuracy response. In this circumstance, in this paper, a novel algorithm based on a combination of deep learning and camera geometry is proposed for vehicle distance estimation. The proposed algorithm demonstrates fast processing duration which is contributed by the deep learning base technique. Moreover, thanks to the simplicity of camera geometry, the proposed algorithm requires minimal data to perform the estimation. By combining deep learning and camera geometry, the proposed algorithm can provide high estimation accuracy while keeping the model simple and fast. The performance of the proposed algorithm is verified through experiments results.",34,https://doi.org/10.1109/ICOIN50884.2021.9333922,1,
878,A Feasible Study of Cube Sensing Organization Map for Cognitive Spectrum Allocation.,"Spectrum availability has been considered as one of the most crucial factors for the success of serving dense user devices in fifth generation (5G) mobile communications and beyond. To this end, cognitive spectrum sensing technologies can be utilized to detect unused wireless channels in the coverage area of the user devices, which are possibly used for device-to-device communications efficiently. However, frequent wide range sensing of spectrum extremely exhausts energy resource of the user devices. In this paper, we propose a probabilistic cognitive spectrum sensing (PCSS) mechanism to overcome this challenge. In the PCSS mechanism, a cube sensing organization map (CSOM) is dimensionalized on the basis of channels and locations in the coverage area. Weight of each block in the CSOM represents the probability that a user device successfully senses the corresponding channel idle in the corresponding location. The weight of each block is learned after every sensing periods to reinforce the CSOM. Consequently, user devices sense the list of channels in their proximity with the probabilities indicated by the weight of the best matching CSOM blocks. Simulation results show that the PCSS mechanism overcomes the existing techniques in terms of energy efficiency and positive sensing ratio.",34,https://doi.org/10.1109/ICOIN50884.2021.9333998,1,
879,A hybrid genetic algorithm for the traveling salesman problem with drone.,"This paper addresses the traveling salesman problem with drone (TSP-D), in which a truck and drone are used to deliver parcels to customers. The objective of this problem is to either minimize the total operational cost (min-cost TSP-D) or minimize the completion time for the truck and drone (min-time TSP-D). This problem has gained a lot of attention in the last few years reflecting the recent trends in a new delivery method among logistics companies. To solve the TSP-D, we propose a hybrid genetic search with dynamic population management and adaptive diversity control based on a split algorithm, problem-tailored crossover and local search operators, a new restore method to advance the convergence and an adaptive penalization mechanism to dynamically balance the search between feasible/infeasible solutions. The computational results show that the proposed algorithm outperforms two existing methods in terms of solution quality and improves many best known solutions found in the literature. Moreover, various analyses on the impacts of crossover choice and heuristic components have been conducted to investigate their sensitivity to the performance of our method.",34,https://doi.org/10.1007/s10732-019-09431-y,1,
880,A New Variant of Truck Scheduling for Transporting Container Problem.,"We consider in this paper the truck scheduling for container transportation problem in which trucks and trailers are separate objects which can attach together for transporting containers. It is called Truck-Trailer-Container Routing Problem (TTCRP). In this context, trucks, trailers, and containers are located in different places. A truck can be scheduled to take a trailer, and then transports different containers from depots to customers or from customers to terminals or from terminals to customers or from customers to depots. In a planning, a truck can carry one 40ft container or two 20ft containers at a time. Moreover, in case of a long waiting time for unloading, a truck can be detached from a trailer at a customers and attached to another trailer for other itineraries. We describe the problem formulation and propose heuristics algorithms for solving large instances of the problem. We analyze the efficiency of the proposed algorithms in different problem instances.",34,https://doi.org/10.1145/3368926.3369721,1,
881,A time-dependent model with speed windows for share-a-ride problems: A case study for Tokyo transportation.,"This paper introduces a new fully time-dependent model of a public transportation system in the urban context that allows sharing a taxi between one passenger and parcels with speed widows consideration. The model contains many real-life case features and is presented by a mathematical formulation. We study both static and dynamic scenarios in comparison to traditional strategies, i.e., the direct delivery model. Moreover, we classify speed windows by different zones and congestion levels during a day in the urban context. Different speed windows induce the dynamic graph model for road networks and make the problem much more difficult to solve. Because of the complex model, the preprocessing steps on data as well as on dynamic graphs are very important. We use a greedy algorithm to initiate the solution and then use some local search techniques to improve the solution quality. The experimental data set is recorded by Tokyo-Musen Taxi company. The data set includes more than 20000 requests per day, more than 4500 used taxis per day and more than 130000 crossing points on the Tokyo map. Experimental results are analyzed on various factors such as the total benefit, the accumulating traveling time during the day, the number of used taxis and the number of shared requests.",34,https://doi.org/10.1016/j.datak.2017.06.002,1,
882,Traveling Salesman Problem with Multiple Drones.,"Combining trucks and drones in delivering parcels is an emerging research in recent years. In this paper, we investigate an extension of the TSP-D problem in which a truck travels with m (m > 1) drones (called TSP-mD) instead of one drone in TSP-D. We adapt the greedy randomized adaptive search procedure (GRASP) (proposed by Ha et al.) and propose an adaptive large neighborhood search (ALNS) heuristic to the resolution of this problem. Experimental results on different instances show that by combining a truck with more than one drone, the GRASP can bring more efficient solution. Moreover, the ALNS is more efficient than the GRASP in this context.",34,https://doi.org/10.1145/3287921.3287932,1,
883,A Constraint-Based Local Search for Offline and Online General Vehicle Routing.,"Vehicle routing is a class of combinatorial optimization problems arising in the industry of transportation and logistics. The goal of these problems is to compute an optimal route plan for a set of vehicles for serving transport requests of customers. There are many variants of the vehicle routing problems: routing for delivering goods, routing for demand responsive transport (taxi, school bus, …). Each problem might have different constraints, objectives. In this paper, we introduce a Constraint-Based Local Search (CBLS) framework for general offline and online vehicle routing problems. We extend existing neighborhood structures in the literature by proposing new neighborhoods to facilitate the resolution of different class of vehicle routing problems in a unified platform. A novel feature of the framework is the available APIs for online vehicle routing problems where requests arrive online during the execution of the computed route plan. Experimental results on three vehicle routing problems (the min-max capacitated vehicle routing problem, the multi-vehicle covering tour problem, and the online people-andparcel share-a-ride taxis problem) show the modelling flexibility, genericity, extensibility and efficiency of the proposed framework.",34,https://doi.org/10.1142/S021821301750004X,1,
884,Prediction-based optimization for online People and Parcels share a ride taxis.,One of the major problems faced in the urban environment is the efficient public transportation of people and goods. Clients make a call to the company to request a transportation service for people or parcels. A good transport scheduling will bring better profits to companies while satisfying people demands and reducing negative social impact such as traffic jam and pollution. We extend the work in [10] on a people and parcel share-a-ride taxis transportation model and propose algorithms to schedule taxis that exploit prediction on requests in an online scenario.,34,https://doi.org/10.1109/KSE.2017.8119432,1,
885,Exact methods for solving the elementary shortest and longest path problems.,"We consider in this paper the problems of finding the elementary shortest and longest paths on a graph containing negative and positive cycles. These problems are NP-hard. We propose exact algorithms based on mixed integer programming for their solution, employing different valid inequalities. Moreover, we propose decomposition techniques which are very efficient for cases with special structure. Experimental results show the efficiency of our algorithms compared with state of the art exact algorithms.",34,https://doi.org/10.1007/s10479-016-2116-5,1,
886,Improving the connectivity of a bus system: a case study of Ho Chi Minh city.,Public transportation modes like buses plays an important role in our real life. Designing a good bus system can bring advantages to both passengers and the society. One of key design criteria is the connectivity which can be measured by the minimum number of bus lines a passenger must take to travel from one station to another. This paper considers the bus system design problem with the objective of improving the connectivity of the system from the existing infrastructure. We defined the problem in the form of an optimization problem and propose an algorithm for the solution. Results obtained from the real instances derived from the bus system of Ho Chi Minh city are analysed and reported.,34,https://doi.org/10.1145/3011077.3011094,1,
887,A Java library for Constraint-Based Local Search: Application to the master thesis defense timetabling problem.,"Constraint-Based Local Search (CBLS) is an architecture for local search that uses constraints and objectives to control the local search. CBLS has many advantages in designing and implementing local search programs such as compositionality, modularity and reusability. We implement in this paper a Java library for CBLS. The implemented library will then be applied to the resolution of a combinatorial optimization problem arising in the education management in most of Vietnamese universities: the master thesis defense timetabling problem. Experimental results show the modelling flexibility and the efficiency of the constructed library. Our constructed library will be released as open source.",34,https://doi.org/10.1145/2833258.2833300,1,
888,Solving the TimeTabling problem at FPT University.,"TimeTabling is a classical problem in combinatorial optimization. There are many variants of the problem depending on specific requirements of the given institution. In this paper, we describe a new variant of the timetabling problem arising in FPT university called FUTimeTabling. We propose an algorithm for solving FUTimeTabling problem. We conduct expriments on a real data test which show the feasibility of the proposed technique.",34,https://doi.org/10.1145/2833258.2833311,1,
889,Solving the Quorumcast Routing Problem as a Mixed Integer Program.,"The quorumcast routing problem is a generalization of multicasting which arises in many distributed applications. It consists of finding a minimum cost tree that spans the source node and at least q out of m specified nodes on a given undirected weighted graph. In this paper, we solve this problem as a mixed integer program. The experimental results show that our four approaches outperform the state of the art. A sensitivity analysis is also performed on values of q and m.",34,https://doi.org/10.1007/978-3-319-07046-9_4,1,
890,Solving the Agricultural Land Allocation Problem by Constraint-Based Local Search.,"Agricultural land allocation is a problem that exists in most provinces in Vietnam. Each household owns many disconnected parcels, which reduces agricultural development. The solution to the problem is to repartition this agricutural land among the households, while satisfying some criteria. Historically, this problem has been approached neither using optimization technology nor computer science. The present paper describes the formulation of the problem and proposes a constraint-based local search algorithm for solving it. Experimental results on real data in Dong Trung village show that the solution computed by our algorithm is better than traditional solutions.",34,https://doi.org/10.1007/978-3-642-40627-0_55,1,
891,LS(Graph): a constraint-based local search for constraint optimization on trees and paths.,"Constrained optimum tree (COT) and constrained optimum path (COP) problems arise in many real-life applications and are ubiquitous in communication networks. They have been traditionally approached by dedicated algorithms, which are often hard to extend with side constraints and to apply widely. This paper proposes a constraint-based local search framework for COT/COP applications, bringing the compositionality, reuse, and extensibility at the core of constraint-based local search and constraint programming systems. The modeling contribution is the ability to express compositional models for various COT/COP applications at a high level of abstraction, while cleanly separating the model and the search procedure. The main technical contribution is a connected neighborhood based on rooted spanning trees to find high-quality solutions to COP problems. This framework is applied to some COT/COP problems, e.g., the quorumcast routing problem, the edge-disjoint paths problem, and the routing and wavelength assignment with delay side constraints problem. Computational results show the potential importance of the approach.",34,https://doi.org/10.1007/s10601-012-9124-0,1,
893,Solving the Longest Simple Path Problem with Constraint-Based Techniques.,"The longest simple path problem on graphs arises in a variety of context, e.g., information retrieval, VLSI design, robot patrolling. Given an undirected weighted graph G = (V,E), the problem consists of finding the longest simple path (i.e., no vertex occurs more than once) on G. We propose in this paper an exact and a tabu search algorithm for solving this problem. We show that our techniques give competitive results on different kinds of graphs, compared with recent genetic algorithms.",34,https://doi.org/10.1007/978-3-642-29828-8_19,1,
894,Genetic Algorithm for Solving the Master Thesis Timetabling Problem with Multiple Objectives.,"Master thesis defense scheduling problem is a real-world practical problem that arises from the Vietnamese Universities. In this paper, we give the formulation of the problem based on realistic requirements. We then show that the considered problem is NP-hard and propose a genetic algorithm for solving it. We experiment the proposed algorithm on the real problem instances taken from Hanoi University of Science and Technology. Experimental results show the feasibility of proposed algorithm.",34,https://doi.org/10.1109/TAAI.2012.50,1,
895,A literature-based method to automatically detect learning styles in learning management systems.,"Efficiency and effectiveness of learning process can be improved by adaptations to learners' learning styles. But for the time being, most of existing education systems lack of adaptation or personalization; every learner is delivered the same learning contents. Many researchers have been studying to find out an efficient way of students' learning style identification for a better personalization. In our study, we concentrate on intelligent agents that can provide the learners with personal assistants to carry out learning activities according to their learning styles and knowledge level. In this paper, we present a new literature-based method that uses learners' behaviours on learning objects as indicators for estimating students' learning styles during an online course conducted in our POLCA learning management system. The evaluation of learning style estimation and adaptation from our experiment show a high precision. Together with the mentioned benefits of learning style adaptation, this result indicates that our method is capable for wide use.",34,https://doi.org/10.1145/2254129.2254186,1,
896,LS(Graph): a constraint-based local search framework for constrained optimum tree and path problems on graphs.,"onstrained Optimum Tree (COT) and Constrained Optimum Path (COP) are two classes of problems which arise in many real-life applications and are ubiquitous in communication networks, transportations, very large scale integration (VLSI) and distributed systems. Most of these problems are computationally very hard to solve. They have been traditionally approached by dedicated algorithms including heuristics and exact algorithms, which are often hard to extend with side constraints and to apply widely because they depend strongly on the problem structures. Moreover, it is required huge research and programming efforts for solving new problems. In this thesis, we construct a constraint-based local search (CBLS) framework, called LS(Graph), for solving COT/COP applications, bringing the compositionality, reuse, and extensibility at the core of CBLS and CP systems. The modeling contribution is the ability to express compositional models for various COT/COP applications at a high level of abstraction, while cleanly separating the model and the search procedure. The LS(Graph) framework will strengthen the modeling benefits of CBLS. By using LS(Graph), users can quickly develop a local search algorithm for a new problem which gives, in most of cases, an acceptable solution while waiting for experts who do research with huge efforts for dedicated algorithms. Moreover, this solution can be used as the initial solution in more complex and hybrid algorithms. The main technical contribution is a connected neighborhood based on rooted spanning trees. The idea behind is to use rooted spanning tree for representing solutions which are paths and their neighborhoods. This approach enables the genericity of the framework from both modeling and computation standpoints. The constructed framework is applied to some three COT (i.e., the edge-weighted k-cardinality tree problem, the quorumcast routing problem, the problem of minimizing congestions on ethernet networks) and four COP problems (i.e., the resource constrained shortest path problem, the edge-disjoint paths problem, the routing and wavelength assignment with delay side constraint problem, and the routing for network covering problem). Experimental results show the potential benefits of the approach. On the one hand, we show the facility and the genericity of the resolution of the COT/COP applications which can be extended with side constraints. On the other hand, for the quorumcast routing and the edge-disjoint paths problems, we show competitive results in comparing with existing techniques.",34,https://hdl.handle.net/2078.1/70746,1,
897,An Architecture and a Domain Ontology for Personalized Multi-agent e-Learning Systems.,"Semantic web and intelligent agents are main technologies to implement personalization for e-learning systems. Ontology is the key concept in semantic web, and it plays an important role in knowledge representation, sharing of didactic material and content personalization. Furthermore, ontologies are machine-readable. Therefore they are faster and more convenient to query and retrieval educational material with the Semantic Web services. Intelligent agents can provide the learners with personal assistants to carry out learning activities according to their learning styles and their knowledge levels. In this paper, we present the architecture of a multi-agent e-learning system that supports pre-defining students' learning styles and re-estimating them during the course for a better personalization. We also present a domain ontology that is suitable for adaptive e-learning environments. The ontology describes learning material that composes a course as well as describes learners and their learning styles.",34,https://doi.org/10.1109/KSE.2011.35,1,
898,Constraint-based local search for fields partitioning problem.,"Partitioning a field into parcels is one step of the project of merging and repartitioning agricultural lands which emerges in many provinces in Vietnam. Historically, this problem has been conducted based on human experiences and without using optimization technology nor computer science. In this paper, we propose a model for the problem of fields partitioning taking into account real issues. We also propose local search algorithms for solving this problem with typical shapes of fields i.e., rectangle, trapezium, triangle. Experimental results show that our local search approach gives better solutions than those realized by human experiences.",34,https://doi.org/10.1145/2069216.2069224,1,
899,An efficient exact algorithm for finding two link-disjoint paths with related path cost and QoS constraint.,"We consider here a problem of QoS routing in survivable networks which provides protection against link failures. In this problem, a pair of link-disjoint paths between the source and the destination has to be established in which they satisfy the total delay path constraint and a related paths cost is optimal (the SNOP-D problem). Although this is a complex problem, the NP-hard behavior seems only to occur in specially constructed graphs, which are unlikely to occur in realistic communication networks. That is the main reason to consider exact multi-constrained routing algorithms. We propose in this paper an efficient exhaustive search algorithm SNOP-D for solving it which is based on the brach-and-bound technique. The major contribution of this paper is an efficient algorithm for finding a primal upper bound of the optimal objective value of this problem. Based on this primal bound, the brach-and-bound search can then perform efficiently. Simulation results show the feasibility of the proposed",34,https://doi.org/10.1145/2069216.2069225,1,
900,Constraint-Based Local Search for Constrained Optimum Paths Problems.,"Constrained Optimum Path (COP) problems arise in many real-life applications and are ubiquitous in communication networks. They have been traditionally approached by dedicated algorithms, which are often hard to extend with side constraints and to apply widely. This paper proposes a constraint-based local search (CBLS) framework for COP applications, bringing the compositionality, reuse, and extensibility at the core of CBLS and CP systems. The modeling contribution is the ability to express compositional models for various COP applications at a high level of abstraction, while cleanly separating the model and the search procedure. The main technical contribution is a connected neighborhood based on rooted spanning trees to find high-quality solutions to COP problems. The framework, implemented in COMET, is applied to Resource Constrained Shortest Path (RCSP) problems (with and without side constraints) and to the edge-disjoint paths problem (EDP). Computational results show the potential significance of the approach",34,https://doi.org/10.1007/978-3-642-13520-0_29,1,
902,Using local search for traffic engineering in switched Ethernet networks.,"Large switched Ethernet networks are deployed in campus networks, data centers and metropolitan area networks to support various types of services. Congestion is usually handled by deploying more switches and installing higher bandwidth link bundles, although a better use of the existing infrastructure would allow to deal with congestion at lower cost. In this paper, we use constrained-based local search and the COMET language to develop an efficient traffic engineering technique that improves the use of the infrastructure by the spanning tree protocol. We evaluate the performance of our scheme by considering several types of network topologies and traffic matrices. We also compare the performance of our technique with the performance that a routing-based deployment supported by an IP traffic engineering technique would obtain.",34,https://doi.org/10.1109/ITC.2010.5608731,1,
903,Constant approximation for opportunistic sensing in mobile air quality monitoring system.,"Monitoring air quality plays a critical role in the sustainable development of developing regions where the air is severely polluted. Air quality monitoring systems based on static monitors often do not provide information about the area each monitor represents or represent only small areas. In addition, they have high deployment costs that reflect the efforts needed to ensure sufficient quality of measurements. Meanwhile, the mobile air quality monitoring system, such as the one in this work, shows the feasibility of solving those challenges. The system includes environmental sensors mounted on buses that move along their routes, broadening the monitoring areas. In such a system, we introduce a new optimization problem named opportunistic sensing that aims to find (1) optimal buses to place the sensors and (2) the optimal monitoring timing to maximize the number of monitored critical regions. We investigate the optimization problem in two scenarios: simplified and general bus routes. Initially, we mathematically formulate the targeted problem and prove its NP-hardness. Then, we propose a polynomial-time -, -approximation algorithm for the problem with the simplified, general routes, respectively. To show the proposed algorithms’ effectiveness, we have evaluated it on the real data of real bus routes in Hanoi, Vietnam. The evaluation results show that the former algorithm guarantees an average performance ratio of 72.68%, while the latter algorithm achieves the ratio of 63.87%. Notably, when the sensors can be on (e.g., enough energy) during the whole route, the -approximation algorithm achieves the approximation ratio of . Such ratio, which is almost twice as , enlarges the average performance ratio to 78.42%.",36,https://doi.org/10.1016/j.comnet.2021.108646,1,
904,The equidistribution of some Mahonian statistics over permutations avoiding a pattern of length three.,"We prove the equidistribution of several multistatistics over some classes of permutations avoiding a 3-length pattern. We deduce the equidistribution, on the one hand of inv and  statistics, and on the other hand that of maj and makl statistics, over these classes of pattern avoiding permutations. Here inv and maj are the celebrated Mahonian statistics,  is one of the statistics defined in terms of generalized patterns in the 2000 pioneering paper of Babson and Steingrímsson, and makl is one of the statistics defined by Clarke, Steingrímsson and Zeng in (1997) [5]. These results solve several conjectures posed by Amini in (2018) [1].",36,https://doi.org/10.1016/j.disc.2021.112684,1,
905,Realizing Mobile Air Quality Monitoring System: Architectural Concept and Device Prototype.,"Air pollution is a critical issue in cities in developing countries like Hanoi, Vietnam. An efficient and comprehensive air quality monitoring system may reduce the harmfulness and improve the cities' sustainability. This paper presents a novel approach to realize such a system in which the air monitoring sensors are mobile. More specifically, we introduce a three-tier architecture for the air quality system, including sensing, communication, and application layers. Initially, we discuss each layer concept to bypass the limitation of the traditional stationary monitoring system. We then describe our design and implementation of air quality monitoring devices installed on vehicles, such as buses. The device is carefully designed to satisfy the conditions of impedance matching and power integrity. Besides, it fully functions in measuring parameters from the ambient environment. The device is aware of its location (using GPS) and uses Wi-Fi and 4G (LTE) to transmit sensing data on the Internet. We have conducted various experiments, including a trial deployment of the devices on a vehicle running in Hanoi. The results show our device achieves sensing data transmission with high-reliability levels (i.e., 97%, 100% on Wi-Fi, 4G (LTE), respectively). Moreover, the trial deployment confirms the feasible operation of our device in actual condition.",36,https://doi.org/10.1109/APCC49754.2021.9609931,1,
906,A dynamic programming algorithm for the maximum induced matching problem in permutation graphs.,"For a finite undirected graph G = (V, E) and a positive integer k ≥ 1, an edge set M ⊆ E is a distance-k matching if the pairwise distance of edges in M is at least k in G. The special case k = 2 has been studied under the name maximum induced matching (MIM for short), i.e., a maximum matching which forms an induced subgraph in G. MIM arises in many applications, such as artificial intelligence, game theory, computer networks, VLSI design and marriage problems. In this paper, we design an O(n2) solution for finding MIM in permutation graphs based on a dynamic programming method on edges with the aid of the sweep line technique. Our result is better than the best known algorithm.",36,https://doi.org/10.1145/3287921.3287961,1,
907,Right-Justified Characterization for Generating Regular Pattern Avoiding Permutations.,"ECO-method and its corresponding succession rules allow to recursively define and construct combinatorial objects. The induced generating trees can be coded by corresponding pattern avoiding permutations. We refine succession rules by using succession functions in case when avoided patterns are regular or c-regular. Although regular patterns are hard to be recognized in general, we give a characterization for its right-justified property which is a prerequisite in the definition of the regular pattern. Based on this characterization, we show the (c-)regularity for various classes of permutations avoiding sets of patterns with variable lengths. Last, the technique of succession functions permits to construct general recursive generating models for classes of (c-) regular pattern avoiding permutations, which are constant amortized time for all classes mentioned in the paper.",36,https://doi.org/10.1007/978-3-319-72453-9_25,1,
908,ECO-generation for some restricted classes of compositions.,"We study several restricted classes of compositions by giving one-to-one maps between them and different classes of restricted binary strings or pattern avoiding permutations. Inspired by the ECO method [8], new succession rules for these classes are presented. Finally, we obtain generating algorithms in Constant Amortized Time (CAT) for theses classes.",36,http://www.math.md/publications/csjm/issues/v21-n1/11377/,1,
909,An O(n(log n)3) algorithm for maximum matching in trapezoid graphs.,"Trapezoid graphs are intersection graphs of trapezoids between two horizontal lines. Many graph problems that are NP-hard in general case have polynomial time algorithms for trapezoid graphs. A matching in a graph is a set of pairwise non-adjacent edges, and a maximum matching is a matching whose cardinality is maximum. In this paper, we define a modified range tree data structure, called S-Range tree, which allows to report the maximum label of points in a rectangular region and update the label of a point efficiently. We use this data structure to construct an O(n(log n) 3 ) algorithm for finding a maximum matching in trapezoid graphs based on their box representation. In addition, we generalize this algorithm for a larger graph class, k-trapezoid graph by using multidimensional range tree. To the best of our knowledge, this is the first efficient maximum matching algorithm for trapezoid graphs.",36,https://doi.org/10.1109/RIVF.2013.6719886,1,
910,An efficient exact algorithm for finding two link-disjoint paths with related path cost and QoS constraint.,"We consider here a problem of QoS routing in survivable networks which provides protection against link failures. In this problem, a pair of link-disjoint paths between the source and the destination has to be established in which they satisfy the total delay path constraint and a related paths cost is optimal (the SNOP-D problem). Although this is a complex problem, the NP-hard behavior seems only to occur in specially constructed graphs, which are unlikely to occur in realistic communication networks. That is the main reason to consider exact multi-constrained routing algorithms. We propose in this paper an efficient exhaustive search algorithm SNOP-D for solving it which is based on the brach-and-bound technique. The major contribution of this paper is an efficient algorithm for finding a primal upper bound of the optimal objective value of this problem. Based on this primal bound, the brach-and-bound search can then perform efficiently. Simulation results show the feasibility of the proposed technique.",36,https://doi.org/10.1145/2069216.2069225,1,
911,Constraint-based local search for solving non-simple paths problems on graphs: application to the routing for network covering problem.,"Routing problems have been considered as central problems in the fields of transportation, distribution and logistics. LS(Graph) is a generic framework allowing to model and solve constrained optimum paths problems on graphs by local search where paths are known to be elementary (i.e., edges, vertices cannot be repeated on paths). In many real-world situations, the paths to be determined are not known to be neither simple nor elementary. In this paper, we extend the LS(Graph) framework by designing and implementing abstractions that allow to model and solve constrained paths problem where edges, vertices can be repeated on paths (call non-simple paths). We also propose an instance of such problem class: the routing for network covering (RNC) problem which arises in the context of rescue after a natural disaster in which we have to route a fleet of identical vehicles with limited capacity on a transportation network in order to collect the informations of the disaster. Given an undirected weighted graph G = (V, E) representing a transportation network and a vertex v0 ∈ V representing the depot, the RNC problem consists of routing a fleet of unlimited number of identical vehicles with limited capacity that cannot perform a path of length > L such that each vehicle starts from and teminates at the depot and all the edges of a given set S (S ⊆ E) must be visited. The objective of the routing plan is to minimize the number of vehicles used. This paper discusses the challenge around this problem and applies the constructed framework to the resolution of this problem. The proposed model is generic; it allows to solve some variants of the problem where side constraints are required to be added.",36,https://doi.org/10.1145/1852611.1852613,1,
913,ViTransPAD: Video Transformer using convolution and self-attention for Face Presentation Attack Detection.,"Face Presentation Attack Detection (PAD) is an important measure to prevent spoof attacks for face biometric systems. Many works based on Convolution Neural Networks (CNNs) for face PAD formulate the problem as an image-level binary classification task without considering the context. Alternatively, Vision Transformers (ViT) using self-attention to attend the context of an image become the mainstreams in face PAD. Inspired by ViT, we propose a Video-based Transformer for face PAD (ViTransPAD) with short/long-range spatio-temporal attention which can not only focus on local details with short attention within a frame but also capture long-range dependencies over frames. Instead of using coarse image patches with single-scale as in ViT, we propose the Multi-scale Multi-Head Self-Attention (MsMHSA) architecture to accommodate multi-scale patch partitions of Q, K, V feature maps to the heads of transformer in a coarse-to-fine manner, which enables to learn a fine-grained representation to perform pixel-level discrimination for face PAD. Due to lack inductive biases of convolutions in pure transformers, we also introduce convolutions to the proposed ViTransPAD to integrate the desirable properties of CNNs by using convolution patch embedding and convolution projection. The extensive experiments show the effectiveness of our proposed ViTransPAD with a preferable accuracy-computation balance, which can serve as a new backbone for face PAD.",38,https://doi.org/10.48550/arXiv.2203.01562,1,
914,A Survey on Anti-Spoofing Methods for Facial Recognition with RGB Cameras of Generic Consumer Devices.,"The widespread deployment of facial recognition-based biometric systems has made facial presentation attack detection (face anti-spoofing) an increasingly critical issue. This survey thoroughly investigates facial Presentation Attack Detection (PAD) methods that only require RGB cameras of generic consumer devices over the past two decades. We present an attack scenario-oriented typology of the existing facial PAD methods, and we provide a review of over 50 of the most influenced facial PAD methods over the past two decades till today and their related issues. We adopt a comprehensive presentation of the reviewed facial PAD methods following the proposed typology and in chronological order. By doing so, we depict the main challenges, evolutions and current trends in the field of facial PAD and provide insights on its future research. From an experimental point of view, this survey paper provides a summarized overview of the available public databases and an extensive comparison of the results reported in PAD-reviewed papers. ",38,https://doi.org/10.3390/jimaging6120139,1,
915,Predicting Brain Degeneration with a Multimodal Siamese Neural Network *.,"To study neurodegenerative diseases, longitudinal studies are carried on volunteer patients. During a time span of several months to several years, they go through regular medical visits to acquire data from different modalities, such as biological samples, cognitive tests, structural and functional imaging. These variables are heterogeneous but they all depend on the patient’s health condition, meaning that there are possibly unknown relationships between all modalities. Some information may be specific to some modalities, others may be complementary, and others may be redundant. Some data may also be missing. In this work we present a neural network architecture for multimodal learning, able to use imaging and clinical data from two time points to predict the evolution of a neurodegenerative disease, and robust to missing values. Our multimodal network achieves 92.5% accuracy and an AUC score of 0.978 over a test set of 57 subjects. We also show the superiority of the multimodal architecture, for up to 37.5% of missing values in test set subjects’ clinical measurements, compared to a model using only the clinical modality.",38,https://doi.org/10.1109/IPTA50016.2020.9286657,1,
916,Vector space model adaptation and pseudo relevance feedback for content-based image retrieval.,"Image retrieval is an important problem for researchers in computer vision and content-based image retrieval (CBIR) fields. Over the last decades, many image retrieval systems were based on image representation as a set of extracted low-level features such as color, texture and shape. Then, systems calculate similarity metrics between features in order to find similar images to a query image. The disadvantage of this approach is that images visually and semantically different may be similar in the low level feature space. So, it is necessary to develop tools to optimize retrieval of information. Integration of vector space models is one solution to improve the performance of image retrieval. In this paper, we present an efficient and effective retrieval framework which includes a vectorization technique combined with a pseudo relevance model. The idea is to transform any similarity matching model (between images) to a vector space model providing a score. A study on several methodologies to obtain the vectorization is presented. Some experiments have been undertaken on Wang, Oxford5k and Inria Holidays datasets to show the performance of our proposed framework",38,https://doi.org/10.1007/s11042-017-4463-x,1,
917,A robust CBIR framework in between bags of visual words and phrases models for specific image datasets.,"One objective of the Content Based Image Retrieval research field is to propose new methodologies and tools to manage the increasing number of images available. Linked to a specific context of small expert datasets without prior knowledge, our research work focuses on improving the discriminative power of the image representation while keeping the same efficiency for retrieval. Based on the well-known bag of visual words model, we propose three different methodologies inspired by the visual phrase model effectiveness and a compression technique which ensures the same effectiveness for retrieval than the BoVW model. Our experimental results study the performance of our proposals on different well known benchmark datasets and show its good performance compared to other recent approaches.",38,https://doi.org/10.1007/s11042-018-5841-8,1,
918,Study of CNN Based Classification for Small Specific Datasets.,"Recently, deep learning and particularly, Convolutional Neural Network (CNN), has become predominant in many application fields, including visual image classification. In an applicative context of detecting areas with hazard of dengue fever, we propose a classification framework using deep neural networks on a limited dataset of images showing urban sites. For this purpose, we have to face multiple research issues: (i) small number of training data; (ii) images belonging to multiple classes; (iii) non-mutually exclusive classes. Our framework overcomes those issues by combining different techniques including data augmentation and multi-scale/region-based classification, in order to extract the most discriminative information from the data. Experiment results present our framework performance using several CNN architectures with different parameter sets, without and with transfer learning. Then, we analyze the effect of data augmentation and multiscale region based classification. Finally, we show that adding a classification weighting scheme allows the global framework to obtain more than 90% average precision for our classification task.",38,https://doi.org/10.1007/978-3-319-76081-0_30,1,
919,Deep CNN and Data Augmentation for Skin Lesion Classification.,"Deep CNN techniques have dramatically become the state of the art in image classification. However, applying high-capacity Deep CNN in medical image analysis has been impeded because of scarcity of labeled data. This study has two primary contributions: first, we propose a classification model to improve performance of classification of skin lesion using Deep CNN and Data Augmentation. Second, we demonstrate the use of image data augmentation for overcoming the problem of data limitation and examine the influence of different number of augmented samples on the performance of different classifiers. The proposed classification system is evaluated using the largest public skin lesion testing dataset, containing 600 testing images, and 6,162 training images. New state-of-the-art performance result is archived with AUC (89.2% vs. 87.4%), AP (73.9% vs. 71.5%), and ACC (89.0% vs. 87.2%). In additional, we explore the influence of each image augmentation on the three classifiers and observe that performance of each classifier is influenced differently by each augmentation and has better results comparing with traditional methods. Thus, it is suggested that the performance of skin cancer classification and medial image classification could be improved further by applying data augmentation.",38,https://doi.org/10.1007/978-3-319-75420-8_54,1,
920,FaceLiveNet: End-to-End Networks Combining Face Verification with Interactive Facial Expression-Based Liveness Detection.,"The effectiveness of the state-of-the-art face verifi-cation/recognition algorithms and the convenience of face recognition greatly boost the face-related biometric authentication applications. However, existing face verification architectures seldom integrate any liveness detection or keep such stage isolated from face verification as if it was irrelevant. This may potentially result in the system being exposed to spoof attacks between the two stages. This work introduces FaceLiveNet, a holistic end-to-end deep networks which can perform face verification and liveness detection simultaneously. An interactive scheme for facial expression recognition is proposed to perform liveness detection, providing better generalization capacity and higher security level. The proposed framework is low-cost as it relies on commodity hardware instead of costly sensors, and lightweight with much fewer parameters comparing to the other popular deep networks such as VGG16 and FaceNet. Experimental results on the benchmarks LFW, YTF, CK+, OuluCASIA, SFEW, FER2013 demonstrate that the proposed FaceLiveNet can achieve state-of-art performance or better for both face verification and facial expression recognition. We also introduce a new protocol to evaluate the global performance for face authentication with the fusion of face verification and interactive facial expression-based liveness detection.",38,https://doi.org/10.1109/ICPR.2018.8545274,1,
921,Dichotomic lattices and local discretization for Galois lattices.,"The present paper deals with supervised classification methods based on Galois lattices and decision trees. Such ordered structures require attributes discretization and it is known that, for decision trees, local discretization improves the classification performance compared with global discretization. While most literature on discretization for Galois lattices relies on global discretization, the presented work introduces a new local discretization algorithm for Galois lattices which hinges on a property of some specific lattices that we introduce as dichotomic lattices. Their properties, co-atomicity and ∨-complementarity are proved along with their links with decision trees. Finally, some quantitative and qualitative evaluations of the local discretization are proposed.",38,https://doi.org/10.1007/s11634-015-0225-7,1,
922,DocCreator: A New Software for Creating Synthetic Ground-Truthed Document Images.,"Most digital libraries that provide user-friendly interfaces, enabling quick and intuitive access to their resources, are based on Document Image Analysis and Recognition (DIAR) methods. Such DIAR methods need ground-truthed document images to be evaluated/compared and, in some cases, trained. Especially with the advent of deep learning-based approaches, the required size of annotated document datasets seems to be ever-growing. Manually annotating real documents has many drawbacks, which often leads to small reliably annotated datasets. In order to circumvent those drawbacks and enable the generation of massive ground-truthed data with high variability, we present DocCreator, a multi-platform and open-source software able to create many synthetic image documents with controlled ground truth. DocCreator has been used in various experiments, showing the interest of using such synthetic images to enrich the training stage of DIAR tools.",38,https://doi.org/10.3390/jimaging3040062,1,
923,Simple Triplet Loss Based on Intra/Inter-Class Metric Learning for Face Verification.,"Recently, benefiting from the advances of the deep convolution neural networks (CNNs), significant progress has been made in the field of the face verification and face recognition. Specially, the performance of the FaceNet has overpassed the human level performance in terms of the accuracy on the datasets ""Labeled Faces in the Wild (LFW)"" and ""Youtube Faces in the Wild (YTF)"". The triplet loss used in the FaceNet has proved its effectiveness for face verification. However, the number of the possible triplets is explosive when using a large scale dataset to train the model. In this paper, we propose a simple class-wise triplet loss based on the intra/inter-class distance metric learning which can largely reduce the number of the possible triplets to be learned. However the simplification of the classic triplet loss function has not degraded the performance of the proposed approach. The experimental evaluations on the most widely used benchmarks LFW and YTF show that the model with the proposed class-wise simple triplet loss can reach the state-of-the-art performance. And the visualization of the distribution of the learned features based on the MNIST dataset has also shown the effectiveness of the proposed method to better separate the classes and make the features more discriminative in comparison with the other state-of-the-art loss function.",38,https://doi.org/10.1109/ICCVW.2017.194,1,
924,"Massive, Free and Reproducible Grountruthed Document Image Databases Generation with DocCreator.","Whether your research is focused on image restoration, layout analysis, text-graphic separation, binarization, OCR, etc. you need a groundtruthed database to train your method or to evaluate it. This article presents DocCreator, a multi-platform and open-source software able to create many synthetic image documents with controlled groundtruth. With DocCreator, you can create complete synthetic images choosing the text, font, background and layout to use, add various realistic degradations (bleed-through, light defect, paper deformation, ink degradation, etc.) on original images, or combine both to increase the size of your database. DocCreator comes as an online (easy to test version) and a desktop solution (fast calculation process, and no need to upload copyrighted data). DocCreator is useful for retraining tasks and to know precisely whether your algorithm is robust. It has already been used favorably and could help other DIAR researchers to produce and share groundtruthed databases.",38,https://doi.org/10.1109/ICDAR.2017.188,1,
925,Impact of OCR Errors on the Use of Digital Libraries: Towards a Better Access to Information.,"Digital collections are increasingly used for a variety of purposes. In Europe only, we can conservatively estimate that tens of thousands of users consult digital libraries daily. The usages are often motivated by qualitative and quantitative research. However, caution must be advised as most digitized documents are indexed through their OCRed version, which is far from perfect, especially for ancient documents. In this paper, we aim to estimate the impact of OCR errors on the use of a major online platform: The Gallica digital library from the National Library of France. It accounts for more than 100M OCRed documents and receives 80M search queries every year. In this context, we introduce two main contributions. First, an original corpus of OCRed documents composed of 12M characters along with the corresponding gold standard is presented and provided, with an equal share of English- and French-written documents. Next, statistics on OCR errors have been computed thanks to a novel alignment method introduced in this paper. Making use of all the user queries submitted to the Gallica portal over 4 months, we take advantage of our error model to propose an indicator for predicting the relative risk that queried terms mismatch targeted resources due to OCR errors, underlining the critical extent to which OCR quality impacts on digital library access.",38,https://doi.org/10.1109/JCDL.2017.7991582,1,
926,Improving the Discriminative Power of Bag of Visual Words Model.,"With the exponential increase of image database, Content Based Image Retrieval research field has started a race to always propose more effective and efficient tools to manage massive amount of data. In this paper, we focus on improving the discriminative power of the well-known bag of visual words model. To do so, we present n-BoVW, an approach that combines visual phrase model effectiveness keeping the efficiency of visual words model with a binary based compression algorithm. Experimental results on widely used datasets (UKB, INRIA Holidays, Corel1000 and PASCAL 2012) show the effectiveness of the proposed approach.",38,https://doi.org/10.1007/978-3-319-51814-5_21,1,
929,Towards an approach using metric learning for interactive semi-supervised clustering of images.,"The problem of unsupervised and semi-supervised clustering is extensively studied in machine learning. In order to involve user in image data clustering, we proposed in [1] a new approach for interactive semi-supervised clustering that translates user feedback (expressed at the level of individual images) into pairwise constraints between groups of images, these groups being formed thanks to the underlying hierarchical clustering solution and user feedback. Recently, the need for appropriate measures of distance or similarity between data led to the emergence of distance metric learning approaches. In this paper 1 , we propose a method incorporating metric learning in the existing system to improve performance and reduce the computational time. Our preliminary experiments performed on the Wang dataset show that metric learning methods improve the performances and computational time of the existing system.",38,https://doi.org/10.1109/KSE.2016.7758080,1,
930,Unsupervised word spotting using a graph representation based on invariants.,"We are currently working on the concept of an interactive word retrieval system for ancient document collection navigation, based on query composition for non-expert users. We have introduced a new notion: invariants, which are writing pieces automatically extracted from the old document collection. The invariants can be used in query making process in where the user selects and composes appropriate invariants to make the query. The invariants can be also used as descriptor to characterize word images. We introduced our unsupervised method for extracting invariants in our earlier paper. In this paper, we present a new structural word spotting system using a graph representation based on invariants as a descriptor. Through experiments, we conclude that our proposed system can adapt to different types of homogenous alphabetic languages documents (regardless of language/script, antiquity, handwritten or printed).",38,https://doi.org/10.1109/ICDAR.2015.7333835,1,
931,Text and non-text segmentation based on connected component features.,"Document image segmentation is crucial to OCR and other digitization processes. In this paper, we present a learning-based approach for text and non-text separation in document images. The training features are extracted at the level of connected components, a mid-level between the slow noise-sensitive pixel level, and the segmentation-dependent zone level. Given all types, shapes and sizes of connected components, we extract a powerful set of features based on size, shape, stroke width and position of each connected component. Adaboosting with Decision trees is used for labeling connected components. Finally, the classification of connected components into text and non-text is corrected based on classification probabilities and size as well as stroke width analysis of the nearest neighbors of a connected component. The performance of our approach has been evaluated on the two standard datasets: UW-III and ICDAR-2009 competition for document layout analysis. Our results demonstrate that the proposed approach achieves competitive performance for segmenting text and non-text in document images of variable content and degradation.",38,https://doi.org/10.1109/ICDAR.2015.7333930,1,
933,Unsupervised and Interactive Semi-supervised Clustering for Large Image Database Indexing and Retrieval.," The feature space structuring methods play a very important role in finding information in large image databases. They organize indexed images in order to facilitate, accelerate and improve the results of further retrieval. Clustering, one kind of feature space structuring, may organize the dataset into groups of similar objects without prior knowledge (unsupervised clustering) or with a limited amount of prior knowledge (semi-supervised clustering). In this paper, we present both formal and experimental comparisons of different unsupervised clustering methods for structuring large image databases. We use different image databases of increasing sizes (Wang, PascalVoc2006, Caltech101, Corel30k) to study the scalability of the different approaches. Then, we present a new interactive semi-supervised clustering model, which allows users to provide feedback in order to improve the clustering results according to their wishes. Moreover,we also compare, experimentally, our proposed model with the semi-supervised HMRF-kmeans clustering method.",38,https://doi.org/10.3233/FI-2014-988,1,
934,A new interactive semi-supervised clustering model for large image database indexing.,"Indexing methods play a very important role in finding information in large image databases. They organize indexed images in order to facilitate, accelerate and improve the results for later retrieval. Alternatively, clustering may be used for structuring the feature space so as to organize the dataset into groups of similar objects without prior knowledge (unsupervised clustering) or with a limited amount of prior knowledge (semi-supervised clustering).

In this paper, we introduce a new interactive semi-supervised clustering model where prior information is integrated via pairwise constraints between images. The proposed method allows users to provide feedback in order to improve the clustering results according to their wishes. Different strategies for deducing pairwise constraints from user feedback were investigated. Our experiments on different image databases (Wang, PascalVoc2006, Caltech101) show that the proposed method outperforms semi-supervised HMRF-kmeans ",38,https://doi.org/10.1016/j.patrec.2013.06.014,1,
936,Document Retrieval Based on Logo Spotting Using Key-Point Matching.,"In this paper, we present an approach to retrieve documents based on logo spotting and recognition. A document retrieval system is proposed inspired from our previous method for logo spotting and recognition. First, the key-points from both the query logo images and a given set of document images are extracted and described by SIFT descriptor, and are matched in the SIFT feature space. They are filtered by the nearest neighbor matching rule based on the two nearest neighbors and are then post-filtered with BRIEF descriptor. Secondly, logo segmentation is performed using spatial density-based clustering, and homography is used to filter the matched key-points as a post processing. Finally, for ranking, we use two measures which are calculated based on the number of matched key-points. Tested on a well-known benchmark database of real world documents containing logos Tobacco-800, our approach achieves better performance than the state-of-the-art methods.",38,https://doi.org/10.1109/ICPR.2014.527,1,
937,The ICDAR/GREC 2013 Music Scores Competition: Staff Removal.,"The first competition on music scores that was organized at ICDAR and GREC in 2011 awoke the interest of researchers, who participated in both staff removal and writer identification tasks. In this second edition, we focus on the staff removal task and simulate a real case scenario concerning old and degraded music scores. For this purpose, we have generated a new set of semi-synthetic images using two degradation models that we previously introduced: local noise and 3D distortions. In this extended paper we provide an extended description of the dataset, degradation models, evaluation metrics, the participant’s methods and the obtained results that could not be presented at ICDAR and GREC proceedings due to page limitations.",38,https://doi.org/10.1007/978-3-662-44854-0_16,1,
938,An efficient parametrization of character degradation model for semi-synthetic image generation.,"This paper presents an efficient parametrization method for generating synthetic noise on document images. By specifying the desired categories and amount of noise, the method is able to generate synthetic document images with most of degradations observed in real document images (ink splotches, white specks or streaks). Thanks to the ability of simulating different amount and kind of noise, it is possible to evaluate the robustness of many document image analysis methods. It also permits to generate data for algorithms that employ a learning process. The degradation model presented in [7] needs eight parameters for generating randomly noise regions. We propose here an extension of this model which aims to set automatically the eight parameters to generate precisely what a user wants (amount and category of noise). Our proposition consists of three steps. First, Nsp seed-points (i.e. centres of noise regions) are selected by an adaptive procedure. Then, these seed-points are classified into three categories of noise by using a heuristic rule. Finally, each size of noise region is set using a random process in order to generate degradations as realistic as possible.",38,https://doi.org/10.1145/2501115.2501127,1,
939,Generation of learning samples for historical handwriting recognition using image degradation.,"Historical documents pose challenging problems for training handwriting recognition systems. Besides the high variability of character shapes inherent to all handwriting, the image quality can also differ greatly, for instance due to faded ink, ink bleed-through, wrinkled and stained parchment. Especially when only few learning samples are available, it is difficult to incorporate this variability in the morphological character models. In this paper, we investigate the use of image degradation to generate synthetic learning samples for historical handwriting recognition. With respect to three image degradation models, we report significant improvements in accuracy for recognition with hidden Markov models on the medieval Saint Gall and Parzival data sets.",38,https://doi.org/10.1145/2501115.2501123,1,
940,Improving Logo Spotting and Matching for Document Categorization by a Post-Filter Based on Homography.,"Digital document categorization based on logo spotting and recognition has raised a great interest in the research community because logos in documents are sources of information for categorizing documents with low costs. In this paper, we present an approach to improve the result of our method for logo spotting and recognition based on key point matching and presented in our previous paper [7]. First, the key points from both the query document images and a given set of logos (logo gallery) are extracted and described by SIFT, and are matched in the SIFT feature space. Secondly, logo segmentation is performed using spatial density-based clustering. The contribution of this paper is to add a third step where homography is used to filter the matched key points as a post-processing. And finally, in the decision stage, logo classification is performed by using an accumulating histogram. Our approach is tested using a well-known benchmark database of real world documents containing logos, and achieves good performances compared to state-of-the-art approaches.",38,https://doi.org/10.1109/ICDAR.2013.61,1,
941,A Discriminative Approach to On-Line Handwriting Recognition Using Bi-character Models.,"Unconstrained on-line handwriting recognition is typically approached within the framework of generative HMM-based classifiers. In this paper, we introduce a novel discriminative method that relies, in contrast, on explicit grapheme segmentation and SVM-based character recognition. In addition to single character recognition with rejection, bi-characters are recognized in order to refine the recognition hypotheses. In particular, bi-character recognition is able to cope with the problem of shared character parts. Whole word recognition is achieved with an efficient dynamic programming method similar to the Viterbi algorithm. In an experimental evaluation on the Unipen-ICROW-03 database, we demonstrate improvements in recognition accuracy of up to 8% for a lexicon of 20,000 words with the proposed method when compared with an HMM-based baseline system. The computational speed is on par with the baseline system.",38,https://doi.org/10.1109/ICDAR.2013.80,1,
942,Semi-synthetic Document Image Generation Using Texture Mapping on Scanned 3D Document Shapes.,"This article presents a method for generating semi-synthetic images of old documents where the pages might be torn (not flat). By using only 2D deformation models, most existing methods give non-realistic synthetic document images. Thus, we propose to use 3D approach for reproducing geometric distortions in real documents. First, a new proposed texture coordinate generation technique extracts texture coordinates of each vertex in the document shape (mesh) resulting from 3D scanning of a real degraded document. Then, any 2D document image can be overlayed on the mesh by using an existing texture image mapping method. As a result, many complex real geometric distortions can be integrated in generated synthetic images. These images then can be used for enriching training sets or for performance evaluation. The degradation method here is jointly used with the character degradation model we proposed in [1] to generate the 6000 semi-synthetic degraded images of the music score removal staff line competition of ICDAR 2013.",38,https://doi.org/10.1109/ICDAR.2013.104,1,
943,Invariants Extraction Method Applied in an Omni-language Old Document Navigating System.,"We are currently working on the concept of an omni script and interactive word retrieval system for ancient document collection navigation, based on query composition for non-expert users. To make the query, the user selects and composes writing pieces, which are invariants automatically extracted from the old document collection. In order to extract invariants from documents, strokes must be first extracted and clustered. Stroke extraction raises two main difficulties: detecting the ambiguous zones so as to extract primary strokes (writing pieces which do not contain any ambiguous zone) and grouping the primary strokes so as to form invariants. In this paper, we present existing methods for ambiguity zones detection and compare these methods on documents of different languages and periods to find out which one is more adapted in our context. Once ambiguous zones have been extracted, some neighboring primary strokes are grouped so as to obtain strokes and our clustering algorithm is applied over these strokes to find their representatives, i.e. the invariants. These invariants can further be used by the user to compose his/her query and to retrieve words from the document collection.",38,https://doi.org/10.1109/ICDAR.2013.268,1,
944,An experimental comparison of clustering methods for content-based indexing of large image databases.,"In recent years, the expansion of acquisition devices such as digital cameras, the development of storage and transmission techniques of multimedia documents and the development of tablet computers facilitate the development of many large image databases as well as the interactions with the users. This increases the need for efficient and robust methods for finding information in these huge masses of data, including feature extraction methods and feature space structuring methods. The feature extraction methods aim to extract, for each image, one or more visual signatures representing the content of this image. The feature space structuring methods organize indexed images in order to facilitate, accelerate and improve the results of further retrieval. Clustering is one kind of feature space structuring methods. There are different types of clustering such as hierarchical clustering, density-based clustering, grid-based clustering, etc. In an interactive context where the user may modify the automatic clustering results, incrementality and hierarchical structuring are properties growing in interest for the clustering algorithms. In this article, we propose an experimental comparison of different clustering methods for structuring large image databases, using a rigorous experimental protocol. We use different image databases of increasing sizes (Wang, PascalVoc2006, Caltech101, Corel30k) to study the scalability of the different approaches.",38,https://doi.org/10.1007/s10044-011-0261-7,1,
945,A character degradation model for grayscale ancient document images.,"Kanungo noise model is widely used to test the robustness of different binary document image analysis methods towards noise. This model only works with binary images while most document images are in grayscale. Because binarizing a document image might degrade its contents and lead to a loss of information, more and more researchers are currently focusing on segmentation-free methods (Angelika et al [2]). Thus, we propose a local noise model for grayscale images. Its main principle is to locally degrade the image in the neighbourhoods of “seed-points” selected close to the character boundary. These points define the center of “noise regions”. The pixel values inside the noise region are modified by a Gaussian random distribution to make the final result more realistic. While Kanungo noise models scanning artifacts, our model simulates degradations due to the age of the document itself and printing/writing process such as ink splotches, white specks or streaks. It is very easy for users to parameterize and create a set of benchmark databases with an increasing level of noise. These databases will further be used to test the robustness of different grayscale document image analysis methods (i.e. text line segmentation, OCR, handwriting recognition).",38,https://ieeexplore.ieee.org/document/6460227/,1,
946,Logo spotting for document categorization.,"Logo spotting is of a great interest because it enables to categorize the document images of a digital library of scanned documents according to their sources, without any costly semantic analysis of their textual transcript. In this paper, we present an approach for logo spotting, based on the matching of keypoints extracted both from the query document images and a given set of logos (gallery) using SIFT. In order to filter the matching points and keep only the most relevant, we compare the spatial distribution of the matching keypoints in the query image and in the logo gallery. We test our approach using a large collection of real world documents using a well-known benchmark database of logos and show that our approach achieves good performances compared to state-of-the-art approaches.",38,https://ieeexplore.ieee.org/document/6460915/,1,
947,A protocol to characterize the descriptive power and the complementarity of shape descriptors.,"Most document analysis applications rely on the extraction of shape descriptors, which may be grouped into different categories, each category having its own advantages and drawbacks (O.R. Terrades et al. in Proceedings of ICDAR’07, pp. 227–231, 2007). In order to improve the richness of their description, many authors choose to combine multiple descriptors. Yet, most of the authors who propose a new descriptor content themselves with comparing its performance to the performance of a set of single state-of-the-art descriptors in a specific applicative context (e.g. symbol recognition, symbol spotting...). This results in a proliferation of the shape descriptors proposed in the literature. In this article, we propose an innovative protocol, the originality of which is to be as independent of the final application as possible and which relies on new quantitative and qualitative measures. We introduce two types of measures: while the measures of the first type are intended to characterize the descriptive power (in terms of uniqueness, distinctiveness and robustness towards noise) of a descriptor, the second type of measures characterizes the complementarity between multiple descriptors. Characterizing upstream the complementarity of shape descriptors is an alternative to the usual approach where the descriptors to be combined are selected by trial and error, considering the performance characteristics of the overall system. To illustrate the contribution of this protocol, we performed experimental studies using a set of descriptors and a set of symbols which are widely used by the community namely ART and SC descriptors and the GREC 2003 database.",38,https://doi.org/10.1007/s10032-010-0125-5,1,
948,Navigala: an Original Symbol Classifier Based on Navigation through a Galois Lattice.,"This paper deals with a supervised classification method, using Galois Lattices based on a navigation-based strategy. Coming from the field of data mining techniques, most literature on the subject using Galois lattices relies on selection-based strategies, which consists of selecting/choosing the concepts which encode the most relevant information from the huge amount of available data. Generally, the classification step is then processed by a classical classifier such as the k-nearest neighbors rule or the Bayesian classifier. Opposed to these selection-based strategies are navigation-based approaches which perform the classification stage by navigating through the complete lattice (similar to the navigation in a classification tree), without applying any selection operation. Our approach, named Navigala, proposes an original navigation-based approach for supervised classification, applied in the context of noisy symbol recognition. Based on a state of the art dealing with Galois Lattices classification based methods, including a comparison between possible selection and navigation strategies, this paper proposes a description of NAVIGALA and its implementation in the context of symbol recognition. Some objective quantitative and qualitative evaluations of the approach are proposed, in order to highlight the relevance of the method.",38,https://doi.org/10.1142/S0218001411008634,1,
949,A New Adaptive Structural Signature for Symbol Recognition by Using a Galois Lattice as a Classifier.,"In this paper, we propose a new approach for symbol recognition using structural signatures and a Galois lattice as a classifier. The structural signatures are based on topological graphs computed from segments which are extracted from the symbol images by using an adapted Hough transform. These structural signatures-that can be seen as dynamic paths which carry high-level information-are robust toward various transformations. They are classified by using a Galois lattice as a classifier. The performance of the proposed approach is evaluated based on the GREC'03 symbol database, and the experimental results we obtain are encouraging.",38,https://doi.org/10.1109/TSMCB.2011.2108646,1,
950,A Local Discretization of Continuous Data for Lattices: Technical Aspects.,"Since few years, Galois lattices (GLs) are used in data mining
and defining a GL from complex data (i.e. non binary) is a recent challenge [1,2]. Indeed GL is classically defined from a binary table (called
context), and therefore in the presence of continuous data a discretization
step is generally needed to convert continuous data into discrete data.
Discretization is classically performed before the GL construction in a
global way. However, local discretization is reported to give better classification rates than global discretization when used jointly with other
symbolic classification methods such as decision trees (DTs). Using a result of lattice theory bringing together set of objects and specific nodes of
the lattice, we identify subsets of data to perform a local discretization
for GLs. Experiments are performed to assess the efficiency and the effectiveness of the proposed algorithm compared to global discretization",38,http://ceur-ws.org/Vol-959/paper_short2.pdf,1,
951,Writer Identification Using TF-IDF for Cursive Handwritten Word Recognition.,"In this paper, we present two text-independent writer identification methods in a closed-world context. Both methods use on-line and off-line features jointly with a classifier inspired from information retrieval methods. These methods are local, respectively based on the character and grapheme levels. This writer identification engine may be used to personalize our cursive word recognition engine [1] to the handwriting style of the writer, resulting in an adaptive cursive word recognizer. Experiments assess the effectiveness of the proposed approaches in a context of writer identification as well as integrated to our cursive word recognizer to make it adaptive.",38,https://doi.org/10.1109/ICDAR.2011.173,1,
952,Local Discretization of Numerical Data for Galois Lattices.,"Galois lattices' (GLs) definition is defined for a binary table (called context). Therefore, in the presence of continuous data, a discretization step is needed. Discretization is classically performed before the lattice construction in a global way. However, local discretization is reported to give better classification rates than global discretization when used jointly with other symbolic classification methods such as decision trees (DTs). We present a new algorithm performing local discretization for GLs using the lattice properties. Our local discretization algorithm is applied iteratively to particular nodes (called concepts) of the GL. Experiments are performed to assess the efficiency and the effectiveness of the proposed algorithm compared to global discretization.",38,https://doi.org/10.1109/ICTAI.2011.148,1,
954,Cursive On-line Handwriting Word Recognition Using a Bi-character Model for Large Lexicon Applications.,"This paper deals with on-line handwriting recognition in a closed-world environment with a large lexicon. Several applications using handwriting recognition have been developed, but most of them consider a lexicon of limited size. Many difficulties, in particular confusions during the segmentation stage, are linked to the use of a large lexicon, with large writing variations and an increased complexity of the connections between characters. In order to circumvent these problems, we introduce in this paper an original method based on a new analytical approach using two levels of recognition models: an isolated character recognizer and an original bi-character recognition model. The idea behind the bi-character model is to recognize jointly two neighboring characters. The objective is to reduce the confusions between characters occurring during the segmentation step. Experiments show an interesting improvement of the recognition rate when introducing the bi-character model, as the recognition rate is increased of 7.2% for a 1000 words lexicon, of 9.1% for a 2000 words lexicon, and up to 15% for a 10000 words lexicon.",38,https://doi.org/10.1109/ICFHR.2010.38,1,
956,Normalized radial basis function networks and bilinear discriminant analysis for face recognition.,"In this paper, we present a novel approach for face recognition, using a new subspace method called bilinear discriminant analysis (BDA) and normalized radial basis function networks (NRBFNs). In a first step, BDA extracts the features that enhance separation between classes by using a generalized bilinear projection-based Fisher criterion, computed from image matrices directly. In a second step, the features are fed into a NRBFN that learns class conditional probabilities. This results in an efficient and computationally simple open-world identification process. Experimental results assess the performance and robustness of the proposed algorithm compared to other subspace methods combined with NRBFNs, in the presence of variations in head poses, facial expressions, and partial occlusions.",38,https://doi.org/10.1109/AVSS.2005.1577292,1,
957,Face Recognition Using Modular Bilinear Discriminant Analysis.,"In this paper, we present a new approach for face recognition, named Modular Bilinear Discriminant Analysis (MBDA). In a first step, a set of experts is created, each one being trained independently on specific face regions using a new supervised technique named Bilinear Discriminant Analysis (BDA). BDA relies on the maximization of a generalized Fisher criterion based on bilinear projections of face image matrices. In a second step, the experts are combined to assign an identity with a confidence measure to each of the query faces. A series of experiments is performed in order to evaluate and compare the effectiveness of MBDA with respect to BDA and to the Modular Eigenspaces method. The experimental results indicate that MBDA is more effective than both BDA and the Modular Eigenspaces approach for face recognition.",38,https://doi.org/10.1007/11590064_3,1,
958,Two-Dimensional-Oriented Linear Discriminant Analysis for Face Recognition.,"In this paper, a new statistical projection-based method called Two-Dimensional- Oriented Linear Discriminant Analysis (2DO-LDA) is presented. While in the Fisherfaces method the 2D image matrices are first transformed into 1D vectors by merging their rows of pixels, 2DO-LDA is directly applied on matrices, as 2D-PCA. Within and between-class image covariance matrices are generalized, and 2DO-LDA aims at finding a projection space jointly maximizing the second and minimizing the first by considering a generalized Fisher criterion defined on image matrices. A series of experiments was performed on various face image databases in order to evaluate and compare the effectiveness and robustness of 2DO-LDA to 2D-PCA and the Fisherfaces method. The experimental results indicate that 2DO-LDA is more efficient than both 2D-PCA and LDA when dealing with variations in lighting conditions, facial expression and head pose.",38,https://doi.org/10.1007/1-4020-4179-9_147,1,
959,Comparing Robustness of Two-Dimensional PCA and Eigenfaces for Face Recognition.,"In this paper, we aim at evaluating the robustness of 2D-PCA for face recognition, and comparing it with the classical eigenfaces method. For most applications, a sensory gap exists between the images collected and those used for training. Consequently, methods based upon statistical projection need several preprocessing steps: face detection and segmentation, rotation, rescaling, noise removal, illumination correction, etc... This paper determines, for each preprocessing step, the minimum accuracy required in order to allow successful face recognition with 2D-PCA and compares it with the eigenfaces method. A series of experiments was conducted on a subset of the FERET database and digitally-altered versions of this subset. The tolerances of both methods to eight different artifacts were evaluated and compared. The experimental results show that 2D-PCA is significantly more robust to a wide range of preprocessing artifacts than the eigenfaces method.",38,https://doi.org/10.1007/978-3-540-30126-4_87,1,
963,A graph convolutional topic model for short and noisy text streams.,"Learning hidden topics from data streams has become absolutely necessary but posed challenging problems such as concept drift as well as short and noisy data. Using prior knowledge to enrich a topic model is one of potential solutions to cope with these challenges. Prior knowledge that is derived from human knowledge (e.g. Wordnet) or a pre-trained model (e.g. Word2vec) is very valuable and useful to help topic models work better. However, in a streaming environment where data arrives continually and infinitely, existing studies are limited to exploiting these resources effectively. Especially, a knowledge graph, that contains meaningful word relations, is ignored. In this paper, to aim at exploiting a knowledge graph effectively, we propose a novel graph convolutional topic model (GCTM) which integrates graph convolutional networks (GCN) into a topic model and a learning method which learns the networks and the topic model simultaneously for data streams. In each minibatch, our method not only can exploit an external knowledge graph but also can balance the external and old knowledge to perform well on new data. We conduct extensive experiments to evaluate our method with both a human knowledge graph (Wordnet) and a graph built from pre-trained word embeddings (Word2vec). The experimental results show that our method achieves significantly better performances than state-of-the-art baselines in terms of probabilistic predictive measure and topic coherence. In particular, our method can work well when dealing with short texts as well as concept drift.",42,https://doi.org/10.1016/j.neucom.2021.10.047,1,
964,From implicit to explicit feedback: A deep neural network for modeling sequential behaviours and long-short term preferences of online users.,"In this work, we examine the advantages of using multiple types of behaviours in recommendation systems. Intuitively, each user often takes some implicit actions (e.g., click) before making an explicit decision (e.g., purchase). Previous studies show that implicit and explicit feedback has different roles for a useful recommendation. However, these studies either exploit implicit and explicit behaviours separately or ignore the semantics of sequential interactions between users and items. In addition, we go from the hypothesis that a user’s preferences at a time are combinations of long-term and short-term interests. In this paper, we propose some Deep Learning architectures. The first one is Implicit to Explicit (ITE), to exploit users’ interests through the sequence of their actions. The second and third ones are two versions of ITE with Bidirectional Encoder Representations from Transformers based (BERT-based) architecture called BERT-ITE and BERT-ITE-Si, which combine users’ long- and short-term preferences without and with side information to enhance users’ representations. The experimental results show that our models outperform previous state-of-the-art ones and also demonstrate our views on the effectiveness of exploiting the implicit to explicit order as well as combining long- and short-term preferences in three large-scale datasets.",42,https://doi.org/10.1016/j.neucom.2022.01.023,1,
965,Revisiting Supervised Word Embeddings.,"Word embeddings are playing a crucial role in a variety of applications. However, most previous works focus on word embeddings which are either non-discriminative or hardly interpretable. In this work, we investigate a novel approach, referred to as SWET, which learns supervised word embeddings using topic models from labeled corpora. SWET inherits the interpretability of topic models, the discriminativeness of supervised inference from labels. More importantly, SWET enables us to directly exploit a large class of existing unsupervised and supervised topic models to learn supervised word embeddings. Extensive experiments show that SWET outperforms unsupervised approaches by a large margin, and are highly competitive with supervised baselines.",42,http://jise.iis.sinica.edu.tw:80/JISESearch/pages/View/PaperView.jsf?keyId=185_2501,1,
966,Boosting prior knowledge in streaming variational Bayes.,"Exploiting prior/human knowledge is an effective way to enhance Bayesian models, especially in cases of sparse or noisy data, for which building an entirely new model is not always possible. There is a lack of studies on the effect of external prior knowledge in streaming environments, where the data come sequentially and infinitely. In this work, we show the problem of vanishing prior knowledge in streaming variational Bayes. This is a serious drawback in various applications. We then develop a simple framework to boost the external prior when learning a Bayesian model from data streams. By boosting, the prior knowledge can be maintained and efficiently exploited through each minibatch of streaming data. We evaluate the performance of our framework in four scenarios: streaming in synthetic data, streaming sentiment analysis, streaming learning for latent Dirichlet allocation, and streaming text classification, in comparison with the methods that do not keep priors. From extensive experiments, we find that when provided good external knowledge, our framework can improve the performance of a Bayesian model, often by a significant margin for noisy and short text streams.",42,https://doi.org/10.1016/j.neucom.2020.10.026,1,
967,Structured Dropout Variational Inference for Bayesian Neural Networks.,"Approximate inference in Bayesian deep networks exhibits a dilemma of how to yield high fidelity posterior approximations while maintaining computational efficiency and scalability. We tackle this challenge by introducing a novel variational structured approximation inspired by the Bayesian interpretation of Dropout regularization. Concretely, we focus on the inflexibility of the factorized structure in Dropout posterior and then propose an improved method called Variational Structured Dropout (VSD). VSD employs an orthogonal transformation to learn a structured representation on the variational Gaussian noise with plausible complexity, and consequently induces statistical dependencies in the approximate posterior. Theoretically, VSD successfully addresses the pathologies of previous Variational Dropout methods and thus offers a standard Bayesian justification. We further show that VSD induces an adaptive regularization term with several desirable properties which contribute to better generalization. Finally, we conduct extensive experiments on standard benchmarks to demonstrate the effectiveness of VSD over state-of-the-art variational methods on predictive accuracy, uncertainty estimation, and out-of-distribution detection.",42,https://proceedings.neurips.cc/paper/2021/hash/80a160ff31266be2f93012a2a3eca713-Abstract.html,1,
968,Generalization of GANs under Lipschitz continuity and data augmentation.,"Generative adversarial networks (GANs) are so complex that the existing learning theories do not provide a satisfactory explanation for why GANs have great success in practice. The same situation also remains largely open for deep neural networks. To fill this gap, we introduce a Lipschitz theory to analyze generalization. We demonstrate its simplicity by analyzing generalization and consistency of overparameterized neural networks. We then use this theory to derive Lipschitz-based generalization bounds for GANs. Our bounds show that penalizing the Lipschitz constant of the GAN loss can improve generalization. This result answers the long mystery of why the popular use of Lipschitz constraint for GANs often leads to great success, empirically without a solid theory. Finally but surprisingly, we show that, when using Dropout or spectral normalization, both \emph{truly deep} neural networks and GANs can generalize well without the curse of dimensionality.",42,https://arxiv.org/abs/2104.02388,1,
969,Neural Poisson Factorization.,"In this work, we focus on dealing with a sparse users' feedback matrix and short descriptions/contents of items in recommender systems. We propose the Neural Poisson factorization (NPF) model which is a hybrid of deep learning and Poisson factorization. While Poisson factorization is suitable to model discrete, massive and sparse feedback, using a deep neural network and pre-trained word embeddings can learn hidden semantic in short item descriptions well. Therefore, NPF overcomes the limitation of existing models when dealing with short texts and a sparse feedback matrix. Moreover, we develop a random view algorithm based on stochastic learning for our model, in which each user is only viewed a random subset of items and his/her feedback on the subset is used to update his/her representation in each iteration. This approach is reasonable because each user can only know or view a partial subset of items when surfing a system. Extensive experiments illustrate the significant advantages of NPF over content-based matrix factorization methods and others that ignore item descriptions.",42,https://doi.org/10.1109/ACCESS.2020.2994239,1,
970,"MAP Estimation With Bernoulli Randomness, and Its Application to Text Analysis and Recommender Systems.","MAP estimation plays an important role in many probabilistic models. However, in many cases, the MAP problem is non-convex and intractable. In this work, we propose a novel algorithm, called BOPE, which uses Bernoulli randomness for Online Maximum a Posteriori Estimation. We show that BOPE has a fast convergence rate. In particular, BOPE implicitly employs a prior which plays as regularization. Such a prior is different from the one of the MAP problem and will be vanishing as BOPE does more iterations. This property of BOPE is significant and enables to reduce severe overfitting for probabilistic models in ill-posed cases, including short text, sparse data, and noisy data. We validate the practical efficiency of BOPE in two contexts: text analysis and recommender systems. Both contexts show the superior of BOPE over the baselines.",42,https://doi.org/10.1109/ACCESS.2020.3008534,1,
971,Bag of biterms modeling for short texts.,"Analyzing texts from social media encounters many challenges due to their unique characteristics of shortness, massiveness, and dynamic. Short texts do not provide enough context information, causing the failure of the traditional statistical models. Furthermore, many applications often face with massive and dynamic short texts, causing various computational challenges to the current batch learning algorithms. This paper presents a novel framework, namely bag of biterms modeling (BBM), for modeling massive, dynamic, and short text collections. BBM comprises of two main ingredients: (1) the concept of bag of biterms (BoB) for representing documents, and (2) a simple way to help statistical models to include BoB. Our framework can be easily deployed for a large class of probabilistic models, and we demonstrate its usefulness with two well-known models: latent Dirichlet allocation (LDA) and hierarchical Dirichlet process (HDP). By exploiting both terms (words) and biterms (pairs of words), the major advantages of BBM are: (1) it enhances the length of the documents and makes the context more coherent by emphasizing the word connotation and co-occurrence via bag of biterms, and (2) it inherits inference and learning algorithms from the primitive to make it straightforward to design online and streaming algorithms for short texts. Extensive experiments suggest that BBM outperforms several state-of-the-art models. We also point out that the BoB representation performs better than the traditional representations (e.g., bag of words, tf-idf) even for normal texts.",42,https://doi.org/10.1007/s10115-020-01482-z,1,
972,Predictive Coding for Locally-Linear Control.,"High-dimensional observations and unknown dynamics are major challenges when applying optimal control to many real-world decision making tasks. The Learning Controllable Embedding (LCE) framework addresses these challenges by embedding the observations into a lower dimensional latent space, estimating the latent dynamics, and then performing control directly in the latent space. To ensure the learned latent dynamics are predictive of next-observations, all existing LCE approaches decode back into the observation space and explicitly perform next-observation prediction—a challenging high-dimensional task that furthermore introduces a large number of nuisance parameters (i.e., the decoder) which are discarded during control. In this paper, we propose a novel information-theoretic LCE approach and show theoretically that explicit next-observation prediction can be replaced with predictive coding. We then use predictive coding to develop a decoder-free LCE model whose latent dynamics are amenable to locally-linear control. Extensive experiments on benchmark tasks show that our model reliably learns a controllable latent space that leads to superior performance when compared with state-of-the-art LCE baselines.",42,http://proceedings.mlr.press/v119/shu20a.html,1,
973,Graph Convolutional Topic Model for Data Streams.,"Learning hidden topics from data streams has become absolutely necessary but posed challenging problems such as concept drift as well as short and noisy data. Using prior knowledge to enrich a topic model is one of potential solutions to cope with these challenges. Prior knowledge that is derived from human knowledge (e.g. Wordnet) or a pre-trained model (e.g. Word2vec) is very valuable and useful to help topic models work better. However, in a streaming environment where data arrives continually and infinitely, existing studies are limited to exploiting these resources effectively. Especially, a knowledge graph, that contains meaningful word relations, is ignored. In this paper, to aim at exploiting a knowledge graph effectively, we propose a novel graph convolutional topic model (GCTM) which integrates graph convolutional networks (GCN) into a topic model and a learning method which learns the networks and the topic model simultaneously for data streams. In each minibatch, our method not only can exploit an external knowledge graph but also can balance the external and old knowledge to perform well on new data. We conduct extensive experiments to evaluate our method with both a human knowledge graph (Wordnet) and a graph built from pre-trained word embeddings (Word2vec). The experimental results show that our method achieves significantly better performances than state-of-the-art baselines in terms of probabilistic predictive measure and topic coherence. In particular, our method can work well when dealing with short texts as well as concept drift. The implementation of GCTM is available ",42,https://arxiv.org/abs/2003.06112,1,
974,Dynamic transformation of prior knowledge into Bayesian models for data streams.,"We consider how to effectively use prior knowledge when learning a Bayesian model from streaming environments where the data come infinitely and sequentially. This problem is highly important in the era of data explosion and rich sources of precious external knowledge such as pre-trained models, ontologies, Wikipedia, etc. We show that some existing approaches can forget any knowledge very fast. We then propose a novel framework that enables to incorporate the prior knowledge of different forms into a base Bayesian model for data streams. Our framework subsumes some existing popular models for time-series/dynamic data. Extensive experiments show that our framework outperforms existing methods with a large margin. In particular, our framework can help Bayesian models generalize well on extremely short text while other methods overfit. The implementation of our framework is available at ",42,https://arxiv.org/abs/2003.06123,1,
975,Eliminating overfitting of probabilistic topic models on short and noisy text: The role of dropout.,"Probabilistic topic models are powerful tools for discovering hidden structures/semantics in discrete data, e.g., texts, images, links. However, on short and noisy texts, directly applying topic models may not work well or face severe overfitting. In this article, we investigate the benefits of dropout for preventing topic models from overfitting. We integrate dropout into several stochastic methods for learning latent Dirichlet allocation (LDA). From extensive experiments on four large-scale datasets, our findings are: (1) dropout helps to prevent overfitting and significantly enhance predictiveness and generalization of LDA on short texts; (2) for long documents, dropout may provide little benefit; (3) dropout can be easily integrated into any learning methods to avoid overfitting for short and noisy text. Furthermore, dropout can be straightforwardly employed in a wide range of topic models. In evidence, we apply dropout to BTM (Biterm topic model), one of the state-of-the-art models for short texts. Our experiments illustrate that BTM with dropout not only remains its good results in term of predictiveness, but also improves the learning time significantly.",42,https://doi.org/10.1016/j.ijar.2019.05.010,1,
976,Employing the Correspondence of Relations and Connectives to Identify Implicit Discourse Relations via Label Embeddings.,"It has been shown that implicit connectives can be exploited to improve the performance of the models for implicit discourse relation recognition (IDRR). An important property of the implicit connectives is that they can be accurately mapped into the discourse relations conveying their functions. In this work, we explore this property in a multi-task learning framework for IDRR in which the relations and the connectives are simultaneously predicted, and the mapping is leveraged to transfer knowledge between the two prediction tasks via the embeddings of relations and connectives. We propose several techniques to enable such knowledge transfer that yield the state-of-the-art performance for IDRR on several settings of the benchmark dataset (i.e., the Penn Discourse Treebank dataset).",42,https://doi.org/10.18653/v1/p19-1411,1,
977,Infinite Dropout for training Bayesian models from data streams.,"The ability to continuously train Bayesian models in streaming environments is highly important in the era of big data. However, it has to face the famous stability-plasticity dilemma and the problem of noisy and sparse data. We propose a novel and easy-to-implement framework, called Infinite Dropout (iDropout), to address these challenges. iDropout has an easy mechanism to balance between old and new information, which allows models to trade off stability against plasticity. Thanks to the ability to reduce overfitting and the ensemble property of Dropout, our framework obtains better generalization, thus effectively handles undesirable effects of noise and sparsity. Further, iDropout is able to adapt quickly to abnormal changes in data streams. We theoretically analyze the equivalence of Dropout in iDropout to a regularizer, well applied to a much larger context than what was known before. Extensive experiments show that iDropout significantly outperforms the state-of-the-art baselines.",42,https://doi.org/10.1109/BigData47090.2019.9005544,1,
978,Marking Mechanism in Sequence-to-sequence Model for Mapping Language to Logical Form.,"Semantic parsing in Natural language processing (NLP) is a challenging task, which has been studied for many years. The main purpose is to model the language as a logical form like a machine translation task. Recently, an approach which uses a Neural network with Sequence to sequence model (Seq2seq) has achieved positive results. However, there are many challenges which have not been solved thoroughly yet, especially in the problem of rare words. Rare words in a natural sentence are usually the name of an object, a place or number, time, etc. Although these words are very various and difficult for the model to capture meaning, it holds a key information role in human communication (for example: name all the rivers in colorado ?). There are some methods to solve this problem such as using Attention or using Copy mechanism. However, these methods still difficult to copy phrase rare words, especially in case these phrases are variable in size. This paper proposes a novel approach to solve this problem, namely Marking mechanism in Seq2seq. The main idea is to label special words which are rare-words in a sentence by the encoder (marking step) and the decoder represents the logical form based on those labels (transforming step). Our experiments demonstrate that this approach works effectively, achieved a competitive result with old methods on all 3 datasets Geo, Atis, Jobs and special outperformed on our Artificial dataset.",42,https://doi.org/10.1109/KSE.2019.8919471,1,
979,Enriching User Representation in Neural Matrix Factorization.,"Personalized recommendation is a crucial problem in the era of information overload. One of the most prominent methods addressing this problem is collaborative filtering (CF). Recently, NeuMF [1] has employed Deep Neural Network to facilitate traditional CF methods. Although NeuMF achieved the state-of-the-art performance, it is expected that predictive accuracy can be improved by enriching user representation. In this paper, we propose a neural network incorporating latent features and personalized information of users, namely Feature based Neural Matrix Factorization (FeaNMF). In FeaNMF, personalized information can be considered as auxiliary features which are generated by associating past behavior of users (e.g, purchase history, browsing activity, watching habits) with item categories. By explicitly modeling personalized information in the metric of item categories, the major advantages of FeaNMF are: (1) it enriches user representation and thus enhances the predictive capability, (2) it inherits the advantages of NeuMF to model the latent features in a more comprehensive approach, rather than applying a simple linear function as existing methods (e.g. Matrix Factorization), and (3) it is easy to extend FeaNMF for employing a large amount of external sources to construct user preferences. Experiments show that our model significantly improves predictive accuracy compared to the three previous approaches including NeuMF.",42,https://doi.org/10.1109/RIVF.2019.8713692,1,
981,Collaborative Topic Model for Poisson distributed ratings.,"We present Collaborative Topic Model for Poisson distributed ratings (CTMP), a hybrid and interpretable probabilistic content-based collaborative filtering model for recommender system. The model enables both content representation by admixture topic modelling, and computational efficiency from Poisson factorization living together under one tightly coupled probabilistic model, thus addressing the limitation of previous methods. CTMP excels in predictive performance under different real-world recommendation contexts, and easily scales to big datasets, while recovering interpretable user profiles. Moreover, our empirical study also shows strong evidence that sparsity in the estimates of topic mixture can be recovered via learning, despite not being specified in the model. The sparse representation derived from CTMP would allow efficient storage of the item contents, consequently providing a computational advantage for other tasks in industrial settings.",42,https://doi.org/10.1016/j.ijar.2018.02.001,1,
982,A Fast Algorithm for Posterior Inference with Latent Dirichlet Allocation.,"Latent Dirichlet Allocation (LDA) [1], among various forms of topic models, is an important probabilistic generative model for analyzing large collections of text corpora. The problem of posterior inference for individual texts is very important in streaming environments, but is often intractable in the worst case. To avoid directly solving this problem which is NP-hard, some proposed existing methods for posterior inference are approximate but do not have any guarantee on neither quality nor convergence rate. Based on the idea of Online Frank-Wolfe algorithm by Hazan [2] and improvement of Online Maximum a Posteriori Estimation algorithm (OPE) by Than [3, 4], we propose a new effective algorithm (so-called NewOPE) solving posterior inference in topic models by combining Bernoulli distribution, stochastic bounds, and approximation function. Our algorithm has more attractive properties than existing inference approaches, including theoretical guarantees on quality and fast convergence rate. It not only maintains the key advantages of OPE but often outperforms OPE and existing algorithms before. Our new algorithm has been employed to develop two effective methods for learning topic models from massive/streaming text collections. Experimental results show that our approach is more efficient and robust than the state-of-the-art methods.",42,https://doi.org/10.1007/978-3-319-75420-8_13,1,
983,Evaluating Named-Entity Recognition Approaches in Plant Molecular Biology.,"Text mining research is becoming an important topic in biology with the aim to extract biological entities from scientific papers in order to extend the biological knowledge. However, few thorough studies are developed for plant molecular biology data, especially rice, thus resulting a lack of datasets available to exploit advanced machine learning methods able to detect entities such as genes and proteins. In this article, we first developed a dataset from the Ozyzabase - a database of rice gene, and used it as the benchmark. Then, we evaluated the performance of two Name Entities Recognition (NER) methods for sequence tagging: a Long Short Term Memory (LSTM) model, combined with Conditional Random Fields (CRFs), and a hybrid method based on the dictionary lookup combining with some machine learning systems to improve result. We analyzed the performance of these methods when apply to the Oryzabase dataset and improved the results. On average, the result from LSTM-CRF reaching 86% in F1 is more exploitable.

",42,https://doi.org/10.1007/978-3-030-03014-8_19,1,
984,Reducing Class Overlapping in Supervised Dimension Reduction.,"Dimension reduction is to find a low-dimensional subspace to project high-dimensional data on, such that the discriminative property of the original higher-dimensional data is preserved. In supervised dimension reduction, class labels are integrated into the lower-dimensional representation, to produce better results on classification tasks. The supervised dimension reduction (SDR) framework by [17] is one of the state-of-the-art methods that takes into account not only the class labels but also the neighborhood graphs of the data, and have some advantages in preserving the within-class local structure and widening the between-class margin. However, the reduced-dimensional representation produced by the SDR framework suffers from the class overlapping problem - in which, data points lie closer to a different class rather than the class they belong to. The class overlapping problem can hurt the quality on the classification task. In this paper, we propose a new method to reduce the overlap for the SDR framework in [17]. The experimental results show that our method reduces the size of the overlapping set by an order of magnitude. As a result, our method outperforms the pre-existing framework on the classification task significantly. Moreover, visualization plots show that the reduced-dimensional representation learned by our method is more scattered for within-class data and more separated for between-class data, as compared to the pre-existing SDR framework.",42,https://doi.org/10.1145/3287921.3287925,1,
985,An effective and interpretable method for document classification.,"As the number of documents has been rapidly increasing in recent time, automatic text categorization is becoming a more important and fundamental task in information retrieval and text mining. Accuracy and interpretability are two important aspects of a text classifier. While the accuracy of a classifier measures the ability to correctly classify unseen data, interpretability is the ability of the classifier to be understood by humans and provide reasons why each data instance is assigned to a label. This paper proposes an interpretable classification method by exploiting the Dirichlet process mixture model of von Mises–Fisher distributions for directional data. By using the labeled information of the training data explicitly and determining automatically the number of topics for each class, the learned topics are coherent, relevant and discriminative. They help interpret as well as distinguish classes. Our experimental results showed the advantages of our approach in terms of separability, interpretability and effectiveness in classification task of datasets with high dimension and complex distribution. Our method is highly competitive with state-of-the-art approaches.",42,https://doi.org/10.1007/s10115-016-0956-6,1,
986,Keeping Priors in Streaming Bayesian Learning.,"Exploiting prior knowledge in the Bayesian learning process is one way to improve the quality of Bayesian model. To the best of our knowledge, however, there is no formal research about the influence of prior in streaming environment. In this paper, we address the problem of using prior knowledge in streaming Bayesian learning, and develop a framework for keeping priors in streaming learning (KPS) that maintains knowledge from the prior through each minibatch of streaming data. We demonstrate the performance of our framework in two scenarios: streaming learning for latent Dirichlet allocation and streaming text classification in comparison with methods that do not keep prior.",42,https://doi.org/10.1007/978-3-319-57529-2_20,1,
987,Sparse Stochastic Inference with Regularization.,"The massive amount of digital text information and delivering them in streaming manner pose challenges for traditional inference algorithms. Recently, advances in stochastic inference algorithms have made it feasible to learn topic models from very large-scale collections of documents. In this paper, we however point out that many existing approaches are prone to overfitting for extremely large/infinite datasets. The possibility of overfitting is particularly high in streaming environments. This finding suggests to use regularization for stochastic inference. We then propose a novel stochastic algorithm for learning latent Dirichlet allocation that uses regularization when updating global parameters and utilizes sparse Gibb sampling to do local inference. We study the performance of our algorithm on two massive data sets and demonstrate that it surpasses the existing algorithms in various aspects.",42,https://doi.org/10.1007/978-3-319-57454-7_35,1,
988,Enabling Hierarchical Dirichlet Processes to Work Better for Short Texts at Large Scale.,"Analyzing texts from social media often encounters many challenges, including shortness, dynamic, and huge size. Short texts do not provide enough information so that statistical models often fail to work. In this paper, we present a very simple approach (namely, bag-of-biterms) that helps statistical models such as Hierarchical Dirichlet Processes (HDP) to work well with short texts. By using both terms (words) and biterms to represent documents, bag-of-biterms (BoB) provides significant benefits: (1) it naturally lengthens representation and thus helps us reduce bad effects of shortness; (2) it enables the posterior inference in a large class of probabilistic models including HDP to be less intractable; (3) no modification of existing models/methods is necessary, and thus BoB can be easily employed in a wide class of statistical models. To evaluate those benefits of BoB, we take Online HDP into account in that it can deal with dynamic and massive text collections, and we do experiments on three large corpora of short texts which are crawled from Twitter, Yahoo Q&A, and New York Times. Extensive experiments show that BoB can help HDP work significantly better in both predictiveness and quality",42,https://doi.org/10.1007/978-3-319-31750-2_34,1,
989,Effective and Interpretable Document Classification Using Distinctly Labeled Dirichlet Process Mixture Models of von Mises-Fisher Distributions.,"Document Classification is essential to information retrieval and text mining. Accuracy and interpretability are two important aspects of text classifiers. This paper proposes an interpretable classification method (DLDPvMFs) by using the Dirichlet process mixture (DPM) model to discover the hidden topics distinctly within each label for classification of directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. We use a mean-field variational inference algorithm when developing DLDPvMFs. By using the label information of the training data explicitly and determining automatically the number of topics for each label to find the topical space, class topics are coherent, relevant and discriminative and since they help us interpret class’s label as well as distinguish classes. Our experimental results showed the advantages of our approach via significant criteria such as separability, interpretability and effectiveness in classification task of large datasets with high dimension and complex distribution. Our obtained results are highly competitive with state-of-the-art approaches.",42,https://doi.org/10.1007/978-3-319-18123-3_9,1,
990,Modeling the diversity and log-normality of data.,"We investigate two important properties of real data: diversity and log-normality. Log-normality accounts for the fact that data follow the lognormal distribution, whereas diversity measures variations of the attributes in the data. To our knowledge, these two inherent properties have not been paid much attention from the machine learning community, especially from the topic modeling community. In this article, we fill in this gap in the framework of topic modeling. We first investigate whether or not these two properties can be captured by the most well-known Latent Dirichlet Allocation model (LDA), and find that LDA behaves inconsistently with respect to diversity. Particularly, it favors data of low diversity, but works badly on data of high diversity. Then, we argue that these two inherent properties can be captured well by endowing the topic-word distributions in LDA with the lognormal distribution. This treatment leads to a new model, named Dirichlet-lognormal topic model (DLN). Using the lognormal distribution complicates the learning and inference of DLN, compared with those of LDA. Hence, we used variational method, in which model learning and inference are reduced to solving convex optimization problems. Extensive experiments strongly suggest that (1) the predictive power of DLN is consistent with respect to diversity, and that (2) DLN works consistently better than LDA for datasets whose diversity is large, and for datasets which contain many log-normally distributed attributes. Justifications for these results require insights into the used statistical distributions and will be discussed in the article.",42,https://doi.org/10.3233/IDA-140685,1,
991,An effective framework for supervised dimension reduction.,"We consider supervised dimension reduction (SDR) for problems with discrete inputs. Existing methods are computationally expensive, and often do not take the local structure of data into consideration when searching for a low-dimensional space. In this paper, we propose a novel framework for SDR with the aims that it can inherit scalability of existing unsupervised methods, and that it can exploit well label information and local structure of data when searching for a new space. The way we encode local information in this framework ensures three effects: preserving inner-class local structure, widening inter-class margin, and reducing possible overlap between classes. These effects are vital for success in practice. Such an encoding helps our framework succeed even in cases that data points reside in a nonlinear manifold, for which existing methods fail.

The framework is general and flexible so that it can be easily adapted to various unsupervised topic models. We then adapt our framework to three unsupervised models which results in three methods for SDR. Extensive experiments on 10 practical domains demonstrate that our framework can yield scalable and qualitative methods for SDR. In particular, one of the adapted methods can perform consistently better than the state-of-the-art method for SDR while enjoying 30–450 times faster speed.",42,https://doi.org/10.1016/j.neucom.2014.02.017,1,
992,Dual online inference for latent Dirichlet allocation.,"Latent Dirichlet allocation (LDA) provides an efficient tool to analyze very large text collections. In this paper, we discuss three novel contributions: (1) a proof for the tractability of the MAP estimation of topic mixtures under certain conditions that might fit well with practices, even though the problem is known to be intractable in the worst case; (2) a provably fast algorithm (OFW) for inferring topic mixtures; (3) a dual online algorithm (DOLDA) for learning LDA at a large scale. We show that OFW converges to some local optima, but under certain conditions it can converge to global optima. The discussion of OFW is very general and hence can be readily employed to accelerate the MAP estimation in a wide class of probabilistic models. From extensive experiments we find that DOLDA can achieve significantly better predictive performance and more interpretable topics, with lower runtime, than stochastic variational inference. Further, DOLDA enables us to easily analyze text streams or millions of documents.",42,http://proceedings.mlr.press/v39/than14.html,1,
993,Simplicial nonnegative matrix factorization.,"Nonnegative matrix factorization (NMF) plays a crucial role in machine learning and data mining, especially for dimension reduction and component analysis. It is employed widely in different fields such as information retrieval, image processing, etc. After a decade of fast development, severe limitations still remained in NMFs methods including high complexity in instance inference, hard to control sparsity or to interpret the role of latent components. To deal with these limitations, this paper proposes a new formulation by adding simplicial constraints for NMF. Experimental results in comparison to other state-of-the-art approaches are highly competitive.",42,https://doi.org/10.1109/RIVF.2013.6719865,1,
994,Probable convexity and its application to Correlated Topic Models.,"Non-convex optimization problems often arise from probabilistic modeling, such as estimation of posterior distributions. Non-convexity makes the problems intractable, and poses various obstacles for us to design efficient algorithms. In this work, we attack non-convexity by first introducing the concept of \emph{probable convexity} for analyzing convexity of real functions in practice. We then use the new concept to analyze an inference problem in the \emph{Correlated Topic Model} (CTM) and related nonconjugate models. Contrary to the existing belief of intractability, we show that this inference problem is concave under certain conditions. One consequence of our analyses is a novel algorithm for learning CTM which is significantly more scalable and qualitative than existing methods. Finally, we highlight that stochastic gradient algorithms might be a practical choice to resolve efficiently non-convex problems. This finding might find beneficial in many contexts which are beyond probabilistic modeling.",42,http://arxiv.org/abs/1312.4527,1,
995,Fully Sparse Topic Models.,"In this paper, we propose Fully Sparse Topic Model (FSTM) for modeling large collections of documents. Three key properties of the model are: (1) the inference algorithm converges in linear time, (2) learning of topics is simply a multiplication of two sparse matrices, (3) it provides a principled way to directly trade off sparsity of solutions against inference quality and running time. These properties enable us to speedily learn sparse topics and to infer sparse latent representations of documents, and help significantly save memory for storage. We show that inference in FSTM is actually MAP inference with an implicit prior. Extensive experiments show that FSTM can perform substantially better than various existing topic models by different performance measures. Finally, our parallel implementation can handily learn thousands of topics from large corpora with millions of terms.",42,https://doi.org/10.1007/978-3-642-33460-3_37,1,
996,Supervised dimension reduction with topic models.,"We consider supervised dimension reduction (SDR) for problems with discrete variables. Existing methods are computationally expensive, and often do not take the local structure of data into consideration when searching for a low-dimensional space. In this paper, we propose a novel framework for SDR which is (1) general and fiexible so that it can be easily adapted to various unsupervised topic models, (2) able to inherit scalability of unsupervised topic models, and (3) can exploit well label information and local structure of data when searching for a new space. Extensive experiments with adaptations to three models demonstrate that our framework can yield scalable and qualitative methods for SDR. One of those adaptations can perform better than the state-of-the-art method for SDR while enjoying significantly faster speed.",42,http://proceedings.mlr.press/v25/than12.html,1,
997,"Managing sparsity, time, and quality of inference in topic models.","Inference is an integral part of probabilistic topic models, but is often non-trivial to derive an efficient algorithm for a specific model. It is even much more challenging when we want to find a fast inference algorithm which always yields sparse latent representations of documents. In this article, we introduce a simple framework for inference in probabilistic topic models, denoted by FW. This framework is general and flexible enough to be easily adapted to mixture models. It has a linear convergence rate, offers an easy way to incorporate prior knowledge, and provides us an easy way to directly trade off sparsity against quality and time. We demonstrate the goodness and flexibility of FW over existing inference methods by a number of tasks. Finally, we show how inference in topic models with nonconjugate priors can be done efficiently.",42,http://arxiv.org/abs/1210.7053,1,
998,Adaptive Ranking Relevant Source Files for Bug Reports Using Genetic Algorithm.,"Precisely locating buggy files for a given bug report is a cumbersome and time-consuming task, particularly in a large-scale project with thousands of source files and bug reports. An efficient bug localization module is desirable to improve the productivity of the software maintenance phase. Many previous approaches rank source files according to their relevance to a given bug report based on simple lexical matching scores. However, the lexical mismatches between natural language expressions used to describe bug reports and technical terms of software source code might reduce the bug localization system’s accuracy. Incorporating domain knowledge through some features such as the semantic similarity, the fixing frequency of a source file, the code change history and similar bug reports is crucial to efficiently locating buggy files. In this paper, we propose a bug localization model, BugLocGA that leverages both lexical and semantic information as well as explores the relation between a bug report and a source file through some domain features. Given a bug report, we calculate the ranking score with every source files through a weighted sum of all features, where the weights are trained through a genetic algorithm with the aim of maximizing the performance of the bug localization model using two evaluation metrics: mean reciprocal rank (MRR) and mean average precision (MAP). The empirical results conducted on some widely-used open source software projects have showed that our model outperformed some state of the art approaches by effectively recommending relevant files where the bug should be fixed.",44,https://doi.org/10.3233/FAIA210042,1,
999,Applying PNZ Model in Reliability Prediction of Component-Based Systems and Fault Tolerance Structures Technique.,"Reliability is the chief quality that one wishes for in anything. Reliability is also the main issue with computer systems. One of the purposes of system reliability analysis is to identify the weakness in a system and to quantify the impact of component failures. However, existing reliability prediction approaches for component-based software systems are limited in their applicability because they either neglect or do not support modeling explicitly several factors like error propagation, software fault tolerance mechanisms. In this paper, we evaluate reliability prediction of component-based system and fault tolerance structures technique by applying Pham Nordmann Zhang (PNZ) model, one of the best models based on non homogeneous Poisson process. Our approach uses a reliability modeling schema whose models are automatically transformed by a reliability prediction tool into PNZ models for reliability predictions and sensitivity analyses. Via these our case studies, we demonstrate its applicability and introduce how much reliability of software system can be improved by using fault tolerance structures technique.",45,https://doi.org/10.1007/978-3-319-29236-6_27,1,
1001,The connected p-median problem on complete multi-layered graphs.,"We address in this paper the connected p-median problem on complete multi-layered graphs. We first solve the corresponding 1-median problem on the underlying graph in linear time based on the structure of the induced path. For p≥2, we prove that the connected p-median contains at least one vertex in the layered set corresponding to the 1-median or its adjacent vertices in the induced path. Then, we develop an O(nlogn) algorithm that solves the problem on a complete multi-layered graph with n vertices.",45,https://doi.org/10.1142/S1793830921501184,1,
1002,The minmax regret inverse maximum weight problem.,"Let a ground set  and a prespecified element be given. We address the problem of modifying the weight of each element in  at minimum cost so that the weight of the prespecified element become the maximum one in the perturbed set. Moreover, as modifying costs are usually uncertain in many real life situations, we measure the robustness by taking into account the minmax regret inverse maximum weight problem on . In order to solve the problem, we first prove that there are exactly two scenarios that lead to the maximum regret of the cost function. Based on the convexity of the objective function, we develop a combinatorial algorithm that solves the corresponding problem in linear time.",45,https://doi.org/10.1016/j.amc.2021.126328,1,
1003,The inverse connected p-median problem on block graphs under various cost functions.,"We address the problem of modifying vertex weights of a block graph at minimum total cost so that a predetermined set of p connected vertices becomes a connected p-median on the perturbed block graph. This problem is the so-called inverse connected p-median problem on block graphs. We consider the problem on a block graph with uniform edge lengths under various cost functions, say rectilinear norm, Chebyshev norm, and bottleneck Hamming distance. To solve the problem, we first find an optimality criterion for a set that is a connected p-median. Based on this criterion, we can formulate the problem as a convex or quasiconvex univariate optimization problem. Finally, we develop combinatorial algorithms that solve the problems under the three cost functions in O(nlogn) time, where n is the number of vertices in the underlying block graph.",45,https://doi.org/10.1007/s10479-020-03651-3,1,
1004,A linear time algorithm for the p-maxian problem on trees with distance constraint.,"This paper concerns the p-maxian problem on trees with an upper bound on the distance of new facilities. We first consider the case p=2 and show that the optimal objective is obtained if the constraint holds with equality. By this result, we further explore the characteristic of the optimal solution, which helps to develop a linear time algorithm to solve the constrained 2-maxian problem. The result can be extended to the constrained p-maxian on trees based on the nestedness property. We also discuss the constrained p-maxian problem on trees in relation to the unconstrained p-maxian problem and the 1-maxian problem on the underlying tree.",45,https://doi.org/10.1007/s10878-020-00650-9,1,
1005,Computational aspects of the inverse single facility location problem on trees under lk-norm.,"We consider in this paper the inverse versions of the two popular problems in location theory, say the 1-median and the 1-center problems on trees. The cost for modifying vertex weights is measured under -norm for a positive integer k and hence the cost function is nonlinear. For the inverse 1-median problem, we develop a linear time algorithm based on the optimal solution of the induced unconstrained problem. For the inverse 1-center problem, we prove that the problem can be decomposed into linearly many subproblems and the objective function in each subproblem is piecewise-convex. Furthermore, we also discuss an  time algorithm for the inverse 1-center problem under -norm based on the convexity of the cost function of each subproblem.",45,https://doi.org/10.1016/j.tcs.2020.08.010,1,
1006,Monitoring Rice Crop with Dense Segmentation on Satellite Images: A case study in Vietnamese Mekong Delta.,"Rice is the major crop in Vietnam with the crucial roles in national food security and export. While the demand of this crop is increasing overtime, the land use for this purpose is restricted due to various factors such as urban expansion and seawater intrusion. Therefore, a sustainable and effective rice paddy area monitoring is highly desirable, as the traditional methods are costly, inaccurate and requires heavy human-related workload. In this paper, we propose a novel rice monitoring framework using the satellite image, thanks to its high quality and free-to-access. Our framework first preprocesses the raw satellite image to alleviate the effect of adversarial factors such as geometric discrepancy and solar radiation. Then we design a deep learning model inspiring from u-net architecture to exploit simultaneously the spatial, spectral and temporal nature of satellite images. The empirical result conducted over data collected in Vietnamese Mekong Delta shows that our framework outperforms others state-of-the-art baselines and achieves the accuracy of 92.7% without the need of any handcraft feature engineering or expert knowledge.",45,https://doi.org/10.1109/KSE50997.2020.9287368,1,
1007,Natural Language Understanding in Smartdialog: A Platform for Vietnamese Intelligent Interactions.,"Nowadays in the modern world, interactive smart dialogs with text or voice are gaining traction as the main digital interaction channel between human and machine. However, most of the current platforms do not support or have not fully developed for Vietnamese. In this paper, the authors propose a smart conversational platform through a text channel and/or voice channel in Vietnamese language, including these main steps: (i) Input Conversion and Pre-Processing, (ii) Entity Recognition, (iii) Intent Classification, (iv) Action Prediction and Execution, and (v) Output Generation. This paper focuses on presenting problems related to natural language understanding. To recognize entities in a sentence, the authors studied and optimized the features for Vietnamese with the Conditional Random Field model. With the problem of predicting user intent, this work proposed, experimented, and compared of Random Forest and BiLSTM deep learning model to optimize for the Vietnamese language. A platform was built and deployed for Milo smart speaker application (LUMI smart home) and VADI driver virtual assistant with the accuracy of around 98.7%.",45,https://doi.org/10.1145/3342827.3342857,1,
1008,Severity Detection Tool for Patients with Infectious Disease.,"Hand, foot and mouth disease (HFMD) and tetanus are serious infectious diseases in low and middle income countries. Tetanus in particular has a high mortality rate and its treatment is resource-demanding. Furthermore, HFMD often affects a large number of infants and young children. As a result, its treatment consumes enormous healthcare resources, especially when outbreaks occur. Autonomic nervous system dysfunction (ANSD) is the main cause of death for both HFMD and tetanus patients. However, early detection of ANSD is a difficult and challenging problem. In this paper, we aim to provide a proof-of-principle to detect the ANSD level automatically by applying machine learning techniques to physiological patient data, such as electrocardiogram (ECG) and photoplethysmogram (PPG) waveforms, which can be collected using low-cost wearable sensors. Efficient features are extracted that encode variations in the waveforms in the time and frequency domains. A support vector machine is employed to classify the ANSD levels. The proposed approach is validated on multiple datasets of HFMD and tetanus patients in Vietnam. Results show that encouraging performance is achieved in classifying ANSD levels. Moreover, the proposed features are simple, more generalisable and outperformed the standard heart rate variability (HRV) analysis. The proposed approach would facilitate both the diagnosis and treatment of infectious diseases in low and middle income countries, and thereby improve overall patient care.",45,http://arxiv.org/abs/1912.05345,1,
1009,On the complexity of inverse convex ordered 1-median problem on the plane and on tree networks.,"An ordered median function is used in location theory to generalize a class of problems, including median and center problems. In this paper we consider the complexity of inverse ordered 1-median problems on the plane and on trees, where the multipliers are sorted nondecreasingly. Based on the convexity of the objective function, we prove that the problems with variable weights or variable coordinates on the line are NP-hard. Then we can directly get the NP-hardness result for the corresponding problem on the plane. We finally develop a cubic time algorithm that solves the inverse convex ordered 1-median problem on trees with relaxation on modification bounds.",45,https://doi.org/10.1007/s00186-018-0632-6,1,
1010,Cloud-enabled data sharing model.,"Database-as-a-Service is becoming more and more popular for many organizations. Storing data on the cloud can significantly reduce costs in terms of maintenance costs and initial investment costs. But due to data privacy and security concerns, many companies are still reluctant to use outsourced database service. In this paper, we present a solution to protect data privacy on the cloud, at the same time, allowing multiple users accessing the shared database with security assurance.",45,https://doi.org/10.1109/ISI.2012.6281922,1,
1011,Reducing the Network Load in CREPEnvironment.,"This paper presents a car navigation system for a future integrated transportation system forming a component of an Intelligent Transportation System- ITS. The system focuses on the mechanisms of how to detect the current position of each vehicle and to navigate each vehicle. Individual vehicular data is collected using the Global Positioning System - GPS, for location data and it will then be transmitted to a control center via a mobile phone. For the purpose of this paper, the device will be referred to as “Reporting Equipment for current geographic Position” or REP. It is suspected that if a great number of REP equipped vehicles report their positions to the control center simultaneously, there would be a heavy load on the computational communications network (“network”). This paper proposes an algorithm to reduce the network load associated with large numbers of vehicles reporting their position at the same time; if a car skips some reports then it is not possible for the control center to estimate its correct position therefore we need an algorithm that aims to decrease the frequency of the reporting without sacrificing the proper level of accuracy for the position. Compared to periodical reporting, the load effectiveness shows a 50-66% improvement.",45,https://doi.org/10.2197/ipsjjip.19.12,1,
1012,Improved word alignment in patent domain.,"This paper presents a new method for word alignment in patent domain which incorporates both generative and discriminative models. In this framework, the advantage of generative model that can learn large numbers of parameters from a sentence-aligned parallel corpus automatically in a unsupervised way can be kept, as well as get an improvement through discriminative models which can deploy various features in a supervised way. Even with only 300 word-aligned Chinese-English sentence pairs, incorporates with a 1M parallel Chinese-English patent sentences released by NTCIR9, experiments show that our method can get a promising performance.",45,https://doi.org/10.1109/NLPKE.2011.6138196,1,
1013,Mining Informative Rule Set for Prediction over a Sliding Window.,"We study the problem of mining informative (association) rule set for prediction over data streams. On dense datasets and low minimum support threshold, the generating of informative rule set does not use all mined frequent itemsets (FIs). Therefore, we will waste a portion of FIs if we run existing algorithms for finding FIs from data streams as the first stage to mine informative rule set. We propose an algorithm for mining informative rule set directly from data streams over a sliding window. Our experiments show that our algorithm not only attains high accurate results but also out performs the two-stage process, find FIs and then generate rules, of mining informative rule set.",45,https://doi.org/10.1007/978-3-642-12101-2_44,1,
1015,High-bit-rate packet transfer to fast mobile terminals in an IP based micro-cellular network with parallel polling scenario.,"This work presents an all IP based micro-cellular network configuration used in the metropolitan area. It aims to provide high-bit-rate packet transfer to fast-moving terminals. With the high transfer rate and guaranteed security, it can be considered as the extension of corporate LAN for business uses. For avoiding contention based access delay and packet re-transfer delay during handover, a new parallel polling scenario is proposed. Operating within the range of logical macro cell (LMC), detailed packet transfer procedures of polling are explained. Also automatic switching-over of LMC that locate the mobile terminal (MT) at the center cell of LMC all along can reduce the burden of strict time frame faced by the MT during high frequency handover between micro-cells.",45,https://doi.org/10.1109/WCNC.2004.1311334,1,
1016,SAFL: A Self-Attention Scene Text Recognizer with Focal Loss.,"In the last decades, scene text recognition has gained worldwide attention from both the academic community and actual users due to its importance in a wide range of applications. Despite achievements in optical character recognition, scene text recognition remains challenging due to inherent problems such as distortions or irregular layout. Most of the existing approaches mainly leverage recurrence or convolution-based neural networks. However, while recurrent neural networks (RNNs) usually suffer from slow training speed due to sequential computation and encounter problems as vanishing gradient or bottleneck, CNN endures a trade-off between complexity and performance. In this paper, we introduce SAFL, a self-attention-based neural network model with the focal loss for scene text recognition, to overcome the limitation of the existing approaches. The use of focal loss instead of negative log-likelihood helps the model focus more on low-frequency samples training. Moreover, to deal with the distortions and irregular texts, we exploit Spatial TransformerNetwork (STN) to rectify text before passing to the recognition network. We perform experiments to compare the performance of the proposed model with seven benchmarks. The numerical results show that our model achieves the best performance.",45,https://arxiv.org/abs/2201.00132,1,
1017,PM2.5 Prediction Using Genetic Algorithm-Based Feature Selection and Encoder-Decoder Model.,"The concentration of fine particulate matter (PM2.5), which represents inhalable particles with diameters of 2.5 micrometers and smaller, is a vital air quality index. Such particles can penetrate deep into the human lungs and severely affect human health. This paper studies accurate PM2.5 prediction, which can potentially contribute to reducing or avoiding the negative consequences. Our approach’s novelty is to utilize the genetic algorithm (GA) and an encoder-decoder (E-D) model for PM2.5 prediction. The GA benefits feature selection and remove outliers to enhance the prediction accuracy. The encoder-decoder model with long short-term memory (LSTM), which relaxes the restrictions between the input and output of the model, can be used to effectively predict the PM2.5 concentration. We evaluate the proposed model on air quality datasets from Hanoi and Taiwan. The evaluation results show that our model achieves excellent performance. By merely using the E-D model, we can obtain more accurate (up to 53.7%) predictions than those of previous works. Moreover, the GA in our model has the advantage of obtaining the optimal feature combination for predicting the PM2.5 concentration. By combining the GA-based feature selection algorithm and the E-D model, our proposed approach further improves the accuracy by at least 13.7%.",45,https://doi.org/10.1109/ACCESS.2021.3072280,1,
1018,QIH: An Efficient Q-Learning Inspired Hole-Bypassing Routing Protocol for WSNs.,"This paper addresses the local minimum phenomenon , routing path enlargement, and load imbalance problems of geographic routing in wireless sensor networks (WSNs) with holes. These issues may degrade the network lifetime of WSNs since they cause a long detour path and a traffic concentration around the hole boundary. Aiming to solve these problems, in this work, we propose a novel geographic routing protocol for WSNs, namely, Q-learning Inspired Hole bypassing (QIH), which is lightweight and efficient. QIH’s conceptual idea is to leverage Q-learning to estimate the distance from a node to the holes. QIH makes routing decisions following the nodes’ residual energy, their estimated distance to the holes, and their distance to the destination. We first confirm the effectiveness of QIH by theoretical analysis. Then, we conduct extensive simulations of QIH in comparison to state-of-the-art protocols. The simulation results show that QIH outperforms the other protocols in terms of network lifetime, packet latency, and energy consumption.",45,https://doi.org/10.1109/ACCESS.2021.3108156,1,
1019,An On-Demand Charging for Connected Target Coverage in WRSNs Using Fuzzy Logic and Q-Learning.,"In wireless rechargeable sensor networks (WRSNs), a mobile charger (MC) moves around to compensate for sensor nodes’ energy via a wireless medium. In such a context, designing a charging strategy that optimally prolongs the network lifetime is challenging. This work aims to solve the challenges by introducing a novel, on-demand charging algorithm for MC that attempts to maximize the network lifetime, where the term “network lifetime” is defined by the interval from when the network starts till the first target is not monitored by any sensor. The algorithm, named Fuzzy Q-charging, optimizes both the time and location in which the MC performs its charging tasks. Fuzzy Q-charging uses Fuzzy logic to determine the optimal charging-energy amounts for sensors. From that, we propose a method to find the optimal charging time at each charging location. Fuzzy Q-charging leverages Q-learning to determine the next charging location for maximizing the network lifetime. To this end, Q-charging prioritizes the sensor nodes following their roles and selects a suitable charging location where MC provides sufficient power for the prioritized sensors. We have extensively evaluated the effectiveness of Fuzzy Q-charging in comparison to the related works. The evaluation results show that Fuzzy Q-charging outperforms the others. First, Fuzzy Q-charging can guarantee an infinite lifetime in the WSRNs, which have a sufficient large sensor number or a commensurate target number. Second, in other cases, Fuzzy Q-charging can extend the time until the first target is not monitored by 6.8 times on average and 33.9 times in the best case, compared to existing algorithms.",45,https://doi.org/10.3390/s21165520,1,
1020,Realizing Mobile Air Quality Monitoring System: Architectural Concept and Device Prototype.,"Air pollution is a critical issue in cities in developing countries like Hanoi, Vietnam. An efficient and comprehensive air quality monitoring system may reduce the harmfulness and improve the cities' sustainability. This paper presents a novel approach to realize such a system in which the air monitoring sensors are mobile. More specifically, we introduce a three-tier architecture for the air quality system, including sensing, communication, and application layers. Initially, we discuss each layer concept to bypass the limitation of the traditional stationary monitoring system. We then describe our design and implementation of air quality monitoring devices installed on vehicles, such as buses. The device is carefully designed to satisfy the conditions of impedance matching and power integrity. Besides, it fully functions in measuring parameters from the ambient environment. The device is aware of its location (using GPS) and uses Wi-Fi and 4G (LTE) to transmit sensing data on the Internet. We have conducted various experiments, including a trial deployment of the devices on a vehicle running in Hanoi. The results show our device achieves sensing data transmission with high-reliability levels (i.e., 97%, 100% on Wi-Fi, 4G (LTE), respectively). Moreover, the trial deployment confirms the feasible operation of our device in actual condition.",45,https://doi.org/10.1109/APCC49754.2021.9609931,1,
1021,Multi-Agent Multi-Armed Bandit Learning for Offloading Delay Minimization in V2X Networks.,"In a three-tier Vehicle to X (V2X) network, a vehicle can offload the computational tasks to the edge computing component at a roadside unit (RSU) or a base station with cloud computing (gNB). Moreover, an RSU can also offload to gNB, forming three offloading paths: vehicle-to-RSU, vehicle-to-gNB, and RSU-to-gNB. This paper aims to minimize the offloaded tasks' average latency while dealing with the network dynamic. Note that the existing works assume the fixed network parameters, hence have failed to address the dynamic. As a solution, we use the multi-agent multi-armed bandits (MBA) learning for offloading that can adapt to the network dynamic and optimize the latency. More importantly, we propose a new MBA offloading scheme with an exploration mechanism based on the Sigmoid function. We conduct an extensive evaluation to evaluate and show the superiority of our proposal. First, the proposed Sigmoid exploration mechanism reduces the tasks' average latency by 35% compared to a basic MBA using negative rewarding. Second, the simulation results show our proposed offloading algorithm shortens the task latency by 18.5% on average and 56.9% in the best case, compared to the state-of-the-art.",45,https://doi.org/10.1109/EUC53437.2021.00016,1,
1022,Efficient Prediction of Discharge and Water Levels Using Ensemble Learning and Singular-Spectrum Analysis-Based Denoising.,"This work addresses forecasting two essential factors in river hydrodynamics, which are discharge (Q) and water (H) levels. The accurate forecast of the two has long been a challenge in hydrological researches and flood prediction. While the traditional statistical models fail to capture the peak discharge during flooding seasons (i.e., due to the excessive level values), the simulation’s numerical models face the difficulty of precise input parameters (e.g., measured values of surface zones, root zones, etc.). The emerging deep learning shows a lot of potential in solving the challenges of Q and H prediction. However, applying deep learning in such a context is not straightforward due to the following critical issues. First, the amount of training data is insufficient due to the data collection is non-trivial. Second, although lacking, the collected data incurs noises (e.g., measurement errors). We aim to overcome those shortcomings in a newly proposed deep learning model that accurately predicts Q and H. The model is a new ensemble of the one-dimensional convolutional neural network (1D-CNN), long short term memory (LSTM) models, to handle the insufficient data issue. Moreover, we adopt the Singular-Spectrum Analysis technique to eliminate noise from the collected data. The experimental results show that our proposed approach outperforms existing methods.",45,https://doi.org/10.1007/978-3-030-79463-7_4,1,
1023,Q-learning-based Opportunistic Communication for Real-time Mobile Air Quality Monitoring Systems.,"We focus on real-time air quality monitoring systems that rely on devices installed on automobiles in this research. We investigate an opportunistic communication model in which devices can send the measured data directly to the air quality server through a 4G communication channel or via Wi-Fi to adjacent devices or the so-called Road Side Units deployed along the road. We aim to reduce 4G costs while assuring data latency, where the data latency is defined as the amount of time it takes for data to reach the server. We propose an offloading scheme that leverages Q-learning to accomplish the purpose. The experiment results show that our offloading method significantly cuts down around 40-50% of the 4G communication cost while keeping the latency of 99.5% packets smaller than the required threshold.",45,https://doi.org/10.1109/IPCCC51483.2021.9679398,1,
1024,MC-OCR Challenge 2021: A Multi-modal Approach for Mobile-Captured Vietnamese Receipts Recognition.,"Mobile captured receipts OCR (MC-OCR) recognizes text from structured and semi-structured receipts and invoices captured by mobile devices. This process plays a critical role in streamlining document-intensive processes and office automation in many financial, accounting, and taxation areas. Although many efforts have been devoted, MC-OCR still faces significant challenges due to mobile captured images’ complexity. First, receipts might be crumpled, or the content might be blurred. Second, different from scanned images, the quality of photos taken by mobile devices shows high diversity due to the light condition and the dynamic environment (e.g., indoor, out-door, complex background, etc.) where the receipts were captured. These difficulties lead to a low accuracy of the recognition results. In this challenge, we target two tasks to address these issues, including (1) evaluating the quality of the captured receipts, and (2) recognizing required fields of the receipts. Our idea is to leverage a multi-modal approach which can take advantage of both areas: computer vision and natural language processing, two of the main interests of the RIVF community. The paper presents the BK-OCR team’s methodology and results in the Mobile-Captured Image Document Recognition for Vietnamese Receipts 2021.",45,https://doi.org/10.1109/RIVF51545.2021.9642088,1,
1025,Monitoring agriculture areas with satellite images and deep learning.,"Agriculture applications rely on accurate land monitoring, especially paddy areas, for timely food security control and support actions. However, traditional monitoring requires field works or surveys performed by experts, which is costly, slow, and sparse. Agriculture monitoring systems are looking for sustainable land use monitoring solutions, starting with remote sensing on satellite data for cheap and timely paddy mapping. The aim of this study is to develop an autonomous and intelligent system built on top of imagery data streams, which is available from low-Earth orbiting satellites, to differentiate crop areas from non-crop areas. However, such agriculture mapping framework poses unique challenges for satellite image processing, including the seasonal nature of crop, the complexity of spectral channels, and adversarial conditions such as cloud and solar radiance. In this paper, we propose a novel multi-temporal high-spatial resolution classification method with an advanced spatio-temporal–spectral deep neural network to locate paddy fields at the pixel level for a whole year long and for each temporal instance. Our method is built and tested on the case study of Landsat 8 data due to its high spatial resolution. Empirical evaluations on real imagery datasets of different landscapes from 2016 to 2018 show the superior of our mapping model against the baselines with over 0.93 F1-score, the importance of each model design, the robustness against seasonal effects, and the visual mapping results.",45,https://doi.org/10.1016/j.asoc.2020.106565,1,
1026,Automated Test Input Generation via Model Inference Based on User Story and Acceptance Criteria for Mobile Application Development.,"There has been observed explosive growth in the development of mobile applications (apps) for Android and iOS operating systems, which has led to the direct impact towards mobile app development. In order to design and propose quality-oriented apps, it is the primary responsibility of developers to devote time and sufficient efforts towards testing to make the apps bug-free and operational in the hands of end-users without any hiccup. Manual testing procedures take a prolonged amount of time in writing test cases, and in some cases, the full testing requirements are not met. Besides, the insufficient knowledge of tester also impacts the overall quality and bug-free apps. To overcome the obstacles of testing, we propose a new testing methodology cum tool called “AgileUATM” which works primarily towards white-box and black-box testing. To evaluate the validity of the proposed tool, we put the tool in a real-time operational environment concerning mobile test apps. By using this tool, all the acceptance criteria are determined via user stories. The testers/developers specify requirements with formal specifications based on programs properties, predicates, invariants, and constraints. The results show that the proposed tool generated effective and accurate test cases, test input. Meanwhile, expected output was also generated in a unified fashion from the user stories to meet acceptance criteria. The proposed solution also reduced the development time to identify test data as compared to manual Behavior-Driven Development (BDD) methodologies. This tool can support the developers to get a better idea about the required tests and able to translate the customer’s natural languages to computer languages as well. This paper fulfills an approach to suitably test mobile application development",45,https://doi.org/10.1142/s0218194020500163,1,
1027,"A Path-Length Efficient, Low-Overhead, Load-Balanced Routing Protocol for Maximum Network Lifetime in Wireless Sensor Networks with Holes.","In wireless sensor networks (WSNs) with holes, designing efficient routing protocols, which prolong the network lifetime, is one of the most critical issues. To this end, this paper proposes a new geographic routing protocol for WSNs named the load Balanced and constant Stretch protocol for bypassing Multiple Holes (i.e., BSMH). In contrast to the existing works in the literature, the design of BSMH simultaneously takes into account the three factors that impacted the network lifetime, including routing path length, control packet overhead, and load balance among the nodes. Moreover, BSMH aims at minimizing the routing path length and the control overhead, while maximizing the load balance. We theoretically prove the efficiency of BSMH and extensively evaluate BSMH against the state-of-the-art protocols. The evaluation results show that the proposed protocol outperforms the others in various investigated metrics, not only network lifetime, but also routing path stretch, load balance, and control overhead. Specifically, BSMH prolongs the network lifetime by 30% compared to the existing protocols while guaranteeing that the routing path stretch is under",45,https://doi.org/10.3390/s20092506,1,
1028,"Q-learning-based, Optimized On-demand Charging Algorithm in WRSN.","This paper introduces a novel charging strategy for wireless rechargeable sensor networks (WRSNs), in which a mobile charger (MC) moves and wirelessly transfers the power to the sensor nodes. The first distinct point of this work is designing the MC's charging algorithm under the consideration of target coverage and connectivity. As a solution, we introduce a novel on-demand charging scheme for WRSNs that optimize the charging time at each MC's charging location. Moreover, we take advantage of the Q-learning technique (i.e., hence named our algorithms Q-charging) to maximize the number of monitored targets. Q-charging can prioritize the sensor nodes, which play a more critical role in the network. Hence, Q-charging can select a suitable charging location aiming to provide sufficient power for the prioritized sensors. We have evaluated our proposal in comparison to the previous works. The evaluation results show that Q-charging can prolong the time until the first target is not monitored by 5.2 times on the average, and 14.3 times in the best case, compared to existing algorithms.",45,https://doi.org/10.1109/NCA51143.2020.9306695,1,
1029,A Method for Improvement the Parameter Estimation of Non-linear Regression in Growth Model to Predict Project Cost at Completion.,"In this paper, we propose a comparison between existing parameter estimation methods of a nonlinear regression-based growth model to forecast project duration as well as its cost at completion. To analyze and perform some experiments, we used the Gompertz growth model and the dataset comes from a number of similar previous studies. Two other nonlinear models were also applied to compare the results with the Gompertz model in terms of fitness-function score. The performance of the proposed methods is also the key for further studies in fitting the S- curve and predicting the Estimate-to-Complete and the Estimate-at-Completion of certain projects.",45,https://doi.org/10.1109/RIVF48685.2020.9140765,1,
1030,Extending Network Lifetime by Exploiting Wireless Charging in WSN.,"In recent years, wireless energy transfer technology has emerged as a promising solution to supply energy for sensors and extend the network lifetime. In the literature, although significant efforts have been devoted to optimizing the charging schedule, it still remains a challenging problem. In this paper, we study how to prolong the network lifetime by optimizing the charging time. Specifically, we aim at proposing a charging schedule that maximizes the minimum residual energy of sensors. We first mathematically formulate the problem under mixed integer and linear programming. Then, we propose a lightweight heuristic algorithm. The experiment results show that our proposed algorithms extend network lifetime more than 10% compared to the state-of-the-art.",45,https://doi.org/10.1109/RIVF48685.2020.9140727,1,
1031,A Novel Surface Descriptor for Automated 3-D Object Recognition and Localization.,"This paper presents a novel approach to the automated recognition and localization of 3-D objects. The proposed approach uses 3-D object segmentation to segment randomly stacked objects in an unstructured point cloud. Each segmented object is then represented by a regional area-based descriptor, which measures the distribution of surface area in the oriented bounding box (OBB) of the segmented object. By comparing the estimated descriptor with the template descriptors stored in the database, the object can be recognized. With this approach, the detected object can be matched with the model using the iterative closest point (ICP) algorithm to detect its 3-D location and orientation. Experiments were performed to verify the feasibility and effectiveness of the approach. With the measured point clouds having a spatial resolution of 1.05 mm, the proposed method can achieve both a mean deviation and standard deviation below half of the spatial resolution",45,https://doi.org/10.3390/s19040764,1,
1032,Network Lifetime Maximization for Full Area Coverage in Wireless Sensor Networks.,"Sensor scheduling for maximizing the network lifetime and achieving the full area coverage is a paramount problem in wireless sensor networks. Although considerable effort has been devoted, this problem is still a challenge to the research community. The approximation algorithms proposed so far couldn't guarantee the performance ratio. In this paper, we first formulate the problem under linear programming model which can help to determine the exact optimal solution. Then, in order to reduce the time complexity, we propose a (1 +∊)-approximation algorithm based on divide-and-conquer technique. The main idea is to divide the network into sub-regions, then determine the suboptimal solution for every sub-region and combine them to obtain the total solution of the whole network. Moreover, with the aim of speeding up the suboptimal solution finding process, we propose an approximation algorithm using the column generation approach. The experiment results show the superiority of our proposed algorithms over the existing ones.",45,https://doi.org/10.1109/APCC47188.2019.9026510,1,
1033,Exploiting Q-Learning in Extending the Network Lifetime of Wireless Sensor Networks with Holes.,"Geographic routing is one of the most popular routing protocols in wireless sensor networks (WSNs) due to its simplicity and efficiency. However, with the occurrence of holes, geographic routing incurs with the so-called local minimum problem that may lead to a long hole detour path as well as the traffic concentration around the hole boundary. In consequence, the network lifetime is shortened. In this paper, we aim at proposing a lightweight distributed geographic routing protocol, which can prolong the lifetime of WSNs under the hole occurrence. Our main idea is to exploiting Q-learning technique to estimate the distance from a node to the holes. The routing decision is then determined based on the residual energy of the nodes, their estimated distance to the holes, and their distance to the destination. The simulation experiments show that our protocol strongly outperforms state-of-the-art protocols in terms of the network lifetime, packet latency and energy consumption. Specifically, our proposed protocol extends the network lifetime by more than 12% compared to the existing protocols.",45,https://doi.org/10.1109/ICPADS47876.2019.00091,1,
1034,A Dynamic Routing Protocol for Maximizing Network Lifetime in WSNs with Holes.,"Extending network lifetime is one of the most critical issues in handling wireless sensor networks. The network lifetime is contributed by three factors: routing path length, control packet overhead and load balance among the nodes. In the literature, many routing protocols have been proposed. However, none of them jointly considers all of these factors, thus they can not solve the network lifetime maximization problem thoroughly. In this paper, we aim at designing a routing protocol whose objective is to minimize the routing path length and the control overhead, while maximizing the load balance. The experiment results show that our proposed protocol strongly outperforms state-of-the-art protocols concerning many metrics including network lifetime, routing path stretch, load balance and control overhead.",45,https://doi.org/10.1145/3368926.3369725,1,
1035,Load balanced and constant stretch routing in the vicinity of holes in WSNs.,"Because of its simplicity and scalability, geographic routing is a popular approach in wireless sensor networks, which can achieve a near-optimal routing path in the networks without holes (i.e., regions without working sensors). With the occurrence of holes, however, geographic routing faces the problems of load imbalance and routing path enlargement. In the literature, several proposals have attempted to fix these issues, but the majority of them considers only the cases when both the source and the destination stay fairly far from the holes. Recently, a few work has been proposed to tackle the problem of routing in the vicinity of routing holes. However, none of them addresses the two problems (i.e., load imbalance and routing path enlargement) concurrently, and none of them can solve the problem of load imbalance thoroughly. In this paper, we introduce a novel approach in dealing with routing in the vicinity of holes, that is the first to target and solve both the load imbalance and path enlargement problems. The theoretical analysis proves that the routing path stretch of our proposed protocol can be controlled to be as small as 1 + ε (for any predefined ε> 0) and the simulation experiments show that our protocol strongly outperforms the existing protocols in terms of load balancing.",45,https://doi.org/10.1109/CCNC.2018.8319190,1,
1036,A Delay-Guaranteed Geographic Routing Protocol with Hole Avoidance in WSNs.,"Wireless sensor networks (WSNs) are used in many mission-critical applications, such as target tracking on a battlefield, emergency alarms, and disaster detection. In such applications, QoS provisioning in the timeliness domain is indispensable. Moreover, because of the diversity of sensory data, QoS provisioning should support not only one but multiple levels of end-to-end delay constraints. As a result of several characteristics such as the limitations on the energy supply, available storage and computational capacity of the sensor nodes, guaranteeing timely delivery in WSNs is a challenging problem. To overcome these limitations, several lightweight and stateless QoS-based geographic routing protocols have been proposed. The existing protocols work well in networks without routing holes (i.e., regions with no working sensors). However, with the occurrence of routing holes, they suffer from the so-called local minimum phenomenon and traffic congestion around the hole boundary. In this paper, we consider the presence of routing holes and propose a delay-guaranteed geographic routing protocol called DEHA that can support multiple end-to-end delay levels. The main idea is to achieve early awareness of the presence of a routing hole and then to utilize this awareness in determining a routing path that can avoid the hole. Simulation results show that our protocol outperforms the existing protocols in terms of several performance metrics, including packet delivery ratio, energy efficiency, and load balancing.",45,https://doi.org/10.1109/MASS.2017.16,1,
1037,Constant stretch and load balanced routing protocol for bypassing multiple holes in wireless sensor networks.,"The occurrence of multiple holes in wireless sensor networks poses many challenges in designing routing protocols. The traditional scheme is forwarding packets along the hole perimeters. However, this scheme leads to two serious problems: data concentration around the hole boundaries and routing path enlargement Recently, several approaches have been proposed to address these two problems, wherein a common idea is to form forbidden areas around the holes from which packets are kept to stay away. However, due to the static nature of the forbidden areas and routing paths, the existing protocols cannot solve these two problems thoroughly. In this paper, we propose a novel protocol for bypassing multiple holes in wireless sensor networks which can balance the traffic over the network while ensuring the constant stretch property of the routing path. Our main idea is to use elastic forbidden areas and dynamic routing paths. The theoretical analysis proves that the routing path stretch of the proposed protocol can be controlled to be as small as 1 + ϵ (for any predefined ϵ > 0), and the simulation experiments show that our protocol strongly outperforms state-of-the-art protocols in terms of load balancing.",45,https://doi.org/10.1109/NCA.2017.8171330,1,
1038,Safe Incremental Design of UML Architectures.,"IDF is an Incremental Development Framework which
supports the development and the verification of UML models for reactive systems. IDF offers refinement and extension techniques allowing liveness properties to be preserved
during the model developments. Here, we improve the
framework in order to analyze models from a safety point
of view. For this purpose, we associate IDF with the experienced tools of safety analysis based on the BIP language
by translating UML models into BIP. We demonstrate on a
basic example the complementarity of liveness and safety
analyses.",45,https://doi.org/10.18293/SEKE2017-033,1,
1039,Component-based verification using incremental design and invariants.,"We propose invariant-based techniques for the efficient verification of safety and deadlock-freedom properties of component-based systems. Components and their interactions are described in the BIP language. Global invariants of composite components are obtained by combining local invariants of their constituent components with interaction invariants that take interactions into account. We study new techniques for computing interaction invariants. Some of these techniques are incremental, i.e., interaction invariants of a composite hierarchically structured component are computed by reusing invariants of its constituents. We formalize incremental construction of components in the BIP language as the process of building progressively complex components by adding interactions (synchronization constraints) to atomic components. We provide sufficient conditions ensuring preservation of invariants when new interactions are added. When these conditions are not satisfied, we propose methods for generating new invariants in an incremental manner by reusing existing invariants from the constituents in the incremental construction. The reuse of existing invariants reduces considerably the overall verification effort. The techniques have been implemented in the D-Finder toolset. Among the experiments conducted, we have been capable of verifying safety properties and deadlock-freedom of sub-systems of the functional level of the DALA autonomous robot. This work goes far beyond the capacity of existing monolithic verification tools.",45,https://doi.org/10.1007/s10270-014-0410-8,1,
1040,Improve the Performance of Mobile Applications Based on Code Optimization Techniques Using PMD and Android Lint.,"Analyzing, testing, and optimizing source code are techniques that improve software quality and the performance of features and energy consumption of systems. Source code analysis includes analyzing the source code of an application and checking aspects to detect potential problems based on previous experience. In this paper, we investigate the rules and techniques of analyzing and optimizing Java source code by using PMD and Android lint. An automatic code-analyzing and code refactoring tool is developed with a set of rules based on the Eclipse Refactoring API (plug-in) to get optimized code that consumes less energy and improves performance for Android applications. The optimized code was tested in real environments with positive results. It reveals that programmer could use these techniques and support tool for developing Android applications with high quality source code and reliable and performance.",45,https://doi.org/10.1007/978-3-319-49046-5_29,1,
1041,Runtime verification of component-based systems in the BIP framework with formally-proved sound and complete instrumentation.,"Verification of component-based systems still suffers from limitations such as state space explosion since a large number of different components may interact in a heterogeneous environment. These limitations entail the need for complementary verification methods such as runtime verification. Runtime verification is a dynamic analysis technique and is prone to scalability. In this paper, we integrate runtime verification into the BIP (Behavior, Interaction and Priority) framework. BIP is a powerful and expressive component-based framework for the formal construction of heterogeneous systems. Our method augments BIP systems with monitors to check specifications at runtime. This method has been implemented in RV-BIP, a prototype tool that we used to validate the whole approach on a robotic application.",45,https://doi.org/10.1007/s10270-013-0323-y,1,
1043,Rigorous Component-Based System Design Using the BIP Framework.,"Rigorous system design requires the use of a single powerful component framework allowing the representation of the designed system at different detail levels, from application software to its implementation. A single framework allows the maintenance of the overall coherency and correctness by comparing different architectural solutions and their properties. The authors present the BIP (behavior, interaction, priority) component framework, which encompasses an expressive notion of composition for heterogeneous components by combining interactions and priorities. This allows description at different abstraction levels from application software to mixed hardware/software systems. A rigorous design flow that uses BIP as a unifying semantic model derives a correct implementation from an application software, a model of the target architecture, and a mapping. Implementation correctness is ensured by applying source-to-source transformations that preserve correctness of essential design properties. The design is fully automated and supported by a toolset including a compiler, the D-Finder verification tool, and model transformers. The authors present an autonomous robot case study to illustrate BIP's use as a modeling formalism as well as crucial aspects of the design flow for ensuring correctness.",45,https://doi.org/10.1109/MS.2011.27,1,
1044,Efficient deadlock detection for concurrent systems.,"Concurrent systems are prone to deadlocks that arise from competing access to shared resources and synchronization between the components. At the same time, concurrency leads to a dramatic increase of the possible state space due to interleavings of computations, which makes standard verification techniques often infeasible. Previous work has shown that approximating the state space of component based systems by computing invariants allows to verify much larger systems then standard methods that compute the exact state space. The approach comes with the drawback, though, that not all of the reported specification violations may be reachable in the system. This paper deals with that problem by combining the information from the invariant with model checking techniques and strategies for reducing the memory footprint. The approach is implemented as post processing step for generating the exact set of reachable specification violations along with traces to demonstrate the error.",45,https://doi.org/10.1109/MEMCOD.2011.5970518,1,
1045,D-Finder 2: Towards Efficient Correctness of Incremental Design.,"D-Finder 2 is a new tool for deadlock detection in concurrent systems based on effective invariant computation to approximate the effects of interactions among modules. It is part of the BIP framework, which provides various tools centered on a component-based language for incremental design. The presented tool shares its theoretical roots with a previous implementation, but was completely rewritten to take advantage of a new version of BIP and various new results on the theory of invariant computation. The improvements are demonstrated by comparison with previous work and reports on new results on a practical case study.",45,https://doi.org/10.1007/978-3-642-20398-5_32,1,
1046,Runtime Verification of Component-Based Systems.,"Verification of component-based systems still suffers from limitations such as state space explosion since a large number of different components may interact in an heterogeneous environment. Those limitations entail the need for complementary verification methods such as runtime verification based on dynamic analysis and prone to scalability. In this paper, we integrate runtime verification into the BIP (Behavior, Interaction, and Priority) framework. BIP is a powerful component-based framework for the construction of heterogeneous systems. Our method augments BIP systems with monitors checking a user-provided specification. This method has been implemented in RV-BIP, a prototype tool that we used to validate the whole approach on a robotic application.",45,https://doi.org/10.1007/978-3-642-24690-6_15,1,
1047,On the Simulation of Time-Triggered Systems on a Chip with BIP.,"In this report, we present functional models for software and hardware components of Time-Triggered Systems on a Chip (TTSoC). These are modeled in the asynchronous component based language BIP. We demonstrate the usability of our components for simulation of software which is developed for the TTSoC. Our software comprises services and an application part. Our approach allows us to simulate and validate aspects of the software system at an early stage in the development process and without the need to have the TTSoC hardware at hand.",45,http://arxiv.org/abs/1109.5505,1,
1048,Compositional verification for component-based systems and application.,The authors present a compositional method for the verification of component-based systems described in a subset of the behaviour-interaction-priority language encompassing multi-party interaction without data transfer. The method is based on the use of two kinds of invariants. Component invariants are over-approximations of components' reachability sets. Interaction invariants are global constraints on the states of components involved in interactions. The method has been implemented in the D-Finder tool and has been applied for checking deadlock-freedom. The experimental results on non-trivial examples show that this method allows either to prove deadlock-freedom or to identify very few deadlock configurations that can be analysed by using state-space exploration.,45,https://doi.org/10.1049/iet-sen.2009.0011,1,
1049,Invariants and Robustness of BIP Models.,"In this paper we present on-going work addressing the problem of automatically generating realistic and guaranteed correct invariants. Since invariant generation mechanisms are error-prone, after the computation of invariants by a verification tool, we formally prove that the generated invariants are indeed invariants of the considered systems using a higher-order theorem prover and automated techniques. We regard invariants for BIP models. BIP (behavior, interaction, priority) is a language for specifying asynchronous component based systems. Proving that an invariant holds often requires an induction on possible system execution traces. For this reason, apart from generating invariants that precisely capture a system’s behavior, inductiveness of invariants is an important goal. We establish a notion of robust BIP models. These can be automatically constructed from our original non-robust BIP models and over-approximate their behavior. We motivate that invariants of robust BIP models capture the behavior of systems in a more natural way than invariants of corresponding non-robust BIP models. Robust BIP models take imprecision due to values delivered by sensors into account. Invariants of robust BIP models tend to be inductive and are also invariants of the original non-robust BIP model. Therefore they may be used by our verification tools and it is easy to show their correctness in a higher-order theorem prover. The presented work is developed to verify the results of a deadlock-checking tool for embedded systems after their computations. Therewith, we gain confidence in the provided analysis results.",45,https://doi.org/10.29007/prxp,1,
1050,Incremental Invariant Generation for Compositional Design.,"We consider a compositional method for the verification of component-based systems described in a subset of the BIP language encompassing multi-party interactions. The method is based on the use of two kinds of invariants. Component invariants are over-approximations of components' reach ability sets. Interaction invariants are constraints on the states of components involved in interactions. In this paper we propose fixed point characterization for computing interaction invariants. We also propose a new technique that takes the incremental design of the system into account. In many situations, the technique will help to avoid redoing all the verification process each time an interaction is added in the design. Our two techniques have been implemented as extension of the D-Finder toolset. The result has been applied to check deadlock-freedom on several case studies. Our experiments show that our new methodology is generally much faster than existing ones.",45,https://doi.org/10.1109/TASE.2010.23,1,
1051,Designing autonomous robots.,"Autonomous robots are complex systems that require the interaction or cooperation of numerous heterogeneous software components. Nowadays, robots are getting closer to humans and as such are becoming critical systems that must meet safety properties including logical, temporal, and real-time constraints.",45,https://doi.org/10.1109/MRA.2008.931631,1,
1052,D-Finder: A Tool for Compositional Deadlock Detection and Verification.,"D-Finder tool implements a compositional method for the verification of component-based systems described in BIP language encompassing multi-party interaction. For deadlock detection, D-Finder applies proof strategies to eliminate potential deadlocks by computing increasingly stronger invariants.",45,https://doi.org/10.1007/978-3-642-02658-4_45,1,
1053,Incremental Component-Based Construction and Verification of a Robotic System.,"Autonomous robots are complex systems that require the interaction/cooperation of numerous heterogeneous software components. Nowadays, robots are critical systems and must meet safety properties including in particular temporal and real-time constraints. We present a methodology for modeling and analyzing a robotic system using the BIP component framework integrated with an existing framework and architecture, the LAAS Architecture for Autonomous System, based on GenoM. The BIP componentization approach has been successfully used in other domains. In this study, we show how it can be seamlessly integrated in the preexisting methodology. We present the componentization of the functional level of a robot, the synthesis of an execution controller as well as validation techniques for checking essential “safety” properties.",45,https://doi.org/10.3233/978-1-58603-891-5-631,1,
1054,Genetic Algorithm for Solving the Master Thesis Timetabling Problem with Multiple Objectives.,"Master thesis defense scheduling problem is a real-world practical problem that arises from the Vietnamese Universities. In this paper, we give the formulation of the problem based on realistic requirements. We then show that the considered problem is NP-hard and propose a genetic algorithm for solving it. We experiment the proposed algorithm on the real problem instances taken from Hanoi University of Science and Technology. Experimental results show the feasibility of proposed algorithm.",46,https://doi.org/10.1109/TAAI.2012.50,1,
1055,Evaluating Feasibility and Effectiveness of Digital Game-Based Instructional Technology.,"Higher education institutions encounters difficulties when the Covid-19 pandemic happens because learners cannot go to class and absorb knowledge directly. In Vietnam, universities have used online learning to help learners keep learning in this pandemic context. However, the quality and effectiveness of the learning process are questioned by the learner and the teaching staff. In this context, our research is looking for solutions to apply new technology in the teaching process to ensure the quality and effectiveness of learning. Among these new technologies, digital games are considered due to most students playing digital games as well as their positive effects in teaching reflected in recent publications. Under the approach of technological process, our team evaluate the feasibility and effectiveness of the digital game-based instructional technology at HUST. In terms of feasibility, research results show that most students agree that learning skills can be formed by using digital games in learning process. About the efficiency, there are three key findings: First, although male students play video games more than female students, the results show that both men and women have the same rate to-wards the effectiveness of digital games in learning. Second, there is no difference in assessing the effectiveness of digital games in learning between IT students and non-IT students although IT students are more exposed to technology. Third, there is a difference in the views of second-year students and third-year students. This difference shows the reason to build blended-learning courses that use digital games effectively for third-year students who starts more indeed engineering major at universities.",47,https://online-journals.org/index.php/i-jet/article/view/23829,1,
1056,A bi-level optimized charging algorithm for energy depletion avoidance in wireless rechargeable sensor networks.,"In Wireless Rechargeable Sensor Networks (WRSNs), charging scheme optimization is one of the most critical issues, which plays an essential role in deciding the sensors’ lifetime. An effective charging scheme should simultaneously consider both the charging path and the charging time. Existing works, however, mainly focus on determining the optimal charging path and adopt the full charging strategy. The full charging approach may increase the sensors’ charging delay and eventually lead to sensor energy depletion. This paper studies how to optimize the charging path and the charging time at the same time to avoid energy depletion in WRSNs. We first formulate the investigated problem with a Mixed-Integer Linear Programming model. We then leverage the bi-level optimization approach and represent the targeted problem with two levels: the charging path optimization at the upper level and the charging time optimization at the lower level. A combination of Genetic Algorithm and Greedy method is proposed to determine the optimal charging path. Besides, to reduce the computational complexity of charging time identification level, we propose a Particle Swarm Optimization (PSO) algorithm to optimize the charging time of the best charging path in each evolutionary generation. The experimental validation on various network scenarios demonstrates our proposed charging scheme’s superiority over the existing algorithms.",49,https://doi.org/10.1007/s10489-021-02775-8,1,
1057,Constant approximation for opportunistic sensing in mobile air quality monitoring system.,"Monitoring air quality plays a critical role in the sustainable development of developing regions where the air is severely polluted. Air quality monitoring systems based on static monitors often do not provide information about the area each monitor represents or represent only small areas. In addition, they have high deployment costs that reflect the efforts needed to ensure sufficient quality of measurements. Meanwhile, the mobile air quality monitoring system, such as the one in this work, shows the feasibility of solving those challenges. The system includes environmental sensors mounted on buses that move along their routes, broadening the monitoring areas. In such a system, we introduce a new optimization problem named opportunistic sensing that aims to find (1) optimal buses to place the sensors and (2) the optimal monitoring timing to maximize the number of monitored critical regions. We investigate the optimization problem in two scenarios: simplified and general bus routes. Initially, we mathematically formulate the targeted problem and prove its NP-hardness. Then, we propose a polynomial-time -, -approximation algorithm for the problem with the simplified, general routes, respectively. To show the proposed algorithms’ effectiveness, we have evaluated it on the real data of real bus routes in Hanoi, Vietnam. The evaluation results show that the former algorithm guarantees an average performance ratio of 72.68%, while the latter algorithm achieves the ratio of 63.87%. Notably, when the sensors can be on (e.g., enough energy) during the whole route, the -approximation algorithm achieves the approximation ratio of . Such ratio, which is almost twice as , enlarges the average performance ratio to 78.42%.",49,https://doi.org/10.1016/j.comnet.2021.108646,1,
1058,A Host-based Investigation of IPv6 in Academia: The Cases of Japan and Vietnam.,"This paper introduces a host-based method for investigating IPv6 adoption and performance in academia that targets universities in Japan and Vietnam. Unlike other works, the investigation has been utilized on a host native IPv6 host with a standard tool (i.e., curl). We first probe the IPv6 capabilities of the universities’ websites. Second, within the IPv6-supported websites, we compare the performances of IPv4 and IPv6 when letting the two IP versions access the websites concurrently. We find that, despite the popularity of IPv6 in the two countries, a significant number of academic websites are not yet IPv6 capable. The native IPv6 client can only access fifty of the more than one thousand websites in Japan. Furthermore, there are no IPv6-supported websites at Vietnamese universities. Among the accessible IPv6 websites in Japan, we observe that the web access performances of IPv4 and IPv6 are similar.",49,https://doi.org/10.1109/CCNC49033.2022.9700604,1,
1059,Usability and Aesthetics: Better Together for Automated Repair of Web Pages.,"With the recent explosive growth of mobile devices such as smartphones or tablets, guaranteeing consistent web appearance across all environments has become a significant problem. This happens simply because it is hard to keep track of the web appearance on different sizes and types of devices that render the web pages. Therefore, fixing the inconsistent appearance of web pages can be difficult, and the cost incurred can be huge, e.g., poor user experience and financial loss due to it. Recently, automated web repair techniques have been proposed to automatically resolve inconsistent web page appearance, focusing on improving usability. However, generated patches tend to disrupt the webpage's layout, rendering the repaired webpage aesthetically unpleasing, e.g., distorted images or misalignment of components.
In this paper, we propose an automated repair approach for web pages based on meta-heuristic algorithms that can assure both usability and aesthetics. The key novelty that empowers our approach is a novel fitness function that allows us to optimistically evolve buggy web pages to find the best solution that optimizes both usability and aesthetics at the same time. Empirical evaluations show that our approach is able to successfully resolve mobile-friendly problems in 94% of the evaluation subjects, significantly outperforming state-of-the-art baseline techniques in terms of both usability and aesthetics.",49,https://arxiv.org/abs/2201.00117,1,
1060,SAFL: A Self-Attention Scene Text Recognizer with Focal Loss.,"In the last decades, scene text recognition has gained worldwide attention from both the academic community and actual users due to its importance in a wide range of applications. Despite achievements in optical character recognition, scene text recognition remains challenging due to inherent problems such as distortions or irregular layout. Most of the existing approaches mainly leverage recurrence or convolution-based neural networks. However, while recurrent neural networks (RNNs) usually suffer from slow training speed due to sequential computation and encounter problems as vanishing gradient or bottleneck, CNN endures a trade-off between complexity and performance. In this paper, we introduce SAFL, a self-attention-based neural network model with the focal loss for scene text recognition, to overcome the limitation of the existing approaches. The use of focal loss instead of negative log-likelihood helps the model focus more on low-frequency samples training. Moreover, to deal with the distortions and irregular texts, we exploit Spatial TransformerNetwork (STN) to rectify text before passing to the recognition network. We perform experiments to compare the performance of the proposed model with seven benchmarks. The numerical results show that our model achieves the best performance.",49,https://arxiv.org/abs/2201.00132,1,
1061,QIH: An Efficient Q-Learning Inspired Hole-Bypassing Routing Protocol for WSNs.,"This paper addresses the local minimum phenomenon , routing path enlargement, and load imbalance problems of geographic routing in wireless sensor networks (WSNs) with holes. These issues may degrade the network lifetime of WSNs since they cause a long detour path and a traffic concentration around the hole boundary. Aiming to solve these problems, in this work, we propose a novel geographic routing protocol for WSNs, namely, Q-learning Inspired Hole bypassing (QIH), which is lightweight and efficient. QIH’s conceptual idea is to leverage Q-learning to estimate the distance from a node to the holes. QIH makes routing decisions following the nodes’ residual energy, their estimated distance to the holes, and their distance to the destination. We first confirm the effectiveness of QIH by theoretical analysis. Then, we conduct extensive simulations of QIH in comparison to state-of-the-art protocols. The simulation results show that QIH outperforms the other protocols in terms of network lifetime, packet latency, and energy consumption.",49,https://doi.org/10.1109/ACCESS.2021.3108156,1,
1062,Realizing Mobile Air Quality Monitoring System: Architectural Concept and Device Prototype.,"Air pollution is a critical issue in cities in developing countries like Hanoi, Vietnam. An efficient and comprehensive air quality monitoring system may reduce the harmfulness and improve the cities' sustainability. This paper presents a novel approach to realize such a system in which the air monitoring sensors are mobile. More specifically, we introduce a three-tier architecture for the air quality system, including sensing, communication, and application layers. Initially, we discuss each layer concept to bypass the limitation of the traditional stationary monitoring system. We then describe our design and implementation of air quality monitoring devices installed on vehicles, such as buses. The device is carefully designed to satisfy the conditions of impedance matching and power integrity. Besides, it fully functions in measuring parameters from the ambient environment. The device is aware of its location (using GPS) and uses Wi-Fi and 4G (LTE) to transmit sensing data on the Internet. We have conducted various experiments, including a trial deployment of the devices on a vehicle running in Hanoi. The results show our device achieves sensing data transmission with high-reliability levels (i.e., 97%, 100% on Wi-Fi, 4G (LTE), respectively). Moreover, the trial deployment confirms the feasible operation of our device in actual condition.",49,https://doi.org/10.1109/APCC49754.2021.9609931,1,
1063,Speaker count: A new building block for speaker diarization.,"In daily communication, several people sometimes talk simultaneously, resulting in overlapped speech segments. Such segments challenge machine listening tasks like speaker diarization or speech recognition. This paper presents a speaker diarization framework where speaker count, a building block to predict the number of active speakers in each analyzing audio window, is integrated. Such speaker count block can be developed independently with existing speaker diarization systems; its output is then used in the re-segmentation step of existing systems to better label active speakers in each considered window. We further investigate the effect of analyzing window size in diarization performance in an oracle setting. Our preliminary theoretical analysis shows that the overlap speech detection, a special case of speaker count, is helpful to reduce diarization error rate when the window size is small enough. Finally, experiment results obtained from two state-of-the-art diarization systems on a benchmark dataset confirm the potential benefit of the proposed approach.",49,https://ieeexplore.ieee.org/document/9689476,1,
1064,Multi-Agent Multi-Armed Bandit Learning for Offloading Delay Minimization in V2X Networks.,"In a three-tier Vehicle to X (V2X) network, a vehicle can offload the computational tasks to the edge computing component at a roadside unit (RSU) or a base station with cloud computing (gNB). Moreover, an RSU can also offload to gNB, forming three offloading paths: vehicle-to-RSU, vehicle-to-gNB, and RSU-to-gNB. This paper aims to minimize the offloaded tasks' average latency while dealing with the network dynamic. Note that the existing works assume the fixed network parameters, hence have failed to address the dynamic. As a solution, we use the multi-agent multi-armed bandits (MBA) learning for offloading that can adapt to the network dynamic and optimize the latency. More importantly, we propose a new MBA offloading scheme with an exploration mechanism based on the Sigmoid function. We conduct an extensive evaluation to evaluate and show the superiority of our proposal. First, the proposed Sigmoid exploration mechanism reduces the tasks' average latency by 35% compared to a basic MBA using negative rewarding. Second, the simulation results show our proposed offloading algorithm shortens the task latency by 18.5% on average and 56.9% in the best case, compared to the state-of-the-art.",49,https://doi.org/10.1109/EUC53437.2021.00016,1,
1065,GCRINT: Network Traffic Imputation Using Graph Convolutional Recurrent Neural Network.,"Missing values appear in most multivariate time series, especially in the monitored network traffic data due to high measurement cost and unavoidable loss. In the networking fields, missing data prevents advanced analysis and downgrades downstream applications such as traffic engineering and anomaly detection. Despite the great potential, existing imputation approaches based on tensor decomposition and deep learning techniques have shown limitations in addressing missing values of traffic data due to its dynamic behavior. In this paper, we propose Graph Convolutional Recurrent Neural Network for Imputing Network Traffic (GCRINT), a combination between Recurrent Neural Network (RNN) and Graph Convolutional Neural Network, for filling the missing values of network traffic data. We use a bidirectional Long Short-Term Memory network and Graph Neural Network to efficiently learn the spatial-temporal correlations in partially observed data. We conducted extensive experiments to evaluate our model by using two different datasets and various missing scenarios. The experiment results show that GCRINT achieves significantly low imputation errors and reduces the error by 35% compared to the state-of-the-art methods. GCRINT also helps to obtain a stable performance in the traffic engineering problem.",49,https://doi.org/10.1109/ICC42927.2021.9500687,1,
1066,Efficient Prediction of Discharge and Water Levels Using Ensemble Learning and Singular-Spectrum Analysis-Based Denoising.,"This work addresses forecasting two essential factors in river hydrodynamics, which are discharge (Q) and water (H) levels. The accurate forecast of the two has long been a challenge in hydrological researches and flood prediction. While the traditional statistical models fail to capture the peak discharge during flooding seasons (i.e., due to the excessive level values), the simulation’s numerical models face the difficulty of precise input parameters (e.g., measured values of surface zones, root zones, etc.). The emerging deep learning shows a lot of potential in solving the challenges of Q and H prediction. However, applying deep learning in such a context is not straightforward due to the following critical issues. First, the amount of training data is insufficient due to the data collection is non-trivial. Second, although lacking, the collected data incurs noises (e.g., measurement errors). We aim to overcome those shortcomings in a newly proposed deep learning model that accurately predicts Q and H. The model is a new ensemble of the one-dimensional convolutional neural network (1D-CNN), long short term memory (LSTM) models, to handle the insufficient data issue. Moreover, we adopt the Singular-Spectrum Analysis technique to eliminate noise from the collected data. The experimental results show that our proposed approach outperforms existing methods.",49,https://doi.org/10.1007/978-3-030-79463-7_4,1,
1067,Fuzzy Deep Q-learning Task Offloading in Delay Constrained Vehicular Fog Computing.,"In the age of the ever-growing number of tasks being generated from IoT devices, one of the most crucial problems with enhancing the Quality of Service in multi-access computing is the system's limited resources. To this end, Vehicular Fog Computing (VFC) has emerged as a potential solution that utilizes the idle resources of vehicles to reduce the load imposed on the edge servers. In this paper, we leverage the advantages of both deep reinforcement learning and Fuzzy logic to propose Fuzzy Deep Q-learning base Offloading scheme (FDQO), a real-time offloading scheme in delay constrained VFC. Our objective is to maximize the Quality of Experiences (QoE), which indicates how the task meets its delay constraint. The experiment results show that our proposed approach significantly outperforms the existing algorithms. Specifically, FDQO improves the average QoE by 37.72% compared to using only Deep Q-learning, 7.47% compared to using only Fuzzy logic, and 19% compared to the ∊ -greedy strategy for multi-armed bandits.",49,https://doi.org/10.1109/IJCNN52387.2021.9533615,1,
1068,Multi-time-step Segment Routing based Traffic Engineering Leveraging Traffic Prediction.,"Based on the concept of source routing, Segment Routing (SR) allows the source or ingress node to inject a sequence of segment labels into the packet header and specify the routing path. Due to the routing flexibility, SR has been widely used to solve traffic engineering problems such as minimizing the maximum link utilization. However, most of the prior works only solve the problem in a single snapshot without considering network traffic dynamics, resulting in frequent traffic reroute. To cope with this problem, we focus on solving the segment routing based traffic engineering problem by taking into account the future traffic changes. We formulate the multi-time-step segment routing problem and leverage traffic prediction to extend the length of routing cycles. Due to the large search space of multi-time-step segment routing problem, we further propose a heuristic algorithm for incrementally recomputing the segment routing paths in sub-second. Through extensive experiments on real backbone network traffic datasets, we show that our proposal can achieve a near-optimal performance in term of maximum link utilization while significantly reducing the number of routing changes.",49,https://ieeexplore.ieee.org/document/9463952,1,
1069,Q-learning-based Opportunistic Communication for Real-time Mobile Air Quality Monitoring Systems.,"We focus on real-time air quality monitoring systems that rely on devices installed on automobiles in this research. We investigate an opportunistic communication model in which devices can send the measured data directly to the air quality server through a 4G communication channel or via Wi-Fi to adjacent devices or the so-called Road Side Units deployed along the road. We aim to reduce 4G costs while assuring data latency, where the data latency is defined as the amount of time it takes for data to reach the server. We propose an offloading scheme that leverages Q-learning to accomplish the purpose. The experiment results show that our offloading method significantly cuts down around 40-50% of the 4G communication cost while keeping the latency of 99.5% packets smaller than the required threshold.",49,https://doi.org/10.1109/IPCCC51483.2021.9679398,1,
1070,Visual-guided audio source separation: an empirical study.,"Real-world video scenes are usually very complicated as they are mixtures of many different audio-visual objects. Humans with normal hearing ability can easily locate, identify and differentiate sound sources which are heard simultaneously. However, this is an extremely difficult task for machines as the creation of machine listening algorithms that can automatically separate sound sources in difficult mixing conditions has remained very challenging. In this paper, we consider the use of a visual-guided audio source separation approach for separating sounds of different instruments in the video, where detected visual objects are used to assist the sound separation process. We particularly investigate the use of different object detectors for the task. In addition, as an empirical study, we analyze the effect of training datasets on separation performance. Finally, experiment results obtained from a benchmark dataset MUSIC confirm the advantages of the new object detector investigated in the paper.",49,https://doi.org/10.1109/MAPR53640.2021.9585244,1,
1071,MC-OCR Challenge 2021: A Multi-modal Approach for Mobile-Captured Vietnamese Receipts Recognition.,"Mobile captured receipts OCR (MC-OCR) recognizes text from structured and semi-structured receipts and invoices captured by mobile devices. This process plays a critical role in streamlining document-intensive processes and office automation in many financial, accounting, and taxation areas. Although many efforts have been devoted, MC-OCR still faces significant challenges due to mobile captured images’ complexity. First, receipts might be crumpled, or the content might be blurred. Second, different from scanned images, the quality of photos taken by mobile devices shows high diversity due to the light condition and the dynamic environment (e.g., indoor, out-door, complex background, etc.) where the receipts were captured. These difficulties lead to a low accuracy of the recognition results. In this challenge, we target two tasks to address these issues, including (1) evaluating the quality of the captured receipts, and (2) recognizing required fields of the receipts. Our idea is to leverage a multi-modal approach which can take advantage of both areas: computer vision and natural language processing, two of the main interests of the RIVF community. The paper presents the BK-OCR team’s methodology and results in the Mobile-Captured Image Document Recognition for Vietnamese Receipts 2021.",49,https://doi.org/10.1109/RIVF51545.2021.9642088,1,
1072,"A Path-Length Efficient, Low-Overhead, Load-Balanced Routing Protocol for Maximum Network Lifetime in Wireless Sensor Networks with Holes.","In wireless sensor networks (WSNs) with holes, designing efficient routing protocols, which prolong the network lifetime, is one of the most critical issues. To this end, this paper proposes a new geographic routing protocol for WSNs named the load Balanced and constant Stretch protocol for bypassing Multiple Holes (i.e., BSMH). In contrast to the existing works in the literature, the design of BSMH simultaneously takes into account the three factors that impacted the network lifetime, including routing path length, control packet overhead, and load balance among the nodes. Moreover, BSMH aims at minimizing the routing path length and the control overhead, while maximizing the load balance. We theoretically prove the efficiency of BSMH and extensively evaluate BSMH against the state-of-the-art protocols. The evaluation results show that the proposed protocol outperforms the others in various investigated metrics, not only network lifetime, but also routing path stretch, load balance, and control overhead. Specifically, BSMH prolongs the network lifetime by 30% compared to the existing protocols while guaranteeing that the routing path stretch is under",49,https://doi.org/10.3390/s20092506,1,
1073,A 1/2-Approximation Algorithm for Target Coverage Problem in Mobile Air Quality Monitoring Systems.,"So far, air quality monitoring is usually handled by monitoring stations located at fixed locations. However, due to the cost of installation, deployment, and operation, the number of monitoring stations deployed is often tiny; thus, the monitored area is limited. To deal with this problem, in this paper, we consider a mobile air quality monitoring system that relies on sensors mounted on buses to broaden the monitoring area. Specifically, we investigate the optimal buses to place the sensors as well as the optimal monitoring timings to maximize the number of critical regions that are monitored. We mathematically formulate the targeted problem and prove its NP-hardness. Then, we exploit the greedy and dynamic programming approaches to propose a polynomial-time 1/2-approximation algorithm. We use the data of real bus routes in Hanoi, Vietnam, for the experimentation and show that the proposed algorithm guarantees an average performance ratio of 72.68%.",49,https://doi.org/10.1109/GLOBECOM42002.2020.9322079,1,
1074,A Reinforcement Learning Algorithm for Resource Provisioning in Mobile Edge Computing Network.,"Mobile edge computing (MEC) is a model that allows integration of computing power into telecommunications networks, to improve communication and data processing efficiency. In general, providing power to ensure the computing power of edge servers in the MEC network is very important. In many cases, ensuring continuous power supply to the system is not possible because servers are deployed in hard-to-reach areas such as outlying areas, forests, islands, etc. This is when renewable energy prevails as a viable source of power for ensuring stable operation. This paper addresses resource provisioning in the MEC network using renewable energy. We formulate the problem as a Markov Decision Problem and introduce a new approach to optimize this problem in terms of energy and time costs by using a reinforcement learning technique. Our simulation validates the efficacy of our algorithm, which results in a cost three times better than the other methods.",49,https://doi.org/10.1109/IJCNN48605.2020.9206947,1,
1075,An $\frac{e-1}{2e-1}$-Approximation Algorithm for Maximizing Coverage Capability in Mobile Air Quality Monitoring Systems.,"In this paper, we focus on broadening the monitoring area of a mobile air quality monitoring system, in which the sensors mounted on buses. In particular, we investigate the optimal buses to place the sensors and the optimal monitoring timings to maximize the number of monitored critical regions. We mathematically formulate the targeted problem. Then, we leverage the greedy approach to propose a polynomial-time e−12e−1 approximation algorithm. We use the data of real bus routes in Hanoi, Vietnam, for the experimentation and show that the proposed algorithm guarantees an average performance ratio of 63.87%.",49,https://doi.org/10.1109/NCA51143.2020.9306692,1,
1076,Extending Network Lifetime by Exploiting Wireless Charging in WSN.,"In recent years, wireless energy transfer technology has emerged as a promising solution to supply energy for sensors and extend the network lifetime. In the literature, although significant efforts have been devoted to optimizing the charging schedule, it still remains a challenging problem. In this paper, we study how to prolong the network lifetime by optimizing the charging time. Specifically, we aim at proposing a charging schedule that maximizes the minimum residual energy of sensors. We first mathematically formulate the problem under mixed integer and linear programming. Then, we propose a lightweight heuristic algorithm. The experiment results show that our proposed algorithms extend network lifetime more than 10% compared to the state-of-the-art.",49,https://doi.org/10.1109/RIVF48685.2020.9140727,1,
1077,Network Lifetime Maximization for Full Area Coverage in Wireless Sensor Networks.,"Sensor scheduling for maximizing the network lifetime and achieving the full area coverage is a paramount problem in wireless sensor networks. Although considerable effort has been devoted, this problem is still a challenge to the research community. The approximation algorithms proposed so far couldn't guarantee the performance ratio. In this paper, we first formulate the problem under linear programming model which can help to determine the exact optimal solution. Then, in order to reduce the time complexity, we propose a (1 +∊)-approximation algorithm based on divide-and-conquer technique. The main idea is to divide the network into sub-regions, then determine the suboptimal solution for every sub-region and combine them to obtain the total solution of the whole network. Moreover, with the aim of speeding up the suboptimal solution finding process, we propose an approximation algorithm using the column generation approach. The experiment results show the superiority of our proposed algorithms over the existing ones.",49,https://doi.org/10.1109/APCC47188.2019.9026510,1,
1078,Generating Test Data for Blackbox Testing from UML-Based Web Engineering Content and Presentation Models.,"Software testing is a process that produces and consumes huge amounts of data. Thus, the test data is usually either gathered manually by the testers or randomly generated by tools. The manual method consumes lot of time and highly depends on the testers’ experience while the random approach faces the problem of redundant test data caused by identical use cases. By leveraging the concept of Model-based testing, this paper provides a novel method of testing to save the cost of manual testing and to increase the reliability of the testing processes. In Model-based testing, test cases and test data can be derived from different models. In this paper, we present a technique to generate test data from UML-based Web Engineering (UWE) presentation model for web application testing by using formal specification and Z3 SMT solver. We also build a model-based testing Eclipse Plug-in tool called TESTGER-UWE that generates test data based on the model of UWE for the web application. We evaluate the proposed methods by applying them to generate test data for an Address Book project of UWE. Experimental results show that our proposed methods can reduce the time significantly when generating test data for automation test tools such as Selenium, Katalon, Unit test, etc.",49,https://doi.org/10.1007/978-3-030-30149-1_17,1,
1079,A Dynamic Routing Protocol for Maximizing Network Lifetime in WSNs with Holes.,"Extending network lifetime is one of the most critical issues in handling wireless sensor networks. The network lifetime is contributed by three factors: routing path length, control packet overhead and load balance among the nodes. In the literature, many routing protocols have been proposed. However, none of them jointly considers all of these factors, thus they can not solve the network lifetime maximization problem thoroughly. In this paper, we aim at designing a routing protocol whose objective is to minimize the routing path length and the control overhead, while maximizing the load balance. The experiment results show that our proposed protocol strongly outperforms state-of-the-art protocols concerning many metrics including network lifetime, routing path stretch, load balance and control overhead.",49,https://doi.org/10.1145/3368926.3369725,1,
1080,Development of Rules and Algorithms for Model-Driven Code Generator with UWE Approach.,"UML-based Web Engineering (UWE) is an object-driven method for model-driven Web application development based on the UML modeling language. Focusing on the principles of the Model-Driven Engineering (MDE) paradigm, UWE helps to develop Web applications speedily and efficiently. This approach provides four separated models for representing different web-application concerns, i.e., content, navigation, processing and presentation models. In this paper, we exploit UWE to propose rules and algorithms in order to automatically generate code from different models. We also develop a model-driven code generator tool named CODEGER-UWE which is embedded with the proposed rules and algorithms. CODEGER-UWE has been experimented to semi-automatically generate code for a typical example, i.e., a web-based address book application. We then evaluate the performance of CODEGER-UWE with respect to some metrics including the number of generated source codes, the generation speed and the completion level.",49,https://doi.org/10.3233/FAIA190078,1,
1081,A Delay-Guaranteed Geographic Routing Protocol with Hole Avoidance in WSNs.,"Wireless sensor networks (WSNs) are used in many mission-critical applications, such as target tracking on a battlefield, emergency alarms, and disaster detection. In such applications, QoS provisioning in the timeliness domain is indispensable. Moreover, because of the diversity of sensory data, QoS provisioning should support not only one but multiple levels of end-to-end delay constraints. As a result of several characteristics such as the limitations on the energy supply, available storage and computational capacity of the sensor nodes, guaranteeing timely delivery in WSNs is a challenging problem. To overcome these limitations, several lightweight and stateless QoS-based geographic routing protocols have been proposed. The existing protocols work well in networks without routing holes (i.e., regions with no working sensors). However, with the occurrence of routing holes, they suffer from the so-called local minimum phenomenon and traffic congestion around the hole boundary. In this paper, we consider the presence of routing holes and propose a delay-guaranteed geographic routing protocol called DEHA that can support multiple end-to-end delay levels. The main idea is to achieve early awareness of the presence of a routing hole and then to utilize this awareness in determining a routing path that can avoid the hole. Simulation results show that our protocol outperforms the existing protocols in terms of several performance metrics, including packet delivery ratio, energy efficiency, and load balancing.",49,https://doi.org/10.1109/MASS.2017.16,1,
1082,Constant stretch and load balanced routing protocol for bypassing multiple holes in wireless sensor networks.,"The occurrence of multiple holes in wireless sensor networks poses many challenges in designing routing protocols. The traditional scheme is forwarding packets along the hole perimeters. However, this scheme leads to two serious problems: data concentration around the hole boundaries and routing path enlargement Recently, several approaches have been proposed to address these two problems, wherein a common idea is to form forbidden areas around the holes from which packets are kept to stay away. However, due to the static nature of the forbidden areas and routing paths, the existing protocols cannot solve these two problems thoroughly. In this paper, we propose a novel protocol for bypassing multiple holes in wireless sensor networks which can balance the traffic over the network while ensuring the constant stretch property of the routing path. Our main idea is to use elastic forbidden areas and dynamic routing paths. The theoretical analysis proves that the routing path stretch of the proposed protocol can be controlled to be as small as 1 + ϵ (for any predefined ϵ > 0), and the simulation experiments show that our protocol strongly outperforms state-of-the-art protocols in terms of load balancing.",49,https://doi.org/10.1109/NCA.2017.8171330,1,
1083,A Distributed Algorithm For Monitoring An Expanding Hole In Wireless Sensor Networks.,"Holes in sensor networks are regions that have no operating nodes and that may occur due to several reasons, including cases caused by natural obstacles or disaster suffered areas. Determining the location and shape of holes can help to monitor these disasters (such as volcano, tsunami, etc.) or help to make smart, early routing decisions for circumventing a hole. There are many hole determination algorithms proposed in the literature, however, these only consider the networks with static holes i.e. with stable boundary nodes. Moreover, most of these are designed in a centralized manner which is not suitable to the unstable situation of networks with an expanding hole. In this paper, we propose an algorithmic scheme not only for determining the initial shape but also for monitoring and quickly reporting about the area of a hole gradually expanding. Our algorithms are designed in a distributed manner and our initial simulation results show that our protocol is lightweight and feasible with monitoring sensor networks with an expanding hole. ",49,http://www.informatica.si/index.php/informatica/article/view/1279,1,
1084,Hole Approximation-Dissemination Scheme for Bounded-Stretch Routing in Sensor Networks.,"The routing stretch in a sensor network with a large hole can be controlled if the network nodes have certain information about the hole location and shape, possibly by using some mechanism for hole boundary approximation and information dissemination. However, a finer approximation (for smaller stretch) leads to higher cost of dissemination, which could be unaffordable for the sensors. We provide an insightful analysis to this fundamental trade-off of the hole approximation problem, using a geometric model. Based on this we discuss an efficient approximation-dissemination scheme using a natural, heuristic approximation technique. For reasonable conditions (network is dense enough apart from the hole), we assure that the routing stretch is under a predefined threshold > 1 while the dissemination cost is also under tight control. Our initial experiment results also confirm that our approach is significantly more efficient and economical, compared to existing proposals.",49,https://doi.org/10.1109/DCOSS.2014.17,1,
1086,A Hybrid Method for Vietnamese Text Normalization.,"This paper presents a hybrid method for normalizing written text often found on newspapers to its spoken form. To normalize raw text with a number of non-standard words (NSWs), a two-step model is proposed. The first step involves classifying NSWs into different categories using Random Forest. The latter one is to expand them, depending on their NSW types, into pronounceable syllables using a hybrid method. Most of numeric types can be expanded by well-defined rules while most of alphabetic ones must be expanded by a deep learning (i.e. sequence-to-sequence) model and a post adjustment. The experiment on a Vietnamese corpus with proposed NSW categories shows that the most ambiguous cases of the classification model are for abbreviation and read-as-sequence types, hence combined into one category for the latter expansion with more complex model and better context. The classification model gives an enhanced result of 99.20% with the category combination and the feature optimization. In the expansion, the sequence-to-sequence model shows a good result of 96.53% for abbreviations and 96.25% for loanwords with a post-adjustment for some completely wrong cases. This model can predict effectively the expansions of abbreviations in context.",50,https://doi.org/10.1145/3342827.3342851,1,
1087,Natural Language Understanding in Smartdialog: A Platform for Vietnamese Intelligent Interactions.,"Nowadays in the modern world, interactive smart dialogs with text or voice are gaining traction as the main digital interaction channel between human and machine. However, most of the current platforms do not support or have not fully developed for Vietnamese. In this paper, the authors propose a smart conversational platform through a text channel and/or voice channel in Vietnamese language, including these main steps: (i) Input Conversion and Pre-Processing, (ii) Entity Recognition, (iii) Intent Classification, (iv) Action Prediction and Execution, and (v) Output Generation. This paper focuses on presenting problems related to natural language understanding. To recognize entities in a sentence, the authors studied and optimized the features for Vietnamese with the Conditional Random Field model. With the problem of predicting user intent, this work proposed, experimented, and compared of Random Forest and BiLSTM deep learning model to optimize for the Vietnamese language. A platform was built and deployed for Milo smart speaker application (LUMI smart home) and VADI driver virtual assistant with the accuracy of around 98.7%.",50,https://doi.org/10.1145/3342827.3342857,1,
1088,Enhanced Genetic Algorithm for Single Document Extractive Summarization.,"In extractive summarization, summaries are generated by selecting the most salient sentences from the original text. The text summarization can be seen as a classification of sentences into two groups: in-summary/not-in-summary. Many approaches have been proposed to extract key sentences in which using Genetic Algorithms (GAs) has shown some promising results. In this paper, we propose an enhanced genetic algorithm in order to improve the quality of extractive text summarization. More concisely, we first evaluate the role of some sentence features and their contribution to improve the fitness function. We second investigate some crossover and mutation mechanisms in order to augment the accuracy of summarization as well as the performance of our model. The experiment has been conducted for the Daily Mail dataset to assess the proposed model and previous works. The empirical results show that our proposed GA gives better accuracy in comparison with TextRank and SummaRunNer, i.e., increasing the accuracy by 7.2% and 6.9% respectively.",50,https://doi.org/10.1145/3368926.3369729,1,
1089,Abstractive Text Summarization Using Pointer-Generator Networks With Pre-trained Word Embedding.,"Abstractive text summarization is the task of generating a summary that captures the main content of a text document. As a state-of-the-art method for abstractive summarization, the pointer-generator network produces more fluent summaries and solves two shortcomings of reproducing factual details inaccurately and phrase repetition. Though this network can generate Out-Of-Vocabulary (OOV) words, it cannot completely represent them in the context and may face the information loss problem. This paper aims to improve the quality of abstractive summarization with an extra pretrained layer of word embedding for the pointer-generator network. This mechanism helps to maintain the meaning of words in more various contexts. This assures that every word has its own representation, even though it does not exist in the vocabulary. We modify the network with the two latest word embedding mechanisms, i.e. Word2vec and Fasttext, to represent the semantic information of words more accurately. Some OOV words which are marked as unknown tokens now can have their right embeddings and be well considered in summary generation. The experiments on the CNN/Daily Mail corpus shows that the new mechanism outperforms the only pointer-generator network in all 3 ROUGE scores (R1, R2, RL).",50,https://doi.org/10.1145/3368926.3369728,1,
1090,Enhancing extractive summarization using non-negative matrix factorization with semantic aspects and sentence features.,"The main task in extractive text summarization is to evaluate the important of sentences in a document. This paper aims at improving the quality of an unsupervised summarization method, i.e. non-negative matrix factorization, by using sentence features and considering semantically related words using word embeddings (i.e. word2vec) in sentence scoring. The experiments were carried out with different scenario using the DUC 2007 dataset. Experimental results showed that when NMF was combined three types of sentence features (i.e., surface, content, and relevant features) and word2vec, the system got best performance with 42.34% for Rouge-1 and 10.77% for Rouge-2, increasing 0.67% Rouge-1 and 0.78% Rouge-2 in compared with only NMF.",50,https://doi.org/10.1145/3155133.3155188,1,
1091,Sender - Receiver cooperation and buffer observation for fine-grained adaptive transmission of video over mobile ad hoc networks.,"Deployment of video communication services over mobile ad hoc networks faces numerical obstacles since the networks are characterized by limited and unpredictable bandwidth. Following the tenet of service-orientation, this study proposes a cross-layer design for fine-grained adaptive transmission of real-time video over time-varying channels. Specifically, the sender accurately estimates the current effective bandwidth by observing its transmission buffer and negative acknowledgments (NACKs) from the receiver. Information on route quality provided by routing protocols may also be exploited to control the real-time stream efficiently. At the same time, the sender handles NACK messages to predict route goodness and orders retransmissions for improving video quality. We have evaluated the performance of the proposed design by both simulations and experiments of ad hoc nodes delivering H264/AVC, which consistently demonstrated the soundness and feasibility of the framework with respect to video quality and resource utilization.",50,https://doi.org/10.1109/PCCC.2010.5682333,1,
1092,Checking Interface Interaction Protocols Using Aspect-Oriented Programming.,"We propose an approach based on aspect-oriented programming to specify and to verify interaction protocols in the component interfaces of Java programs. First, based on method patterns of AspectJ we introduce a simple protocol specification language that specifies temporal orders of operations of an interface. We then develop an algorithm that takes a protocol specification and produces aspect code that will check for protocol conformance of programs that use the interface at runtime. Finally, we propose several extensions that enable the integration of our approach with static techniques and allow checking protocols over multiple components.",50,https://doi.org/10.1109/SEFM.2008.32,1,
1093,Prosodic Boundary Prediction Model for Vietnamese Text-To-Speech.,"This research aims to build a prosodic boundary prediction model for improving the naturalness of Vietnamese speech synthesis. This model can be used directly to predict prosodic boundaries in the synthesis phase of the statistical parametric or end-to-end speech systems. Beside conventional features related to Part-Of-Speech (POS), this paper proposes two efficient features to predict prosodic boundaries: syntactic blocks and syntactic links, based on a thorough analysis of a Vietnamese dataset. Syntactic blocks are syntactic phrases whose sizes are bounded in their constituent syntactic tree. A syntactic link of two adjacent words is calculated based on the distance between them in the syntax tree. The experimental results show that the two proposed predictors improve the quality of the boundary prediction model using a decision tree classification algorithm, about 36.4% (F1 score) higher than the model with only POS features. The final boundary prediction model with POS, syntactic block, and syntactic link features using the LightGBM algorithm gives the best F1-score results at 87.0% in test data. The proposed model helps the TTS systems, developed by either HMM-based, DNN-based, or End-to-end speech synthesis techniques, improve about 0.3 MOS points (i.e. 6 to 10%) compared to the ones without the proposed model.",50,https://doi.org/10.21437/Interspeech.2021-125,1,
1094,A Hypercuboid-Based Machine Learning Algorithm for Malware Classification.,"Malware attacks have been among the most serious threats to cyber security in the last decade. Antimalware software can help safeguard information systems and minimize their exposure to the malware. Most of anti-malware programs detect malware instances based on signature or pattern matching. Data mining and machine learning techniques can be used to automatically detect models and patterns behind different types of malware variants. However, traditional machine-based learning techniques such as SVM, decision trees and naive Bayes seem to be only suitable for detecting malicious code, not effective enough for complex problems such as classification. In this article, we propose a new prototype extraction method for non-traditional prototype-based machine learning classification. The prototypes are extracted using hypercuboids. Each hypercuboid covers all training data points of a malware family. Then we choose the data points nearest to the hyperplanes as the prototypes. Malware samples will be classified based on the distances to the prototypes. Experiments results show that our proposition leads to F1 score of 96.5% for classification of known malware and 97.7% for classification of unknown malware, both better than the original prototype-based classification method.",50,https://doi.org/10.1109/RIVF51545.2021.9642093,1,
1095,"Comparison of Machine Learning Methods for Estimating Mangrove Above-Ground Biomass Using Multiple Source Remote Sensing Data in the Red River Delta Biosphere Reserve, Vietnam.","This study proposes a hybrid intelligence approach based on an extreme gradient boosting regression and genetic algorithm, namely, the XGBR-GA model, incorporating Sentinel-2, Sentinel-1, and ALOS-2 PALSAR-2 data to estimate the mangrove above-ground biomass (AGB), including small and shrub mangrove patches in the Red River Delta biosphere reserve across the northern coast of Vietnam. We used the novel extreme gradient boosting decision tree (XGBR) technique together with genetic algorithm (GA) optimization for feature selection to construct and verify a mangrove AGB model using data from a field survey of 105 sampling plots conducted in November and December of 2018 and incorporated the dual polarimetric (HH and HV) data of the ALOS-2 PALSAR-2 L-band and the Sentinel-2 multispectral data combined with Sentinel-1 (C-band VV and VH) data. We employed the root-mean-square error (RMSE) and coefficient of determination (R2) to evaluate the performance of the proposed model. The capability of the XGBR-GA model was assessed via a comparison with other machine-learning (ML) techniques, i.e., the CatBoost regression (CBR), gradient boosted regression tree (GBRT), support vector regression (SVR), and random forest regression (RFR) models. The XGBR-GA model yielded a promising result (R2 = 0.683, RMSE = 25.08 Mg·ha−1) and outperformed the four other ML models. The XGBR-GA model retrieved a mangrove AGB ranging from 17 Mg·ha−1 to 142 Mg·ha−1 (with an average of 72.47 Mg·ha−1). Therefore, multisource optical and synthetic aperture radar (SAR) combined with the XGBR-GA model can be used to estimate the mangrove AGB in North Vietnam. The effectiveness of the proposed method needs to be further tested and compared to other mangrove ecosystems in the tropics",50,https://doi.org/10.3390/rs12081334,1,
1098,Intonation issues in HMM-based speech synthesis for Vietnamese.,"n an HMM-based Text-To-Speech system, contextual features, including phonetic and prosodic factors have a significant influence to the spectrum, F0 and duration of the synthetic voice. This paper proposes prosodic features aiming at improving the naturalness of an HMM-based TTS system (VTed) for a tonal language, Vietnamese. The ToBI (Tones and Break Indices) features are used to learn two crucial prosodic cues i.e. intonation (boundary tones) and pause (break indices), concurrently with another set of features. The result of MOS test showed that the general quality of synthetic voice is rather good, 1.21 point lower than the natural voice. About 55% of the voice trained with ToBI boundary tone feature are perceived as similar to the voice trained without this feature, while a 10% difference in favour of the voice trained without this ToBI feature is observed. This may be linked with F0 contour lowering or raising regardless of lexical tones. This brought two main problems in the synthetic voice: discontinuity in spectrum and F0 or unexpected voice quality. This paper then concluded the need of much more work on intonation modeling that should take into account the Vietnamese tones. A new prosody model can be designed, which may consider the ToBI model, with respect to lexical tones and the syntactic structure of Vietnamese.

Index Terms: Text-to-speech (TTS), speech synthesis, tonal language, Vietnamese, HMM-based speech synthesis, intonation, ToBI",50,http://www.isca-speech.org/archive/sltu_2014/nguyen14_sltu.html,1,
1099,"A model of F0 contour for Vietnamese questions, applied in speech synthesis.","This paper presents some initial results in modeling F0 contour for Vietnamese questions, which can be applied in speech synthesis. Perceptual tests were carried out to find out the pertinent parameters which have an important influence on intonation: the normalized register ratio and the increasing slope. The intonation of Vietnamese questions is then generated from the result of perceptual tests. Firstly, an intonation contour for the sentence is generated using our intonation model for the statement sentence. The whole contour is then raised by a number of percentages of the F0 mean called alpha (normalized register ratio). And finally, the contour of the last syllable is raised by a number of percentages of the F0 mean called beta (increasing slope). Some experiments were carried out to prove and verify for the proposed model.",50,https://doi.org/10.1145/2069216.2069250,1,
1100,Multifactorial Evolutionary Algorithm for Simultaneous Solution of TSP and TRP.,"We study two problems called the Traveling Repairman Problem (TRP) and Traveling Salesman Problem (TSP). The TRP wants to minimize the total time for all customers that have to wait before being served, while the TSP aims to minimize the total time to visit all customers. In this sense, the TRP takes a customer-oriented view, whereas the TSP is server-oriented. In the literature, there exist numerous algorithms that are developed for two problems. However, these algorithms are designed to solve each problem independently. Recently, Multifactorial Evolutionary Algorithm (MFEA) has been a variant of Evolutionary Algorithm (EA) aiming to solve multiple optimization tasks simultaneously. The MFEA framework has yet to be fully exploited, but the realm has recently attracted much interest from the research community. This paper proposed a new approach using the MFEA framework to solve these two problems simultaneously. The MFEA has two tasks simultaneously: the first is solving the TRP problem, and the second is solving the TSP. Experiment results show the efficiency of the proposed MFEA: 1. for small instances, the algorithm reaches the optimal solutions of both problems; 2. for large instances, our solutions are better than those of the previous MFEA algorithms.",54,https://doi.org/10.31577/cai_2021_6_1370,1,
1101,Variable neighbourhood search-based algorithm to solve the minimum back-walk-free latency problem.,"The Minimum Back-Walk-Free Latency Problem (MBLP) is an extension of the
Minimum Latency Problem (MLP). It aims to find a tour with minimum latency while ignoring the
back-walking costs. One of the justifications for ignoring the back-walking costs is in applications
of message broadcasting for mobile devices. Obviously, the MBLP is NP-hard problem because it is
a particular variant of MLP. Therefore, metaheuristic needs to be developed to provide near-optimal
solutions within a short computation time. However, the main issue of metaheuristics is that they
fall into local optima in some cases since the search space of the problem is combinatorial
explosion. To overcome the drawback, we propose a metaheuristic algorithm combined between the
Variable Neighbourhood Search (VNS) and shaking technique to solve the problem. The aim of
VNS is to generate diverse neighbourhoods by using various neighbourhood searches while the
shaking technique guides the search towards an unexplored part of the solution space. Moreover, a
technique called Adaptive Memory (AM) is applied to balance between diversity and
intensification. The computational results show that the effectiveness and efficiency of the proposed
algorithm are comparable with the state-of-the-art algorithms.
Keywords: MBLP; minimum back-walk-free latency probl",54,https://doi.org/10.1504/IJCAT.2021.113642,1,
1102,Applying Metaheuristic for Time-Dependent Traveling Salesman Problem in Postdisaster.,"The Time-Dependent Traveling Salesman Problem (TDTSP) is a generalization of the Traveling Salesman Problem (TSP) and Traveling Repairman Problem (TRP). In the TSP and TRP, the travel time to travel is assumed to be constant. However, in practice, the travel times vary according to several factors that naturally depend on the time of day. Therefore, the TDTSP is closer to several real practical situations than the TSP. In this paper, we introduce a new variant of the TDTSP, that is, the Time-Dependent Traveling Salesman Problem in Postdisaster (TDTSP-PD). In the problem, the travel costs need to be added debris removal times after a disaster occurs. To solve the TDTSP-PD, we present an effective population-based algorithm that combines the diversification power of the Genetic Algorithm (GA) and the intensification strength of Local Search (LS). Therefore, our metaheuristic algorithm maintains a balance between diversification and intensification. The results of the experimental simulation are compared with the well-known and successful metaheuristic algorithms. These results show that the proposed algorithm reaches better solutions in many cases.",54,https://doi.org/10.2991/ijcis.d.210226.001,1,
1103,An efficient branch-and-cut algorithm for the parallel drone scheduling traveling salesman problem.,"We propose an efficient branch-and-cut algorithm to exactly solve the parallel drone scheduling traveling salesman problem. Our algorithm can find optimal solutions for all but two existing instances with up to 229 customers in a reasonable running time. To make the problem more challenging for future methods, we introduce two new sets of 120 larger instances with the number of customers varying from 318 to 783 and test our algorithm and investigate the performance of state-of-the-art metaheuristics on these instances.",54,https://arxiv.org/abs/2111.11307,1,
1104,An Effective Metaheuristic for Multiple Traveling Repairman Problem with Distance Constraints.,"Multiple Traveling Repairman Problem with Distance Constraints (MTRPD) is an extension of the NP-hard Multiple Traveling Repairman Problem. In MTRPD, a fleet of identical vehicles is dispatched to serve a set of customers with the following constraints. First, each vehicle's travel distance is limited by a threshold. Second, each customer must be visited exactly once. Our goal is to find the visiting order that minimizes the sum of waiting times. To solve MTRPD we propose to combine the Insertion Heuristic (IH), Variable Neighborhood Search (VNS), and Tabu Search (TS) algorithms into an effective two-phase metaheuristic that includes a construction phase and an improvement phase. In the former phase, IH is used to create an initial solution. In the latter phase, we use VNS to generate various neighborhoods, while TS is employed to mainly prohibit from getting trapped into cycles. By doing so, our algorithm can support the search to escape local optima. In addition, we introduce a novel neighborhoods’ structure and a constant time operation which are efficient for calculating the cost of each neighboring solution. To show the efficiency of our proposed metaheuristic algorithm, we extensively experiment on benchmark instances. The results show that our algorithm can find the optimal solutions for all instances with up to 50 vertices in a fraction of seconds. Moreover, for instances from 60 to 80 vertices, almost all found solutions fall into the range of 0.9 %-1.1 % of the optimal solutions' lower bounds in a reasonable duration. For instances with a larger number of vertices, the algorithm reaches good-quality solutions fast. Moreover, in a comparison to the state-of-the-art metaheuristics, our proposed algorithm can find better solutions.",54,http://www.cai.sk/ojs/index.php/cai/article/view/2019_4_883,1,
1105,Robust Adaptive Image Coding for Frame Memory Reduction in LCD Overdrive.,"Summary form only given. In this paper, Robust Adaptive Image Coding (RAIC) is proposed to increase compression performance for Frame Memory Reduction in LCD Overdrive. The RAIC contains two techniques to improve the quality of decompressed images. The first is a Min-Max (of Block) Adaptive Uniform Quantization Coding (MMAUQC) to improve the quality of decompressed block images. The second is a Multiple Adaptive Quantization Coding (MAQC), which is combined of Range-based Bit Distribution Technique (RBBDT) and MMAUQC. This combination gives multiple adaptive ability that improving the image quality while preserving fixed word length compression feature. RAIC shows the flexible ability in bit distribution of RBBDT, which can expand the search capability to other optimize models. Experimental results show that, compared with other coding methods used in the same type of application, RAIC has outperforming features.",54,https://doi.org/10.1109/DCC.2013.77,1,
1106,Generating Test Data for Blackbox Testing from UML-Based Web Engineering Content and Presentation Models.,"Software testing is a process that produces and consumes huge amounts of data. Thus, the test data is usually either gathered manually by the testers or randomly generated by tools. The manual method consumes lot of time and highly depends on the testers’ experience while the random approach faces the problem of redundant test data caused by identical use cases. By leveraging the concept of Model-based testing, this paper provides a novel method of testing to save the cost of manual testing and to increase the reliability of the testing processes. In Model-based testing, test cases and test data can be derived from different models. In this paper, we present a technique to generate test data from UML-based Web Engineering (UWE) presentation model for web application testing by using formal specification and Z3 SMT solver. We also build a model-based testing Eclipse Plug-in tool called TESTGER-UWE that generates test data based on the model of UWE for the web application. We evaluate the proposed methods by applying them to generate test data for an Address Book project of UWE. Experimental results show that our proposed methods can reduce the time significantly when generating test data for automation test tools such as Selenium, Katalon, Unit test, etc.",56,https://doi.org/10.1007/978-3-030-30149-1_17,1,
1107,Development of Rules and Algorithms for Model-Driven Code Generator with UWE Approach.,"UML-based Web Engineering (UWE) is an object-driven method for model-driven Web application development based on the UML modeling language. Focusing on the principles of the Model-Driven Engineering (MDE) paradigm, UWE helps to develop Web applications speedily and efficiently. This approach provides four separated models for representing different web-application concerns, i.e., content, navigation, processing and presentation models. In this paper, we exploit UWE to propose rules and algorithms in order to automatically generate code from different models. We also develop a model-driven code generator tool named CODEGER-UWE which is embedded with the proposed rules and algorithms. CODEGER-UWE has been experimented to semi-automatically generate code for a typical example, i.e., a web-based address book application. We then evaluate the performance of CODEGER-UWE with respect to some metrics including the number of generated source codes, the generation speed and the completion level.",56,https://doi.org/10.3233/FAIA190078,1,
1108,Adaptive Ranking Relevant Source Files for Bug Reports Using Genetic Algorithm.,"Precisely locating buggy files for a given bug report is a cumbersome and time-consuming task, particularly in a large-scale project with thousands of source files and bug reports. An efficient bug localization module is desirable to improve the productivity of the software maintenance phase. Many previous approaches rank source files according to their relevance to a given bug report based on simple lexical matching scores. However, the lexical mismatches between natural language expressions used to describe bug reports and technical terms of software source code might reduce the bug localization system’s accuracy. Incorporating domain knowledge through some features such as the semantic similarity, the fixing frequency of a source file, the code change history and similar bug reports is crucial to efficiently locating buggy files. In this paper, we propose a bug localization model, BugLocGA that leverages both lexical and semantic information as well as explores the relation between a bug report and a source file through some domain features. Given a bug report, we calculate the ranking score with every source files through a weighted sum of all features, where the weights are trained through a genetic algorithm with the aim of maximizing the performance of the bug localization model using two evaluation metrics: mean reciprocal rank (MRR) and mean average precision (MAP). The empirical results conducted on some widely-used open source software projects have showed that our model outperformed some state of the art approaches by effectively recommending relevant files where the bug should be fixed.",56,https://doi.org/10.3233/FAIA210042,1,
1109,Emotion Recognition with Capsule Neural Network.,"For human-machine communication to be as effective as human-to-human communication, research on speech emotion recognition is essential. Among the models and the classifiers used to recognize emotions, neural networks appear to be promising due to the network’s ability to learn and the diversity in configuration. Following the convolutional neural network, a capsule neural network (CapsNet) with inputs and outputs that are not scalar quantities but vectors allows the network to determine the part-whole relationships that are specific 6 for an object. This paper performs speech emotion recognition based on CapsNet. The corpora for speech emotion recognition have been augmented by adding white noise and changing voices. The feature parameters of the recognition system input are mel spectrum images along with the characteristics of the sound source, vocal tract and prosody. For the German emotional corpus EMO-DB, the average accuracy score for 4 emotions, neutral, boredom, anger and happiness, is 99.69%. For Vietnamese emotional corpus BKEmo, this score is 94.23% for 4 emotions, neutral, sadness, anger and happiness. The accuracy score is highest when combining all the above feature parameters, and this score increases significantly when combining mel spectrum images with the features directly related to the fundamental frequency.",63,https://doi.org/10.32604/csse.2022.021635,1,
1110,Emotional Speech Recognition Using Deep Neural Networks.,"The expression of emotions in human communication plays a very important role in the information that needs to be conveyed to the partner. The forms of expression of human emotions are very rich. It could be body language, facial expressions, eye contact, laughter, and tone of voice. The languages of the world’s peoples are different, but even without understanding a language in communication, people can almost understand part of the message that the other partner wants to convey with emotional expressions as mentioned. Among the forms of human emotional expression, the expression of emotions through voice is perhaps the most studied. This article presents our research on speech emotion recognition using deep neural networks such as CNN, CRNN, and GRU. We used the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus for the study with four emotions: anger, happiness, sadness, and neutrality. The feature parameters used for recognition include the Mel spectral coefficients and other parameters related to the spectrum and the intensity of the speech signal. The data augmentation was used by changing the voice and adding white noise. The results show that the GRU model gave the highest average recognition accuracy of 97.47%. This result is superior to existing studies on speech emotion recognition with the IEMOCAP corpus",63,https://doi.org/10.3390/s22041414,1,
1111,PBX Autoresponder System for Information Lookup of Pupil Records.,"This paper presents the integration of a Vietnamese recognition module into the digital PBX Asterisk and construction of a PBX autoresponder system for information lookup of pupil records. This system allows direct interaction with the user entirely in Vietnamese, as it receives requests from users and then responds in Vietnamese. An experiment has been performed to test Vietnamese recognition using a set of commands containing from 1 to 6 syllables. The acoustic unit selection method was tested. The test results showed that use of acoustic units with diphones and rhyming syllables gives better recognition results than use of units with single phonemes. The scores of the Vietnamese recognition system in the command mode were 78.52 % for one-syllable commands and 74.71 % for six-syllable commands.",63,https://doi.org/10.1007/978-3-662-49390-8_50,1,
1112,Text-dependent speaker recognition for vietnamese.,"This paper presents a new method for Vietnamese text-dependent speaker recognition. The system is modeled for each speaker using mixture model Gaussian GMM (Gaussian Mixture Model). The phonemes in the keywords are represented by hidden Markov models HMM. The prior and posterior probabilities for keywords and speakers have been combined together to identify speakers. The results showed that in the case the speaker did not say a long enough phrase, this approach has increased performance of speaker identification.",63,https://doi.org/10.1109/SOCPAR.2013.7054126,1,
1113,Automatic Speech Recognition for Vietnamese Using HTK System.,"This paper presents our research on Vietnamese speech recognition using HTK1 system. Our method is the adaptation of Vietnamese data into HTK system: text corpus was collected from Web and then bigram language model was built by the CMU SLM toolkit. The experiments were carried out with different acoustic models. Further more, the experiment using speaker adaptation technique was implemented by MLLR algorithm. For our results, WA (Word Accuracy) in the case of speaker independent recognition test is 71.37% and 75.96% with the test of speaker adaptation",63,https://doi.org/10.1109/RIVF.2010.5633587,1,
1114,Using tone information for Vietnamese continuous speech recognition.,"This paper presents our study on the use of tone information in a large vocabulary for a Vietnamese continuous speech recognition system. Firstly, a new module of tone recognition using Hidden Markov model is presented. Then, a new methodology for integrating this module into the Speeral system is given. The experiments were implemented on VNSpeechCorpus. The results showed that the direct use of tone score in the Speeral system would increase the performance of the system, e.g., 28.6% relative reduction in word error rate.",63,https://doi.org/10.1109/RIVF.2008.4586340,1,
1116,An Uncertainty-Aware Encoder for Aspect Detection.,"Aspect detection is a fundamental task in opinion mining. Previous works use seed words either as priors of topic models, as anchors to guide the learning of aspects, or as features of aspect classifiers. This paper presents a novel weakly-supervised method to exploit seed words for aspect detection based on an encoder architecture. The encoder maps segments and aspects into a low-dimensional embedding space. The goal is approximating similarity between segments and aspects in the embedding space and their ground-truth similarity generated from seed words. An objective function is proposed to capture the uncertainty of ground-truth similarity. Our method outperforms previous works on several benchmarks in various domains.",64,https://doi.org/10.18653/v1/2021.findings-emnlp.69,1,
1118,Convolutional Neural Network Modelling for MODIS Land Surface Temperature Super-Resolution.,"Nowadays, thermal infrared satellite remote sensors enable to extract very interesting information at large scale, in particular Land Surface Temperature (LST). However such data are limited in spatial and/or temporal resolutions which prevents from an analysis at fine scales. For example, MODIS satellite provides daily acquisitions with 1Km spatial resolutions which is not sufficient to deal with highly heterogeneous environments as agricultural parcels. Therefore, image super-resolution is a crucial task to better exploit MODIS LSTs. This issue is tackled in this paper. We introduce a deep learning-based algorithm, named Multi-residual U-Net, for super-resolution of MODIS LST single-images. Our proposed network is a modified version of U-Net architecture, which aims at super-resolving the input LST image from 1Km to 250m per pixel. The results show that our Multi-residual U-Net outperforms other state-of-the-art methods.",65,https://arxiv.org/abs/2202.10753,1,
1119,A comparative study of operational engineering for environmental and compute-intensive applications.,"The study presented in this work deals with environmental and compute-intensive applications and their operational management aimed to achieve maximum effectiveness on high-performance systems. These applications, mainly compute-intensive modeling and simulations, are often required to deliver accurate forecasts having far-reaching effects on many facets of our society. The extreme growing amounts of observational data, along with remarkable advances in computing power and technology significantly improve the ability to model and simulate challenging issues. However, it is well known that designing, implementing as well as deploying such high-fidelity applications intended for running on compute systems is still time-consuming and challenging work for domain experts. In this study, we show that it is a systematic process requiring an expertise and skills in the scientific domain of interest, mathematics, and computer science. The study outcomes present and analyze successful benefits of high computational power for compute-intensive applications especially in the environmental domain.",65,https://doi.org/10.1016/j.array.2021.100096,1,
1120,Measuring Satisfaction and Loyalty of Guests Based on Vietnamese Hotel Online Reviews.,"Measuring customer satisfaction is a key task for hotels today. Analyzing online reviews of experienced guests will help the managers to know if guests are satisfied or dissatisfied with the service that they provided. Hence, they have solutions to improve service quality. This study presents a method to measure guest satisfaction based on sentiment lexicon that is developed for hospitality domain to increase the accuracy of the analysis results. Actual data is downloaded from TripAdvisor with 35 four-star to five-star hotels of five cities in Vietnam to analyze guest satisfaction that shows that nearly 80% of customers are satisfied with the quality of Vietnamese hotels. Based on data analysis, the study also evaluating guest loyalty through phrases like “came here several,” “come back,” “recommend,” etc. This rate corresponds to the number that was reported by the Vietnam National Administration of Tourism.
",65,https://doi.org/10.4018/IJEEI.2021070101,1,
1121,nQSV-Net: a novel queuing search variant for global space search and workload modeling.,"In this paper, we present our research that goes through two steps: (1) using meta-heuristic optimization for global space search; (2) applying the proposed optimization to multivariate workload modeling and prediction. In the first step, we pay attention to the improvement of the Queuing Search optimization by the space-walk combination of Levy-flight trajectory to improve population diversity and Opposition-based learning to speed up the convergence process. To evaluate our solution’s effectiveness, we compare it with six well-known optimization algorithms using CEC 2014 benchmark functions. The achieved results show the significant effect of our nQSV designs in avoiding local optima and speed up the convergence process. In the second step, to prove the feasibility of solving real problems, we apply nQSV to train a neural network to model multiple variables of distributed workload simultaneously. The model is called nQSV-Net as the whole. The gained outcomes from extensive experiments with three real datasets show the accuracy and stability of nQSV-Net as a solution in the domain.",65,https://doi.org/10.1007/s12652-020-02849-4,1,
1122,Longitudinal Motion Control of Electric Vehicles: Glocal Model and Design Using Passivity.,"This article presents a framework to model and design the longitudinal motion control (LMC) system for electric vehicles (EVs). Thanks to the passivity property of EVs, a hierarchical control configuration including three layers is proposed. In the lower layer, each actuator is provided a disturbance observer (DOB) for locally preventing the wheel slip. A global controller is designed in the upper layer for the cruising purpose. In addition, the middle layer serves as the aggregation and distribution channels.",65,https://doi.org/10.1109/MVT.2021.3086449,1,
1123,Multi-objective Sparrow Search Optimization for Task Scheduling in Fog-Cloud-Blockchain Systems.,"In this paper, we first design a system that integrates a blockchain network into a fog-cloud environment to improve the transparency as well as avoid the attack from suspicious parties. We identify task scheduling as the most prominent and crucial issue in such a system and model it as a multi-objective optimization problem. To solve this problem, we propose a multi-objective version of the Sparrow Search Algorithm (SSA), called MO-SSA. Our experimental studies compare MO-SSA with other well-known optimization algorithms (NSGA-II, NSGA-III, and MO-ALO). Results show that MO-SSA archives better results than most of the baseline algorithms on several performance metrics.",65,https://doi.org/10.1109/SCC53864.2021.00065,1,
1124,A Novel Approach for Developing Decentralized Storage and Sharing Systems.,"In recent years, the proliferation of blockchain technology has opened many research directions. In this context, the combination of blockchain-based techniques and traditional methods to improve existing systems has received significant interest from researchers. In this paper, we present a decentralized storage and sharing system based on a combination of IPFS (Inter-Planetary File System), encryption technologies (including Advanced Encryption Standard (AES), Elliptic Curve Cryptosystem (ECC), ABE (Attribute-based Encryption), Multi-Authority ABE (MA-ABE)), and multichain. In particular, we facilitate the advantages of the IPFS network to store user's data in a distributed manner. Furthermore, we make use of a cryptographer to protect the privacy of data. The hash returned by the IPFS network will be stored in our multichain architecture to provide transparency for all users participating in the system. To the best of our knowledge, it is the first storage and sharing system using IPFS, cryptographer, and multichain to ensure decentralized, trustworthy, transparent characteristics for storing and sharing data.",65,https://doi.org/10.1145/3457337.3457845,1,
1125,MPoC - A Metaheuristic Proof of Criteria Consensus Protocol for Blockchain Network.,"Over the last years, many blockchain platforms opt for Delegated Proof-of-Stake (DPoS) consensus protocol for their network deployment. However, using the DPoS, nodes on the network tend to vote for the nodes that are more likely to become block producers after several voting rounds. Hence, with DPoS, the number of block producers is quite small compared to the total number of nodes in the blockchain network. Besides, with DPoS consensus, the distribution of block producers often centralizes into several individual nodes. This paper proposes a novel consensus protocol called Meta-heuristic Proof of Criteria (MPoC) to overcome the disadvantages. Our protocol enables multiple operational criteria to evaluate all blockchain nodes during the block producer selection process. We apply meta-heuristic algorithms to optimize the decentralized level of the node selection based on the set of criteria weights to increase the democracy of the blockchain network.We carry out experiments to evaluate the block producer decentralization of MPoC as compared with DPoS. The achieved results show that our consensus protocol improves the decentralization of block producers as expected. We also show the feasibility of applying different meta-heuristic algorithms to our consensus protocol of blockchain through experiments.",65,https://doi.org/10.1109/ICBC51069.2021.9461113,1,
1126,Passivity Based Hierarchically Decentralized Range Extension Control of In-wheel-motor Vehicles.,"Range extension control of in-wheel-motor electric vehicles have been studied for years. However, there is still lack of theoretical results for rigorously stabilizing the overall control system which suffers nonlinear tire-force characteristics. In addition, it is required to locally guarantee safe motion of each driven wheel while globally increase the cruising range per charge. To deal with aforementioned issues, this paper utilizes passivity theory to present a hierarchical range extension control system. Each wheel is provided a local disturbance observer based anti-slip controller. By managing the aggregation of local wheel velocities, the global controller gives the total driving command which is optimally allocated to each wheel. The torque of each wheel is, therefore, the sum of the local control signal and the distributed signal from the global controller. Beside proving the stability rigorously, the effectiveness of the proposed control system is evaluated using Carsim-Matlab co-simulator.",65,https://doi.org/10.1109/ICM46511.2021.9385666,1,
1127,Disturbance Observer Based Glocal Control of Mechatronics Systems: A Passivity Approach.,"DOB has been one of the most popular robust control tools. However, there is still lack of practical design techniques for DOB-based applications, particularly for complex systems. To overcome this challenge, we investigate in this paper the possibility of DOB-based glocal control for a class of multi-actuator mechatronics systems possessing passivity property. In the lower-layer, each actuator is provided a DOB that locally compensates the external disturbances and model uncertainties. There exists in the upper-layer a global controller that controls an aggregation signal from the lower-layer. The aggregation and distribution channels connecting the lower/upper-layers can be treated as a middle one. Utilizing passivity theory, we rigorously obtain a condition for the global/local controllers to guarantee the stability of the overall system. We further investigate the other role of the middle-layer: attaining additional global objectives by optimization of the aggregation/distribution vectors.",65,https://doi.org/10.1109/ICM46511.2021.9385688,1,
1128,Fuzzy Deep Q-learning Task Offloading in Delay Constrained Vehicular Fog Computing.,"In the age of the ever-growing number of tasks being generated from IoT devices, one of the most crucial problems with enhancing the Quality of Service in multi-access computing is the system's limited resources. To this end, Vehicular Fog Computing (VFC) has emerged as a potential solution that utilizes the idle resources of vehicles to reduce the load imposed on the edge servers. In this paper, we leverage the advantages of both deep reinforcement learning and Fuzzy logic to propose Fuzzy Deep Q-learning base Offloading scheme (FDQO), a real-time offloading scheme in delay constrained VFC. Our objective is to maximize the Quality of Experiences (QoE), which indicates how the task meets its delay constraint. The experiment results show that our proposed approach significantly outperforms the existing algorithms. Specifically, FDQO improves the average QoE by 37.72% compared to using only Deep Q-learning, 7.47% compared to using only Fuzzy logic, and 19% compared to the ∊ -greedy strategy for multi-armed bandits.",65,https://doi.org/10.1109/IJCNN52387.2021.9533615,1,
1129,Absolute Stability of Disturbance Observer Based Altitude Control for Multirotor Flying Vehicles.,"Although altitude control of multirotor flying vehicles has been studied for years, it is still required to design and analyze the system by a practical-oriented procedure. The most challenge is due to the nonlinearities and uncertainties of thrust force characteristics. To overcome the aforementioned issues, this paper proposes a systematic approach for altitude control using disturbance observer. In particularly, this paper shows that the overall system can be equivalently modelled as a multi-agent system with rank-1 physical interaction and sector bounded nonlinearities. Thanks to this modelling, a condition for absolute stability is derived. The condition is independent of the number of propeller actuators, and hence, considerably reducing the complexity of system design. The effectiveness of the proposed approach is demonstrated by both numerical simulations and experiments using a quadrotor vehicle system.",65,https://doi.org/10.1109/ISIE45552.2021.9576178,1,
1130,A New Workload Prediction Model Using Extreme Learning Machine and Enhanced Tug of War optimization.,"Time series data is widely accessible in many life areas like economy, weather, stock price, retail sales, distributed system workloads. While many studies have focused on improving existing prediction techniques on accuracy aspect, less efforts is paid towards simple but efficient forecasting models in order to keep the balance between computation cost and prediction accuracy. In this work, we propose a novel time series prediction model, which aims to both model simplicity and accuracy. The core of the model is built based on extreme learning machine. Due to the random determination process for input weights and hidden biases, extreme learning machine requires a large number of hidden neurons to achieve good results and this increases the model complexity. To overcome this drawback, we propose a new opposition-based tug of war optimization to select optimally input weights, and hidden biases then apply to extreme learning machine. Two real public traffic monitoring datasets from Internet service providers were employed to evaluate our design. The achieved outcomes demonstrate that our proposed solution works effectively with satisfied performance in comparison with existing models for distributed system workload prediction.",65,https://doi.org/10.1016/j.procs.2020.03.063,1,
1131,Distributed Stabilization by Probability Control for Deterministic-Stochastic Large Scale Systems: Dissipativity Approach.,"By using dissipativity approach, we establish the stability condition for the feedback connection of a deterministic dynamical system and a stochastic memoryless map. After that, we extend the result to the class of large-scale systems in which: the deterministic system consists of many sub-systems; and the stochastic map consists of many stochastic actuators. We will demonstrate the proposed approach by showing the design procedures to globally stabilize the manufacturing systems while locally balance the stock levels in any production process.",65,https://doi.org/10.1109/CDC42340.2020.9304476,1,
1132,A Reinforcement Learning Algorithm for Resource Provisioning in Mobile Edge Computing Network.,"Mobile edge computing (MEC) is a model that allows integration of computing power into telecommunications networks, to improve communication and data processing efficiency. In general, providing power to ensure the computing power of edge servers in the MEC network is very important. In many cases, ensuring continuous power supply to the system is not possible because servers are deployed in hard-to-reach areas such as outlying areas, forests, islands, etc. This is when renewable energy prevails as a viable source of power for ensuring stable operation. This paper addresses resource provisioning in the MEC network using renewable energy. We formulate the problem as a Markov Decision Problem and introduce a new approach to optimize this problem in terms of energy and time costs by using a reinforcement learning technique. Our simulation validates the efficacy of our algorithm, which results in a cost three times better than the other methods.",65,https://doi.org/10.1109/IJCNN48605.2020.9206947,1,
1133,Modeling Multi-constrained Fog-cloud Environment for Task Scheduling Problem.,"A fog-cloud system allows end devices to be integrated with computing resources, including fog equipment and cloud services in the IoT environment. Due to the heterogeneity and diversity of the components in a fog-cloud system, many factors such as computation, storage, latency, power consumption, and costs affect the operational effectiveness of that system. This paper deals with the problem of task scheduling in fog-cloud systems. Thus, we formulated a general model of the fog-cloud system with different constraints and then applied metaheuristic methods to schedule data processing tasks in this environment to solve a multi-objective optimization problem. Our simulated experiments proved that the proposed fog-cloud model could be applied to practical use cases. Also, the use of metaheuristic algorithms for the task scheduling problem performs well compared to the traditional methods like Round-Robin.",65,https://doi.org/10.1109/NCA51143.2020.9306718,1,
1134,Towards Blockchainizing Land Valuation Certificate Management Procedures in Vietnam.,"In this paper, we concentrate on the process of managing the land valuation certificate using blockchain in Vietnam. To approach this problem, we base on the e-government application framework published by the Ministry of Information and Communication and Ministry of Natural Resources and Environment to develop a blockchain-based component which will be linked to the framework. Our component has three layers corresponding to three main layers of the architecture. After that, we develop a decentralized application serving the procedures of land valuation certificate management. With our application, certificates are stored and can be traced from the blockchain network. Our component hence helps create the transparency of management processes. We also carry out preliminary experiments using the proposed system to prove its feasibility during test operation processes. Our research also paves the way for applying blockchain technology to the state management field in general.",65,https://doi.org/10.1109/RIVF48685.2020.9140766,1,
1135,Realization Problem of Passivity-Based Cyber Physical Systems in Power Networks.,"In this paper, we deal with a realization problem of cyber physical systems. Cyber physical systems are known to include many heterogeneous dynamical modules such as physical systems, control rules, or optimization algorithms. The connections between modules, which can be different kind of subsystems, need ""transformation of signals"" between physical quantities and information signals. In the realization of such transformation, we should care about what kind of mechanism is possible or not for its purpose. For example, the passivity based methodology to connect subsystems is convenient for guaranteeing the stability of the total systems, however the problem of the physical realization of the feedback connection emerges. From above discussion, in this paper, we focus on our previously proposed hierarchical control and optimization of power networks based on passivity methodology and discuss a problem what structures are physically realizable.",65,https://ieeexplore.ieee.org/document/9240371,1,
1136,Glocal Control of Manufacturing Factories by Probability Control of AGV Transportation Mechanism: A Dissipativity Approach.,"This paper presents a glocal control of manufacturing systems which utilizes swarms of vehicles for transporting the materials in the network of multiple subprocesses. With respect to the increasing in system's size, the stochastic properties of transportation mechanism, and the transportation delay, the following question is non-trivial: How to locally balance the stock levels of the buffers in any process while globally stabilizing the overall networks? To this end, we model the overall system by the feedback connection of a deterministic dynamical system and a stochastic memoryless map. The stability of the system is guaranteed by a proposed dissipativity approach. Agent-based simulations show that both the global and local objectives are attained even if the transportation delay varies much longer than the control period.",65,https://ieeexplore.ieee.org/document/9240366,1,
1137,BML: A Data Mapping Language for Blockchain Platforms.,"Currently, all blockchain-based applications conduct two primary operations, i.e., writing data on blockchain networks and reading these data from the networks. These tasks require users to have considerable knowledge in blockchain technology, and they become even more challenging if users want to utilize different blockchain platforms to write and read data. So far, we have not had a uniform mechanism to perform write and read operations on various blockchain platforms. In addition, writing a huge amount of data on blockchain networks is a time-consuming task and requires considerable transaction fees. To address these issues, we present in the paper a data mapping language named BML. BML allows developers to uniformly define mappings for data transformation from traditional data storage mechanisms into blockchain networks. Conversely, this language also assists users in reading transformed data. Currently, BML accepts five input data sources, including XML, JSON, XLSX, SQL (relational database), NoSQL, and supports two output platforms, including Hyperledger Sawtooth and Ethereum",65,https://doi.org/10.1109/TrustCom50675.2020.00174,1,
1138,A Discussion on Stabilization of Frequency Control for Power Systems.,"How to practically maintain the frequency stability of large-scale power systems by a decentralized way is a simple but non-trivial question. In other words, is it possible to design any local controller without understanding the other controlled areas and with less understanding of network structure? With respect to the special properties of physical interaction between the local areas, this paper suggests two existing theories for tackling this issue. Firstly, passivity theory is shown to be a candidate for frequency control problem using swing equation. Based on the passivity of swing dynamics, it is possible to guarantee the system stability by designing for each local area a passive controller. We further extend the passivity approach to the hierarchically decentralized control system with unknown communication delay. Secondly, we discuss the application of generalized frequency variable (GFV) to the frequency control problem using area-control-error. Each local controller is designed such that each local subsystem follows a nominal model set. Utilizing GFV theory, we present a triad of conditions that sufficiently guarantee the system stability. The conditions can be tested conveniently by a limited set of inequalities established from the GFV and the eigenvalues of the physical interaction matrix. The effectiveness, limitation, and challenge of two theories are discussed by design examples with numerical simulations.",65,https://arxiv.org/abs/2010.05389,1,
1139,Distributed Control of Stochastic Manufacturing Processes Based on Consensus Algorithms.,"In this paper, we deal with a distributed control of stock levels of buffers in a large-scale manufacturing factory. We suppose that the manufacturing factory is composed of many production processes and transportation processes. Each production process has several input buffers to stock materials for production temporarily and an output buffer to stock the produced materials temporarily. The transportation process is assumed to be realized by a swarm of vehicles, which flexibly move between buffers and transport the materials conveniently. We model the production processes as deterministic dynamical systems and the transportation processes as stochastic events. Then, we propose a distributed control method for the production rates and the transportation probabilities by employing an idea of consensus algorithms and rigorously prove that all the normalized stock levels of the buffers are globally stable at consensus values in a probabilistic sense. We finally demonstrate the effectiveness of our proposed method by numerical simulations.",65,https://doi.org/10.23919/ACC.2019.8815071,1,
1140,An Information-centric Approach for Slice Monitoring from Edge Devices to Clouds.,"Internet of Things (IoT) has enabled physical devices and virtual objects to be connected to share data, coordinate, and automatically make smart decisions to server people. Recently, many IoT resource slicing studies that allow managing devices, IoT platforms, network functions, and clouds under a single unified programming interface have been proposed. Although they helped IoT developers to create IoT services more easily, the efforts still have not dealt with the monitoring problem for the slice components. This could cause an issue: thing states could not be tracked continuously, and hence the effectiveness of controlling the IoT components would be decreased significantly because of updated information lack. In this paper, we introduce an information-centric approach for multiple sources monitoring issue in IoT. The proposed model thus is designed to provide generic and extensible data format for diverse IoT objects. Through this model, IoT developers can build smart services smoothly without worrying about the diversity as well as heterogeneity of collected data. We also propose an overall monitoring architecture for the information-centric model to deploy in IoT environment and its monitoring API prototype. This document also presents our experiments and evaluations to prove feasibility of the proposals in practice.",65,https://doi.org/10.1016/j.procs.2018.04.046,1,
1141,Glocal Control of Load Frequency for Electrical Power Networks with Multiple Shared Model Set.,"Load frequency control of electrical power networks to balance the demand and supply based on automatic generation control (AGC) has been widely studied for years. As the power networks become more complex, we face the essential issue: How to design the load frequency control systematically to assure the stability of the overall system while optimizing the global/local control performances? To this end, in this paper, we propose a framework for glocal (global/local) control of load frequency. The command to each local governor includes two signals: the local signal through the corresponding area-control error (ACE) in the lower-layer, and the global signal distributed from the upper-layer by controlling the average frequency aggregation. We will clarify the role-sharing of the upper and lower layers via two model sets which are shared in both control layers. Finally, we demonstrate the trade-off between upper and lower performances through the volumes of model sets, and verify the effectiveness of the proposed method by simulations.",65,https://doi.org/10.1109/CCTA.2018.8511474,1,
1142,Managing Cloud Data Centers with Three-State Server Model Under Job Abandonment Phenomenon.,"In fact, to improve the quality of system service, for user job requests, which already have waited for a long time in queues of a busy cluster, cloud vendors often migrate the jobs to other available clusters. This strategy brings about the occurrence of job abandonment phenomenon in data centers, which disturbs the server management mechanisms in the manner of decreasing effectiveness control, increasing energy consumptions, and so on. In this paper, based on the three-state model proposed in previous works, we develop a novel model and its management strategy for cloud data centers using a finite queue. Our proposed model is tested in a simulated cloud environment using CloudSim. The achieved outcomes show that our three-state server model for data centers operates well under the job abandonment phenomenon.

",65,https://doi.org/10.1007/978-3-319-93713-7_63,1,
1143,PSPChord - A Novel Fault Tolerance Approach for P2P Overlay Network.,"In this paper, we propose a novel approach called PSPChord to provide efficient fault tolerance solution for Chord-based P2P overlay networks. In our proposal, the successor list is removed, instead, we design the partition-based data replication and modify finger tables. While the partition strategy is used to distribute data replicas evenly on Chord ring to reduce and balance the cost of lookup request, the finger table is added links to successor and predecessor of neighboring nodes to pass over faulty nodes. By simulating, our experiments already showed the performance of PSPChord as compared with original Chord in resolving fault tolerance problem on P2P overlay network.",65,https://doi.org/10.1007/978-3-030-05755-8_38,1,
1144,Hierarchically decentralized control for in-wheel-motored electric vehicles with global and local objectives.,"Motion control of electric vehicle has been widely studies for many years. Usually, the control system is designed hierarchically, including the upper-layer that performs car motion global objectives, and the lower-layer that controls the local actuators. However, to the best of our knowledge, there exists no systematic theory to analyze and design the vehicle system as a whole. Almost the published works treat the upper-layer and lower-layer separately and neglect the actuators' physical interaction. This paper proposes a novel hierarchically decentralized control for electric vehicle driven by multi in-wheel-motors. Our main idea is to clarify the role-sharing of the upper and lower layers via a shared model set which should be considered in both layers. We clearly represent the trade-off between the global and local objectives through the size of the model set. The effectiveness of the designed control system is verified using Carsim-Matlab simulation, where we show that the robustness of the overall system can be assured even if the driving force distribution to each local motor is time-varying.",65,https://doi.org/10.1109/ASCC.2017.8287337,1,
1145,Stability analysis of tire force distribution for multi-actuator electric vehicles using generalized frequency variable.,"Tire force distribution is an important topic in motion control of electric vehicles. For years, many researchers have focused on optimal distribution by minimizing a certain cost function. However, no effort has been paid to the stability analysis of tire force distribution theoretically. Let the actuators (in-wheel-motors and steering motors) be the local agents, the electric vehicle can be seen as a special type of multi-agent system. The agents are not decoupled but they physically interact with each other via the vehicle body. As the increasing of the actuator number, stability analysis becomes more and more complex. By modeling the EV system with generalized frequency variable, we propose a systematic scheme that can considerably reduce the burden of stability analysis for tire force distribution.",65,https://doi.org/10.1109/CCA.2016.7587827,1,
1147,Manufacturing of weather forecasting simulations on high performance infrastructures.,"The Weather Research and Forecasting (WRF) model represents a mesoscale numerical weather prediction system designed for both the atmospheric research and operational forecasting use. WRF offers several computationally efficient platforms while providing serial and parallel models running with or without multi-threading. The main goal of the work described in this paper was to develop management software tools facilitating the execution of WRF simulations on High Performance Computing (HPC) infrastructures. The management tools enable easily to carry out the complex simulation process, representing a workflow of central jobs, i.e. preprocessing, modeling and post-processing, which are running primarily on our local HPC cluster with the possibility to exploit also the computational power of the European Grid infrastructure. The second objective of this work was the investigation of parallel MPI and hybrid parallel MPI+OpenMP WRF models using the created tools to run simulations. The aim was to find out the most suitable combination of hardware and software configuration setting for the realization of the WRF simulation process with the given input scenario. Performance results of real simulation processes running on various hardware configurations of the cluster are presented.",65,https://doi.org/10.1109/eScience.2016.7870932,1,
1148,Fusion of large-time-delay measurement with non-delay measurement based on upper-bound scheme.,"This paper describes a filtering method for state estimation using the fusion of large-time-delay and non-delay measurements. To overcome the difficulty in covariance computation and keep the affordable computational cost, the norm-bound scheme is applied to obtain the estimation gain. The effectiveness of the proposed algorithm is verified through experiment with a visual servo system.",65,https://doi.org/10.1109/ICMECH.2015.7083978,1,
1149,Estimation and control of lateral displacement of electric vehicle using WPT information.,"This work proposes a method to estimate lateral displacement of an electric vehicle from wireless power transfer information and vehicles motion measurement through the Un-scented Kalman filter. By using the estimated lateral displacement as a feedback variable, the trajectory of vehicle is controlled to the area of high performance transmission. The proposed method is validated by simulation and experiment.",65,https://doi.org/10.1109/ICMECH.2015.7083997,1,
1151,Multirate Estimation and Control of Body Slip Angle for Electric Vehicles Based on Onboard Vision System.,"A new method for vehicle body-slip-angle estimation using nontraditional sensor configuration and system model is proposed, which enables robust estimation against vehicle parameter uncertainties. In this approach, a linear vehicle bicycle model is augmented with a simple visual model. As the visual model contains few uncertain parameters and increases the observer's design freedom, the combined model-based estimator provides more accurate estimation result compared with the traditional bicycle-model-based one. However, two issues are raised by the combined vehicle and vision models: 1) image processing introduces delay in the visual measurements, and 2) the sampling time of a normal camera is much longer than that of other onboard sensors. For electric vehicles, the control period of motors is much shorter than the sampling time of a normal camera. Considering the aforementioned delay and multirate problems, a multirate Kalman filter with intersample compensation is designed, and the estimation performance can be improved during the sampling intervals of the vision system. Then, a two-degree-of-freedom controller is designed using the estimated body slip angle as feedback for reference tracking. With the proposed multirate estimator, the controller achieves better tracking performance than the singlerate method. The effectiveness of the proposed estimator and controller is demonstrated by both simulations and experiments.",65,https://doi.org/10.1109/TIE.2013.2271596,1,
1152,Four wheel optimal autonomous steering for improving safety in emergency collision avoidance manoeuvres.,"The objective of this work is to demonstrate that autonomous collision avoidance manoeuvres based on optimal control motion planning improve the safety margins and can fully exploit the vehicle dynamics. In particular we analyse the performance of a traditional two-wheel-steering vehicle (2WS) versus a four-wheel-steering vehicle (4WS) in steer-to-avoid manoeuvres. We show how the vehicle manoeuvrability is improved using 4WS that allows to avoid an obstacle quite later with respect to the traditional 2WS vehicle. A control algorithm with novel features is also proposed to autonomously track the optimised manoeuvres, which has been successfully tested on a 4WS electric vehicle.",65,https://doi.org/10.1109/AMC.2014.6823309,1,
1153,Dual rate Kalman filter considering delayed measurement and its application in visual servo.,"This paper describes a Kalman filter for a class of linear system in which: 1) The sampling time of measurement is longer than the control period. 2) The measurement is delayed. Using the delayed measurement and the present and past time predicted state, the pseudo-measurement in present time is constructed. The optimal Kalman gain is designed to minimize the trace of the covariance of estimation error. A dual rate recursive algorithm is proposed to estimate the state synchronously with the control signal. A visual servo system is demonstrated to verify the effectiveness of the proposed method.",65,https://doi.org/10.1109/AMC.2014.6823331,1,
1155,Towards a semantic model of resource in cloud environment.,"Cloud computing has become fundamental technologies for most of IT-services today. This is due to primarily to its elastic nature: users or customers can acquire and release resources on-demand, and pay only for the resources they need (pay-per-use or pay-as-you-go model). Although the number and quality of cloud services are daily rising, but on the other aspect, the user requirements for the services also increase very fast and diversely. One of the prominent user demands is the need of a solution that can help them automatically choose suitable resources to deploy appliances into cloud environment. Based on the previous work presented in [1], in this paper, we go further to develop a primitive semantic model of cloud resources derived from multi-infrastructures. This semantic model allows users to create service and automatically deploy them to suitable cloud according to specific requirements of each. In this way, the approach enables appliance interoperability and elasticity among clouds. Our solution thus achieves value-added feature for cloud providers and simultaneously lowers barriers for users when using cloud computing.",65,https://doi.org/10.1145/2676585.2676610,1,
1156,Electric vehicle stability control based on disturbance accommodating Kalman filter using GPS.,"This paper describes a new electronic stability control system of electric vehicles based on sideslip angle estimation through Kalman filter. Vehicle course angle obtained from single antenna GPS receiver and yaw rate obtained from gyroscope are used as measurements for Kalman filter. By treating the combination of model errors and external disturbances as extended states in the Kalman filter algorithm, accurate sideslip angle estimation was achieved. A new scheme was proposed for integrating active front steering angle and yaw moment as control inputs. The extended states are utilized for disturbance rejection that improves the robustness of the control system.",65,https://doi.org/10.1109/ICMECH.2013.6518567,1,
1157,A novel approach for developing interoperable services in cloud environment.,"Cloud computing has seen a tremendous growth in the last five years. Along with the growth, many cloud models have been marketed. They deliver hardware and software as virtualization enabled services to users. Although cloud computing offers considerable advantages such as unlimited resources, manageability and lower investment costs but there are still barriers to exploit it. One of the barriers is the difficulties, which users are being faced when developing and deploying their own services into clouds. In this paper, we present a novel approach for developing interoperable services that can be deployed in different cloud infrastructures at the same time. The approach provides an instrument with emphasis on abstraction, inheritance and code reuse. Using the approach, cloud-based services are developed easily by extending existing abstractions classes provided by the instrument or other developers. The interoperability between different clouds is solved via the basic abstraction class of the instrument and all services are inherited and benefited from this advantage.",65,https://doi.org/10.1109/ICOIN.2013.6496382,1,
1158,Fast and accurate vision-based positioning control employing multi-rate Kalman filter.,"To achieve fast and accurate positioning control, a controller based on a visual servo employing a multi-rate Kalman filter was implemented and experimentally evaluated. Although a visual servo is expected to achieve robust positioning control by eliminating positional errors due to operation environments, applying visual information to positioning control faces three challenges, namely, lower sampling speed, longer delay, and lower positional accuracy than those possible by using a conventional position encoder. To overcome these problems, a multi-rate Kalman filter with a time-delay compensation technique is employed to combine visual information with encoder-based positional information. Two models describing a positioning-control experimental system using a one-axis linear stage with a camera were investigated to determine the optimum structure of the Kalman filter. Evaluation of the experimental system showed that the designed controller achieves high-precision (±10 µm) positioning control within approximately 10-ms settling time while simultaneously adjusting the environmental positional errors.",65,https://doi.org/10.1109/IECON.2013.6700201,1,
1159,Vision-based integrated lateral control system for electric vehicles considering multi-rate and measurable uneven time delay issues.,"Driver assistance systems such as automatic steering for lane keeping are of particular importance for vehicle's lateral safety, and onboard look-ahead cameras are widely employed to acquire surrounding information for realization of such applications. In fact, vision-based lane keeping and automatic steering have been intensively studied in the past few decades. Meanwhile, electric vehicles, as green solutions for future transportation, are gaining increasing attentions nowadays. From the viewpoint of motion control, differential torque between left and right in-wheel-motors is considered as an effective method for lateral safety control of electric vehicles. Nevertheless, the sampling rate of normal cameras is 30 Hz, which is much slower compared with that of motors and other kinds of onboard sensors. Moreover, image processing brings uneven time delay (which is usually measurable) into the visual signals, which further complicate the system sampling sequence. Many previous research simply adapt the whole system's sampling frequency to the camera; however, the held and random delayed feedbacks deteriorate system performance and may cause instability. In this paper, the two problems are solved by a multi-rate Kalman filter with random measurement delays compensation using a residual estimation technique.",65,https://doi.org/10.1109/ISIE.2013.6563772,1,
1160,An Abstraction Model for High-Level Application Programming on the Cloud.,"One of the biggest obstacles in the widespread take-up of cloud computing is the difficulties that users meet in developing and deploying applications into clouds. Although PaaS cloud type offers advanced features such as available platform, automatic scaling, it ties users/developers into certain technologies provided by vendors. Meanwhile, due to the gap of a suitable programming environment for IaaS cloud type, the development and deployment applications into IaaS clouds become a complex task. In this paper, we present a novel abstraction model for programming and deploying applications on the cloud. The approach also enables service migration and interoperability among different clouds.",65,https://doi.org/10.1007/978-3-319-00293-4_22,1,
1162,Vision-based vehicle body slip angle estimation with multi-rate Kalman filter considering time delay.,"Body slip angle is one of the most important information for vehicle motion control; as specific sensors for body slip angle measurement are expensive, it is necessary to investigate estimation methods using existing popular sensors such as gyro sensor, encoder, camera, etc. For EV (electric vehicle), in particular, the motor response is several milliseconds which enables high performance control with short control period; fast signal feedback is consequently desired. Nevertheless, the sampling rate of a normal camera is much slower compared with other kinds of onboard sensors and the time delay caused by image processing cannot be neglected. In this paper, the two problems are solved using a multi-rate Kalman filter with measurement delay included; the estimated body slip angle can be updated every 1 ms. First of all, vehicle model and visual model are explained followed with experimental setup introduction; then, real-time image processing techniques are briefly introduced; and then, single-rate and multi-rate Kalman filters considering time delay are designed to estimate body slip angle; finally, conclusion and further works are presented.",65,https://doi.org/10.1109/ISIE.2012.6237314,1,
1163,Abstraction layer for cloud computing.,"In this paper, we will present an abstraction layer for cloud computing. The abstraction layer will allow users to manipulate virtual machines as objects and simplify the process of porting applications to cloud computing. This approach could improve the flexibility of cloud computing, enable interoperability and simplify the creation of more complex systems in the cloud.
",65,http://www.scpe.org/index.php/scpe/article/view/731,1,
1165,Recent Advances in LWIR Type-II InAs/GaSb Superlattice Photodetectors and Focal Plane Arrays at the Center for Quantum Devices.,"In recent years, Type-II InAs/GaSb superlattice photodetectors have experienced significant improvements in material quality, structural designs, and imaging applications. They now appear to be a possible alternative to the state-of-the-art HgCdTe (MCT) technology in the long (LWIR) and very long wavelength infrared regimes. At the Center for Quantum Devices, we have successfully realized very high quantum efficiency, very high dynamic differential resistance R 0 A-product LWIR Type-II InAs/GaSb superlattice photodiodes with efficient surface passivation techniques. The demonstration of high-quality LWIR focal plane arrays that were 100% fabricated in-house reaffirms the pioneer position of this university-based laboratory.",65,https://doi.org/10.1109/JPROC.2009.2017108,1,
1167,Improved Genetic Algorithm for Solving Optimal Communication Spanning Tree Problem.,"Optimal Communication Spanning Tree (OCST) is a well-known NP-hard problem on the graph that seeks for the spanning tree with the lowest cost. The tree cost depends on the communication volume between each pair of nodes. This paper proposed an improved Genetic Algorithm combining with Ahujia and Murty’s Tree Improvement Procedure. The proposed algorithm was experimented on known benchmark tests which used in many papers related to OCST problem, and random instances from 200 to 500 vertexes. The experimental results show that the proposed algorithm is better than the heuristic and out-performance the most recent evolutionary algorithm approaches.",67,https://doi.org/10.1007/978-3-642-37502-6_49,1,
1168,Modeling and debugging numerical constraints of cyber-physical systems design.,"To design and analyze Cyber-Physical Systems (CPSs), engineers should consider computation and physical processes at the same time. Engineers require a design description which is easy to understand. SysML provides diagrammatic notations which are independent of specific disciplines. To express numerical constraints of SysML diagrams, we extend OCL with real arithmetic. Both SysML and extended-OCL require an executable design language to check inconsistencies of constraints. To achieve the above idea, we bridge a gap between SysML and VDM-RT. We propose a framework to embed SysML/extended-OCL into VDM-RT, where we prepare some libraries for extended-OCL. It is unnecessary for engineers to know VDM-RT. Using a unified CPS design description, our approach automatically debugs inconsistencies in whole CPS design. This paper illustrates the proposed method with a car race system.",68,https://doi.org/10.1145/2542050.2542068,1,
1169,Linked widgets: An approach to exploit open government data,"More and more governments publish Open Government Data (OGD) for their citizens, which receives great interest; because they are not only covering many domains, but also are meaningful and trustful. Since the data are not ready to be linked to other sources, some governments have started to follow the Linked Open Data paradigm as a mean to improve data quality. However, exploiting Linked Data is still a challenging task, which requires a profound understanding of underlying datasets, as well as considerable technical expertise. In this paper, we present an approach to remove the barrier between end users and Linked Data generally, and Open Government Linked Data particularly.",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:d1gkVwhDpl0C,,1
1170,Open Linked Widgets Mashup Platform.,"Since the emergence of the mashup concept on the web around 2005, a large stream of academic research and industrial development resulted in numerous architecture proposals, platforms and editing tools. This strong initial interest in mashup technologies and promising use case demonstrations notwithstanding, however, commercial platforms such as Microsoft Popfly, IBM Mashup Center, and Google Mashup Editor failed to gain widespread adoption by consumers and enterprises and were eventually discontinued. This failure may be attributed to a number of common limitations of these platforms:(i) they are each useful only for a single or a limited number of restricted problems in specific domains;(ii) they are closed, ie, developers cannot contribute and share their widgets;(iii) widgets, which are crucial elements of any mashup platform, are usually not modeled in sufficient semantic detail to support widget search and composition features that facilitate reuse. This paper addresses these limitations by introducing an open mashup platform based on semantic web technologies. We present a novel architecture in which widgets equipped with a semantic, graph-based model can cooperate with each other in a mashup created by end users through simple drag and drop operations. Widgets created freely by independent developers and hosted on arbitrary servers can be discovered and combined easily through our introduced semantic search feature.",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:qjMakFHDy7sC,,1
1171,Distributed mashups: a collaborative approach to data integration," – This paper aims to present a collaborative mashup platform for dynamic integration of heterogeneous data sources. The platform encourages sharing and connects data publishers, integrators, developers and end users. ",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:zYLM7Y9cAGgC,,1
1172,Towards an Environmental Information System for Semantic Stream Data.,"The future of the earth’s environmental systems will to a major extent be decided in cities as already more than 50% of the population is concentrated there. Pervasively available sensors and the data they generate can help to address pressing environmental challenges in urban areas by making crucial information available to researchers and decision-makers. However, environmental data is at present typically stored in disparate systems and formats, which inhibits reuse and recombination. Furthermore, the large amounts of environmental data that stream in continuously require novel processing approaches. So far, however, research at the intersection of environmental sciences and urban data to overcome these barriers has been scarce. To address these issues, we develop a novel framework using semantic web technologies. We apply ontological concepts and semantic stream processing technologies in order to facilitate combination, comparison, and visualization of heterogeneous data from various sources. The platform for environmental data stream analysis introduced in this paper can inform and support decision-making by non-expert users. We propose and discuss a three-step framework and outline initial results.",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:W7OEmFMy1HYC,,1
1173,Blockchain for Education: Verification and Management of Lifelong Learning Data.,"In recent years, blockchain technology has been applied in the educational domain because of its salient advantages, i.e., transparency, decentralization, and immutability. Available systems typically use public blockchain networks such as Ethereum and Bitcoin to store learning results. However, the cost of writing data on these networks is significant, making educational institutions limit data sent to the target network, typically containing only hash codes of the issued certificates. In this paper, we present a system based on a private blockchain network for lifelong learning data authentication and management named B4E (Blockchain For Education). B4E stores not only certificates but also learners’ training data such as transcripts and educational programs in order to create a complete record of the lifelong education of each user and verify certificates that they have obtained. As a result, B4E can address two types of fake certificates, i.e., certificates printed by unlawful organizations and certificates issued by educational institutions for learners who have not met the training requirements. In addition, B4E is designed to allow all participants to easily deploy software packages to manage, share, and check stored information without depending on a single point of access. As such, the system enhances the transparency and reliability of the stored data. Our experiments show that B4E meets expectations for deployment in reality.",70,https://doi.org/10.32604/csse.2022.023508,1,
1174,A Novel Approach for Developing Decentralized Storage and Sharing Systems.,"In recent years, the proliferation of blockchain technology has opened many research directions. In this context, the combination of blockchain-based techniques and traditional methods to improve existing systems has received significant interest from researchers. In this paper, we present a decentralized storage and sharing system based on a combination of IPFS (Inter-Planetary File System), encryption technologies (including Advanced Encryption Standard (AES), Elliptic Curve Cryptosystem (ECC), ABE (Attribute-based Encryption), Multi-Authority ABE (MA-ABE)), and multichain. In particular, we facilitate the advantages of the IPFS network to store user's data in a distributed manner. Furthermore, we make use of a cryptographer to protect the privacy of data. The hash returned by the IPFS network will be stored in our multichain architecture to provide transparency for all users participating in the system. To the best of our knowledge, it is the first storage and sharing system using IPFS, cryptographer, and multichain to ensure decentralized, trustworthy, transparent characteristics for storing and sharing data.",70,https://doi.org/10.1145/3457337.3457845,1,
1175,MPoC - A Metaheuristic Proof of Criteria Consensus Protocol for Blockchain Network.,"Over the last years, many blockchain platforms opt for Delegated Proof-of-Stake (DPoS) consensus protocol for their network deployment. However, using the DPoS, nodes on the network tend to vote for the nodes that are more likely to become block producers after several voting rounds. Hence, with DPoS, the number of block producers is quite small compared to the total number of nodes in the blockchain network. Besides, with DPoS consensus, the distribution of block producers often centralizes into several individual nodes. This paper proposes a novel consensus protocol called Meta-heuristic Proof of Criteria (MPoC) to overcome the disadvantages. Our protocol enables multiple operational criteria to evaluate all blockchain nodes during the block producer selection process. We apply meta-heuristic algorithms to optimize the decentralized level of the node selection based on the set of criteria weights to increase the democracy of the blockchain network.We carry out experiments to evaluate the block producer decentralization of MPoC as compared with DPoS. The achieved results show that our consensus protocol improves the decentralization of block producers as expected. We also show the feasibility of applying different meta-heuristic algorithms to our consensus protocol of blockchain through experiments.",70,https://doi.org/10.1109/ICBC51069.2021.9461113,1,
1176,"A Novel Model Using CDN, P2P, and IPFS for Content Delivery.","The use of multimedia data is increasingly popular due to their outstanding advantages compared to traditional textual data. The demand for live streaming and the proliferation of new standards such as Full High Definition (FHD) and Ultra High Definition(UHD) makes it increasingly difficult to satisfy users. To overcome this challenge, content providers can utilize a hybrid architecture relying on a Content Delivery Network (CDN) and Peer-to-Peer communications. Many efforts have been conducted in order to evaluate and improve the performance of this architecture. However, research on applying blockchain-related technologies to the problem of content delivery is still scarce. In this context, our paper aims to investigate the ability of applying InterPlanetary File system (IPFS) - one of the salient technologies related to blockchain to the traditional hybrid architecture. We conduct different experiments to provide preliminary evaluations of the performance of the proposed system. Experimental results show that the proposed architecture is promising and effective in providing content delivery service.",70,https://doi.org/10.1007/978-981-33-4370-2_4,1,
1177,Exploration and Integration of Job Portals in Vietnam.,"In recent years, a huge number of job portals have been published on the web. Published job advertisements not only open a great opportunity for learners in identifying their job chances in the future, but also support lecturers and education managers in determining contents needed to be improved in curriculum. Although initial steps towards web of Vietnam's labour market have been established, a united system which can collect, integrate, and analyze job advertisements from different sources has not existed yet. To address this problem, we introduce a system named VNJobSpace. VNJobSpace is composed by six main components which are responsible for collecting, integrating and analyzing job advertisements. In addition, it contains three databases to store job data, schema of each source, and collected curricula. At present, VNJobSpace provides access to more than 700,000 advertisements from 14 job portals in Vietnam. End users can search collected information, obtain prediction for job opportunities, and receive recommendations on essential courses to obtain a desired job",70,https://doi.org/10.1109/RIVF48685.2020.9140732,1,
1178,BML: A Data Mapping Language for Blockchain Platforms.,"Currently, all blockchain-based applications conduct two primary operations, i.e., writing data on blockchain networks and reading these data from the networks. These tasks require users to have considerable knowledge in blockchain technology, and they become even more challenging if users want to utilize different blockchain platforms to write and read data. So far, we have not had a uniform mechanism to perform write and read operations on various blockchain platforms. In addition, writing a huge amount of data on blockchain networks is a time-consuming task and requires considerable transaction fees. To address these issues, we present in the paper a data mapping language named BML. BML allows developers to uniformly define mappings for data transformation from traditional data storage mechanisms into blockchain networks. Conversely, this language also assists users in reading transformed data. Currently, BML accepts five input data sources, including XML, JSON, XLSX, SQL (relational database), NoSQL, and supports two output platforms, including Hyperledger Sawtooth and Ethereum.",70,https://doi.org/10.1109/TrustCom50675.2020.00174,1,
1179,Towards Open Data Mashups for Data Journalism.,"In line with a broad social and political movement in recent years, public and private sector actors have started to open up their data and to publish it on various portals. However, the actual use of the available open data sets is still rather limited in many sectors. Consequently, the potentials of available open data sets are not fully realized, which may discourage open data publishers. To encourage a broader adoption of open data and to contribute towards making democratic processes more transparent, critical data journalism is essential. In this demo paper, we propose an approach called Open Data Mashups for Data Journalism (ODMOJO) that aims to bridge open data publishers and their consumers, i.e., journalists and society at large, with Linked Data technologies. Specifically, our approach will facilitate the access, reuse, and integration of open data for General Data Journalism. We plan to evaluate our approach with potential journalism partners as well as open data publishers in Austria, to foster further adoption and utilization of open data through data journalism and Linked Data technologies.",70,http://ceur-ws.org/Vol-2044/paper17/,1,
1180,Linked Widgets Platform for Rapid Collaborative Semantic Mashup Development.,"In recent years, data has become vital in supporting our everyday lives. Along with large volumes of open data available on the web, various types of public, private, and enterprise data are stored in the cloud or distributed over multiple devices. The value of this data would increase drastically if we were able to integrate it. This would enable more sophisticated presentation and analysis of previously disparate data. So far, however, it is challenging for non-expert users to efficiently make use of such data because (i) data heterogeneity hampers integration of different kinds of data that are stored in various formats and spread among storage infrastructures; (ii) manual data integration processes are typically neither reproducible, nor reusable; and (iii) the lack of support for exploration does not allow for the integration of arbitrary data sources. This paper tackles these challenges by introducing a mashup platform that combines semantic web and mashup concepts to help users obtain insights and make informed decisions. To this end, we leverage a semantic model of mashup components for automated techniques that support the user in exploring available data. Moreover, we introduce a collaborative and distributed model to create and execute mashups. This facilitates distributed ad-hoc integration of heterogeneous data contributed by multiple stakeholders.",70,https://doi.org/10.1007/978-3-319-53174-8_4,1,
1181,Leveraging the web of data via linked widgets.,"Machine-readable datasets that have increasingly become available in open formats in recent years have great potential as a foundation for innovative applications and services. Linked Data in particular-a set of best practices for publishing and connecting structured data on the Web-has facilitated significant progress in evolving the Web of documents into a Web of Data. However, although this concept has opened up many opportunities for data sharing and collaboration, integrating data is still a challenging task that requires considerable technical expertise and a profound understanding of the underlying datasets. In this paper, we introduce a novel approach to provide knowledge workers with the necessary tools to leverage the fast growing Linked Data Cloud by creating semantic-aware dataflow processes. To this end, we introduce the “Linked Widget” concept as an enhancement of standard Web Widgets. These widgets are based on a semantic data model that facilitates powerful mechanisms for gathering, processing, integration, and visualization of data in a user-friendly Mashup environment. By allowing knowledge workers to easily create complex Linked Data applications in an adhoc manner, our approach should contribute towards reducing existing barriers of Linked Data adoption.",70,https://doi.org/10.1007/s12927-014-0001-9,1,
1182,Multiscale Exploration of Spatial Statistical Datasets: A Linked Data Mashup Approach.,". Many national and international organizations today leverage semantic web technologies to make statistical datasets available as
Linked Open Data (LOD). A key advantages of this approach is that
the data not only becomes publicly available, but also machine-readable
and hence suitable for automated discovery and exploration. Whereas
this has great potential to support interesting use cases, it remains difficult for end users today to utilize and combine these statistical Linked
Data. Three challenges are: (i) directing users to relevant data sources
based on a specified location; (ii) facilitating data integration despite a
lack of outgoing links between datasets; and (iii) offering flexible means
to integrate and aggregate data from various sources. As time and location are highly relevant dimensions in most statistical data, we address
the identified challenges by first constructing geographical metadata for
statistical sources. Following a mashup approach, we introduce mechanisms to recommend interesting datasets to end users and automatically
enable data integration, visualization, and comparisons based on userdefined criteria.",70,http://ceur-ws.org/Vol-1550/article-08.pdf,1,
1183,Open Mashup Platform - A Smart Data Exploration Environment.,"The number of applications designed around Linked Open
Data (LOD) has expanded rapidly in recent years. However, these applications typically do not make use of the vast amounts of LOD datasets,
but only provide access to predefined, domain-specific subsets. Exceptions that do allow for more flexible exploration of LOD are not targeted
at end users, which excludes users who have limited experience with
Semantic Web technologies from realizing the potential of the so-called
LOD cloud. This paper introduces a Mashup Platform that models, manages, reuses, and interconnects LOD web applications, thereby encouraging initiative and creativity of potential users. Figuratively, our approach
allows developers to implement building blocks whereas the platform provides the cement so that end users can build houses by themselves.",70,http://ceur-ws.org/Vol-1272/paper_45.pdf,1,
1184,Implementing Linked Widgets: lessons learned for Linked Data developers.,"Seven years after Linked Data has been introduced as a concept to publish data on the web, an abundant cloud of Linked Open Data
(LOD) built upon standard web technologies has emerged. To facilitate
and encourage widespread use of that data, a critical step is now to
streamline the process for creating applications on top of LOD. This paper discusses lessons learned while developing an open standards-based
platform that aims to achieve that by means of Linked Widgets. Whereas
resources are already connected in the LOD cloud, Linked Widgets in a
similar vein aim to alleviate LOD application development in an open
and interlinked fashion. Through reuse, we aim to foster both users’ and
developers’ productivity and creativity",70,http://ceur-ws.org/Vol-1268/paper5.pdf,1,
1185,Exploring linked statistical data using linked widgets.,"The Open Data movement has gained momentum among governments, in the business world, and in the public sector in recent years. This movement has resulted in a growing number of open and accessible datasets that have established a solid basis for enhanced service offerings and improved experiences for citizens and businesses. Statistical data, which embodies a big portion of Open Data, comprises a wide range of domains including finance, demographics, transportation, employment, etc. Statistical data plays an important role in public policy formation and as a facilitator for informed decision-making in the private sector. Linked Statistical Data is an evolving concept that combines the richness of Linked Data (a set of best practices for publishing and connecting structured data on the Web) with the descriptiveness of statistical data to integrate data from multiple sources and put it in a semantic context. In this short paper, Linked Statistical Data limitations and challenges are explored before introducing Linked Widgets as an innovative approach.",70,https://doi.org/10.1145/2676585.2676589,1,
1186,Deep neural network based learning to rank for address standardization.,"Address standardization is the process of converting and mapping free-form addresses into a standard structured format. For many business cases, the addresses are entered into the information systems by end-users. They are often noisy, uncompleted, and in different formatted styles. In this paper, we propose a deep learning-based approach to the address standardization challenge. Our key idea is to leverage a Siamese neural network model to embed raw inputs and standardized addresses into a single latent multi-dimensional space. Thus, the corresponding of the raw input address is the one with the highest-ranking score. Our experiments demonstrate that our best model achieved 95.41% accuracy, which is 6.6% improvement from the current state of the art.",71,https://doi.org/10.1109/RIVF51545.2021.9642079,1,
1187,A Novel Conditional Random Fields Aided Fuzzy Matching in Vietnamese Address Standardization.,"Address standardization is the process of recognizing and normalizing free-form addresses into a common standard format. In today's digital economy, this process is increasingly challenging such as in ecommerce fulfillment, logistic planning, geographical data analysis, real-estate, and social network mining, etc. Traditional approaches mostly follow two directions: Named Entity Recognition (NER) and fuzzy matching. Particularly, for Vietnamese address, neither these two approaches are efficient due to sparse and erroneous data. In this paper, we propose a novel approach that leverages NER model as a suggestion to re-rank potential address candidates obtained by the fuzzy matching stage. We develop a log-linear model for this re-ranking purpose. Our experiments showed that it outperforms both NER and fuzzy matching approaches with an accuracy of 88%, and suggested further applications on different language data.",71,https://doi.org/10.1145/3368926.3369687,1,
1188,Machine Translation between Vietnamese and English: an Empirical Study.,"Machine translation is shifting to an end-to-end approach based on deep neural networks. The state of the art achieves impressive results for popular language pairs such as English - French or English - Chinese. However for English - Vietnamese the shortage of parallel corpora and expensive hyper-parameter search present practical challenges to neural-based approaches. This paper highlights our efforts on improving English-Vietnamese translations in two directions: (1) Building the largest open Vietnamese - English corpus to date, and (2) Extensive experiments with the latest neural models to achieve the highest BLEU scores. Our experiments provide practical examples of effectively employing different neural machine translation models with low-resource language pairs.
",71,http://arxiv.org/abs/1810.12557,1,
1189,Towards State-of-the-art English-Vietnamese Neural Machine Translation.,"Machine translation is one of the most challenging topics in natural language processing. The common approaches to machine translation base on either statistical or rule-based methods. Rule-based translation analyzes sentence structures, requires extensive lexicons with morphological, syntactic, semantic information, and large sets of manually created rules. Statistics-based translation faces the challenge of collecting bilingual text corpora, which is particularly difficult for low resource language pairs as English-Vietnamese. This research aims at building state-of-the-art English-Vietnamese machine translation. Our contribution includes: (1) an enormous effort in collecting training dataset, (2) adaptation of current neural machine for English-Vietnamese translation, (3) an experimental result suggested the unnecessary of Vietnamese word segmentation as a common pre-processing step. Our model achieves a highest BLEU score in comparison with other researches.",71,https://doi.org/10.1145/3155133.3155205,1,
1190,Large-scale geographically weighted regression on Spark.,"Geographically Weighted Regression (GWR) is a local version of spatial regression that captures spatial dependency in regression analysis. GWR has many application in practice as a visualization and prediction tool for spatial exploration (e.g in climate, economy, medical). However, this locally regression model is slow in process upon the volume of calculations and the spatial getting bigger. Improving performance of GWR is a critical issue, but their distributed implementations have not been studied. Recently, with the advent of Spark as well MapReduce framework, the development of machine learning applications and parallel programming becomes easier. In this article, we propose several large-scale implementations of distributed GWR, leveraging Spark framework. We implemented and evaluated these approaches with large datasets. To our best knowledge, this is the first work addressing GWR at large-scale.",71,https://doi.org/10.1109/KSE.2016.7758041,1,
1191,A Vietnamese language model based on Recurrent Neural Network.,"Language modeling plays a critical role in many natural language processing (NLP) tasks such as text prediction, machine translation and speech recognition. Traditional statistical language models (e.g. n-gram models) can only offer words that have been seen before and can not capture long word context. Neural language model provides a promising solution to surpass this shortcoming of statistical language model. This paper investigates Recurrent Neural Networks (RNNs) language model for Vietnamese, at character and syllable-levels. Experiments were conducted on a large dataset of 24M syllables, constructed from 1,500 movie subtitles. The experimental results show that our RNN-based language models yield reasonable performance on the movie subtitle dataset. Concretely, our models outperform n-gram language models in term of perplexity score.",71,https://doi.org/10.1109/KSE.2016.7758066,1,
1192,GPSInsights: Towards an efficient framework for storing and mining massive vehicle location data.,"Intelligent Transport System (ITS) has seen growing interest in collecting vehicle location data in order to build up real-time traffic monitoring and analytic systems. However handling these data creates challenges, as they are massive in volume and arriving in near real-time. In this paper, we proposed GPSInsights, a distributed system that is scalable and efficient in processing huge volume of location data stream. GPSInsights is built up on open-source, scalable and distributed components. We demonstrated our system with a scalable map matching implementation and performed experiments with real big data sets.",71,https://doi.org/10.1145/2833258.2833282,1,
1194,Towards scalable array-oriented active storage: the pyramid approach.,"The recent explosion in data sizes manipulated by distributed scientific applications has prompted the need to develop specialized storage systems capable to deal with specific access patterns in a scalable fashion. In this context, a large class of applications focuses on parallel array processing: small parts of huge multi-dimensional arrays are concurrently accessed by a large number of clients, both for reading and writing. A specialized storage system that deals with such an access pattern faces several challenges at the level of data/metadata management. We introduce Pyramid, an active arrayoriented storage system that addresses these challenges. Experimental evaluation demonstrates substantial scalability improvements brought by Pyramid with respect to state-ofart approaches both in weak and strong scaling scenarios, with gains of 100% to 150%.",71,https://doi.org/10.1145/2146382.2146387,1,
1195,Efficient Support for MPI-I/O Atomicity Based on Versioning.,"We consider the challenge of building data management systems that meet an important requirement of today's data-intensive HPC applications: to provide a high I/O throughput while supporting highly concurrent data accesses. In this context, many applications rely on MPI-I/O and require atomic, non-contiguous I/O operations that concurrently access shared data. In most existing implementations, the atomicity requirement is implemented through locking-based schemes, which have proven inefficient, especially for non-contiguous I/O. We claim that using a versioning-enabled storage back-end has the potential to avoid the expensive synchronization induced by locking-based schemes. We describe a prototype implementation on top of ROMIO, and report on promising experimental results with standard MPI-I/O benchmarks specifically designed to evaluate the performance of non-contiguous, overlapped I/O accesses under MPI atomicity guarantees",71,https://doi.org/10.1109/CCGrid.2011.60,1,
1196,Towards a Storage Backend Optimized for Atomic MPI-I/O for Parallel Scientific Applications.,"We consider the challenges of building data management systems that are optimized for parallel I/O of scientific applications. One of those challenges focuses on the requirement of high I/O throughput for concurrent data accesses while obeying MPI atomicity semantics. In most existing implementations, the atomicity is often implemented through locking-based schemes, which have proven inefficient. In this proposal, we argue that a novel versioning-based scheme can avoid the need to perform expensive synchronization by using multiple snapshots of the same data, which is much more efficient. We describe our prototype of a versioning-based storage back-end and report on promising experimental results.",71,https://doi.org/10.1109/IPDPS.2011.376,1,
1197,Towards a Grid File System Based on a Large-Scale BLOB Management Service.,"This paper addresses the problem of building a grid file system for applications that need to manipulate huge data, distributed and concurrently accessed at a very large scale. In this paper we explore how this goal could be reached through a cooperation between the Gfarm grid file system and BlobSeer, a distributed object management system specifically designed for huge data management under heavy concurrency. The resulting BLOB-based grid file system exhibits scalable file access performance in scenarios where huge files are subject to massive, concurrent, fine-grain accesses. This is demonstrated through preliminary experiments of our prototype, conducted on the Grid’5000 testbed.",71,https://doi.org/10.1007/978-1-4419-6794-7_2,1,
1198,Widget-based Exploration of Linked Statistical Data Spaces.,"Today, public statistical data plays an increasingly important role both in public policy formation and as a facilitator for informed decision-making in the private sector. In line with the increasing adoption of open data policies, the amount of data published by governments and organizations on the web is growing rapidly. To increase the value of such data, the W3C recommends the RDF Data Cube Vocabulary to facilitate the publication of data in a more structured and interlinked manner. Although important first steps toward building a web of statistical Linked Datasets have been made, providing adequate facilities for end users to interactively explore and make use of the published data remains an unresolved challenge. This paper presents a widgetbased approach to deal with this issue. In particular, we introduce a mashup platform that allows users lacking advanced skills and knowledge of Semantic Web technologies to interactively analyze datasets through widget compositions and visualizations. Furthermore, we provide mechanisms for the interconnection of datasets to support sophisticated knowledge extraction.",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:u5HHmVD_uO8C,,1
1199,BlazeNeo: Blazing Fast Polyp Segmentation and Neoplasm Detection.,"In recent years, computer-aided automatic polyp segmentation and neoplasm detection have been an emerging topic in medical image analysis, providing valuable support to colonoscopy procedures. Attentions have been paid to improving the accuracy of polyp detection and segmentation. However, not much focus has been given to latency and throughput for performing these tasks on dedicated devices, which can be crucial for practical applications. This paper introduces a novel deep neural network architecture called BlazeNeo, for the task of polyp segmentation and neoplasm detection with an emphasis on compactness and speed while maintaining high accuracy. The model leverages the highly efficient HarDNet backbone alongside lightweight Receptive Field Blocks and a feature aggregation mechanism for computational efficiency. An auxiliary training strategy is proposed to take full advantage of the training data for the segmentation quality. Our experiments on a challenging dataset show that BlazeNeo achieves improvements in latency and model size while maintaining comparable accuracy against state-of-the-art methods. We obtain over 155 fps while outperforming all compared models in terms of accuracy in INT8 precision when deploying on a dedicated edge device with a conventional configuration.",72,https://doi.org/10.1109/ACCESS.2022.3168693,1,
1200,An End-to-End Spatial-Aware Attention Method for Multi-Line License Plate Spotting.,"Previous scene text methods in text spotting often include two tasks: text detection and text recognition. Recently, some methods have been proposed to combine these two tasks into a unified model. However, most of these methods usually detect and recognize text line-by-line. This work proposes a novel spatial-aware attention method for multi-line text spotting and applies it to license plate recognition. Our method provides an end-to-end trainable architecture, significantly reducing the time needed to train the model and inference. Notably, our approach incorporates the 2D positional encoding into Transformer architecture for the recognition branch. The experiments show that our method is very efficient for complex license plate datasets with skew, curve, and multi-line texts.",72,https://doi.org/10.1145/3508072.3508197,1,
1201,NeoUNet : Towards Accurate Colon Polyp Segmentation and Neoplasm Detection.,"Automatic polyp segmentation has proven to be immensely helpful for endoscopy procedures, reducing the missing rate of adenoma detection for endoscopists while increasing efficiency. However, classifying a polyp as being neoplasm or not and segmenting it at the pixel level is still a challenging task for doctors to perform in a limited time. In this work, we propose a fine-grained formulation for the polyp segmentation problem. Our formulation aims to not only segment polyp regions, but also identify those at high risk of malignancy with high accuracy. We then present a UNet-based neural network architecture called NeoUNet, along with a hybrid loss function to solve this problem. Experiments show highly competitive results for NeoUNet on our benchmark dataset compared to existing polyp segmentation models.",72,https://doi.org/10.1007/978-3-030-90436-4_2,1,
1202,"FastTacotron: A Fast, Robust and Controllable Method for Speech Synthesis.","Recent state-of-the-art neural text-to-speech synthesis models have significantly improved the quality of synthesized speech. However, the previous methods have remained several problems. While autoregressive models suffer from slow inference speed, non-autoregressive models usually have a complicated, time and memory-consuming training pipeline. This paper proposes a novel model called FastTacotron, which is an improved text-to-speech method based on ForwardTacotron. The proposed model uses the recurrent Tacotron architecture but replacing its autoregressive attentive part with a single forward pass to accelerate the inference speed. The model also replaces the attention mechanism in Tacotron with a length regulator like the one in FastSpeech for parallel mel-spectrogram generation. Moreover, we introduce more prosodic information of speech (e.g., pitch, energy, and more accurate duration) as conditional inputs to make the duration predictor more accurate. Experiments show that our model matches state-of-the-art models in terms of speech quality and inference speed, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and possible to control the speed and pitch of the generated utterance. More importantly, our model can converge just in few hours of training, which is up to 11.2x times faster than existing methods. Furthermore, the memory requirement of our model grows linearly with sequence length, which makes it possible to predict complete articles at one time with the model. ",72,https://doi.org/10.1109/MAPR53640.2021.9585267,1,
1203,AG-ResUNet++: An Improved Encoder-Decoder Based Method for Polyp Segmentation in Colonoscopy Images.,"Colorectal cancer is one of the most prevalent causes of cancer-related death. Early polyp segmentation in colonoscopy is helpful in diagnosing and preventing colorectal cancer. However, this task a challenging due to variations in the appearance of polyps. This paper proposes a new encoder-decoder-based method called AG-ResUNet++ that leverages attention gate mechanism and residual connections to enhance the performance of the existing UNet++ model in the polyp segmentation task. Our method considerably outperforms other state-of-the-art methods on the popular polyp segmentation datasets, including KvasirSEG and CVC-612.",72,https://doi.org/10.1109/RIVF51545.2021.9642070,1,
1204,An Improved Deep Neural Network Based on a Novel Visual Attention Mechanism for Text Recognition.,"Text recognition from images captured by handheld mobile devices has attracted considerable research interest because of its commercial applications. The state-of-the-art printed text recognition methods are often based on attention mechanisms. However, these methods perform poorly on images captured due to poor illumination conditions, blur, noise, and low resolution. To address these unfavorable conditions, we propose a new text recognition method based on an encoder-decoder model. Particularly, we present a novel attention mechanism using a multi-scale cascade fashion combined with a channel attention gate module. Our model is also strengthened by an EfficientNet-like backbone. Extensive experiments on three popular datasets, including SROIE 2019, B-MOD, and CORD, show that our proposed method outperforms the baseline attention mechanism and achieves competitive accuracy compared to other state-ofthe-art approaches.",72,https://doi.org/10.1109/RIVF51545.2021.9642119,1,
1205,Efficient Brain Tumor Segmentation with Dilated Multi-fiber Network and Weighted Bi-directional Feature Pyramid Network.,"Brain tumor segmentation is critical for precise diagnosis and personalised treatment of brain cancer. Due to the recent success of deep learning, many deep learning based segmentation methods have been developed. However, most of them are computationally expensive due to complicated network architectures. Recently, multi-fiber networks were proposed to reduce the number of network parameters in U-Net based brain tumor segmentation through efficient graph convolution. However, the efficient use of multi-scale features has not been well explored between contracting and expanding paths except simple concatenation. In this paper, we propose a light-weight network where contracting and expanding paths are connected with fused multi-scale features through bi-directional feature pyramid network (BiFPN). The backbone of our proposed network has a dilated multi-fiber (DMF) structure based U-net architecture. First, conventional convolutional layers along the contracting and expanding paths are replaced with a DMF network and an MF network, respectively, to reduce the overall network size. In addition, a learnable weighted DMF network is utilized to take into account different receptive sizes effectively. Next, a weighted BiFPN is utilized to connect contracting and expanding paths, which enables more effective and efficient information flow between the two paths with multi-scale features. Note that the BiFPN block can be repeated as necessary. As a result, our proposed network is able to further reduce the network size without clearly compromising segmentation accuracy. Experimental results on the popular BraTS 2018 dataset demonstrate that our proposed light-weight architecture is able to achieve at least comparable results with the state-of-the-art methods with significantly reduced network complexity and computation time. The source code of this paper will be available at Github.",72,https://doi.org/10.1109/DICTA51227.2020.9363380,1,
1206,A Novel Generative Model to Synthesize Face Images for Pose-invariant Face Recognition.,"Face recognition is an active research area in computer vision, which has been widely used in various applications such as security, video surveillance and personal identification. Although recent studies in this field have achieved great successes, they usually require an enormous amount of data for training and yet still have difficulties in in-the-wild dataset due to large variation of pose, illumination, expression. Among these unconstrained conditions, pose variation is thought to be the factor that harms face recognition accuracy the most. In order to deal with pose variation, one can fulfill the incomplete UV map extracted from in-the-wild faces, then attach the completed UV map to a fitted 3D mesh and finally generate different 2D faces of arbitrary poses, which then can be used for training or testing face recognition models. In this paper, we propose a novel generative model called ResCUNet-GAN to improve UV map completion. Particularly, we improve the original UV-GAN by stacking two U-Nets and enhancing it with multiple-level residual connections and feature fusion. The experiments on the popular Multi-PIE dataset shows that our model outperforms the original UV-GAN model.",72,https://doi.org/10.1109/MAPR49794.2020.9237763,1,
1207,PCA-based 3D Facial Reenactment From Single Image.,"Facial re-enactment from videos and images is one of the essential tasks in character animation applied in the creation of video games and movies. In this paper, we propose an efficient method for 3D facial re-enactment for online facial expression transferring. By approximating the pose, the approximation of expression parameters is reduced to a linear regression problem of which the closed-form solution is readily available, resulting in the improvement in inference speed. We also test the proposed method with multiple variants of initial blendshapes estimation and compare the results with the prior in term of both quality and speed.",72,https://doi.org/10.1109/MAPR49794.2020.9237786,1,
1208,BK.Synapse: A scalable distributed training framework for deep learning.,"Training neural networks efficiently is a thoroughly-researched topic that plays an important role in their adoption. Major advancements have been made, including the use of multiple nodes to further decrease training time. However, training at scale usually means adding on multiple layers of complex deployment logic and parallelization concerns, distracting researchers from the core of their algorithms. This paper presents a framework called BK.Synapse that can facilitate distributed training while maintaining clarity, simplicity, and user-friendliness. The design is modular, allowing flexible and easy deployment on a variety of hardware specifications. The framework is benchmarked in a case study: training a neural network for an object detection problem. Our results show a good amount of improvements over conventional training, with very few modifications to the existing codebase. The resulting model also performs relatively well upon further testing.",72,https://doi.org/10.1145/3368926.3369690,1,
1209,Improving CRNN with EfficientNet-like feature extractor and multi-head attention for text recognition.,"Text recognition is one of the most important and challenging tasks in image-based sequence recognition, which has various potential applications in real life. In this paper, we propose a novel convolutional-recurrent neural network (CRNN) for text recognition. Particularly, we adapt the EfficientNet architecture for extracting deep features and propose multi-head attention mechanisms to improve character localization. The experiments show that our EfficientNet-like feature extractor clearly outperforms other previous CNN feature extractors like VGG and ResNet. In overall, our proposed method yields competitive performance in comparison with other state-of-the-art approaches. Specifically, our F1-score is equivalent to top 3 on the ICDAR 2019 Robust Reading Challenge on Scanned Receipts OCR and Information Extraction.",72,https://doi.org/10.1145/3368926.3369689,1,
1210,An Effective Ensemble Deep Learning Framework for Malware Detection.,"Malware (or malicious software) is any program or file that brings harm to a computer system. Malware includes computer viruses, worms, trojan horses, rootkit, adware, ransomware and spyware. Due to the explosive growth in number and variety of malware, the demand of improving automatic malware detection has increased. Machine learning approaches are a natural choice to deal with this problem since they can automatically discover hidden patterns in large-scale datasets to distinguish malware from benign. In this paper, we propose different deep neural network architectures from simple to advanced ones. We then fuse hand-crafted and deep features, and combine all models together to make an overall effective ensemble framework for malware detection. The experiment results demonstrate the efficiency of our proposed method, which is capable to detect malware with accuracy of 96.24% on our large real-life dataset.",72,https://doi.org/10.1145/3287921.3287971,1,
1211,Two-stream Deep Residual Learning with Fisher Criterion for Human Action Recognition.,"Action recognition is one of the most important areas in the computer vision community. Many previous work use two-stream CNN model to obtain both spatial and temporal clues for predicting task. However, two stream are trained separately and combined later by late fusion. This strategy has overlooked the spatial-temporal features interaction. In this paper, we propose new two-stream CNN architectures that are able to learn the relation between two kinds of features. Furthermore, they can be trained end-to-end with standard back propagation algorithm. We also introduce a Fisher loss that makes features more discriminative. The experiments show that Fisher loss yields higher accuracy than using only the softmax loss.",72,https://doi.org/10.1145/3287921.3287972,1,
1213,Label associated dictionary pair learning for face recognition.,"Dictionary learning (DL) has been successfully applied to various pattern classification tasks. Sparse coding has played a vital role in the success of such DL-based models. However, the popular sparsity constraints using l0 or l1-norm often make the training phase time-consuming. Recently, an emerging trend in using l2-norm has shown its advantages in both accuracy and computational speed. However, the supervised approach that exploits label information in the training phase has not been investigated in such l2-norm based methods. In this paper, we propose a novel supervised dictionary learning method that incorporates label information in the objective function. Based on that, we also propose an effective classification schema. Experiments on three popular face recognition datasets show that our method has promising results. Especially, our method has extremely fast speed in test phase, while maintaining competitive accuracy in comparison with other state-of-the-art models.",72,https://doi.org/10.1145/3011077.3011105,1,
1214,Colour image denoising based on a combined model.,"In this paper, we propose a method to remove noise in RGB-color images. This method is based on a total variation of intensity function of images. Here, the proposed method is a developed version of our previous method to remove a linear combination of Gaussian and Poisson noises in grayscale images. This method works well with the wide range of proportion of Poisson and Gaussian noises. We show here, the developed version can be used to well approximate real noises in color raster images.",72,https://doi.org/10.1145/3011077.3011125,1,
1215,Uniform Detection in Social Image Streams.,"Social media mining from Internet has been an emerging research topic. The problem is challenging because of massive data contents from various sources, especially image data from user upload. In recent years, dictionary learning based image classification has been widely studied and gained significant success. In this paper, we propose a framework for automatic detection of interested uniforms in image streams from social networks. The systems is composed of a powerful feature extraction module based on dense SIFT feature and a state-of-the-art discriminative dictionary learning approach. Beside that, a parallel implementation of feature extraction is deployed to make the system work real time. An extensive set of experiments has been conducted on four real-life datasets. The experimental results show that we can obtain the detection rate up to 100% on some datasets. We also get real time performance with a speed of image stream of about 40 images per second. The framework can be applied to emerging applications such as uniform detection, automated image tagging, content base image retrieval or online advertisement based on image content.",72,https://doi.org/10.1109/KSE.2015.63,1,
1216,A Study on Non-sparse Dictionary Learning for Pattern Classification.,"Dictionary learning (DL) approach has been successfully applied to many pattern classification problems. Sparse property has played an important role in the success of DL-based classification models. However, the sparsity constraints make the learning problem expensive. Recently, there has been an emerged trend in relaxing the sparsity constraints by using L2-norm constraint. The new approach has shown its advantages in both accuracy and classification time. However, the relationship between the quality of the data and the dictionary learning issues that affect the performance of the system has not been investigated. In this paper, we present a comparative study on non-sparse coding dictionary learning for pattern classification. We then propose a dictionary learning model with a non-sparsity constraint on representation coefficients using L2-norm. Our experimental results on three popular benchmark datasets for image classification show that our proposed model can outperform state-of-the-art models and be a promising approach for dictionary learning based classification.",72,https://doi.org/10.1109/KSE.2015.66,1,
1218,Improving semantic texton forests with a Markov random field for image segmentation.,"Semantic image segmentation is a major and challenging problem in computer vision, which has been widely researched over decades. Recent approaches attempt to exploit contextual information at different levels to improve the segmentation results. In this paper, we propose a new approach for combining semantic texton forests (STFs) and Markov random fields (MRFs) for improving segmentation. STFs allow fast computing of texton codebooks for powerful low-level image feature description. MRFs, with the most effective algorithm in message passing for training, will smooth out the segmentation results of STFs using pairwise coherent information between neighboring pixels. We evaluate the performance of the proposed method on two well-known benchmark datasets including the 21-class MSRC dataset and the VOC 2007 dataset. The experimental results show that our method impressively improved the segmentation results of STFs. Especially, our method successfully recognizes many challenging image regions that STFs failed to do.",72,https://doi.org/10.1145/2676585.2676621,1,
1219,Checking and correcting the source code of web pages for accessibility,"This paper aims to propose and experiment a framework for checking and correcting websites for accessibility. Existing tools usually check the WCAG-conformance of HTML client pages (that contain the static elements to be displayed through a browser to end users at a moment). Consequently, web developers have to do tedious works of identifying which parts of server source pages (i.e. server-side source codes that generate HTML client pages) cause non-conformant client elements. Unlike these tools, our framework allows directly reporting and suggesting solutions for the elements in the server source pages. The proposed method composes of four steps. First, the HTML client page and the server source page are parsed. Second, the elements of HTML client page that are non-conformant with the WCAG success criteria are identified and reported. Third, a mapping between the HTML client page and the …",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:3fE2CSJIrl8C,,1
1220,Linked data processing provenance: towards transparent and reusable linked data integration,"The growth of Linked Data has created a promising environment for data exploration and a growing number of tools allow users to interactively integrate data from various sources. Eliciting the reliability of the results of such ad-hoc integration processes, consistently recreating those results, and identifying changes upon re-execution, however, can be difficult. Automated process provenance trail creation can provide major benefits in this context, because (i) it enables users to trace the contribution of individual sources and processing steps to the final outcome and judge whether the result can be trusted;(ii) it ensures repeatability and raises the trustworthiness of results;(iii) it ideally enables reconstruction of Linked Data integration processes from the provenance information embedded in the final result. In this paper, we present a provenance model that facilitates automatic generation of semantic provenance …",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:Y0pCki6q_DkC,,1
1221,A web-based platform for dynamic integration of heterogeneous data,"Whereas the number of open and accessible data sources on the web is growing rapidly, data becomes more heterogeneous and is difficult to use or reuse. Even though many national and international organizations have published their data according to the Linked Data principles, a considerable number of data sources is still available in traditional formats, eg, HTML, XML, CSV, JSON. This results in a challenge of data aggregation and integration. To address this issue, we apply the visual programming paradigm to develop an open web platform. The platform is based on Semantic Web technologies and aims at encouraging and facilitating use of heterogeneous Open Data sources. We define Linked Widgets as user-driven modules which support users in accessing, processing, integrating, and visualizing different kinds of data. By connecting Linked Widgets from different developers, users without …",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:2osOgNQ5qMEC,,1
1222,Toward a Framework for Statistical Data Integration.,"A large number of statistical data sets have been published on the web by various organizations in recent years. The resulting abundance creates opportunities for new analyses and insights, but that frequently requires integrating data from multiple sources. Inconsistent formats, access methods, units, and scales hinder data integration and make it a tedious or infeasible task. Standards such as the W3C RDF data cube vocabulary and the content-oriented guidelines of SDMX provide a foundation to tackle these challenges. In this paper, we introduce a framework that semi-automatically performs semantic data integration on statistical raw data sources at query time. We follow existing standards to transform non-semantic data structures to RDF format. Furthermore, we describe each data set with semantic metadata to deal with inconsistent use of terminologies. This metadata provides the foundation for cross-dataset querying through a mediator that rewrites queries appropriately for each source and returns consolidated results.",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:u-x6o8ySG0sC,,1
1223,A Drag-and-block Approach for Linked Open Data Exploration.,"Since its initial definition in 2007, the concept of Linked Open Data (LOD) has gained strong traction in the scientific community. However, mainstream adoption has been limited and the emergence of an envisioned global linked data space is still in its early stages. One possible explanation is the gap between the large amounts of published LOD datasets and the lack of end-user tools to effectively explore them. Because existing applications are tailored towards specific datasets and do not allow for reuse and extension, novice users have so far had limited means to access the rich data sources being published. To address this issue, we introduce a novel approach to support non-expert users in the flexible exploration of LOD. To this end, we define a formal model that makes use of existing links between interconnected datasets. We implement the model in a mashup platform and illustrate its potential by means of use cases combining Open Data and Linked Open Data sources.",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:KlAtU1dfN6UC,,1
1224,Statspace: A unified platform for statistical data exploration,"In recent years, the amount of statistical data available on the web has been growing fast. Numerous organizations and governments publish data sets in a multitude of formats and encodings, using different scales, and providing access through a wide range of mechanisms. Due to such inconsistent publishing practices, integrated analysis of statistical data is challenging. StatSpace tackles this problem through semantic integration and provides uniform access to disparate statistical data. At present, it incorporates more than 1,800 data sets published by a variety of data providers including the World Bank, the European Union, and the European Environment Agency. StatSpace transparently lifts data from raw sources, maps geographical and temporal dimensions, aligns value ranges, and allows users to explore and integrate the previously isolated data sets. This paper introduces the constituent elements …",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:YsMSGLbcyi4C,,1
1225,Toward a statistical data integration environment: the role of semantic metadata,"In most government and business organizations alike, statistical data provides the foundation for strategic planning and for the management of operations. In this context, the use of increasingly abundant statistical data available on the web creates new opportunities for interesting applications and facilitates more informed decision-making. For the majority of end users, however, viable means to explore statistical data sets available on the web are still scarce. Gathering and relating statistical data from multiple sources is hence typically a tedious manual process that requires significant technical expertise. Data that is being published with associated semantics, using standards such as the W3C RDF Data Cube Vocabulary, lays the foundation to overcome such limitations. In this paper, we develop a semantic metadata repository that describes each statistical data set and develop mechanisms for the interconnection …",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:5nxA0vEk-isC,,1
1226,An Autocomplete Input Box for Semantic Annotation on the Web.,"A large share of websites today allow users to contribute and manage user-generated content. This content is often in textual form and involves names, terms, and keywords that can be ambiguous and difficult to interpret for other users. Semantic annotation can be used to tackle such issues, but this technique has been adopted by only a few websites. This may be attributed to a lack of a standard web input component that allows users to simply and efficiently annotate text. In this paper, we introduce an autocomplete-enabled annotation box that supports users in associating their text with DBpedia resources as they type. This web component can replace existing input fields and does not require particular user skills. Furthermore, it can be used by semantic web developers as a user interface for advanced semantic search and data processing back-ends. Finally, we validate the approach with a preliminary user study.",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:hqOjcs7Dif8C,,1
1227,Integrating streaming data into semantic mashups,"Streaming data, i.e., frequently changing and potentially infinite data flows, are becoming more and more prevalent in today's interconnected world. In a Smart City context, for instance, citizens can access real-time information about a city's weather, pollution, or traffic conditions. However, turning vast real-time data streams into actionable knowledge poses significant challenges. For instance, heterogeneous data from varied sources needs to be normalized and integrated, ideally using a common structured vocabulary. Furthermore, to provide users with both real-time and aggregate views on the data, data stream access requires support for temporal operators. Finally, it is difficult for non-technical users to integrate and leverage streaming data. To tackle these challenges, we propose an extension of the Linked Widgets concept for the streaming data domain. In this paper, we outline a platform prototype for the …",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:IjCSPb-OGe4C,,1
1228,Leveraging the web of data via linked widgets,"Machine-readable datasets that have increasingly become available in open formats in recent years have great potential as a foundation for innovative applications and services. Linked Data in particular-a set of best practices for publishing and connecting structured data on the Web-has facilitated significant progress in evolving the Web of documents into a Web of Data. However, although this concept has opened up many opportunities for data sharing and collaboration, integrating data is still a challenging task that requires considerable technical expertise and a profound understanding of the underlying datasets. In this paper, we introduce a novel approach to provide knowledge workers with the necessary tools to leverage the fast growing Linked Data Cloud by creating semantic-aware dataflow processes. To this end, we introduce the “Linked Widget” concept as an enhancement of standard Web …",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:UeHWp8X0CEIC,,1
1229,Linked widgets platform: Lowering the barrier for open data exploration,"Despite a drastic increase in available Open and Linked Data, unmediated utilization of these data by end users is still relatively uncommon. Applications built on top of Open Data are typically domain-specific and discovering appropriate solutions that fit users’ rapidly shifting needs is a cumbersome process. In line with the Linked Data paradigm, end user tools should be based on openness, foster reusability, and be flexible enough to handle arbitrary data sources. We develop an open platform based on Semantic Web technologies that encourages developers and users to access, process, integrate, and visualize Open Data sources. To help users overcome technological barriers of adoption and get in touch with Open Data, we introduce the concept of Linked Widgets. By connecting Linked Widgets from different developers, users without programming skills can compose and share ad-hoc applications …",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:kNdYIx-mwKoC,,1
1230,Semantic mashup composition from natural language expressions: preliminary results,"Despite an abundance of data available on the web today, satisfying users' complex information needs intelligently by automatically integrating and processing data from various sources remains challenging. In recent years, a large stream of research into mashups as a paradigm of end user development has emerged. These mashups foster combination and reuse of data and services and thereby allow end users to create novel applications. Developing such mashups efficiently and effectively, however, is still difficult for users that lack technical expertise. To address this issue, we extend a mashup platform with automatic mashup composition mechanisms and an agent that assists users in mashup design. To this end, we leverage semantics to simplify the mashup composition process on multiple levels. We associate each widget (ie, mashup component) with a semantic model of inputs and outputs. These …",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:roLk4NBRz8UC,,1
1231,Linked Widgets Platform for Rapid Collaborative Semantic Mashup Development,"In recent years, data has become vital in supporting our everyday lives. Along with large volumes of open data available on the web, various types of public, private, and enterprise data are stored in the cloud or distributed over multiple devices. The value of this data would increase drastically if we were able to integrate it. This would enable more sophisticated presentation and analysis of previously disparate data. So far, however, it is challenging for non-expert users to efficiently make use of such data because (i) data heterogeneity hampers integration of different kinds of data that are stored in various formats and spread among storage infrastructures; (ii) manual data integration processes are typically neither reproducible, nor reusable; and (iii) the lack of support for exploration does not allow for the integration of arbitrary data sources. This paper tackles these challenges by introducing a mashup platform …",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:8k81kl-MbHgC,,1
1232,Mashup-based linked data integration,"The web of data is growing at a staggering pace. A large number of data sources, APIs, services, and data visualizations are publicly available. Satisfying users- complex information needs by integrating and processing data from disparate sources, however, remains challenging. In recent years, a large stream of research into mashup-based data integration has emerged. These mashups foster combination and reuse of data and services and thereby have the potential for rapid creation of rich web applications. Nonetheless, users lacking technical expertise still face enormous barriers when trying to develop such mashups efficiently and effectively. To address this issue, we introduce an approach to compose mashups that integrate heterogeneous data sources in an automatic, collaborative, and distributed manner. We follow a visual programming paradigm and aim for three guiding principles: openness, connectedness, and reusability. The approach is based on semantic web technologies and the concept of Linked Widgets, i.e., web widgets backed by a semantic model. Linked Widgets are designed to effectively tackle data integration challenges by (i) fostering reusability of data processing tasks, (ii) easing data integration via simple operations, (iii) allowing users to explore relevant data sources with regard to their context, (iv) tackling data heterogeneity, and (v) facilitating automatic data integration. This thesis introduces a new model of semantic, collaborative, and distributed mashups. Following semantic web principles for data integration, these ad-hoc mashup-based data integration applications can simultaneously process and combine …",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:ULOm3_A8WrAC,,1
1233,Towards an Environmental Decision-Making System: A Vocabulary to Enrich Stream Data,"The future of the earth’s environmental systems will, to a major extent, be determined in cities, where already more than 50 % of the human population is concentrated. Pervasively available sensors and the data they generate can help to address pressing environmental challenges in urban areas by making crucial information available to researchers and decision-makers. However, environmental data is at present typically stored in disparate systems and formats, which inhibits reuse and integration. Furthermore, the large amounts of environmental data that stream in continuously require novel processing approaches. So far, research at the intersection of environmental sciences and urban data infrastructures has been scarce. To address these issues, we develop a novel framework based on semantic web technologies. We apply data modeling and semantic stream processing technologies in order to …",73,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=e6s_IIoAAAAJ&citation_for_view=e6s_IIoAAAAJ:_FxGoFyzp5QC,,1
1234,Entropy-based Correlation Clustering for Wireless Sensor Networks in Multi-Correlated Regional Environments,"The existence of correlation characteristics brings significant potential advantages to the development of efficient routing protocols in wireless sensor networks. This research proposes a new simple method of clustering sensor nodes into correlation groups in multiple-correlation areas. At first, the evaluation of joint entropy for multiple-sensed data is considered. Based on the evaluation, the definition of correlation region, based on entropy theory, is proposed. Following that, a correlation clustering scheme with less computation is developed. The results are validated with a real data set.",74,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=cJqM-QkAAAAJ&citation_for_view=cJqM-QkAAAAJ:d1gkVwhDpl0C,,1
1235,Entropy correlation-based clustering method for representative data aggregation in wireless sensor networks,"One of the popular data aggregation method in wireless sensor network (WSN) is collecting only local representative data based on correlation of sample data. To recognise the local representative nodes, it is necessary to determine the correlation regions. However, recent correlation models are distance based that is not general and need to be determined beforehand or complicated with high computing cost. Thus, in this paper, a novel entropy correlation model is proposed based on joint entropy approximation. Using the proposed model, an entropy correlation-based clustering method is presented and the selection of representative data that satisfying the desired distortion is proposed. The algorithm is validated with practical data.",74,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=cJqM-QkAAAAJ&citation_for_view=cJqM-QkAAAAJ:UeHWp8X0CEIC,,1
1236,Entropy correlation and its impact on routing with compression in wireless sensor network,"The existence of correlation characteristics brings significant potential advantages for the development of efficient routing protocols in wireless sensor network. However, one of the main problems is identifying the correlation region. This paper proposes an estimated joint entropy model to evaluate joint entropy of multiple sensed data. Using the proposed model, a definition of correlation region based on entropy theory is proposed and followed by a correlation clustering scheme with less computation is presented. In addition, using the proposed model, some routing protocols with compression scheme are considered in order to select the most appropriate scheme. The clustering of correlation region to optimize the communication cost is also considered in this paper.",74,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=cJqM-QkAAAAJ&citation_for_view=cJqM-QkAAAAJ:u5HHmVD_uO8C,,1
1237,Entropy based correlation clustering for wireless sensor network in multi-correlated regional environment,"The existence of correlation characteristics bring significant potential advantages for the development of efficient routing protocols in wireless sensor network. However, there has not an effective approach to divide sensor nodes into correlation group in multiple correlation-areas. This research proposed a new simple method on evaluating joint entropy of multiple sensed data to identify multi-correlation areas. The definition of correlation region based on entropy theory is also proposed. Following, a correlation clustering scheme with less computation is developed using proposed method and definition. The results are validated with real data set.",74,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=cJqM-QkAAAAJ&citation_for_view=cJqM-QkAAAAJ:zYLM7Y9cAGgC,,1
1238,Improvement of feature set based on Apriori algorithm in Android malware classification using machine learning method,"A well-constructed feature set plays an important role in accuracy improvement in malware detection. However, research and evaluation of the relations between features to acquire a good feature set have not been received much attention. In this work, a method based on Apriori algorithm was proposed to improve the feature set. The method studies association rules from the initial feature set to devise the highly correlated and informative features, which will be added to the initial set. The improved feature set will be evaluated via cross validation test using various machine learning algorithms, such as SVM, Random forest and CNN. The accuracy of the test reached is 96.49% with 96.71% improved compared with the test using initial set.",74,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=cJqM-QkAAAAJ&citation_for_view=cJqM-QkAAAAJ:IjCSPb-OGe4C,,1
1242,"Hedge algebras, linguistic-value logic and their application to fuzzy reasoning","People use natural languages to think, to reason, to deduce conclusions, and to make decisions. Fuzzy set theory introduced by L. A. Zadeh has been intensively developed and founded a computational foundation for modeling human reasoning processs. The contribution of this theory both in the theoretical and the applied aspects is well recognized. However, the traditional fuzzy set theory cannot handle linguistic terms directly. In our approach, we have constructed algebraic structures to model linguistic domains, and developed a method of linguistic reasoning, which directly manipulates linguistic terms, In particular, our approach can be applied to fuzzy control problems.",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:LkGwnXOMwfcC,,1
1243,A fuzzy description logic with hedges as concept modifiers,"In this paper we present a fuzzy description logic LCFh, where primitive concepts are modified by means of hedges. LCFh is strictly more expressive than Fuzzy-LC defined in [8]. We show that given a linearly ordered set of hedges primitive concepts can be modified to any desired degree by prefixing them with appropriate chains of hedges. Furthermore, we define a decision procedure for the unsatisfiability problem in LCFh, and discuss truth bounds, expressivity as well as complexity issues.",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:u5HHmVD_uO8C,,1
1244,Link prediction in co-authorship networks based on hybrid content similarity metric,"Link prediction in online social networks is used to determine new interactions among its members which are likely to occur in the future. Link prediction in the co-authorship network has been regarded as one of the main targets in link prediction researches so far. Researchers have focused on analyzing and proposing solutions to give efficient recommendation for authors who can work together in a science project. In order to give precise prediction of links between two ubiquitous authors in a co-authorship network, it is preferable to design a similarity metric between them and then utilizing it to determine the most possible co-author(s). However, the relevant researches did not regard the integration of paper’s content in the metric itself. This is important when considering the collaboration between scientists since it is possible that authors having same research interests are more likely to have a joint paper …",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:hFOr9nPyWt4C,,1
1245,The Fuzzy Description Logic ALCFH with Hedge Algebras as Concept Modifiers.,"In this paper we present the fuzzy description logic ALCFH introduced, where primitive concepts are modified by means of hedges taken from hedge algebras. ALCFH is strictly more expressive than Fuzzy-ALC defined in [11]. We show that given a linearly ordered set of hedges primitive concepts can be modified to any desired degree by prefixing them with appropriate chains of hedges. Furthermore, we define a decision procedure for the unsatisfiability problem in ALCFH, and discuss knowledge base expansion when using terminologies, truth bounds, expressivity as well as complexity issues. We extend [8] by allowing modifiers on non-primitive concepts and extending the satisfiability procedure to handle concept definitions.",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:u-x6o8ySG0sC,,1
1246,Fuzzy linguistic logic programming and its applications,"The paper introduces fuzzy linguistic logic programming, which is a combination of fuzzy logic programming, introduced by P. Vojtáš, and hedge algebras in order to facilitate the representation and reasoning on human knowledge expressed in natural languages. In fuzzy linguistic logic programming, truth values are linguistic ones, e.g., VeryTrue, VeryProbablyTrue and LittleFalse, taken from a hedge algebra of a linguistic truth variable, and linguistic hedges (modifiers) can be used as unary connectives in formulae. This is motivated by the fact that humans reason mostly in terms of linguistic terms rather than in terms of numbers, and linguistic hedges are often used in natural languages to express different levels of emphasis. The paper presents: (a) the language of fuzzy linguistic logic programming; (b) a declarative semantics in terms of Herbrand interpretations and models; (c) a procedural semantics which …",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:roLk4NBRz8UC,,1
1247,The subsumption problem of the fuzzy description logic ALCFH,"We present the fuzzy description logic ALCFH where primitive concepts are modified by means of hedges taken from hedge algebras. ALCFH is strictly more expressive than Fuzzy-ALC defined in [9]. We show that given a linearly ordered set of hedges primitive concepts can be modified to any desired degree by prefixing them with appropriate chains of hedges. Furthermore, we define a decision procedure for the unsatisfiability problem in ALCFH and for the subsumption problem in a fragment of ALCFH.",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:d1gkVwhDpl0C,,1
1248,The fuzzy description logic ALCF LH,"In this paper, we present the fuzzy description logic ALCF LH. ALCF LH is based on ALCFH, but linear hedges are used instead of exponential ones. This allows to solve the entailment and the subsumption problem in a fuzzy description logic, where arbitrary concepts and roles may be modified.",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:2osOgNQ5qMEC,,1
1249,"Fuzziness  Measure, Quantified Semantic Mapping and Interpolative Method of Approximate Reasoning in Medical Expert Systems","1. We hereby assign copyright of our article (the Work) in all forms of media, whether now known or hereafter developed, to the Journal of Computer Science and Cybernetics. We understand that the Journal of Computer Science and Cybernetics will act on my/our behalf to publish, reproduce, distribute and transmit the Work.",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:4TOpqqG69KYC,,1
1250,Connections between discriminants and the root distribution of polynomials with rational generating function,"Let H m (z) be a sequence of polynomials whose generating function∑ m= 0∞ H m (z) t m is the reciprocal of a bivariate polynomial D (t, z). We show that in the three cases D (t, z)= 1+ B (z) t+ A (z) t 2, D (t, z)= 1+ B (z) t+ A (z) t 3 and D (t, z)= 1+ B (z) t+ A (z) t 4, where A (z) and B (z) are any polynomials in z with complex coefficients, the roots of H m (z) lie on a portion of a real algebraic curve whose equation is explicitly given. The proofs involve the q-analogue of the discriminant, a concept introduced by Mourad Ismail.",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:blknAaTinKkC,,1
1251,The fuzzy linguistic description logic ALCFL,"We present the fuzzy linguistic description logic ALCFL, an instance of the description logic framework L− ALC with the certainty lattice characterized by a hedge algebra. Beside constructors of L− ALC, ALCFL allows the modification by hedges.",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:9yKSN-GCB0IC,,1
1252,The root distribution of polynomials with a three-term recurrence,"For any fixed positive integer n, we study the root distribution of a sequence of polynomials H m (z) satisfying the rational generating function∑ m= 0∞ H m (z) t m= 1 1+ B (z) t+ A (z) t n where A (z) and B (z) are any polynomials in z with complex coefficients. We show that the roots of H m (z) which satisfy A (z)≠ 0 lie on a specific fixed real algebraic curve for all large m.",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:YFjsv_pBGBYC,,1
1253,Context matching with reasoning and decision support using hedge algebra with kansei evaluation,"There have been far reaching Societal and Geo-Political developments in healthcare domains locally, nationally, and globally. Healthcare systems are essentially patient centric and decision driven with the clinician focus being on the identification of the best treatment options for patients in uncertain environments. Decision-support systems must focus on knowledge-based decisions using both tacit and explicit knowledge. Decisions are generally made using a qualitative approach in which linguistic (semantic) terms are used to express parameters and preferences to determine the optimal decision from a range of alternative decisions. The study presented in this paper proposes an approach which implements context-matching using hedge algebra integrated with Kansei evaluation. The proposed approach is designed to enable quantification of qualitative factors for linguistic variables while accommodating …",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:Wp0gIr-vW9MC,,1
1254,Fuzzy c-means clustering algorithm with multiple fuzzification coefficients,"Clustering is an unsupervised machine learning technique with many practical applications that has gathered extensive research interest. Aside from deterministic or probabilistic techniques, fuzzy C-means clustering (FCM) is also a common clustering technique. Since the advent of the FCM method, many improvements have been made to increase clustering efficiency. These improvements focus on adjusting the membership representation of elements in the clusters, or on fuzzifying and defuzzifying techniques, as well as the distance function between elements. This study proposes a novel fuzzy clustering algorithm using multiple different fuzzification coefficients depending on the characteristics of each data sample. The proposed fuzzy clustering method has similar calculation steps to FCM with some modifications. The formulas are derived to ensure convergence. The main contribution of this approach is the utilization of multiple fuzzification coefficients as opposed to only one coefficient in the original FCM algorithm. The new algorithm is then evaluated with experiments on several common datasets and the results show that the proposed algorithm is more efficient compared to the original FCM as well as other clustering methods.",75,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=ICrxWekAAAAJ&citation_for_view=ICrxWekAAAAJ:M3NEmzRMIkIC,,1
1255,Local search approach for the pairwise constrained clustering problem,"The pairwise constrained clustering is the problem of partitioning a set of data points into clusters when we know in advance that some pairs of points should be in the same cluster and some pairs should not. Previous studies on this problem can be divided into three categories: modifying a traditional clustering algorithm to incorporate constraints, learning a distance measure or combining both approaches. Local search is a heuristic method for finding high-quality solutions for hard optimization problems in a reasonable computation time. It has been applied to the traditional clustering problem in many studies. However, it has never been used for the pairwise constrained clustering problem. Therefore, this paper proposes Tabu search algorithms for this problem, which were tested on several datasets. The experimental results show that these algorithms are very interesting in comparison with some state-of-the-art …",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:Se3iqnhoufwC,,1
1256,Exact methods for solving the elementary shortest and longest path problems,"We consider in this paper the problems of finding the elementary shortest and longest paths on a graph containing negative and positive cycles. These problems are NP-hard. We propose exact algorithms based on mixed integer programming for their solution, employing different valid inequalities. Moreover, we propose decomposition techniques which are very efficient for cases with special structure. Experimental results show the efficiency of our algorithms compared with state of the art exact algorithms.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:W7OEmFMy1HYC,,1
1257,Solving the agricultural land allocation problem by constraint-based local search,"Agricultural land allocation is a problem that exists in most provinces in Vietnam. Each household owns many disconnected parcels, which reduces agricultural development. The solution to the problem is to repartition this agricutural land among the households, while satisfying some criteria. Historically, this problem has been approached neither using optimization technology nor computer science. The present paper describes the formulation of the problem and proposes a constraint-based local search algorithm for solving it. Experimental results on real data in Dong Trung village show that the solution computed by our algorithm is better than traditional solutions.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:IjCSPb-OGe4C,,1
1258,Designing service sectors for daily maintenance operations in a road network,"The problem of designing service sectors in a road network for the organisation of route maintenance activities is addressed in this paper. The decisions involve determining the locations of a set of depots and assigning road segments to the selected depots to form service sectors. Routing operations must be taken into consideration while partitioning service sectors in order to precisely evaluate deadhead and required number of vehicles. The problem is thus formulated as a location-arc routing problem and solved by a branch-and-cut algorithm. A three-stage heuristic algorithm with sector design component is developed to solve the medium- and large-sized problems. The computational experiments demonstrate the effectiveness of the heuristic algorithm, and the superiority of the heuristic algorithm to the exact method in terms of some specific partition evaluation criteria.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:UeHWp8X0CEIC,,1
1259,Constraint-based local search for fields partitioning problem,"Partitioning a field into parcels is one step of the project of merging and repartitioning agricultural lands which emerges in many provinces in Vietnam. Historically, this problem has been conducted based on human experiences and without using optimization technology nor computer science. In this paper, we propose a model for the problem of fields partitioning taking into account real issues. We also propose local search algorithms for solving this problem with typical shapes of fields ie, rectangle, trapezium, triangle. Experimental results show that our local search approach gives better solutions than those realized by human experiences.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:zYLM7Y9cAGgC,,1
1260,A constraint-based local search for offline and online general vehicle routing,"Vehicle routing is a class of combinatorial optimization problems arising in the industry of transportation and logistics. The goal of these problems is to compute an optimal route plan for a set of vehicles for serving transport requests of customers. There are many variants of the vehicle routing problems: routing for delivering goods, routing for demand responsive transport (taxi, school bus, …). Each problem might have different constraints, objectives. In this paper, we introduce a Constraint-Based Local Search (CBLS) framework for general offline and online vehicle routing problems. We extend existing neighborhood structures in the literature by proposing new neighborhoods to facilitate the resolution of different class of vehicle routing problems in a unified platform. A novel feature of the framework is the available APIs for online vehicle routing problems where requests arrive online during the execution of the …",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:eQOLeE2rZwMC,,1
1261,On the Traveling Salesman Problem with Hierarchical Objective Function,"We address a novel variant of the wellknown Traveling Salesman Problem (TSP) called the Traveling Salesman Problem with Hierarchical Objective (TSPHO). In this problem, the customers are divided in to several groups with decreasing priority levels, i.e., the first group is more important than the second one and the second one is more important than the third one, and so on. The difference between TSPHO and the classical TSP lies in the objective function. The Hierarchical Objective does not minimize the total travel cost, but aims to minimize the completion time of the first group then the completion time of the second group, etc. A transformation of the TSPHO into an equivalent Asymmetric TSP is first proposed from which one can use efficient TSP solvers such as Concorde or Lin-Kernighan-Helsgaun (LKH) to solve the problem. A genetic algorithm is also developed as an alternative solution. Computational …",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:8k81kl-MbHgC,,1
1262,"Modelling and solving complex combinatorial optimization problems: quorumcast routing, elementary shortest path, elementary longest path and agricultural land allocation","The feasible solution set of a Combinatorial Optimization Problem (COP) is discrete  and finite. Solving a COP is to find optimal solutions in the set of feasible solutions such that  the value of a given cost function is minimized or maximized. In the literature, there exist both  complete and incomplete methods for solving COPs. The complete (or exact) methods return  the optimal solutions with the proof of the optimality, for example the branch-and-cut search.  The incomplete methods try to find hight-quality solutions which are as close to the optimal ...",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:YsMSGLbcyi4C,,1
1263,Solving the quorumcast routing problem as a mixed integer program,"The quorumcast routing problem is a generalization of multicasting which arises in many distributed applications. It consists of finding a minimum cost tree that spans the source node and at least q out of m specified nodes on a given undirected weighted graph. In this paper, we solve this problem as a mixed integer program. The experimental results show that our four approaches outperform the state of the art. A sensitivity analysis is also performed on values of q and m.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:Tyk-4Ss8FVUC,,1
1264,On three soft rectangle packing problems with guillotine constraints,"We investigate how to partition a rectangular region of length  and height  into n rectangles of given areas  using two-stage guillotine cuts, so as to minimize either (i) the sum of the perimeters, (ii) the largest perimeter, or (iii) the maximum aspect ratio of the rectangles. These problems play an important role in the ongoing Vietnamese land-allocation reform, as well as in the optimization of matrix multiplication algorithms. We show that the first problem can be solved to optimality in , while the two others are NP-hard. We propose mixed integer linear programming formulations and a binary search-based approach for solving the NP-hard problems. Experimental analyses are conducted to compare the solution approaches in terms of computational efficiency and solution quality, for different objectives.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:UebtZRa9Y70C,,1
1265,Mathematical Programs and Computations for a Class of Anti-aircraft Mission Planning Problems,"The theater defense distribution is an important problem in the military that determines strategies against a sequence of offensive attacks in order to protect his targets. This study focuses on developing mathematical models for three important defense problems that generate anti-aircraft mission plans for a group of missile battalions. The simple Anti-aircraft Launching Assignment problem specifies number of missiles should be launched from each battalion to each fleet of attacking aircraft to maximize the defensive effectiveness, provided that the locations of missile battalions are given. On the other hand, the Anti-aircraft Mission Planning problem maximizes the defender’s effectiveness using all his available battalions, while the Inverse Antiaircraft Mission Planning problem computes necessary weapon resources (battalions and their missiles) to obtain a given defensive effectiveness value. The proposed formulations are Integer Programs and proved as NP-hard. A comprehensive set of experiments is then evaluated to show that these proposed programs can be applied to solve fast real-life instances to optimality.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:5nxA0vEk-isC,,1
1266,Modelling and Solving Anti-aircraft Mission Planning for Defensive Missile Battalions,"The theater defense distribution is an important problem in the military that determines strategies against a sequence of offensive attacks in order to protect his targets. This study focuses on developing mathematical models for two defense problems that generate anti-aircraft mission plans for a group of missile battalions. While the Anti-aircraft Mission Planning problem maximizes the defender’s effectiveness using all his available battalions, the Inverse Anti-aircraft Mission Planning problem computes necessary weapon resources (battalions and their missiles) to obtain a given defensive effectiveness value. The proposed formulations are Mixed Integer Programs that describe not only the positions of missile battalions, but also engage battalions to fleets of attacking aircrafts. We additionally prove that these problems are NP-hard. A comprehensive set of experiments is then evaluated to show that these …",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:hqOjcs7Dif8C,,1
1267,On the mathematical program in theater anti-aircraft distribution problem,The military theater anti-aircraft distribution problem is associated with determining positions of defender's missiles within a potential geographic area of operation. This research focuses on formulating and solving a mathematical model to aid decision makers to maximize defender's efficient course of actions assuming that an aircraft attacking plan is observed. The developed model is a Mixed Integer Program (MIP) and solved using our proposed constraint-based local search algorithm as well as CPLEX solver. Experimental results on the simulated data allow us certainly establish the quality of any defensive plan.,76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:0EnyYjriUFMC,,1
1268,Eliminating overfitting of probabilistic topic models on short and noisy text: The role of dropout,"Probabilistic topic models are powerful tools for discovering hidden structures/semantics in discrete data, e.g., texts, images, links. However, on short and noisy texts, directly applying topic models may not work well or face severe overfitting. In this article, we investigate the benefits of dropout for preventing topic models from overfitting. We integrate dropout into several stochastic methods for learning latent Dirichlet allocation (LDA). From extensive experiments on four large-scale datasets, our findings are: (1) dropout helps to prevent overfitting and significantly enhance predictiveness and generalization of LDA on short texts; (2) for long documents, dropout may provide little benefit; (3) dropout can be easily integrated into any learning methods to avoid overfitting for short and noisy text. Furthermore, dropout can be straightforwardly employed in a wide range of topic models. In evidence, we apply dropout to BTM …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:Wp0gIr-vW9MC,,1
1269,An effective and interpretable method for document classification,"As the number of documents has been rapidly increasing in recent time, automatic text categorization is becoming a more important and fundamental task in information retrieval and text mining. Accuracy and interpretability are two important aspects of a text classifier. While the accuracy of a classifier measures the ability to correctly classify unseen data, interpretability is the ability of the classifier to be understood by humans and provide reasons why each data instance is assigned to a label. This paper proposes an interpretable classification method by exploiting the Dirichlet process mixture model of von Mises–Fisher distributions for directional data. By using the labeled information of the training data explicitly and determining automatically the number of topics for each class, the learned topics are coherent, relevant and discriminative. They help interpret as well as distinguish classes. Our experimental …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:Se3iqnhoufwC,,1
1270,Collaborative topic model for Poisson distributed ratings,"We present Collaborative Topic Model for Poisson distributed ratings (CTMP), a hybrid and interpretable probabilistic content-based collaborative filtering model for recommender system. The model enables both content representation by admixture topic modelling, and computational efficiency from Poisson factorization living together under one tightly coupled probabilistic model, thus addressing the limitation of previous methods. CTMP excels in predictive performance under different real-world recommendation contexts, and easily scales to big datasets, while recovering interpretable user profiles. Moreover, our empirical study also shows strong evidence that sparsity in the estimates of topic mixture can be recovered via learning, despite not being specified in the model. The sparse representation derived from CTMP would allow efficient storage of the item contents, consequently providing a computational …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:4TOpqqG69KYC,,1
1271,Enabling hierarchical Dirichlet processes to work better for short texts at large scale,"Analyzing texts from social media often encounters many challenges, including shortness, dynamic, and huge size. Short texts do not provide enough information so that statistical models often fail to work. In this paper, we present a very simple approach (namely, bag-of-biterms) that helps statistical models such as Hierarchical Dirichlet Processes (HDP) to work well with short texts. By using both terms (words) and biterms to represent documents, bag-of-biterms (BoB) provides significant benefits: (1) it naturally lengthens representation and thus helps us reduce bad effects of shortness; (2) it enables the posterior inference in a large class of probabilistic models including HDP to be less intractable; (3) no modification of existing models/methods is necessary, and thus BoB can be easily employed in a wide class of statistical models. To evaluate those benefits of BoB, we take Online HDP into account in that …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:hqOjcs7Dif8C,,1
1272,Document clustering using dirichlet process mixture model of von mises-fisher distributions,"Document clustering has become an increasingly important technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. This paper proposes a Dirichlet process mixture (DPM) model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. We have developed a mean-field variational inference algorithm for the DPM model of vMFs that is applied to clustering text documents. Using this model, the number of clusters is determined automatically after the clustering process rather than pre-estimated. We conducted extensive experiments to evaluate the proposed approach on a large number of high dimensional text datasets. Empirical experimental results over NMI (Normalized Mutual Information) and Purity evaluation measures demonstrate that our approach …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:kNdYIx-mwKoC,,1
1273,Bag of biterms modeling for short texts,"Analyzing texts from social media encounters many challenges due to their unique characteristics of shortness, massiveness, and dynamic. Short texts do not provide enough context information, causing the failure of the traditional statistical models. Furthermore, many applications often face with massive and dynamic short texts, causing various computational challenges to the current batch learning algorithms. This paper presents a novel framework, namely bag of biterms modeling (BBM), for modeling massive, dynamic, and short text collections. BBM comprises of two main ingredients: (1) the concept of bag of biterms (BoB) for representing documents, and (2) a simple way to help statistical models to include BoB. Our framework can be easily deployed for a large class of probabilistic models, and we demonstrate its usefulness with two well-known models: latent Dirichlet allocation (LDA) and hierarchical …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:ZeXyd9-uunAC,,1
1274,Keeping priors in streaming bayesian learning,"Exploiting prior knowledge in the Bayesian learning process is one way to improve the quality of Bayesian model. To the best of our knowledge, however, there is no formal research about the influence of prior in streaming environment. In this paper, we address the problem of using prior knowledge in streaming Bayesian learning, and develop a framework for keeping priors in streaming learning (KPS) that maintains knowledge from the prior through each minibatch of streaming data. We demonstrate the performance of our framework in two scenarios: streaming learning for latent Dirichlet allocation and streaming text classification in comparison with methods that do not keep prior.",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:qxL8FJ1GzNcC,,1
1275,An interpretable method for text summarization based on simplicial non-negative matrix factorization,"Automatic text summarization plays an important role in information retrieval and text mining. Furthermore, it provides an useful solution to the information overload problem. In this paper, we propose a simplicial NMF-based unsupervised generic document summarization method which can inherit some advantages of simplicial NMF such as easy interpretability, low complexity, convexity and sparsity. By focusing on the major topics contained within every sentence as well as entire document, our method generates better summaries with less repetition. The effectiveness of our method is proved by experimental results. On the summarization performance, our approach obtains mostly higher ROUGE scores than NMF-based method.",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:UebtZRa9Y70C,,1
1276,Boosting prior knowledge in streaming variational Bayes,"Exploiting prior/human knowledge is an effective way to enhance Bayesian models, especially in cases of sparse or noisy data, for which building an entirely new model is not always possible. There is a lack of studies on the effect of external prior knowledge in streaming environments, where the data come sequentially and infinitely. In this work, we show the problem of vanishing prior knowledge in streaming variational Bayes. This is a serious drawback in various applications. We then develop a simple framework to boost the external prior when learning a Bayesian model from data streams. By boosting, the prior knowledge can be maintained and efficiently exploited through each minibatch of streaming data. We evaluate the performance of our framework in four scenarios: streaming in synthetic data, streaming sentiment analysis, streaming learning for latent Dirichlet allocation, and streaming text classification …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:hFOr9nPyWt4C,,1
1277,Neural poisson factorization,"In this work, we focus on dealing with a sparse users' feedback matrix and short descriptions/contents of items in recommender systems. We propose the Neural Poisson factorization (NPF) model which is a hybrid of deep learning and Poisson factorization. While Poisson factorization is suitable to model discrete, massive and sparse feedback, using a deep neural network and pre-trained word embeddings can learn hidden semantic in short item descriptions well. Therefore, NPF overcomes the limitation of existing models when dealing with short texts and a sparse feedback matrix. Moreover, we develop a random view algorithm based on stochastic learning for our model, in which each user is only viewed a random subset of items and his/her feedback on the subset is used to update his/her representation in each iteration. This approach is reasonable because each user can only know or view a partial subset of …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:qUcmZB5y_30C,,1
1278,Infinite Dropout for training Bayesian models from data streams,"The ability to continuously train Bayesian models in streaming environments is highly important in the era of big data. However, it has to face the famous stability-plasticity dilemma and the problem of noisy and sparse data. We propose a novel and easy-to-implement framework, called Infinite Dropout (iDropout), to address these challenges. iDropout has an easy mechanism to balance between old and new information, which allows models to trade off stability against plasticity. Thanks to the ability to reduce overfitting and the ensemble property of Dropout, our framework obtains better generalization, thus effectively handles undesirable effects of noise and sparsity. Further, iDropout is able to adapt quickly to abnormal changes in data streams. We theoretically analyze the equivalence of Dropout in iDropout to a regularizer, well applied to a much larger context than what was known before. Extensive experiments …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:dhFuZR0502QC,,1
1280,From implicit to explicit feedback: A deep neural network for modeling the sequential behavior of online users,"We demonstrate the advantages of taking into account multiple types of behavior in recommendation systems. Intuitively, each user has to do some\textbf {implicit} actions (eg, click) before making an\textbf {explicit} decision (eg, purchase). Previous works showed that implicit and explicit feedback has distinct properties to make a useful recommendation. However, these works exploit implicit and explicit behavior separately and therefore ignore the semantic of interaction between users and items. In this paper, we propose a novel model namely\textit {Implicit to Explicit (ITE)} which directly models the order of user actions. Furthermore, we present an extended version of ITE, namely\textit {Implicit to Explicit with Side information (ITE-Si)}, which incorporates side information to enrich the representations of users and items. The experimental results show that both ITE and ITE-Si outperform existing recommendation systems and also demonstrate the effectiveness of side information in two large scale datasets.",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:QIV2ME_5wuYC,,1
1281,Employing the correspondence of relations and connectives to identify implicit discourse relations via label embeddings,"It has been shown that implicit connectives can be exploited to improve the performance of the models for implicit discourse relation recognition (IDRR). An important property of the implicit connectives is that they can be accurately mapped into the discourse relations conveying their functions. In this work, we explore this property in a multi-task learning framework for IDRR in which the relations and the connectives are simultaneously predicted, and the mapping is leveraged to transfer knowledge between the two prediction tasks via the embeddings of relations and connectives. We propose several techniques to enable such knowledge transfer that yield the state-of-the-art performance for IDRR on several settings of the benchmark dataset (ie, the Penn Discourse Treebank dataset).",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:mVmsd5A6BfQC,,1
1282,Enriching user representation in neural matrix factorization,"Personalized recommendation is a crucial problem in the era of information overload. One of the most prominent methods addressing this problem is collaborative filtering (CF). Recently, NeuMF [1] has employed Deep Neural Network to facilitate traditional CF methods. Although NeuMF achieved the state-of-the-art performance, it is expected that predictive accuracy can be improved by enriching user representation. In this paper, we propose a neural network incorporating latent features and personalized information of users, namely Feature based Neural Matrix Factorization (FeaNMF). In FeaNMF, personalized information can be considered as auxiliary features which are generated by associating past behavior of users (e.g, purchase history, browsing activity, watching habits) with item categories. By explicitly modeling personalized information in the metric of item categories, the major advantages of FeaNMF …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:4DMP91E08xMC,,1
1283,Effective and interpretable document classification using distinctly labeled Dirichlet process mixture models of von Mises-Fisher distributions,"Document Classification is essential to information retrieval and text mining. Accuracy and interpretability are two important aspects of text classifiers. This paper proposes an interpretable classification method (DLDPvMFs) by using the Dirichlet process mixture (DPM) model to discover the hidden topics distinctly within each label for classification of directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. We use a mean-field variational inference algorithm when developing DLDPvMFs. By using the label information of the training data explicitly and determining automatically the number of topics for each label to find the topical space, class topics are coherent, relevant and discriminative and since they help us interpret class’s label as well as distinguish classes. Our experimental results showed the advantages of our approach via …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:8k81kl-MbHgC,,1
1284,Document clustering using mixture model of von Mises-Fisher distributions on document manifold,"Document clustering has become an increasingly important technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. The generative model for document clustering based on the von Mises-Fisher (vMF) distribution generally produces better clustering results than other generative models. However, in fact, it is more natural and reasonable to assume that the document space is a manifold and the probability distribution that generates the data is supported on a document manifold. In this paper, we propose a regularized probabilistic model based on manifold structure for data clustering, called Laplacian regularized vMF Mixture Model (LapvMFs), which explicitly considers the manifold structure. We have developed a generalized mean-field variational inference algorithm for the LapvMFs. Extensive experimental results on a large number of high …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:KlAtU1dfN6UC,,1
1285,Document classification using semi-supervived mixture model of von mises-fisher distributions on document manifold,"Document classifications is essential to information retrieval and text mining. In real life, unlabeled data is readily available whereas labeled ones are often laborious, expensive and slow to obtain. This paper proposes a novel Document Classification approach based on semi-supervised vMF mixture model on document manifold, called Laplacian regularized Semi-Supervised vMF Mixture Model (LapSSvMFs), which explicitly considers the manifold structure of document space to exploit efficiently both labeled and unlabeled data for classification. We have developed a generalized mean-field variational inference algorithm for the LapSSvMFs. Experimental results show that our approach preserves the best accuracy of purely graph-based transductive methods when the data has"" manifold structure"". Furthermore, high accuracy are obtained even for overlapping and fairly skewed datasets in comparison with other …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:MXK_kJrjxJIC,,1
1286,Dynamic transformation of prior knowledge into Bayesian models for data streams,"We consider how to effectively use prior knowledge when learning a Bayesian model from streaming environments where the data come infinitely and sequentially. This problem is highly important in the era of data explosion and rich sources of precious external knowledge such as pre-trained models, ontologies, Wikipedia, etc. We show that some existing approaches can forget any knowledge very fast. We then propose a novel framework that enables to incorporate the prior knowledge of different forms into a base Bayesian model for data streams. Our framework subsumes some existing popular models for time-series/dynamic data. Extensive experiments show that our framework outperforms existing methods with a large margin. In particular, our framework can help Bayesian models generalize well on extremely short text while other methods overfit. The implementation of our framework is available at https://github.com/bachtranxuan/TPS.git.",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:7PzlFSSx8tAC,,1
1287,How to make a machine learn continuously: a tutorial of the Bayesian approach,"How to build a machine that can continuously learn from observations in its life and make accurate inference/prediction? This is one of the central questions in Artificial Intelligence. Many challenges are present, such as the difficulty of learning from infinitely many observations (data), the dynamic nature of the environments, noisy and sparse data, the intractability of posterior inference, etc. This tutorial will discuss how the Bayesian approach provides a natural and efficient answer. We will start from the basic of Bayesian models, and then the variational Bayes method for inference. Next, we will discuss how to learn a Bayesian model from an infinite sequence of data. Some challenges such as catastrophic forgetting phenomenon, concept drifts, and overfitting will be discussed.",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:aqlVkmm33-oC,,1
1296,Local search approach for the pairwise constrained clustering problem,"The pairwise constrained clustering is the problem of partitioning a set of data points into clusters when we know in advance that some pairs of points should be in the same cluster and some pairs should not. Previous studies on this problem can be divided into three categories: modifying a traditional clustering algorithm to incorporate constraints, learning a distance measure or combining both approaches. Local search is a heuristic method for finding high-quality solutions for hard optimization problems in a reasonable computation time. It has been applied to the traditional clustering problem in many studies. However, it has never been used for the pairwise constrained clustering problem. Therefore, this paper proposes Tabu search algorithms for this problem, which were tested on several datasets. The experimental results show that these algorithms are very interesting in comparison with some state-of-the-art …",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:Se3iqnhoufwC,,1
1297,Exact methods for solving the elementary shortest and longest path problems,"We consider in this paper the problems of finding the elementary shortest and longest paths on a graph containing negative and positive cycles. These problems are NP-hard. We propose exact algorithms based on mixed integer programming for their solution, employing different valid inequalities. Moreover, we propose decomposition techniques which are very efficient for cases with special structure. Experimental results show the efficiency of our algorithms compared with state of the art exact algorithms.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:W7OEmFMy1HYC,,1
1298,Solving the agricultural land allocation problem by constraint-based local search,"Agricultural land allocation is a problem that exists in most provinces in Vietnam. Each household owns many disconnected parcels, which reduces agricultural development. The solution to the problem is to repartition this agricutural land among the households, while satisfying some criteria. Historically, this problem has been approached neither using optimization technology nor computer science. The present paper describes the formulation of the problem and proposes a constraint-based local search algorithm for solving it. Experimental results on real data in Dong Trung village show that the solution computed by our algorithm is better than traditional solutions.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:IjCSPb-OGe4C,,1
1299,Designing service sectors for daily maintenance operations in a road network,"The problem of designing service sectors in a road network for the organisation of route maintenance activities is addressed in this paper. The decisions involve determining the locations of a set of depots and assigning road segments to the selected depots to form service sectors. Routing operations must be taken into consideration while partitioning service sectors in order to precisely evaluate deadhead and required number of vehicles. The problem is thus formulated as a location-arc routing problem and solved by a branch-and-cut algorithm. A three-stage heuristic algorithm with sector design component is developed to solve the medium- and large-sized problems. The computational experiments demonstrate the effectiveness of the heuristic algorithm, and the superiority of the heuristic algorithm to the exact method in terms of some specific partition evaluation criteria.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:UeHWp8X0CEIC,,1
1300,Constraint-based local search for fields partitioning problem,"Partitioning a field into parcels is one step of the project of merging and repartitioning agricultural lands which emerges in many provinces in Vietnam. Historically, this problem has been conducted based on human experiences and without using optimization technology nor computer science. In this paper, we propose a model for the problem of fields partitioning taking into account real issues. We also propose local search algorithms for solving this problem with typical shapes of fields ie, rectangle, trapezium, triangle. Experimental results show that our local search approach gives better solutions than those realized by human experiences.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:zYLM7Y9cAGgC,,1
1301,A constraint-based local search for offline and online general vehicle routing,"Vehicle routing is a class of combinatorial optimization problems arising in the industry of transportation and logistics. The goal of these problems is to compute an optimal route plan for a set of vehicles for serving transport requests of customers. There are many variants of the vehicle routing problems: routing for delivering goods, routing for demand responsive transport (taxi, school bus, …). Each problem might have different constraints, objectives. In this paper, we introduce a Constraint-Based Local Search (CBLS) framework for general offline and online vehicle routing problems. We extend existing neighborhood structures in the literature by proposing new neighborhoods to facilitate the resolution of different class of vehicle routing problems in a unified platform. A novel feature of the framework is the available APIs for online vehicle routing problems where requests arrive online during the execution of the …",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:eQOLeE2rZwMC,,1
1302,On the Traveling Salesman Problem with Hierarchical Objective Function,"We address a novel variant of the wellknown Traveling Salesman Problem (TSP) called the Traveling Salesman Problem with Hierarchical Objective (TSPHO). In this problem, the customers are divided in to several groups with decreasing priority levels, i.e., the first group is more important than the second one and the second one is more important than the third one, and so on. The difference between TSPHO and the classical TSP lies in the objective function. The Hierarchical Objective does not minimize the total travel cost, but aims to minimize the completion time of the first group then the completion time of the second group, etc. A transformation of the TSPHO into an equivalent Asymmetric TSP is first proposed from which one can use efficient TSP solvers such as Concorde or Lin-Kernighan-Helsgaun (LKH) to solve the problem. A genetic algorithm is also developed as an alternative solution. Computational …",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:8k81kl-MbHgC,,1
1303,"Modelling and solving complex combinatorial optimization problems: quorumcast routing, elementary shortest path, elementary longest path and agricultural land allocation","The feasible solution set of a Combinatorial Optimization Problem (COP) is discrete  and finite. Solving a COP is to find optimal solutions in the set of feasible solutions such that  the value of a given cost function is minimized or maximized. In the literature, there exist both  complete and incomplete methods for solving COPs. The complete (or exact) methods return  the optimal solutions with the proof of the optimality, for example the branch-and-cut search.  The incomplete methods try to find hight-quality solutions which are as close to the optimal ...",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:YsMSGLbcyi4C,,1
1304,Solving the quorumcast routing problem as a mixed integer program,"The quorumcast routing problem is a generalization of multicasting which arises in many distributed applications. It consists of finding a minimum cost tree that spans the source node and at least q out of m specified nodes on a given undirected weighted graph. In this paper, we solve this problem as a mixed integer program. The experimental results show that our four approaches outperform the state of the art. A sensitivity analysis is also performed on values of q and m.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:Tyk-4Ss8FVUC,,1
1305,On three soft rectangle packing problems with guillotine constraints,"We investigate how to partition a rectangular region of length  and height  into n rectangles of given areas  using two-stage guillotine cuts, so as to minimize either (i) the sum of the perimeters, (ii) the largest perimeter, or (iii) the maximum aspect ratio of the rectangles. These problems play an important role in the ongoing Vietnamese land-allocation reform, as well as in the optimization of matrix multiplication algorithms. We show that the first problem can be solved to optimality in , while the two others are NP-hard. We propose mixed integer linear programming formulations and a binary search-based approach for solving the NP-hard problems. Experimental analyses are conducted to compare the solution approaches in terms of computational efficiency and solution quality, for different objectives.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:UebtZRa9Y70C,,1
1306,Mathematical Programs and Computations for a Class of Anti-aircraft Mission Planning Problems,"The theater defense distribution is an important problem in the military that determines strategies against a sequence of offensive attacks in order to protect his targets. This study focuses on developing mathematical models for three important defense problems that generate anti-aircraft mission plans for a group of missile battalions. The simple Anti-aircraft Launching Assignment problem specifies number of missiles should be launched from each battalion to each fleet of attacking aircraft to maximize the defensive effectiveness, provided that the locations of missile battalions are given. On the other hand, the Anti-aircraft Mission Planning problem maximizes the defender’s effectiveness using all his available battalions, while the Inverse Antiaircraft Mission Planning problem computes necessary weapon resources (battalions and their missiles) to obtain a given defensive effectiveness value. The proposed formulations are Integer Programs and proved as NP-hard. A comprehensive set of experiments is then evaluated to show that these proposed programs can be applied to solve fast real-life instances to optimality.",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:5nxA0vEk-isC,,1
1307,Modelling and Solving Anti-aircraft Mission Planning for Defensive Missile Battalions,"The theater defense distribution is an important problem in the military that determines strategies against a sequence of offensive attacks in order to protect his targets. This study focuses on developing mathematical models for two defense problems that generate anti-aircraft mission plans for a group of missile battalions. While the Anti-aircraft Mission Planning problem maximizes the defender’s effectiveness using all his available battalions, the Inverse Anti-aircraft Mission Planning problem computes necessary weapon resources (battalions and their missiles) to obtain a given defensive effectiveness value. The proposed formulations are Mixed Integer Programs that describe not only the positions of missile battalions, but also engage battalions to fleets of attacking aircrafts. We additionally prove that these problems are NP-hard. A comprehensive set of experiments is then evaluated to show that these …",76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:hqOjcs7Dif8C,,1
1308,On the mathematical program in theater anti-aircraft distribution problem,The military theater anti-aircraft distribution problem is associated with determining positions of defender's missiles within a potential geographic area of operation. This research focuses on formulating and solving a mathematical model to aid decision makers to maximize defender's efficient course of actions assuming that an aircraft attacking plan is observed. The developed model is a Mixed Integer Program (MIP) and solved using our proposed constraint-based local search algorithm as well as CPLEX solver. Experimental results on the simulated data allow us certainly establish the quality of any defensive plan.,76,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=jfV-i0IAAAAJ&citation_for_view=jfV-i0IAAAAJ:0EnyYjriUFMC,,1
1309,Eliminating overfitting of probabilistic topic models on short and noisy text: The role of dropout,"Probabilistic topic models are powerful tools for discovering hidden structures/semantics in discrete data, e.g., texts, images, links. However, on short and noisy texts, directly applying topic models may not work well or face severe overfitting. In this article, we investigate the benefits of dropout for preventing topic models from overfitting. We integrate dropout into several stochastic methods for learning latent Dirichlet allocation (LDA). From extensive experiments on four large-scale datasets, our findings are: (1) dropout helps to prevent overfitting and significantly enhance predictiveness and generalization of LDA on short texts; (2) for long documents, dropout may provide little benefit; (3) dropout can be easily integrated into any learning methods to avoid overfitting for short and noisy text. Furthermore, dropout can be straightforwardly employed in a wide range of topic models. In evidence, we apply dropout to BTM …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:Wp0gIr-vW9MC,,1
1310,An effective and interpretable method for document classification,"As the number of documents has been rapidly increasing in recent time, automatic text categorization is becoming a more important and fundamental task in information retrieval and text mining. Accuracy and interpretability are two important aspects of a text classifier. While the accuracy of a classifier measures the ability to correctly classify unseen data, interpretability is the ability of the classifier to be understood by humans and provide reasons why each data instance is assigned to a label. This paper proposes an interpretable classification method by exploiting the Dirichlet process mixture model of von Mises–Fisher distributions for directional data. By using the labeled information of the training data explicitly and determining automatically the number of topics for each class, the learned topics are coherent, relevant and discriminative. They help interpret as well as distinguish classes. Our experimental …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:Se3iqnhoufwC,,1
1311,Browser Extension-based Crowdsourcing Model for Website Monitoring.,"Websites play an increasingly important role. They are not only a good marketing resource but also a power business tool. To make websites reliable and always available for customers, a large number of website monitoring services have been developed to periodically check the website uptime and performance. These services typically have a very limited number of checkpoints (i.e., geographical locations where we send monitoring requests to check the websites) and internet service providers (ISP) used by these checkpoints; it is because deploying checkpoints with low usage frequency would waste them a huge amount of money. Thus they cannot detect that a website is reachable from a city and an ISP but is unreachable from the others. To address this issue, this paper presents a crowdsourcing-based approach that makes use of browser extensions as checkpoints to monitor websites. Our main contributions include: (i) a website monitoring approach that combines the concepts of crowdsourcing and browser extensions; (ii) a batch processing technique for handling monitoring requests; (iii) the architecture of a conceptual crowdsourcing-based system for website monitoring that can coordinate a large number of crowd members; and (iv) various techniques to enhance the quality of monitoring.",73,https://doi.org/10.1145/3368926.3369706,1,
1312,Towards Open Data Mashups for Data Journalism.,"In line with a broad social and political movement in recent years, public and private sector actors have started to open up their data and to publish it on various portals. However, the actual use of the available open data sets is still rather limited in many sectors. Consequently, the potentials of available open data sets are not fully realized, which may discourage open data publishers. To encourage a broader adoption of open data and to contribute towards making democratic processes more transparent, critical data journalism is essential. In this demo paper, we propose an approach called Open Data Mashups for Data Journalism (ODMOJO) that aims to bridge open data publishers and their consumers, i.e., journalists and society at large, with Linked Data technologies. Specifically, our approach will facilitate the access, reuse, and integration of open data for General Data Journalism. We plan to evaluate our approach with potential journalism partners as well as open data publishers in Austria, to foster further adoption and utilization of open data through data journalism and Linked Data technologies.
",73,http://ceur-ws.org/Vol-2044/paper17/,1,
1313,Multiscale Exploration of Spatial Statistical Datasets: A Linked Data Mashup Approach.,"Many national and international organizations today leverage semantic web technologies to make statistical datasets available as
Linked Open Data (LOD). A key advantages of this approach is that
the data not only becomes publicly available, but also machine-readable
and hence suitable for automated discovery and exploration. Whereas
this has great potential to support interesting use cases, it remains difficult for end users today to utilize and combine these statistical Linked
Data. Three challenges are: (i) directing users to relevant data sources
based on a specified location; (ii) facilitating data integration despite a
lack of outgoing links between datasets; and (iii) offering flexible means
to integrate and aggregate data from various sources. As time and location are highly relevant dimensions in most statistical data, we address
the identified challenges by first constructing geographical metadata for
statistical sources. Following a mashup approach, we introduce mechanisms to recommend interesting datasets to end users and automatically
enable data integration, visualization, and comparisons based on userdefined criteria",73,http://ceur-ws.org/Vol-1550/article-08.pdf,1,
1314,Implementing Linked Widgets: lessons learned for Linked Data developers.,"Seven years after Linked Data has been introduced as a concept to publish data on the web, an abundant cloud of Linked Open Data
(LOD) built upon standard web technologies has emerged. To facilitate
and encourage widespread use of that data, a critical step is now to
streamline the process for creating applications on top of LOD. This paper discusses lessons learned while developing an open standards-based
platform that aims to achieve that by means of Linked Widgets. Whereas
resources are already connected in the LOD cloud, Linked Widgets in a
similar vein aim to alleviate LOD application development in an open
and interlinked fashion. Through reuse, we aim to foster both users’ and
developers’ productivity and creativity",73,http://ceur-ws.org/Vol-1268/paper5.pdf,1,
1315,Open Mashup Platform - A Smart Data Exploration Environment.,"The number of applications designed around Linked Open
Data (LOD) has expanded rapidly in recent years. However, these applications typically do not make use of the vast amounts of LOD datasets,
but only provide access to predefined, domain-specific subsets. Exceptions that do allow for more flexible exploration of LOD are not targeted
at end users, which excludes users who have limited experience with
Semantic Web technologies from realizing the potential of the so-called
LOD cloud. This paper introduces a Mashup Platform that models, manages, reuses, and interconnects LOD web applications, thereby encouraging initiative and creativity of potential users. Figuratively, our approach
allows developers to implement building blocks whereas the platform provides the cement so that end users can build houses by themselves.",73,http://ceur-ws.org/Vol-1272/paper_45.pdf,1,
1316,Exploring linked statistical data using linked widgets.,"The Open Data movement has gained momentum among governments, in the business world, and in the public sector in recent years. This movement has resulted in a growing number of open and accessible datasets that have established a solid basis for enhanced service offerings and improved experiences for citizens and businesses. Statistical data, which embodies a big portion of Open Data, comprises a wide range of domains including finance, demographics, transportation, employment, etc. Statistical data plays an important role in public policy formation and as a facilitator for informed decision-making in the private sector. Linked Statistical Data is an evolving concept that combines the richness of Linked Data (a set of best practices for publishing and connecting structured data on the Web) with the descriptiveness of statistical data to integrate data from multiple sources and put it in a semantic context. In this short paper, Linked Statistical Data limitations and challenges are explored before introducing Linked Widgets as an innovative approach.",73,https://doi.org/10.1145/2676585.2676589,1,
1317,Enhance Java Software Development with Knowledge Acquisition and Management Tools.,"The Semantic Web can be a very promising platform for developing knowledge management systems. It has been applied in many domains, especially in Software Engineering. The main benefit is high improvement in the precision by searching for knowledge, as well as the possibility to retrieve a composition of knowledge sources which are relevant for the software development process. However, the problem is how to represent knowledge in the machine-understandable form, so that relevant knowledge can be found by machine agents. This paper will present a framework that improves Java software development process with our knowledge acquisition and management tools.",73,https://doi.org/10.1109/KSE.2011.19,1,
1319,A New Approach to the Improvement of the Federated Deep Learning Model in a Distributed Environment.,"The federated deep learning model has been successfully studied and applied in a distributed environment. The method aggregates the weight set on the server by averaging the component weight sets. The limitation of the method is that the number of training samples on the clients is different but the weights are averaged, so the importance of the component weight sets cannot be clearly shown. Therefore, this paper proposes a new method to synthesize the weight set for the distributed federated deep learning model based on the importance of the component weight sets. The importance is proportional to the number of training data samples. That is, the larger the dataset size, the more important the weight set. The proposed method is tested with the MNIST dataset by the K-fold method. The improved accuracy compared to the old method is 2.54%.

The federated deep learning model has been successfully studied and applied in a distributed environment. The method aggregates the weight set on the server by averaging the component weight sets. The limitation of the method is that the number of training samples on the clients is different but the weights are averaged, so the importance of the component weight sets cannot be clearly shown. Therefore, this paper proposes a new method to synthesize the weight set for the distributed federated deep learning model based on the importance of the component weight sets. The importance is proportional to the number of training data samples. That is, the larger the dataset size, the more important the weight set. The proposed method is tested with the MNIST dataset by the K-fold method. The improved accuracy compared to the old method is 2.54%.

The federated deep learning model has been successfully studied and applied in a distributed environment. The method aggregates the weight set on the server by averaging the component weight sets. The limitation of the method is that the number of training samples on the clients is different but the weights are averaged, so the importance of the component weight sets cannot be clearly shown. Therefore, this paper proposes a new method to synthesize the weight set for the distributed federated deep learning model based on the importance of the component weight sets. The importance is proportional to the number of training data samples. That is, the larger the dataset size, the more important the weight set. The proposed method is tested with the MNIST dataset by the K-fold method. The improved accuracy compared to the old method is 2.54%.

",74,https://doi.org/10.1007/978-3-030-92666-3_25,1,
1320,Entropy Correlation and Its Impacts on Data Aggregation in a Wireless Sensor Network.,"A correlation characteristic has significant potential advantages for the development of efficient communication protocols in wireless sensor networks (WSNs). To exploit the correlation in WSNs, the correlation model is required. However, most of the present correlation models are linear and distance-dependent. This paper proposes a general distance-independent entropy correlation model based on the relation between joint entropy and the number of members in a group. This relation is estimated using entropy of individual members and entropy correlation coefficients of member pairs. The proposed model is then applied to evaluate two data aggregation schemes in WSNs including data compression and representative schemes. In the data compression scheme, some main routing strategies are compared and evaluated to find the most appropriate strategy. In the representative scheme, with the desired distortion requirement, a method to calculate the number of representative nodes and the selection of these nodes are proposed. The practical validations showed the effectiveness of the proposed correlation model and data reduction schemes",74,https://doi.org/10.3390/s18093118,1,
1321,Collaborative topic model for Poisson distributed ratings,"We present Collaborative Topic Model for Poisson distributed ratings (CTMP), a hybrid and interpretable probabilistic content-based collaborative filtering model for recommender system. The model enables both content representation by admixture topic modelling, and computational efficiency from Poisson factorization living together under one tightly coupled probabilistic model, thus addressing the limitation of previous methods. CTMP excels in predictive performance under different real-world recommendation contexts, and easily scales to big datasets, while recovering interpretable user profiles. Moreover, our empirical study also shows strong evidence that sparsity in the estimates of topic mixture can be recovered via learning, despite not being specified in the model. The sparse representation derived from CTMP would allow efficient storage of the item contents, consequently providing a computational …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:4TOpqqG69KYC,,1
1322,Mathematical fuzzy logic with many dual hedges.,"This paper proposes an axiomatization of mathematical fuzzy logic with many dual hedges. We extend an axiomatization of mathematical fuzzy logic with one truth-stressing and one truth-depressing hedges as an expansion of a core fuzzy logic with new unary connectives by Esteva et al. for many truth-stressing and truth-depressing hedges, in which each hedge can have its own dual one. Our motivation is that, in the real world, we usually use many hedges at the same time, e.g., very, highly, rather, and slightly, to express different levels of emphasis, and each hedge seems to have a dual one, e.g., slightly (true) and rather (true) can be seen as a dual hedge of very (true) and highly (true), respectively. The proposed logic not only covers a large class of hedge functions but also has all completeness properties as the underlying core fuzzy logic w.r.t. the class of their chains as well as a number of special subclasses of their chains, including standard completeness.",75,https://doi.org/10.1145/2676585.2676619,1,
1324,Fixpoint Semantics and Completeness of the Computational Model for Fuzzy Linguistic Logic Programming.,"Book cover
International Conference on Intelligent Computing

ICIC 2008: Advanced Intelligent Computing Theories and Applications. With Aspects of Artificial Intelligence pp 420–429Cite as

Fixpoint Semantics and Completeness of the Computational Model for Fuzzy Linguistic Logic Programming
Van Hung Le, Fei Liu & Dinh Khang Tran 
Conference paper
1723 Accesses

Part of the Lecture Notes in Computer Science book series (LNAI,volume 5227)

Abstract
Fuzzy linguistic logic programming, which is a result of integrating hedge algebras and fuzzy logic programming, is proposed to facilitate the representation and reasoning on knowledge expressed in natural language, in which vague sentences are usually given a degree of truth stated in linguistic terms rather than a number, and linguistic hedges are very often used. To compute the truth value of a query, a computational model which directly manipulates linguistic terms is provided. The computational model has been proved to be sound. This paper presents a fixpoint semantics for fuzzy linguistic logic programs and based on it proves the completeness of the computational model.",75,https://doi.org/10.1007/978-3-540-85984-0_51,1,
1325,"Comments on ""Fuzzy data dependencies and implication of fuzzy data dependencies"".",In this paper we discuss the results given by Wei-Yi Liu in the article entitled “Fuzzy data dependencies and implication of fuzzy data dependencies” (Fuzzy Sets and Systems 92 (1997) 341) and we get some different conclusions,75,https://doi.org/10.1016/j.fss.2004.01.007,1,
1327,Enabling hierarchical Dirichlet processes to work better for short texts at large scale,"Analyzing texts from social media often encounters many challenges, including shortness, dynamic, and huge size. Short texts do not provide enough information so that statistical models often fail to work. In this paper, we present a very simple approach (namely, bag-of-biterms) that helps statistical models such as Hierarchical Dirichlet Processes (HDP) to work well with short texts. By using both terms (words) and biterms to represent documents, bag-of-biterms (BoB) provides significant benefits: (1) it naturally lengthens representation and thus helps us reduce bad effects of shortness; (2) it enables the posterior inference in a large class of probabilistic models including HDP to be less intractable; (3) no modification of existing models/methods is necessary, and thus BoB can be easily employed in a wide class of statistical models. To evaluate those benefits of BoB, we take Online HDP into account in that …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:hqOjcs7Dif8C,,1
1328,Document clustering using dirichlet process mixture model of von mises-fisher distributions,"Document clustering has become an increasingly important technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. This paper proposes a Dirichlet process mixture (DPM) model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. We have developed a mean-field variational inference algorithm for the DPM model of vMFs that is applied to clustering text documents. Using this model, the number of clusters is determined automatically after the clustering process rather than pre-estimated. We conducted extensive experiments to evaluate the proposed approach on a large number of high dimensional text datasets. Empirical experimental results over NMI (Normalized Mutual Information) and Purity evaluation measures demonstrate that our approach …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:kNdYIx-mwKoC,,1
1329,Bag of biterms modeling for short texts,"Analyzing texts from social media encounters many challenges due to their unique characteristics of shortness, massiveness, and dynamic. Short texts do not provide enough context information, causing the failure of the traditional statistical models. Furthermore, many applications often face with massive and dynamic short texts, causing various computational challenges to the current batch learning algorithms. This paper presents a novel framework, namely bag of biterms modeling (BBM), for modeling massive, dynamic, and short text collections. BBM comprises of two main ingredients: (1) the concept of bag of biterms (BoB) for representing documents, and (2) a simple way to help statistical models to include BoB. Our framework can be easily deployed for a large class of probabilistic models, and we demonstrate its usefulness with two well-known models: latent Dirichlet allocation (LDA) and hierarchical …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:ZeXyd9-uunAC,,1
1330,Keeping priors in streaming bayesian learning,"Exploiting prior knowledge in the Bayesian learning process is one way to improve the quality of Bayesian model. To the best of our knowledge, however, there is no formal research about the influence of prior in streaming environment. In this paper, we address the problem of using prior knowledge in streaming Bayesian learning, and develop a framework for keeping priors in streaming learning (KPS) that maintains knowledge from the prior through each minibatch of streaming data. We demonstrate the performance of our framework in two scenarios: streaming learning for latent Dirichlet allocation and streaming text classification in comparison with methods that do not keep prior.",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:qxL8FJ1GzNcC,,1
1331,An interpretable method for text summarization based on simplicial non-negative matrix factorization,"Automatic text summarization plays an important role in information retrieval and text mining. Furthermore, it provides an useful solution to the information overload problem. In this paper, we propose a simplicial NMF-based unsupervised generic document summarization method which can inherit some advantages of simplicial NMF such as easy interpretability, low complexity, convexity and sparsity. By focusing on the major topics contained within every sentence as well as entire document, our method generates better summaries with less repetition. The effectiveness of our method is proved by experimental results. On the summarization performance, our approach obtains mostly higher ROUGE scores than NMF-based method.",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:UebtZRa9Y70C,,1
1332,Boosting prior knowledge in streaming variational Bayes,"Exploiting prior/human knowledge is an effective way to enhance Bayesian models, especially in cases of sparse or noisy data, for which building an entirely new model is not always possible. There is a lack of studies on the effect of external prior knowledge in streaming environments, where the data come sequentially and infinitely. In this work, we show the problem of vanishing prior knowledge in streaming variational Bayes. This is a serious drawback in various applications. We then develop a simple framework to boost the external prior when learning a Bayesian model from data streams. By boosting, the prior knowledge can be maintained and efficiently exploited through each minibatch of streaming data. We evaluate the performance of our framework in four scenarios: streaming in synthetic data, streaming sentiment analysis, streaming learning for latent Dirichlet allocation, and streaming text classification …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:hFOr9nPyWt4C,,1
1333,Neural poisson factorization,"In this work, we focus on dealing with a sparse users' feedback matrix and short descriptions/contents of items in recommender systems. We propose the Neural Poisson factorization (NPF) model which is a hybrid of deep learning and Poisson factorization. While Poisson factorization is suitable to model discrete, massive and sparse feedback, using a deep neural network and pre-trained word embeddings can learn hidden semantic in short item descriptions well. Therefore, NPF overcomes the limitation of existing models when dealing with short texts and a sparse feedback matrix. Moreover, we develop a random view algorithm based on stochastic learning for our model, in which each user is only viewed a random subset of items and his/her feedback on the subset is used to update his/her representation in each iteration. This approach is reasonable because each user can only know or view a partial subset of …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:qUcmZB5y_30C,,1
1334,Infinite Dropout for training Bayesian models from data streams,"The ability to continuously train Bayesian models in streaming environments is highly important in the era of big data. However, it has to face the famous stability-plasticity dilemma and the problem of noisy and sparse data. We propose a novel and easy-to-implement framework, called Infinite Dropout (iDropout), to address these challenges. iDropout has an easy mechanism to balance between old and new information, which allows models to trade off stability against plasticity. Thanks to the ability to reduce overfitting and the ensemble property of Dropout, our framework obtains better generalization, thus effectively handles undesirable effects of noise and sparsity. Further, iDropout is able to adapt quickly to abnormal changes in data streams. We theoretically analyze the equivalence of Dropout in iDropout to a regularizer, well applied to a much larger context than what was known before. Extensive experiments …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:dhFuZR0502QC,,1
1336,From implicit to explicit feedback: A deep neural network for modeling the sequential behavior of online users,"We demonstrate the advantages of taking into account multiple types of behavior in recommendation systems. Intuitively, each user has to do some\textbf {implicit} actions (eg, click) before making an\textbf {explicit} decision (eg, purchase). Previous works showed that implicit and explicit feedback has distinct properties to make a useful recommendation. However, these works exploit implicit and explicit behavior separately and therefore ignore the semantic of interaction between users and items. In this paper, we propose a novel model namely\textit {Implicit to Explicit (ITE)} which directly models the order of user actions. Furthermore, we present an extended version of ITE, namely\textit {Implicit to Explicit with Side information (ITE-Si)}, which incorporates side information to enrich the representations of users and items. The experimental results show that both ITE and ITE-Si outperform existing recommendation systems and also demonstrate the effectiveness of side information in two large scale datasets.",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:QIV2ME_5wuYC,,1
1337,Employing the correspondence of relations and connectives to identify implicit discourse relations via label embeddings,"It has been shown that implicit connectives can be exploited to improve the performance of the models for implicit discourse relation recognition (IDRR). An important property of the implicit connectives is that they can be accurately mapped into the discourse relations conveying their functions. In this work, we explore this property in a multi-task learning framework for IDRR in which the relations and the connectives are simultaneously predicted, and the mapping is leveraged to transfer knowledge between the two prediction tasks via the embeddings of relations and connectives. We propose several techniques to enable such knowledge transfer that yield the state-of-the-art performance for IDRR on several settings of the benchmark dataset (ie, the Penn Discourse Treebank dataset).",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:mVmsd5A6BfQC,,1
1338,Enriching user representation in neural matrix factorization,"Personalized recommendation is a crucial problem in the era of information overload. One of the most prominent methods addressing this problem is collaborative filtering (CF). Recently, NeuMF [1] has employed Deep Neural Network to facilitate traditional CF methods. Although NeuMF achieved the state-of-the-art performance, it is expected that predictive accuracy can be improved by enriching user representation. In this paper, we propose a neural network incorporating latent features and personalized information of users, namely Feature based Neural Matrix Factorization (FeaNMF). In FeaNMF, personalized information can be considered as auxiliary features which are generated by associating past behavior of users (e.g, purchase history, browsing activity, watching habits) with item categories. By explicitly modeling personalized information in the metric of item categories, the major advantages of FeaNMF …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:4DMP91E08xMC,,1
1339,Effective and interpretable document classification using distinctly labeled Dirichlet process mixture models of von Mises-Fisher distributions,"Document Classification is essential to information retrieval and text mining. Accuracy and interpretability are two important aspects of text classifiers. This paper proposes an interpretable classification method (DLDPvMFs) by using the Dirichlet process mixture (DPM) model to discover the hidden topics distinctly within each label for classification of directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. We use a mean-field variational inference algorithm when developing DLDPvMFs. By using the label information of the training data explicitly and determining automatically the number of topics for each label to find the topical space, class topics are coherent, relevant and discriminative and since they help us interpret class’s label as well as distinguish classes. Our experimental results showed the advantages of our approach via …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:8k81kl-MbHgC,,1
1340,Document clustering using mixture model of von Mises-Fisher distributions on document manifold,"Document clustering has become an increasingly important technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. The generative model for document clustering based on the von Mises-Fisher (vMF) distribution generally produces better clustering results than other generative models. However, in fact, it is more natural and reasonable to assume that the document space is a manifold and the probability distribution that generates the data is supported on a document manifold. In this paper, we propose a regularized probabilistic model based on manifold structure for data clustering, called Laplacian regularized vMF Mixture Model (LapvMFs), which explicitly considers the manifold structure. We have developed a generalized mean-field variational inference algorithm for the LapvMFs. Extensive experimental results on a large number of high …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:KlAtU1dfN6UC,,1
1341,Document classification using semi-supervived mixture model of von mises-fisher distributions on document manifold,"Document classifications is essential to information retrieval and text mining. In real life, unlabeled data is readily available whereas labeled ones are often laborious, expensive and slow to obtain. This paper proposes a novel Document Classification approach based on semi-supervised vMF mixture model on document manifold, called Laplacian regularized Semi-Supervised vMF Mixture Model (LapSSvMFs), which explicitly considers the manifold structure of document space to exploit efficiently both labeled and unlabeled data for classification. We have developed a generalized mean-field variational inference algorithm for the LapSSvMFs. Experimental results show that our approach preserves the best accuracy of purely graph-based transductive methods when the data has"" manifold structure"". Furthermore, high accuracy are obtained even for overlapping and fairly skewed datasets in comparison with other …",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:MXK_kJrjxJIC,,1
1342,Dynamic transformation of prior knowledge into Bayesian models for data streams,"We consider how to effectively use prior knowledge when learning a Bayesian model from streaming environments where the data come infinitely and sequentially. This problem is highly important in the era of data explosion and rich sources of precious external knowledge such as pre-trained models, ontologies, Wikipedia, etc. We show that some existing approaches can forget any knowledge very fast. We then propose a novel framework that enables to incorporate the prior knowledge of different forms into a base Bayesian model for data streams. Our framework subsumes some existing popular models for time-series/dynamic data. Extensive experiments show that our framework outperforms existing methods with a large margin. In particular, our framework can help Bayesian models generalize well on extremely short text while other methods overfit. The implementation of our framework is available at https://github.com/bachtranxuan/TPS.git.",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:7PzlFSSx8tAC,,1
1343,How to make a machine learn continuously: a tutorial of the Bayesian approach,"How to build a machine that can continuously learn from observations in its life and make accurate inference/prediction? This is one of the central questions in Artificial Intelligence. Many challenges are present, such as the difficulty of learning from infinitely many observations (data), the dynamic nature of the environments, noisy and sparse data, the intractability of posterior inference, etc. This tutorial will discuss how the Bayesian approach provides a natural and efficient answer. We will start from the basic of Bayesian models, and then the variational Bayes method for inference. Next, we will discuss how to learn a Bayesian model from an infinite sequence of data. Some challenges such as catastrophic forgetting phenomenon, concept drifts, and overfitting will be discussed.",77,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=tZ78MoQAAAAJ&citation_for_view=tZ78MoQAAAAJ:aqlVkmm33-oC,,1
1350,Towards a blockchain-based certificate authentication system in Vietnam,"Anti-forgery information, transaction verification, and smart contract are functionalities of blockchain technology that can change the traditional business processes of IT applications. These functionalities increase the data transparency, and trust of users in the new application models, thus resolving many different social problems today. In this work, we take all the advantages of this technology to build a blockchain-based authentication system (called the Vietnamese Educational Certification blockchain, which stands for VECefblock) to deal with the delimitation of fake certificate issues in Vietnam. In this direction, firstly, we categorize and analyze blockchain research and application trends to make out our contributions in this domain. Our motivating factor is to curb fake certificates in Vietnam by applying the suitability of blockchain technology to the problem domain. This study proposed some blockchain-based application development principles in order to build a step by step VECefblock with the following procedures: designing overall architecture along with business processes, data mapping structure and implementing the decentralized application that can meet the specific Vietnamese requirements. To test system functionalities, we used Hyperledger Fabric as a blockchain platform that is deployed on the Amazon EC2 cloud. Through performance evaluations, we proved the operability of VECefblock in the practical deployment environment. This experiment also shows the feasibility of our proposal, thus promoting the application of blockchain technology to deal with social problems in general as well as certificate management in Vietnam.",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:YsMSGLbcyi4C,,1
1351,Challenges and strategies for developing decentralized applications based on blockchain technology,"This paper reveals challenges and strategies of data models, deployment scenarios, business processes, and consensus models when enterprises or individuals want to apply blockchain to their information system in order to leverage advantages of the technology. A blockchain is a digital ledger, which is encrypted and stored on multiple computers in a public or private network and the data stored therein cannot be altered or deleted by a single individual. The changes for existing applications using blockchain bring difficulties for both managers and developers. The decentralized manner is different from centralized ones in aspects of how to store data in blocks, who has reading and writing permission, and how to validate transactions among many parties. We illustrate our proposal by developing an example blockchain application that enables to authenticate Vietnamese certificates at schools and …",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:Tyk-4Ss8FVUC,,1
1352,HPC-Reuse: efficient process creation for running MPI and Hadoop MapReduce on supercomputers,"Hadoop and Spark analytics are used widely for large-scale data processing on commodity clusters. It is better choice to run them on supercomputers in aspects of productivity and maturity rather than developing new frameworks from scratch. YARN, a key component of Hadoop, is responsible for resource management. YARN adopts dynamic management for job execution and scheduling. We identify three Ds (3D) dynamic characteristics from YARN-like management: on-Demand (processes created during job execution), Diverse job, and Detailed (fine-grained allocation). The dynamic management does not fit into typical resource managers on supercomputers, for example PBS, that are identified having three Ss (3S) static characteristics: Stationary (no newly created process during execution), Single job, and Shallow (coarse-grained allocation). In this paper, we propose HPC-Reuse located between YARN-like …",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:9yKSN-GCB0IC,,1
1353,"B-Box-A Decentralized Storage System Using IPFS, Attributed-based Encryption, and Blockchain","In recent years, centralized storage systems have been extensively adopted by many companies, organizations, and individuals for storing and sharing data. These systems, however, make concerns for users of a single point of failure and the involvement of a centralized entity or third party. Therefore, there is a need for developing decentralized storage systems to overcome the drawbacks of traditional approach. In order to enhance secure and transparent characteristics of decentralized storage systems, in this paper, we present a combination of IPFS (InterPlanetary File System), ABE (Attribute-based Encryption), Multi-Authority ABE (MA-ABE), and Ethereum blockchain. In particular, we facilitate the advantages of IPFS network to store user's data in a distributed manner. Furthermore, we make the use of MA-ABE to encrypt a document, which an user needs to share it among multiple organizations. The hash …",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:eQOLeE2rZwMC,,1
1354,Towards blockchainizing land valuation certificate management procedures in Vietnam,"In this paper, we concentrate on the process of managing the land valuation certificate using blockchain in Vietnam. To approach this problem, we base on the e-government application framework published by the Ministry of Information and Communication and Ministry of Natural Resources and Environment to develop a blockchain-based component which will be linked to the framework. Our component has three layers corresponding to three main layers of the architecture. After that, we develop a decentralized application serving the procedures of land valuation certificate management. With our application, certificates are stored and can be traced from the blockchain network. Our component hence helps create the transparency of management processes. We also carry out preliminary experiments using the proposed system to prove its feasibility during test operation processes. Our research also paves the …",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:WF5omc3nYNoC,,1
1355,Semem: Deployment of mpi-based in-memory storage for hadoop on supercomputers,"This paper reports our experiments to compare various deployment strategies of memcached-like in-memory storage for Hadoop on supercomputers, where each node often does not have a local disk but shares a slow central disk. For the experiments, we developed our own memcached-like file system, named SEMem, for Hadoop. Since SEMem was designed for supercomputers, it uses MPI for communication. SEMem is configurable to adopt various deployment strategies and our experiments revealed that a good deployment strategy was allocating some nodes that work only for in-memory storage but do not directly perform map-reduce computation.",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:u5HHmVD_uO8C,,1
1356,Heatmap rendering from large-scale distributed datasets using cloud computing,"Heatmap is one of the most popular visualizations of gaze behavior, however, increasingly voluminous streams of eye-tracking data make processing of such visualization computationally demanding. Because of high requirements on a single processing machine, real-time visualizations from multiple users are unfeasible if rendered locally. We designed a framework that collects data from multiple eye-trackers regardless of their physical location, analyses these streams, and renders heatmaps in real-time. We propose a cloud computing architecture (EyeCloud) consisting of master and slave nodes on a cloud cluster, and a web interface for fast computation and effective aggregation of the large volumes of eye-tracking data. In experimental studies of the feasibility and effectiveness, we built a cloud cluster on a well-known service, implemented the architecture and reported on a comparison between the proposed …",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:2osOgNQ5qMEC,,1
1357,Real-time Eye tracking Analysis from Large scale Dataset using Cloud computing,"Eye tracking is known in usability and psychology studies and often used mainly in laboratories for research related to human visual system, cognitive process, and human-computer interaction. To our knowledge, there has been no eye tracking technology so far that can be used widely in everyday life since eye tracking is suffering from slow local computation of huge data aggregation and analysis that are preventing it from online processing and pervasive usage. We believe that cloud computing can overcome slow local computation and speed up eye tracking applications. The main purpose of this thesis is to leverage cloud computing infrastructure for eye tracking research firstly and gradually to its real life usage. First of all, we propose a cloud-based architecture (EyeCloud) in which real-time eye tracking applications can be deployed. Secondly, we present designs of three sample applications that are installed successfully on the proposed system: online fixation detection, real-time heatmap rendering from many eye trackers, and classification based on eye movement features. Finally, evaluation of the three applications compared to local processing is given, and benefits and limitations of the cloud computing approach is discussed as well.",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:W7OEmFMy1HYC,,1
1358,BML: A Data Mapping Language for Blockchain Platforms,"Currently, all blockchain-based applications conduct two primary operations, i.e., writing data on blockchain networks and reading these data from the networks. These tasks require users to have considerable knowledge in blockchain technology, and they become even more challenging if users want to utilize different blockchain platforms to write and read data. So far, we have not had a uniform mechanism to perform write and read operations on various blockchain platforms. In addition, writing a huge amount of data on blockchain networks is a time-consuming task and requires considerable transaction fees. To address these issues, we present in the paper a data mapping language named BML. BML allows developers to uniformly define mappings for data transformation from traditional data storage mechanisms into blockchain networks. Conversely, this language also assists users in reading transformed data …",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:_FxGoFyzp5QC,,1
1359,A development platform for embedded domain-specific languages,"The use of domain-specific languages (DSLs) is a promising approach to helping programmers write an efficient program for high-performance computing. The programmers would feel difficulties in writing such a program by hand with only low-level abstractions, such as arrays and loops, provided by a general-purpose language. This chapter presents our new implementation technique for domain-specific languages. Since existing techniques are not satisfactory, we developed our technique called deep reification. This chapter also presents Bytespresso, which is our prototype system to use deep reification. Several Java-embedded DSLs implemented with Bytespresso are presented to assess the effectiveness of deep reification and Bytespresso. Program fragments written in these DSLs are embedded in Java, but they are dynamically off-loaded to native hardware to obtain good execution performance …",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:IjCSPb-OGe4C,,1
1360,A Novel Approach for Developing Decentralized Storage and Sharing Systems,"In recent years, the proliferation of blockchain technology has opened many research directions. In this context, the combination of blockchain-based techniques and traditional methods to improve existing systems has received significant interest from researchers. In this paper, we present a decentralized storage and sharing system based on a combination of IPFS (Inter-Planetary File System), encryption technologies (including Advanced Encryption Standard (AES), Elliptic Curve Cryptosystem (ECC), ABE (Attribute-based Encryption), Multi-Authority ABE (MA-ABE)), and multichain. In particular, we facilitate the advantages of the IPFS network to store user's data in a distributed manner. Furthermore, we make use of a cryptographer to protect the privacy of data. The hash returned by the IPFS network will be stored in our multichain architecture to provide transparency for all users participating in the system. To the …",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:roLk4NBRz8UC,,1
1361,In-memory Hadoop on supercomputers using Memcached-like nodes for data storage only,"In this paper, we propose an in-memory architecture called H-Memcached, which can solve the mismatch of local disks on modern supercomputers. It helps avoid out of memory and makes large-scale datasets fit into memory. Hadoop and Spark are good choices to run MapReduce and Machine learning algorithms, such as PageRank and K-means clustering. A supercomputer is a high-speed network cluster consisting of thousands of compute nodes where local disks are typically not available. All nodes share the same disk called the central disk, and disk I/O to that storage is expensive. To avoid disk I/O, in-memory Hadoop/Spark is required to fit into supercomputer environment. However, memory is limited on each node and out of memory happens when data does not fit into memory. In our proposal, a group of nodes are allocated and used as a storage device instead of the central disk. The uniqueness of H …",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:zYLM7Y9cAGgC,,1
1362,Effective Deployment of Data-intensive Frameworks on Supercomputers,"The goal of this research is to achieve a better performance of popular data-intensive frameworks, for example Hadoop and Spark, with only small modifications when running on modern supercomputers. Big data analytics applications have been becoming more important and widely being demanded to process large-scale datasets in both industry and academia. Compared with developing a new data-intensive application from scratch, using existing popular data-intensive application frameworks to develop is a better choice in aspects of productivity and maturity. Supercomputers are potentially faster than commodity clusters, such as Amazon EC2 cloud, when running data-intensive applications due to their high-performance and high-cost hardware. However, the current supercomputer design focuses more on compute-intensive applications rather than data-intensive ones, so it is hard to achieve the best performance of the hardware when running data-intensive applications on supercomputers. This is partly because it is also important to keep the original data-intensive frameworks’ source code as much as possible since minimizing the cost of changes in the architecture helps increase productivity and easily upgrade to newer versions.",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:Y0pCki6q_DkC,,1
1364,Improving Hadoop MapReduce Performance on the FX10 supercomputer with JVM Reuse,"Hadoop is known as the most popular open-source MapReduce implementation that is used widely in practice to process large datasets. In the aspects of productivity and maturity, it is a good choice to run MapReduce on supercomputers that are urgently dealing with data-intensive problems. Our examination reveals that in iterative MapReduce computation, such as PageRank and K-means clustering, the start-up and initializing time is relatively long in comparison with overall execution time. Furthermore, on the FX10 supercomputer, MPI is the de facto communication and required for high-speed communication, but MPI is not available over Hadoop processes due to the FX10’s specification. In this work, we propose JVM Reuse and its implementation, a process pool, for the sake of Hadoop avoiding the overhead of JVM start-up time and enabling MPI communication. We also present a MPI-based shuffling engine …",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:d1gkVwhDpl0C,,1
1365,fNode: reducing network packet transmission overhead in indoor heterogeneous wireless sensor networks (ユビキタス・センサネットワーク),"Wireless sensor networks (WSNs) have become popular because of the availability of a variety of sensor node hardwares, each having its own unique capability. However, combining several sensor node platforms for a certain wireless sensor network application meets many difficulties such as the differences in radio frequency, network protocols and system architectures. Such WSNs are often referred to as a heterogeneous WSN. In our research, we aim to improve performance of a heterogeneous WSN deployed in indoor building environment under common sensor net-working architectures by reducing the total packet transmission occurring. Our approach is to combine the existing WSN infrastructure with fNode, our implementation of a sensor node capable of forwarding packets of different communication architecture, which is used to replace the redundant nodes within the network. We have implemented and evaluated our work under a real-life indoor environment using 15 sensor nodes. Our experimental results show that fNode reduces the total packet transmission overhead, power consumption and latency within the WSN by approximately 30%.",78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:UeHWp8X0CEIC,,1
1367,author pages,Presents an index of the authors whose articles are published in the conference proceedings record.,78,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bGBBNOcAAAAJ&citation_for_view=bGBBNOcAAAAJ:ufrVoPGSRksC,,1
1368,Diaphragmatic fatigue and breathing pattern during weaning from mechanical ventilation in COPD patients,"The medium-term outcome of weaning from mechanical ventilation in COPD patients is not easy to anticipate because a respiratory fatigue may eventually develop. We evaluated the diaphragmatic function and the breathing pattern during 40 weaning trials on 15 patients ventilated after acute respiratory failure. We formed two groups according to the success (group B, n=18) or failure (group A, n=19) of the medium-term attempt (group A/less than 10 hours; group B/more than 12 hours). Provided the patients showed the classic weaning criteria (tidal volume >5 ml/kg, respiratory frequency <30 breaths per minute, PaO2 >50 mm Hg), the study of the breathing pattern did not allow differentiation between the groups. However, the transdiaphragmatic pressure (Pdi) and the Pdimax, which gave an indication of the power of diaphragm contraction, dropped early in the group that could not stand weaning, with an increase …",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:0EnyYjriUFMC,,1
1369,Web service gateway-A step forward to e-business,"Business-to-business will be a considerable market in the near future of Internet e-business. In this future market, several providers need to be able to integrate or exchange information in providing a global service. The problem that we want to tackle in this paper is related to the existing information sources in the current Internet environment. That is how to integrate existing Web sites each other to become a new Internet service. The difficulty comes from a historical objective. Internet Web sites were developed for human users browsing and so, they do not support machine-understandable as well as interprovider interaction. To overcome this gap, we need a framework to systematically migrate the existing presentation-oriented Web sites to service-oriented one. Evidently, redeveloping all of them is an unacceptable solution. In this paper, we propose a mechanism of Web service gateway in which existing Web …",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:u-x6o8ySG0sC,,1
1370,Research on applying hierachical clustered based routing technique using artificial intelligence algorithms for quality of service of service based routing,"MANET (Mobile Ad-Hoc Network) is autonomous system, not based on existing infrastructure. Nodes usually change their position, network topology changes very fast. Service Based Routing is inherited from the model of Content Based Routing-CBR that manages and classifies many of network services. In order making nodes to communicate quickly and stablely, it requires applying some methodologies to reduce overhead and delay as well as power consumption. Therefore, this paper mentions a hierarchical clustered based routing using Tree. is a data structure with fast algorithms to establish, insert, update. It has been used in many fields effectively and efficiently such as Google Map, Forwarding technique, computer virtual memory. In combination with some artificial intelligence algorithms to make cluster, find optimized routes, multicast trees for effective communication. Fuzzy logic and genetic al. are proved to be very compatible with Manet. A genetic algorithm is used to choose optimized clusters, Fuzzy logic is applied to choose the cluster head and members of each cluster. Multiple paths routing is very importance for routing in MANET that making Quality of Service by Ant Colony Optimization Algorithm.",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:9yKSN-GCB0IC,,1
1371,How to make web sites talk together: web service solution,"Integrating web sites to provide more efficient services is a very promising way in the Internet. For example searching house for rent based on train system or preparing a holiday with several constrains such as hotel, air ticket, etc... From resource view point, current web sites in the Internet already provide quite enough information. However, the challenge is these web sites just provide information but do not support any mechanism to exchange them. As a consequence, it is very often that a human user has to take the role to"" link"" several web sites by browsing each one and get the concrete information. The reason comes from a historical objective. Web sites were developed for human users browsing and so, they do not support any machine-understandable mechanism. Current researches in WWW environment already propose several solutions to make newly web sites become understandable to other web sites …",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:u5HHmVD_uO8C,,1
1372,"Research on innovating, evaluating and applying multicast routing technique for routing messages in service-oriented routing","MANET (short for Mobile Ad-Hoc Network) consists of a set of mobile network nodes, network configuration changes very fast. Each node will act as a router to maintain network operation. There is no central node for controlling entire network. So routing in MANET is very important.",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:UeHWp8X0CEIC,,1
1373,Innovating R tree to create summary filter for message forwarding technique in service-based routing,"In service-oriented routing [5], the problem for storing routing table of filters includes search predicates received from subscribers through subscription messages is an important job. When a content request happens, subscriber will create one subscription message, the subscription message stores several kinds of information, the most important content is filter that is a conjunction of some constraints [5]. One filter is denoted by F character, that has mathematical formula: F=P1 ΛP2 Λ …Λ Pn (1), in which P1 is service request, has format: P1 = ‘Service_name = requested_service_name’. Every Pi (in which i= ) is a constraint that is formed by three components: (Key, op, Value), Key is a keyword for searching, op is an operator, Value is searching condition. Key belongs to the set of name of properties of content that the requested service supplies. Op is operator that depends on the type of data of the Key …",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:qjMakFHDy7sC,,1
1374,Research on innovating and applying evolutionary algorithms based hierarchical clustering and multiple paths routing for guaranteed quality of service on service based routing,"In Service Based Routing (SBR), data is transmitted from a source node to destination nodes are not depended on destination addresses. Hence, it is comfortable with new advanced technology as cloud computing and also flexible and reliable. Genetic and Queen-Bee algorithms (GA, QB) are artificial intelligence techniques for combinatorial optimization problems solving based on some natural rules. In which GA is a branch of the evolutionary strategies that uses some principles of evolution theory, such as natural selection, mutation and crossover. QB is performed based on GA for energy saving. The tree is an effective data structure that can be used to organize a hierarchical clustering network with fast establishing, updating, tuning algorithms. The usage of the Greedy algorithm to find cyclic routes or multiple paths on each trunk by multiple criterions to transmit data effectively.",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:zYLM7Y9cAGgC,,1
1375,"Research on innovating, evaluating and applying multicast routing technique for routing messages in service-oriented routing","MANET (short for Mobile Ad-Hoc Network) consists of a set of mobile network nodes, network configuration changes very fast. Each node will act as a router to maintain network operation. There is no central node for controlling entire network. So routing in MANET is very important.",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:_kc_bZDykSQC,,1
1376,Information processing apparatus and method,"An analysis unit receives a sample of request data sent from a browser to a Web application. The browser and the Web application communicate data using HTTP protocol. The Web application receives the request data using GET method or POST method. The analysis unit analyzes a rule of HTTP protocol based on the sample. The rule includes a number of the request data, a name of the request data, and a format of response data corresponding to the request data. A generation unit generates a protocol converter to connect a communication of a Web service with a communication of the Web application based on the rule.",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:d1gkVwhDpl0C,,1
1377,B2B with Toshiba Web Service Gateway.,"Business-to-Business will be a considerable market in the near future of Internet e-business. In this future market, several providers need to be able to integrate or exchange information in providing a global service. The problem that we want to tackle in this paper is related to the existing information sources in the current Internet environment. That is how to harmonize them with the future Business-to-Business market? The difficulty comes from a historical objective. Internet Web sites were developed for human users browsing and so, they do not support machine-understandable as well as inter-provider interaction. To overcome this gap, we need a framework to systematically migrate the existing presentation-oriented Web sites to service-oriented ones. Evidently, redeveloping all of them is an unacceptable solution.",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:Y0pCki6q_DkC,,1
1379,"Research on Innovating, Applying Multiple Paths Routing Technique Based on Fuzzy Logic and Genetic Algorithm for Routing Messages in Service-Oriented Routing.","MANET (short for Mobile Ad-Hoc Network) consists of a set of mobile network nodes, network configuration changes very fast. In content based routing, data is transferred from source node to request nodes is not based on destination addresses. Therefore, it is very flexible and reliable, because source node does not need to know destination nodes. If We can find multiple paths that satisfies bandwidth requirement, split the original message into multiple smaller messages to transmit concurrently on these paths. On destination nodes, combine separated messages into the original message. Hence it can utilize better network resources, causes data transfer rate to be higher, load balancing, failover. Service Oriented Routing is inherited from the model of content based routing (CBR), combined with several advanced techniques such as Multicast, multiple path routing, Genetic algorithm to increase the data rate, and data encryption to ensure information security. Fuzzy logic is a logical field study evaluating the accuracy of the results based on the approximation of the components involved, make decisions based on many factors relative accuracy based on experimental or mathematical proof. This article presents some techniques to support multiple path routing from one network node to a set of nodes with guaranteed quality of service. By using these techniques can decrease the network load, congestion, use network resources efficiently.",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:Tyk-4Ss8FVUC,,1
1381,Agent-based mobility add-in feature for object transaction service (OTS),"Service session mobility is a new concept in the next telecom service generation. It allows users to move from one terminal in one network to another terminal in another network with service continuity. This concept requires migrating the current service execution related information, including service context, service data, etc, from one terminal to another. For services that execute within transactions, transaction context is a part of service context and must be migrated too. This paper presents our work on this problem, based on CORBA Object Transaction Service. Mobile agent is the enabling technology, which enhances the transaction service with mobility features. This model was initially designed for VHE architecture of VESPER, a European project to design a model for Virtual Home Environment in the next generation of telecom network. However, it can be applied to any other architecture, which require to solve …",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:IjCSPb-OGe4C,,1
1382,Building multiple multicast trees with guarranteed QOS for service based routing using artificial algorithms,"In Service Based Routing (SBR), data is transmitted from a source node to destination nodes are not depended on destination addresses. Hence, it is comfortable with new advanced technology as cloud computing and also flexible and reliable. Multicast routing is advanced technique to deliver data simultaneously from one source node to multiple destination nodes with QOS (quality of service). In this paper, we introduce a technique that is extended from multicast technique with multiple multicast trees that are conformed quality of service routing. This technique is based on Greedy, Ant Colony Optimization, and fuzzy logic to get optimal routes to transmit data from one source to multiple destination node very effectively. The usage of the ANT Colony optimization, Greedy, fuzzy logic algorithms to find cyclic or multiple paths routes on each trunk by multiple criterions to transmit data effectively. ",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:5nxA0vEk-isC,,1
1383,Innovating R Tree and Multicast Routing to Make QOS Multiple Paths for Service Based Routing,"In the advanced routing of new networks communication and network technologies, it not only operates on some lower levels of a network protocol. But it also operates on some upper layers such as the application layer the highest-level layer in network protocol stack of OSI model. The routing process can be known by a more abstract concept. It can process on many layers on network’s stack of OSI model. So this kind of routing may be called the upper layer routing protocol. As the content based routing, in the service based routing protocol, the information can be classified by categories or service classes. Subscribers and publishers can communicate with each other but they don’t know other’s address. So it is more dynamical in processing and more comfortable for ad-hoc network. However the upper routing must be based on lower layers to make routing decisions. But normal routing protocols are used …",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:roLk4NBRz8UC,,1
1384,Apply R Tree to Make Hierarchical Cluster Routing with QOS Guarranteed by Multiple Paths and Multicast Routing,"In the new era of communication and network technologies, a routing protocol not only operates on some lower levels of a network protocol. It also operates on the application layer the highest-level layer in network protocol stack. So we call this is upper layer routing protocol. But it must be based on lower layers to make routing decisions. But normal routing protocols are used for most static network or rather small wireless networks or not high mobility networks. In mobile ad-hoc networks, nodes move very often and fast, so bandwidth of connection between them can be reduced. Therefore the transmission delay may be increased. The paper aims at purpose to increate QOS routing by hierarchical clustering routing by using R+ tree in addition with some advanced techniques multicast routing, multiple paths, use GEN/BEE/ANT to optimize routes to transmit data.",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:MXK_kJrjxJIC,,1
1385,Research on Innovating and Applying Fuzzy Logic for Routing Technique in Service Oriented Routing,"Mobile Ad-hoc Network (Manet) consists of a set of network nodes which change, not based on an existing network infrastructure as well as the centralized control. Network nodes usually have limited energy, bandwidth limitations, so find a way to transfer packets in the network is very important. To minimize packet loss, efficient bandwidth utilization at the node requires routing protocol must be compact, efficient, accurate. Fuzzy logic is a logical field studies evaluating the accuracy of the results based on the approximation of the components involved, make decisions based on many factors relative accuracy based on experimental or mathematical proof. Each component of the logical expression is usually belong to one set of defined unclear (fuzzy set). In this paper, We give some researchs applied fuzzy logic in assessing the network routing and bandwidth management on the basis of a number of existing studies.",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:YOwf2qJgpHMC,,1
1386,Innovating R Tree for Message Forwarding Technique and Hierarchical Network Clustering in Service Based Routing,"In service based routing (SBR)[5], the problem for storing forwarding table that includes searching predicates received from subscribers through subscription messages is an important job. When a content request happens, the subscriber will create a subscription message. The subscription message stores several kinds of information, the most important content is a filter that is a conjunction of some constraints [5]. A filter is denoted by F character, that has mathematical formula: F= C, in which C is service request, has the format: C=‘Service_name= requested service name’. Every C (i= 2.. n) is a constraint that is formed by three components:(Key, op, Value), Key is a keyword for searching, op is an operator, Value is searching condition. Key belongs to the set of name of properties of content messages that the requested service supplies. Op is an operator that depends on the type of data of the Key. Therefore the forwarding table is a set of filters which are received from all subscribers on networks. The algorithms for inserting, updating, deleting and finding filters that match content messages have been published by service providers are very important. In this paper, We mention a technique for forwarding technique on the basic of summary filter for storing and searching filter quickly. This technique is based on some previous researches. In section 8, give an algorithm for finding all network nodes that have matched filters with a content message. Section 7 introduces a cluster routing technique based on summary filter.",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:ULOm3_A8WrAC,,1
1387,"Research on Innovating, Evaluating and Applying Multicast Routing Technique and Genetic Algorithm for Routing Messages in Service-Oriented Routing","MANET (short for Mobile Ad-Hoc Network) consists of a set of mobile network nodes, network configuration changes very fast. Each node will act as a router to maintain network operation. As some standard content based routing protocols, when a node publishes its content, it will broadcast data to network. The published content will be cached at all routers on the network. So when a node publish a request to get some content, it will prepare a subscription message then broadcast it. When any node on network received that subscription, it matches subscription’s content with published content that have been cached on it. If found then matched content will be transferred to requested node. Service Oriented Routing is inherited from the model of content based routing (CBR), combined with several advanced techniques such as Multicast, Genetic algorithm (GA) increase the data rate, and data encryption to ensure information security.",80,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=OmBJT6oAAAAJ&citation_for_view=OmBJT6oAAAAJ:Zph67rFs4hoC,,1
1388,A real-time model based support vector machine for emotion recognition through EEG,"Recently, there has been a significant amount of work on the recognition of human emotions. The results of the work can be applied in real applications, for example in market survey or neuro-marketing. This interesting problem requires to recognize naturally human emotions which come from our mind but ignore the external expressions fully controlled by a subject. A popular approach uses key information from electroencephalography (EEG) signals to identify human emotions. In this paper, we proposed an emotion recognition model based on the Russell's circumplex model, Higuchi Fractal Dimension (HFD) algorithm and Support Vector Machine (SVM) as a classifier. Moreover, we also proposed a method to determine an emotion label of a series of EEG signals. Our model includes two main approaches in machine learning step. In a first approach, machine learning was utilized for all EEG signals from …",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:-f6ydRqryjwC,,1
1389,Dynamic coupling metrics for service–oriented software,"Service-oriented systems have become popular and presented many advantages in develop and maintain process. The coupling is the most important attribute of services when they are integrated into a system. In this paper, we propose a suite of metrics to evaluate service’s quality according to its ability of coupling. We use the coupling metrics to measure the maintainability, reliability, testability, and reusability of services. Our proposed metrics are operated in run-time which bring more exact results.",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:u-x6o8ySG0sC,,1
1390,XML schema automatic matching solution,"Schema matching plays a key role in many different applications, such as schema integration, data integration, data warehousing, data transformation, E-commerce, peer-to-peer data management, ontology matching and integration, semantic Web, semantic query processing, etc. Manual matching is expensive and error-prone, so it is therefore important to develop techniques to automate the schema matching process. In this paper, we present a solution for XML schema automated matching problem which produces semantic mappings between corresponding schema elements of given source and target schemas. This solution contributed in solving more comprehensively and efficiently XML schema automated matching problem. Our solution based on combining linguistic similarity, data type compatibility and structural similarity of XML schema elements. After describing our solution, we present experimental results that demonstrate the effectiveness of this approach.",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:u5HHmVD_uO8C,,1
1391,Monitoring agriculture areas with satellite images and deep learning,"Agriculture applications rely on accurate land monitoring, especially paddy areas, for timely food security control and support actions. However, traditional monitoring requires field works or surveys performed by experts, which is costly, slow, and sparse. Agriculture monitoring systems are looking for sustainable land use monitoring solutions, starting with remote sensing on satellite data for cheap and timely paddy mapping. The aim of this study is to develop an autonomous and intelligent system built on top of imagery data streams, which is available from low-Earth orbiting satellites, to differentiate crop areas from non-crop areas. However, such agriculture mapping framework poses unique challenges for satellite image processing, including the seasonal nature of crop, the complexity of spectral channels, and adversarial conditions such as cloud and solar radiance. In this paper, we propose a novel multi-temporal …",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:B3FOqHPlNUQC,,1
1392,Reliability prediction for component-based software systems: Dealing with concurrent and propagating errors,"One of the most important quality attributes of a software system beyond its functional attributes is its reliability. Techniques for predicting reliability of a software system based on the design models can help software architects in evaluating the impact of their design decisions on the system reliability. This can help to make the system more reliable and avoid costs for fixing the implementation. However, existing reliability prediction approaches for component-based software systems are limited in their applicability because they either neglect or do not support modeling explicitly several factors which influence the system reliability: (i) error propagation, (ii) software fault tolerance mechanisms, and (iii) concurrently present errors. In this paper, we present a reliability modeling and prediction approach for component-based software systems that considers explicitly these reliability-relevant factors. Our approach offers a …",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:hC7cP41nSMkC,,1
1393,Combining maturity with agility: lessons learnt from a case study,"Although both high maturity and agility appeared as different ways to address and overcome issues related to software development (including maximizing resources and minimizing risks), there has been a mixed understanding about the possibility for their co-existence within an organization. Outside of the dogmatic debate regarding their co-existence, however, voices have been raised recently that recognize that both approaches have their merits. This paper presents the results of a case study on the practices that a purely agile organization has put in place in order to profit from the opportunities that higher maturity can offer in respect to value creation for clients. Our conclusion is that both high maturity and agility contribute to customer satisfaction, high quality and waste reduction; and that complying with standards does not necessarily impose restriction on'being agile'. Implication for practice is that companies …",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:eQOLeE2rZwMC,,1
1394,New dissimilarity measures on picture fuzzy sets and applications,"The dissimilarity measures between fuzzy sets/intuitionistic fuzzy sets/picture fuzzy sets are studied and applied in various matters. In this paper, we propose some new dissimilarity measures on picture fuzzy sets. These new dissimilarity measures overcome the restrictions of all existing dissimilarity measures on picture fuzzy sets. After that, we apply these new measures to the pattern recognition problems. Finally, we introduce a multi-criteria decision making (MCDM) method that uses the new dissimilarity measures and apply them in the supplier selection problems.",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:J_g5lzvAfSwC,,1
1395,Optimizing bidders selection of multi-round procurement problem in software project management using parallel max-min ant system algorithm,"This paper presents a Game-theoretic optimization via Parallel Min-Max Ant System (PMMAS) algorithm is used in practice to determine the Nash equilibrium value to resolve the confusion in choosing appropriate bidders of multi-round procurement problem in software project management. To this end, we introduce an approach that proposes: (i) A Game-theoretic model of multi-round procurement problem (ii) A Nash equilibrium strategy corresponds to multi-round strategy bid (iii) An application of PSO for the determination of global Nash equilibrium. The balance point in Nash Equilibrium can help to maintain a sustainable structure not only in terms of project management but also in terms of future cooperation. As an alternative of procuring entities subjectively, a methodology to support decision making has been studied using Nash equilibrium to create a balance point on benefit in procurement where buyers …",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:BrmTIyaxlBUC,,1
1396,A novel particle swarm optimization approach to support decision-making in the multi-round of an auction by game theory,"In this paper, game-theoretic optimization by particle swarm optimization (PSO) is used to determine the Nash equilibrium value, in order to resolve the confusion in choosing appropriate bidders in multi-round procurement. To this end, we introduce an approach that proposes (i) a game-theoretic model of the multi-round procurement problem;(ii) a Nash equilibrium strategy corresponding to the multi-round strategy bid; and (iii) an application of PSO for the determination of the global Nash equilibrium point. The balance point in Nash equilibrium can help to maintain a sustainable structure, not only in terms of project management but also in terms of future cooperation. As an alternative to procuring entities subjectively, a methodology using Nash equilibrium to support decision-making is developed to create a balance point that benefits procurement in which buyers and suppliers need multiple rounds of bidding. To solve complex optimization problems like this, PSO has been found to be one of the most effective meta-heuristic algorithms. These results propose a sustainable optimization procedure for the question of how to choose bidders and ensure a win-win relationship for all participants involved in the multi-round procurement process.",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:sSrBHYA8nusC,,1
1397,A complexity measure for web service,"Today service oriented architecture (SOA) has presented a huge of advantages such as flexibility, maintainability and integration. Therefore, many developers choose it to develop their systems. It leads to the need of evaluation of service-oriented software's quality. In this paper, we propose a suite of complexity metrics for SOA based on Web service definition language (WSDL). These metrics were tested on available Web services and compared with other metrics to prove their ability and reliability in evaluation.",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:3fE2CSJIrl8C,,1
1398,Building the reliability prediction model of component-based software architectures,"Reliability is one of the most important quality attributes of software. Based on the approach of Reussner and the approach of Cheung, we proposed the reliability prediction model of component-based software architectures. Also, the value of the model is shown through the experimental evaluation on a web server system.",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:d1gkVwhDpl0C,,1
1399,Network alignment by representation learning on structure and attribute," Network alignment is the task of recognizing similar network nodes across different networks, which has many applications in various domains. As traditional network alignment methods based on matrix factorization do not scale to large graphs, a variety of representation learning based approaches has been proposed recently. However, these techniques tend to focus on topology consistency between two networks while ignoring other valuable information (e.g. network nodes attribute), which makes them susceptible to structural changes. To alleviate this problem, we propose RAN, a representation-based network alignment model that couples both structure and node attribute information. Our framework first constructs multi-layer networks to represent topology and node attribute information, then computes the alignment result by learning the node embeddings for source and target network. The …",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:uWQEDVKXjbEC,,1
1400,A Bayesian critical path method for managing common risks in software project scheduling,"Although project managers nowadays can use a range of tools and techniques to develop, monitor and control project schedules, the task of creating project schedules is often very difficult since it has to deal with planning against uncertainty. Popular techniques for project scheduling based on the assumption that projects are carried out as planned or scheduled--which hardly happens. This paper takes the advantage of Bayesian Networks in modeling uncertainty and incorporates them in Critical Path Method-one of the most popular means of monitoring project scheduling. The paper also examines common risk factors in project scheduling and proposes a model of 19 common risk factors. A tool was also built, and experiments were carried out to validate the model.",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:2P1L_qKh6hAC,,1
1401,"Vietnamese spelling detection and correction using Bi-gram, Minimum Edit Distance, SoundEx algorithms with some additional heuristics","The spelling checking problem is considered to contain two main phases: the detecting phase and the correcting phase. In this paper, we present a new approach for Vietnamese spelling checking based on Vietnamese characteristics for each phase. Our research approach includes the use of a syllable Bi-gram in combination with parts of speech (POS) to find out suspected syllables. In the correcting phase, we based on minimum edit distance, SoundEx algorithms and some heuristics to build a weight function for assessing suggestion candidates. The training corpus and the test set were collected from e-newspapers.",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:YFjsv_pBGBYC,,1
1402,A method of verifying web service composition,"Service composition is one of the primary tasks in developing service-oriented systems. However, there are currently some challenges to check its correction. In this paper, we propose a visual methodology and a tool for verifying business processes written in BPEL by using the SPIN model checker. We present algorithms to translate BPEL processes into PROMELA programs via labeled control flow graphs. The use of label control graphs in the tool will help regular users understand BPEL business processes and the verification process with a model checker more easily. Finally, the Spin model checker will verify important properties of the PROMELA program that represents a BPEL business process.",85,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Vz24y3sAAAAJ&citation_for_view=Vz24y3sAAAAJ:8k81kl-MbHgC,,1
1403,A graph convolutional topic model for short and noisy text streams.,"Learning hidden topics from data streams has become absolutely necessary but posed challenging problems such as concept drift as well as short and noisy data. Using prior knowledge to enrich a topic model is one of potential solutions to cope with these challenges. Prior knowledge that is derived from human knowledge (e.g. Wordnet) or a pre-trained model (e.g. Word2vec) is very valuable and useful to help topic models work better. However, in a streaming environment where data arrives continually and infinitely, existing studies are limited to exploiting these resources effectively. Especially, a knowledge graph, that contains meaningful word relations, is ignored. In this paper, to aim at exploiting a knowledge graph effectively, we propose a novel graph convolutional topic model (GCTM) which integrates graph convolutional networks (GCN) into a topic model and a learning method which learns the networks and the topic model simultaneously for data streams. In each minibatch, our method not only can exploit an external knowledge graph but also can balance the external and old knowledge to perform well on new data. We conduct extensive experiments to evaluate our method with both a human knowledge graph (Wordnet) and a graph built from pre-trained word embeddings (Word2vec). The experimental results show that our method achieves significantly better performances than state-of-the-art baselines in terms of probabilistic predictive measure and topic coherence. In particular, our method can work well when dealing with short texts as well as concept drift.",77,https://doi.org/10.1016/j.neucom.2021.10.047,1,
1405,Reducing Class Overlapping in Supervised Dimension Reduction.,"Dimension reduction is to find a low-dimensional subspace to project high-dimensional data on, such that the discriminative property of the original higher-dimensional data is preserved. In supervised dimension reduction, class labels are integrated into the lower-dimensional representation, to produce better results on classification tasks. The supervised dimension reduction (SDR) framework by [17] is one of the state-of-the-art methods that takes into account not only the class labels but also the neighborhood graphs of the data, and have some advantages in preserving the within-class local structure and widening the between-class margin. However, the reduced-dimensional representation produced by the SDR framework suffers from the class overlapping problem - in which, data points lie closer to a different class rather than the class they belong to. The class overlapping problem can hurt the quality on the classification task. In this paper, we propose a new method to reduce the overlap for the SDR framework in [17]. The experimental results show that our method reduces the size of the overlapping set by an order of magnitude. As a result, our method outperforms the pre-existing framework on the classification task significantly. Moreover, visualization plots show that the reduced-dimensional representation learned by our method is more scattered for within-class data and more separated for between-class data, as compared to the pre-existing SDR framework.",77,https://doi.org/10.1145/3287921.3287925,1,
1406,An Effective NMF-Based Method for Supervised Dimension Reduction.,"Sparse topic modeling is a potential approach to learning meaningful hidden topics from large datasets with high dimension and complex distribution. We propose a sparse NMF-based method for supervised dimension reduction which aims to detect the particular topics of each class. Beside exploiting constraint convex combination of the hidden topics for each instance, our method separably learns among classes to extract interpretable and meaningful class topics. Our experimental results showed the effectiveness of our approach via significant criteria such as separability, interpretability, sparsity and performance in classification task of large datasets with high dimension and complex distribution. Our obtained results are highly competitive with state-of-the-art NMF-based methods.",77,https://doi.org/10.1007/978-3-319-11680-8_8,1,
1407,Improving Vietnamese Web Page Classification by Combining Hybrid Feature Selection and Label Propagation with Link Information.,"Book cover
International Conference on Context-Aware Systems and Applications

ICCASA 2012: Context-Aware Systems and Applications pp 324–334Cite as

Improving Vietnamese Web Page Classification by Combining Hybrid Feature Selection and Label Propagation with Link Information
Ngo Van Linh, Nguyen Thi Kim Anh & Cao Manh Dat 
Conference paper
1116 Accesses

Part of the Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering book series (LNICST,volume 109)

Abstract
Classification of web pages is essential to many information management and retrieval tasks such as maintaining web directories and focused crawling. One problem in web page classification is that, unlabeled training examples are readily available, while labeled ones are often costly to obtain. Furthermore, the uncontrolled nature of web content presents additional challenges to web page classification, whereas the interconnected characteristic of hypertext can provide useful information for the process. To address these problems, we propose a graph-based semi-supervised classification framework which combines iteratively hybrid semi-supervised feature selection and Label Propagation learning using link information to improve the Vietnamese web page classification. The experimental results show that proposed method outperforms the state-of-the art methods applying to Vietnamese web page classification.",77,https://doi.org/10.1007/978-3-642-36642-0_32,1,
1408,Efficient label propagation for classification on information networks.,"Classification on networked data plays an important role in many problems such as web page categorization, classification of bibliographic information network, etc... Most classification algorithms on information networks work by iteratively propagating information through network graphs. One important issue concerning iterative classifiers is that false inferences made at some point in iteration might propagate further causing an ""avalanche"". To address this problem, we propose an efficient label propagation learning algorithm based on the graph-based regularization framework with adjusting network structure iteratively to improve the accuracy of classification algorithm for noisy data. We show empirically that this adjusting network structure improves significantly the performance of the algorithm for web page classification. In particular, we demonstrate that the proposed algorithm achieves good classification accuracy even for relatively large overlap across the classes.",77,https://doi.org/10.1145/2350716.2350725,1,
1409,Blockchain for Education: Verification and Management of Lifelong Learning Data.,"In recent years, blockchain technology has been applied in the educational domain because of its salient advantages, i.e., transparency, decentralization, and immutability. Available systems typically use public blockchain networks such as Ethereum and Bitcoin to store learning results. However, the cost of writing data on these networks is significant, making educational institutions limit data sent to the target network, typically containing only hash codes of the issued certificates. In this paper, we present a system based on a private blockchain network for lifelong learning data authentication and management named B4E (Blockchain For Education). B4E stores not only certificates but also learners’ training data such as transcripts and educational programs in order to create a complete record of the lifelong education of each user and verify certificates that they have obtained. As a result, B4E can address two types of fake certificates, i.e., certificates printed by unlawful organizations and certificates issued by educational institutions for learners who have not met the training requirements. In addition, B4E is designed to allow all participants to easily deploy software packages to manage, share, and check stored information without depending on a single point of access. As such, the system enhances the transparency and reliability of the stored data. Our experiments show that B4E meets expectations for deployment in reality.",78,https://doi.org/10.32604/csse.2022.023508,1,
1410,Some Improvements of Using the NSGA-II Algorithm for the Problem of Resource Allocation and Scheduling and Its Applying to Inventory Management Strategies.,"Vendor-managed inventory (VMI) is an approach to prevent undesired stocking inventories and hence can lead to a cost reduction of the whole supply chain. One of the main objectives of this approach is to optimize the inventory buffer as safety stock and to optimize the scheduling of inventory and delivery. Such optimization could be considered as a problem of the project's resource scheduling and allocation. In this paper, we present some experimentations for solving this problem by implementing two different algorithms: (i) the Nondominated Sorting Genetic Algorithm (NSGA-II), and (ii) the multi-objective optimization algorithm provided by the MOEA framework. Based on the experimented results, we propose some improvements in using NSGA-II to define an optimized VMI strategy. Such a strategy is implemented and demonstrated through the data collected from a real VMI project.",78,https://doi.org/10.1109/KSE.2019.8919492,1,
1411,Innovating Based on R tree and Artificial Neural Network for Hierarchical Clustering in Order to Make QoS Routes in MANET.,"Book cover
International Conference on e-Infrastructure and e-Services for Developing Countries

AFRICOMM 2016: e-Infrastructure and e-Services for Developing Countries pp 217–231Cite as

Innovating Based on R tree and Artificial Neural Network for Hierarchical Clustering in Order to Make QoS Routes in MANET
Nguyen Thanh Long, Nguyen Duc Thuy & Pham Huy Hoang 
Conference paper
First Online: 10 October 2017
490 Accesses

Part of the Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering book series (LNICST,volume 208)

Abstract
The advanced routing protocol not only operates on lower levels of a network protocol, but it also operates on upper layers such as the application layer of OSI model. The routing task can be operated on a wider scale. It can process based on results of some other protocols for example service based protocol can be operated based on the service discovery protocol. So this kind of routing protocols may be determinized as upper layer routing protocols. Such as the service based routing protocol can operate based on content based protocol and combines some service filters. In service based routing protocol as well as content based routing protocol, subscriber and publisher can communicate with each other but they don’t know the other’s address. So it is more flexible in processing and more comfortable for mobile ad-hoc network. In mobile ad-hoc networks, nodes usually move, so bandwidth of connection between them may be not stable. Therefore transmission delay, overhead and packet loss may be larger than other kinds of networks. The paper aims at purpose to increase QoS of routing by hierarchical clustering routing by using \( R^{ + } \) tree in addition with some advanced techniques such as multicast routing, multiple paths, use can ACO to optimize routes to transmit data. By using R tree structure, the network topology are managed by bottom-up model from leaf level to root of the tree. All the leaf nodes, inner nodes and root of this tree have two roles: (i) Manage a cluster that consisting all nodes that have direct connections with this node; (ii) Operate as a normal node. The paper introduces and analyzes: (i) Establish hierarchical clustering network by using R tree structure; (ii) Make multicast tree from some cluster heads for fast routing; (iii) Make optimized route by Ant Colony Optimization. The paper also uses the Artificial Neural Network to choose optimal cluster head and members for cluster of network.",80,https://doi.org/10.1007/978-3-319-66742-3_21,1,
1412,"Research on Improving, Evaluating and Applying the Ternary Search Tree and Binary Search for Storing and Searching Content - Based Address for Forwarding Technique in Service-Oriented Routing.","Service-based network infrastructure is a new network interface in which the flow of messages is controlled by class of services that generated it. Next is its content, improved shipping address specified by the sender and attached to the message. Networks based on services complement for networks based on traditional unicast and multicast addresses, which provides support for communication patterns based on the service class of large-scale applications, loose connections, multiple partitions and scattered like auctions, information sharing, distributed according to personal information.

With Service Based Routing (SBR), the sender does not indicate message receiver by the unicast or multicast use. Instead it simply pushes messages to the network. It defines the routing based on the messages it cares. It determines the appropriate message class based on message content based on its key-value pairs or regular expressions. Therefore, in SBR routing the receiver determines the transmission of messages, not the sender. Communication based on content services increases the independence, flexibility in the distributed architecture. In SBR the routing table consist of content based addresses, We have to find the structure to store and organize routing table efficiently and save memory and time to search all its items match a content message. In this paper We introduce and improve Ternary Search Tree structure to use for storing and process the SBR routing table.",80,https://doi.org/10.1007/978-3-642-41773-3_20,1,
1413,Correlation-based clustering in wireless sensor network for energy saving protocol.,"Our research concentrates on the energy efficiency in Wireless Sensor Network. One approach is based on the characteristics of environment -- the correlation among sensed data of nodes in a region. The sensor nodes are clustered into highly correlated regions (HCRs) to take advantage of correlation between sensor nodes in order to save energy. However, the determination of HCR is very complex in calculation, thus causes difficulty in implementation. This paper proposes a correlation-based approach that evaluates the correlation between two data sets using a simple calculation and that guarantees the accuracy in correlated evaluation between data. This correlation-based method is proposed to cluster sensor nodes into HCRs. Because of highly correlated characteristics among sensed data of nodes in the same HCRs, some high correlated-nodes would be inactive for energy saving. Simulation results show that the network lifetime of proposed system is 1.75 times longer than that of the conventional protocol.",81,https://doi.org/10.1145/2542050.2542082,1,
1415,A new fall detection system on Android smartphone: Application to a SDN-based IoT system.,"Falls are one of the very serious problems in the healthcare system for the elderly, often resulting in a rapid decline in functionality and death. Serious consequences of sustaining a fall include broken or fractured bones, superficial cuts and abrasions as well as soft tissue damage. Several solutions are proposed to resolve such problems, however, major difficulties they encounter are cost, comfort and performance. In this paper, we propose a new Android application for fall detection based on theory of machine learning and data mining. Concretely, we used a classification method to detect if it is a fall down or another common events (e.g. sitting, jumping, etc.). The obtained experimental results show that our proposal yielded impressive results compared to related works",81,https://doi.org/10.1109/KSE.2017.8119425,1,
1416,A generalized resource allocation framework in support of multi-layer virtual network embedding based on SDN.,"Network Virtualization (NV) allows multiple heterogeneous architectures to simultaneously coexist in a shared infrastructure. Embedding multiple virtual networks (VNs) in a shared substrate deals with efficient mapping of virtual resources in the physical infrastructure and is referred to as the Virtual Network Embedding problem (VNE-problem). Although there is recently a number of research work in the area of network virtualization based on the Software-Defined Networking (SDN) technology, virtual network embedding in SDN remains challenging from both theoretical and practical points of view.

This article focuses on virtual network embedding strategies and related issues for Infrastructure-as-a-Service (IaaS) paradigms under the constraint of fixed virtual node locations. Special considerations are given to the problems related to resource allocation and link sharing of multi-layer virtual networks on top of the physical substrate. Firstly, a heuristic virtual network embedding algorithm is proposed that can improve the mapping acceptance ratio and resource efficiency in the IaaS context. Secondly, REsource reSERvation in generalized Virtual NETworks (ReServNet), a Software-Defined Networking platform designed for embedding multi-level virtual networks in physical infrastructures is developed. By defining new softwarized logical functions, ReServNet allows network administrators to create and manage multiple virtual networks on top of the physical network and allocate bandwidth resources to them accordingly. Moreover, the ReServNet framework allows for designing, prototyping, benchmarking and evaluating the performance of different network embedding algorithms easily in real SDN virtualization environments. Different issues related to virtual network embedding on SDN-based physical substrate are also analyzed and discussed in detail.",81,https://doi.org/10.1016/j.comnet.2015.09.042,1,
1417,An application for diagnosing lung diseases on Android phone.,"Nowadays, lung diseases, such as asthma, chronic obstructive pulmonary disease (COPD), and cystic fibrosis, are increasingly attracting the attention of the world medical community. The main reason is due to the increasing environmental pollution problem. Therefore, detecting pulmonary exacerbations and improving outcomes of chronic lung ailments are in urgent need. Traditionally, patients have to go to pulmonary hospital and use conventional spirometers. However, the challenge of this solution is the cost and the loss of time. Patients may take several hours to queue to see a doctor and have to pay for medical expenses. In order to solve this inconvenience, we proposed an approach, called BKSpiro, for measuring lung function using Android phone, a very popular device for everyone today. BKSpiro has the potential for more rapid recovery, earlier treatment of exacerbations, and reduce health care costs.",81,https://doi.org/10.1145/2833258.2833289,1,
1418,A Lateration-localizing Algorithm for Energy-efficient Target Tracking in Wireless Sensor Networks.,"Target tracking systems, such as battlefield surveillance, wildlife monitoring and border security, need to meet certain real-time constraints in response to transient events, such as fast-moving targets. In addition, a tracking system also needs to address other important properties, such as energy consumption (due to limited power and the processing capability of sensor nodes) and tracking accuracy. In this work, we first propose a Lateration-localizing algorithm to estimate the trajectory of a dynamic target. Based on this Lateration scheme, we propose an energy efficient tracking method that achieves high tracking accuracy and low end-to-end delay. Finally, we present an analytical model and implement an extensive simulation to evaluate the performance of the proposed system. The results confirmed that our system associated with the simple linear estimator Lateration achieves better energy consumption and real time property compared to conventional Extended Kalman Filter while maintaining reasonable tracking accuracy.",81,http://www.oldcitypublishing.com/journals/ahswn-home/ahswn-issue-contents/ahswn-volume-34-number-1-4-2016/ahswn-34-1-p-191-220/,1,
1419,A low-latency communication protocol for target tracking in wireless sensor networks.,"Target tracking applications in wireless sensor networks need to achieve energy efficiency, tracking accuracy, and certain real-time constraints in response to fast-moving targets. From a layer view, an energy-efficient cross-layer communication protocol that consists of a medium access control layer and network routing layer is necessary for joint optimization. Due to the interference and contention over the wireless medium, the limited resources of battery-operated sensor nodes, and the dynamic topology of large-scale networks, this cross-layer design becomes a challenging task. In this research, we exploit a cluster routing algorithm over large-scale networks and propose a low-duty-cycle medium access control (MAC) algorithm to reduce collision, idle-listening, and overhearing. In addition, our work focuses on the joint optimization of routing and a MAC strategy for achieving a good trade-off between low delay, energy efficiency, and tracking accuracy. To deploy this protocol in a real tracking application, we also propose a clustering synchronization procedure that does not require distributing the global timing information over the complete network to achieve network-wide time synchronization. An analytical model and extensive simulations are proposed to evaluate and compare the performance of our work with existing protocols. Simulation and analysis results show that our approach achieves better communication delay and thus better tracking error while maintaining reasonable energy consumption compared to other cases.",81,https://doi.org/10.1186/s13638-016-0517-4,1,
1420,Joint scheduling and mapping in support of downlink fairness and spectral efficiency in ieee 802.16e OFDMA system.,"The next generation broadband wireless networks deploy orthogonal frequency division multiple access (OFDMA) as the enabling technologies for broadband data transmission with QoS capabilities. In such broadband wireless systems, one major issue is how to utilize radio resource efficiently while maintaining fairness between sessions as well as providing adequate QoS. In this work, we propose an approach for OFDMA/time division duplex (TDD) downlink suitable for IEEE802.16e WiMAX systems that combines scheduling and burst mapping algorithms for a trade-off between session fairness, QoS, and spectral efficiency. While optimizing radio resources under QoS and fairness constraints is an NP-hard problem, we follow a heuristic approach that simplifies the complexity of the algorithm. Performance results show that while the new scheme outperforms the Proportional Fair algorithm in terms of fairness, it also improves the overall system spectral efficiency. ",81,https://doi.org/10.1002/dac.3137,1,
1421,Modeling and experimenting combined smart sleep and power scaling algorithms in energy-aware data center networks.,"Recent surveys have shown that the energy consumption in a data center considerably contributes to its operation costs. A remarkable part of the large energy volume consumed in data centers today is due to the over-provisioning of such network resources as switches, links, and servers to meet the stringent requirements on reliability. Therefore performance and energy issues are important factors for the design of large multi-tier data centers that can support multiple services. However, the design, analysis, and experiments of such a large and complex system often suffer from the lack of appropriate experimental infrastructures. In this paper, we firstly propose a new energy saving scheme that combines smart sleeping and power scaling algorithms. An energy analysis model is then proposed to calculate the energy saving bounds in case of low and high traffic utilization. We also present a platform for in-depth analysis of energy-aware data center networks, which is a combination of hardware testbed and emulation. Based on OpenFlow technology, the experimental platform is designed to capture details of energy consumed by all network components such as links, ports, and switches under different scenarios. Analytical and emulation results show that the combined algorithm improves the energy saving under the varied traffic utilization.",81,https://doi.org/10.1016/j.simpat.2013.05.011,1,
1422,Reliable and energy efficient cluster-based and event-driven routing scheme for Wireless Sensor Networks.,"Traditionally, Wireless Sensor Network (WSN) are deployed for monitoring applications based on low rate data collection. Recent research shows that cluster-based and event-driven routing (such as EMRP [1], ARPEES [2], HPEQ [3] and OEDSR [4]) that groups sensor nodes into one cluster in order to efficiently relay the sensed data to the Base Station (BS) has many advantages in term of energy efficiency and load balance. We present in this paper a new cluster-based and event-driven routing algorithm (called Reliable and Energy-efficient Cluster-based and Event-driven Routing algorithm-RECER). Our new scheme was implemented using OMNet++ and can achieves better energy consumption, better load balance, network lifetime and rate delivery compared to other schemes.",81,https://doi.org/10.1109/LANMAN.2013.6528288,1,
1423,Improvement of energy consumption and load balance for LEACH in Wireless Sensors Networks.,"Wireless Sensor Network (WSN) is a promising approach for a variety of applications. Because of limitation of energy resource, memory space and processing capability of sensor nodes, it is very difficult to implement IP-based routing protocols in WSN. Recently, many research focus on developing special routing protocols for WSNs with the main design criteria: energy efficiency, load balance and reliability. The most well known routing protocol class is hierarchical which divides network into many clusters. Each cluster is represented by a cluster head (CH) that is responsible for receiving data from all non-CH members, aggregating these data and sending to the base station (BS). In order to balance energy consumption of the entire network, CHs are not fixed but rotated. Therefore, the hierarchical protocols can decrease and balance the energy consumption and prolong network lifetime. LEACH is one of the first cluster-based for WSN, which includes distributed cluster formation. In this paper, we propose a new hierarchical routing protocol (called EL-LEACH: Energy and Load balance LEACH) which achieves energy efficiency and load balance. Our simulation result shows that our new scheme EL-LEACH achieves better energy consumption, load balance and network life time than other well known LEACH protocols.",81,https://doi.org/10.1109/ICTC.2012.6387206,1,
1424,Multipath routing for cluster-based and event-based protocols in wireless sensor networks.,"Wireless Sensor Network (WSN) that consists of a large number of small sensing self-powered nodes is a promising approach for a variety of applications. Possible applications of WSN are of interest to the most diverse fields: environmental monitoring, warfare, child education, surveillance, micro-surgery and agriculture are only few examples. Due to limited power of sensor nodes that are distributed in a large geographical area, the WSN requires robust but simple, scalable, energy-efficient and also self-configurable routing algorithms. From these characteristics, WSN routing protocol can coarsely be classified in two main classes: flat and hierarchical approaches. Hierarchical solution (also called cluster-based) separates nodes in clusters and the communication within clusters is possible only through the selected leaders or Cluster Head-CH. In the reality, it is not always necessary to divide all the network to clusters because one event can only appear in a small part. In this case, routing protocol needs to form only one cluster with one CH for minimizing the amount of data sending to the base station (BS). Some such routing approaches (called cluster-based and event-based) are proposed as ARPEES [1], OEDSR [2], HPEQ[3]... but they all create only one optimal route from CH to the BS and this route can be broken when the sensor node's power is negligible. In this paper, we propose two multipath solutions called Multiparth ARPEES (MARPEES) and Energy Aware Mesh Routing Protocol (EMRP) for ensuring reliable and fault-tolerance data transmission while balancing node's energy consumption in the network and minimizing the control overhead. The simulation results show that our solutions achieve better performance than other cluster-based and event-based routing protocols.",81,https://doi.org/10.1145/2350716.2350743,1,
1425,Multimedia session continuity with context-aware capability in IMS-based network.,"In this paper we firstly proved that mobile IP protocols are designed for mobility but not for session continuity. In contradiction, mSCTP (mobile stream control transmission protocol) appears to be the best protocol with regard to session continuity support, however it does not support location management mechanisms, thus context-aware QoS as well as other mechanisms based on knowledge of location are hardly accommodated. On the other hand, the standardized IP multimedia subsystem (IMS) solution initially introduced by 3GPP adopts the policy-based QoS model to map user's subscription to a pre-defined QoS level, to apply service-based charging and to prevent theft-of-service. In this article we propose a new approach that integrates mSCTP into IMS networks to facilitate session continuity and to provide mechanisms for service control. It is realized by a proxy based on mSCTP (mobile stream control transmission protocol) that acts as an anchor point for soft vertical handover of mobile nodes, which have multiple physical interfaces (e.g., WLAN/UMTS). The mSCTP-based proxy also supports QoS provisioning and adaptation for the mobile nodes when moving in a heterogeneous wireless environment. Results show that the signaling cost for handover in our approach can be up to 23 times smaller than that in the conventional approach.",81,https://doi.org/10.1109/ISWCS.2009.5285326,1,
1426,The Influence of Proportional Jitter and Delay on End to End Delay in Differentiated Service Network.,"There exist some studies in proportional scheduling algorithms for differentiated service networks which schedule the packets between different classes proportionally based on certain metrics, such as bandwidth, loss and delay. In this paper, the behaviors of RJPS (Relative Jitter Packet Scheduler) and another proportional delay scheduler (WTP), as well as its influence on the playout buffer, which uses the Concord delay adjustment algorithm, are examined. Our simulations show that RJPS is more convergent and produces a smaller delay than WTP via the same network with the same loss rate. Finally, we propose a network using both RJPS and WTP for minimizing the network delay of each class.",81,https://doi.org/10.1109/NCA.2001.962554,1,
1427,Improving Vietnamese Web Page Classification by Combining Hybrid Feature Selection and Label Propagation with Link Information.,"Classification of web pages is essential to many information management and retrieval tasks such as maintaining web directories and focused crawling. One problem in web page classification is that, unlabeled training examples are readily available, while labeled ones are often costly to obtain. Furthermore, the uncontrolled nature of web content presents additional challenges to web page classification, whereas the interconnected characteristic of hypertext can provide useful information for the process. To address these problems, we propose a graph-based semi-supervised classification framework which combines iteratively hybrid semi-supervised feature selection and Label Propagation learning using link information to improve the Vietnamese web page classification. The experimental results show that proposed method outperforms the state-of-the art methods applying to Vietnamese web page classification.",82,https://doi.org/10.1007/978-3-642-36642-0_32,1,
1428,Efficient label propagation for classification on information networks.,"Classification on networked data plays an important role in many problems such as web page categorization, classification of bibliographic information network, etc... Most classification algorithms on information networks work by iteratively propagating information through network graphs. One important issue concerning iterative classifiers is that false inferences made at some point in iteration might propagate further causing an ""avalanche"". To address this problem, we propose an efficient label propagation learning algorithm based on the graph-based regularization framework with adjusting network structure iteratively to improve the accuracy of classification algorithm for noisy data. We show empirically that this adjusting network structure improves significantly the performance of the algorithm for web page classification. In particular, we demonstrate that the proposed algorithm achieves good classification accuracy even for relatively large overlap across the classes.",82,https://doi.org/10.1145/2350716.2350725,1,
1429,Improving Vietnamese web page clustering by combining neighbors' content and using iterative feature selection.,"In recent years, data has become vital in supporting our everyday lives. Along with large volumes of open data available on the web, various types of public, private, and enterprise data are stored in the cloud or distributed over multiple devices. The value of this data would increase drastically if we were able to integrate it. This would enable more sophisticated presentation and analysis of previously disparate data. So far, however, it is challenging for non-expert users to efficiently make use of such data because (i) data heterogeneity hampers integration of different kinds of data that are stored in various formats and spread among storage infrastructures; (ii) manual data integration processes are typically neither reproducible, nor reusable; and (iii) the lack of support for exploration does not allow for the integration of arbitrary data sources. This paper tackles these challenges by introducing a mashup platform that combines semantic web and mashup concepts to help users obtain insights and make informed decisions. To this end, we leverage a semantic model of mashup components for automated techniques that support the user in exploring available data. Moreover, we introduce a collaborative and distributed model to create and execute mashups. This facilitates distributed ad-hoc integration of heterogeneous data contributed by multiple stakeholders.",82,https://doi.org/10.1145/2350716.2350726,1,
1430,Dynamic routing for shared path protection in multidomain optical mesh networks,"The routing problem for shared path protection in multidomain optical mesh networks is more difficult than that in single-domain mesh networks due to the lack of complete and global knowledge of the network topology and bandwidth allocation. To overcome this difficulty, we propose an aggregated network modeling by underestimation with a two-step routing strategy. In the first step, a rough routing solution is sketched in a virtual network that is the topology aggregation of the multidomain network. A complete routing is then determined by solving routing problems within the original single-domain networks. The first step can be solved by either using an exact mathematical program or a heuristic, whereas the second step is always solved by heuristics. Computational results show the relevance of the aggregated network modeling. They also prove the scalability of the proposed routing for multidomain networks and …",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:d1gkVwhDpl0C,,1
1431,Using topology aggregation for efficient shared segment protection solutions in multi-domain networks,"The dynamic routing problem for Overlapping Segment Shared Protection (OSSP) in multi-domain networks has not received a lot of interest so far as it is more complex than in single-domain networks. Difficulties lie in the lack of complete and global knowledge about network topologies and bandwidth allocation whereas this knowledge is easily available in single-domain networks. We propose a two-step routing approach for the OSSP based on a topology aggregation scheme and link cost estimation: an inter-domain step and an intra-domain step. We propose two different heuristics, GROS and DYPOS for the inter-domain step, and a ""Blocking-go-back"" strategy in order to reduce the blocking rate in the intra-domain step. We compare the performance of the two heuristics against an optimal single-domain approach. We show that both heuristics lead to resource efficient solutions that are not far from the optimal …",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:9yKSN-GCB0IC,,1
1432,Recent progress in dynamic routing for shared protection in multidomain networks,"A large number of studies on routing for shared protection focus on minimizing the network transport capacity in a static routing framework. A smaller number of studies have been conducted on dynamic routing. Most of them do not meet the scalability requirements of multidomain networks. This article reviews the recent works in dynamic routing for shared protection in multidomain networks, and proposes a quantitative comparison among the most efficient approaches. Some of the remaining challenges are discussed at the end of the article.",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:qjMakFHDy7sC,,1
1433,A Policy-based approach for user controlled Lightpath Provisioning,"There is a growing need for e2e lightpaths for high volume data transferring applications such as GridFTP and SAN. They wish to dynamically deploy lightpaths over multiple management domains. Research, sponsored by Canarie Incorporated, is underway to enable ""customer-empowered networks"" and to experiment them with the Canadian research network CA*Net4. New signaling and control approaches using Web services have been proposed. One difficulty is that each domain must retain the control of their optical network infrastructure and ensure proper allocation of optical resources. Hence, it is important that the signaling takes into account the management constraints imposed by the different domains. This paper presents a policy-based approach for user-controlled lightpath provisioning. The work builds on the research around the new signaling approaches for realizing customer-empowered networks …",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:2osOgNQ5qMEC,,1
1434,Optimization of survivable mesh long-reach hybrid WDM-TDM PONs,"Long-reach hybrid wavelength-division multiplexing (WDM) and time-division multiplexing (TDM) passive optical networks (PONs) allow deploying access networks for remote service areas with thousands of customers. Typically, several long fiber cables are run between the central office (CO) of the service provider and each service area in order to feed the service area with data flows. In the service area, array waveguide gratings (AWGs) multiplex and demultiplex wavelengths; then, splitters split wavelengths in order to serve multiple optical network units. This paper proposes use of a mesh topology in service areas, i.e., AWGs can feed each other. This architecture has two main advantages. First, mesh linkages between AWGs make the network structure more robust with a high possibility of integrating survivable schemes. Second, fewer fibers are required between the CO and service areas, leading to a …",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:_FxGoFyzp5QC,,1
1435,Backup Path Re-optimizations for Shared Path Protection in Multi-domain Networks.,"Within the context of dynamic routing models for shared path protection in multi-domain networks, we propose a backup path re-optimization phase with possible rerouting of the existing backup paths in order to increase the bandwidth sharing among them while minimizing the network backup cost. The reoptimization phase is activated periodically or when routing a new connection fails because of insufficient capacity. Three reoptimization models are discussed: i) Global rerouting where the re-optimization is performed once for the entire network ii) Local rerouting where the re-optimization is serially performed on one domain at a time or on selected domains, and iii) Local rerouting with least effort, ie, where the smallest possible number of backup path reroutings is performed in order to be able to handle new connection requests. The first model offers the best resource savings while the two others are more scalable in multidomain networks. Comparative performance of the three models are conducted and numerical results are presented.",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:eQOLeE2rZwMC,,1
1436,Topology design and cross-layer optimization for FSO mesh networks impaired by atmospheric turbulence and misalignment fading,"In this paper, we design and optimize free-space optics (FSO) mesh networks over atmospheric turbulence and misalignment fading channels. We propose a heuristic algorithm to choose the best sites to install the FSO transceivers and also the best topology for a given traffic matrix in a region. The algorithm aims to use as few FSO links as possible since this allows us to reduce the network costs. In addition, the algorithm takes into account the influence of turbulence, misalignment, and noise by choosing links with a low bit-error rate (BER). Besides the heuristic, we also propose an optimal integer linear programming model for solving the same problem. The simulation results show that the proposed heuristic runs very fast, even with a large number of choices of FSO sites. Therefore, it is practical to use the heuristic to quickly design a restoration communication network for replacing a regular one affected by a …",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:dhFuZR0502QC,,1
1438,Overlapped segment shared protection in multi-domain networks,"Segment protection is an intermediate protection scheme between path and link protection. The major advantage of segment protection comes from its limited rerouting scope. It then leads to faster recovery over path protection and greater resource utilization efficiency over link protection. Overlapped Segment Shared Protection allows a protection of all nodes and links in addition to bandwidth savings. Routing for Overlapped Segment Shared Protection consists of finding, for each user request, a working path and a set of backup segments to protect it while minimizing the total required bandwidth. The problem in multi-domain optical mesh networks has not received a lot of interest so far although it is more difficult than in single-domain network. The main difficulty lies in the lack of the complete and global knowledge of the network topology as well as the bandwidth allocation meanwhile this knowledge are …",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:IjCSPb-OGe4C,,1
1439,Survivable physical topology design for all-optical metro core networks,"Along with the development of bandwidth consuming services, fiber optic is being widely used, especially in the metro core networks. Many solutions have been proposed for designing optical network topology. However these solutions neither focus on all-optical network nor fault tolerance issue. This paper proposes a solution for designing physical topology of optical metro core networks with the objective of (i) ensuring traffic requirements between the network nodes, (ii) minimizing fiber cost, and (iii) assuring the network survivability. The numerical results show that the proposed solution satisfies those objectives and save more fiber than existing solutions.",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:u5HHmVD_uO8C,,1
1440,A map-and-route approach for segment shared protection in multi-domain networks,Routing and protection with an overlapping segment shared protection (OSSP) scheme in multi-domain networks is more difficult than that in single domain networks because of scalability requirements. We propose a novel approach for OSSP routing where the underlying idea is the prior identification of Potential Intra-domain Paths (PIP) for carrying working and backup traffic between domain border nodes. These PIPs help to reduce the multi-domain network to a simpler aggregated network where routing is performed without unnecessarily going down to the physical links. The novel approach offers an exact and highly scalable routing thanks to the prior identification of the PIPs and the introduction of a maximal share risk group feature. Experiments show that the quality of the proposed approach is close to the optimal single-domain network solution and outperforms the existing multi-domain network solutions.,87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:Tyk-4Ss8FVUC,,1
1441,Protected elastic-tree topology for data center,"Cloud computing and storage solutions enable end users and enterprises to store their data and process their services in third party data centers. The fast growth of data center in size and in number makes them become huge energy consumption points. Some researches claim that 70% energy for a data center focus on server running and cooling. That is why, recent researches proposed to turn off certain switches in data center where little traffic flows through. However, when those switches are turned off, the structure of the data center becomes vulnerable to failures due to low connectivities between servers. This paper aims to overcome this weakness by proposing a way to build a data center where unnecessary switches can still be turned off while the data center remains survivable upon any single failure. The simulation results show that the proposed solution makes data center survivable while still saving a …",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:QIV2ME_5wuYC,,1
1442,A novel Topology Aggregation approach for shared protection in multi-domain networks,"Routing for shared protection in multi-domain networks is more difficult than that in single-domain networks because of the scalability requirements. We propose a novel approach for shared protection routing in multi-domain networks where the key feature is a special Topology Aggregation. In this Topology Aggregation, only some potential intra-domain paths (intra-paths for short) are selected for carrying working and backup traffic between domain border nodes. The abstraction of each intra-path to a virtual edge makes the original multi-domain network to become an aggregated network. On the aggregated network, a single-domain routing algorithm for shared protection can be applied for obtaining the complete routing solutions. The experiments show that the proposed approach is scalable. Moreover it is close to the optimal solution in single-domain networks and outperforms the previously proposed scalable …",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:W7OEmFMy1HYC,,1
1443,A model for designing survivable mesh optical access networks,"In this paper, we propose to use mesh topology for PON based access network in order to strengthen its survivable ability. We propose also an exact model for designing survivable mesh PON with objective of minimizing fiber miles. A greedy heuristic for designing survivable PON is also proposed.",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:zYLM7Y9cAGgC,,1
1444,Impact of mesh topology in cost reduction of survivable hybrid WDM-TDM PON networks,"Along with the development of the Fiber-to-the-Home (FTTH) networks, the Passive Optical Network (PON) is largely studied. PON technology is known as optical access network technology that uses fiber optic and passive devices to connect service providers and end users. Some models for deploying PON have been proposed. In this paper we focus on mesh and star topology for hybrid WDM-TDM PON. We propose three design methods for the two models and compare them to each other in order to see the advantage of mesh topolgy with links between AWGs. These methods are also compared with the solution in [11]. The results show that the proposed methods gains better performance and the use of link between AWGs allows saving a noticeable fiber amount.",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:L8Ckcad2t8MC,,1
1445,Protection partagée pour les réseaux de transport multidomaines,"Protectioll of multi-domain networks is very important becaiise of the highly signif icant impact of a single failure in terms of cost and geographical scope. Although there have been many studies on shared protection, most of them remain limited to single domain networks due to their requirements of complete or partial but aiways global information. These requirements cannot be satisfied in multi-domain networks because of the scalability constraint. Few researches have been conducted specifically for multi-domain networks. Amongst them, we find dedicated protec tion solution, illcomplete shared protection solution which leaves some nodes or links unprotected, or a solution scheme that is valid oniy for a special type of net works. There is thus an actual need for efficient shared protection solutions which deal with general multi-domain networks. This thesis proposes complete solutions for shared path protection, overlapped segment shared protection as well as solutions for re-optimizing existing backup paths leading to the reduction of the backup capacity. These solutions are based on different topology aggregations which allow obtaining a simple network, enriched with aggregate information; and two-step routillgs using the aggregate network at the first step and the original ones in the second step. The use of the aggregate instead of complete and global information iII the routings makes our solutions scalable for multi-dornain networks.",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:MXK_kJrjxJIC,,1
1446,Dimensioning of Free-Space Optical Mesh Network Using High-Altitude Platforms,"High altitude platforms (HAPs) are flying object operated in the stratosphere at 17 km -24 km altitude. HAPs can carry free-space optical (FSO) transceivers for playing the role of intermediate nodes in the communication system between ground FSO transceivers distributed in a large area such as city metropolitan or a province. Installation of HAPs and FSO transceivers on HAP are costly in terms of device cost itself and also maintenance cost. In this paper, we try to answer the questions of where to install HAPs and FSO transceivers on them so that the total cost is minimized. We propose algorithms for designing the topology of HAP networks. The results show graphically locations of HAP as well as the FSO links to be deployed between HAPs.",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:mB3voiENLucC,,1
1447,Design and optimization of FSO mesh networks over atmospheric turbulence and misalignment fading channels,"In this paper, we design and optimize free-space optics (FSO) mesh networks over atmospheric turbulence and misalignment fading channels. We propose an heuristic algorithm to choose the best sites to install the FSO transceivers and also the best topology for a given traffic load in a region. The algorithm aims to use the least of FSO links as possible since this allows to reduce the network costs. In addition, the algorithm takes into account the influence of turbulence, misalignment, and noise by choosing links with low bit-error rate (BER). The simulation results show that the calculation time of the proposed heuristic algorithm is much smaller than optimal integer linear programming (ILP) model. Since the proposed algorithm runs very fast, even with a large number of choices of FSO sites, it is practical to use it in building quickly a restoration communication networks for replacing a regular one after a disaster. The …",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:7PzlFSSx8tAC,,1
1448,How simple routing algorithms are good for solving rwa problem in survival optical networks?,"Since Routing and Wavelength Assignment (RWA) in optical networks is NP-complete problem. Many researches have proposed from simple to complex heuristic algorithms for solving this RWA problem. The shortest path of Dijkstra and sometimes the shortest pair of disjoint paths of Suurballe are used for solving this problem. Such simple heuristic algorithms are usually prejudged as inefficient leading to the conception of complex algorithms. In this paper we analyse quantitatively to see how the simple algorithms of Dijikstra and of Suurballe are good in solving RWA problem in survivable optical networks. The comparisons are made against the optimal solution, when it is available, and against a lower bound, otherwise. The results show that simple algorithms once give solutions, they give very good solutions.",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:KlAtU1dfN6UC,,1
1449,Evaluation of simple algorithms for solving RWA problem in survivable optical networks,"Routing and Wavelength Assignment (RWA) is a well-known problem in optical networks. This problem is generally NP-complete [1]. Many researches have proposed from simple to complex heuristic algorithms for solving RWA problem. The simple heuristics are usually prejudged as inefficient. This paper evaluates the efficiency of some simple algorithms for RWA problem in survivable optical networks. The evaluated RWA algorithms are mainly based on a shortest path algorithm, a shortest pairs of disjoint path algorithm and an simple wavelength assignment algorithm. The challenge is that the optimal solution for the RWA problem is not available in general, especially in medium and large size optical networks. In order to overcome with this challenge, we will find a lower bound of the optimal solution and evaluate the heuristic algorithms against the lower bound instead of the optimal solution. The results show …",87,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=JQnOBnIAAAAJ&citation_for_view=JQnOBnIAAAAJ:Zph67rFs4hoC,,1
1450,Predicting the onset of type 2 diabetes using wide and deep learning with electronic health records,"Diabetes is responsible for considerable morbidity, healthcare utilisation and mortality in both developed and developing countries. Currently, methods of treating diabetes are inadequate and costly so prevention becomes an important step in reducing the burden of diabetes and its complications. Electronic health records (EHRs) for each individual or a population have become important tools in understanding developing trends of diseases. Using EHRs to predict the onset of diabetes could improve the quality and efficiency of medical care. In this paper, we apply a wide and deep learning model that combines the strength of a generalised linear model with various features and a deep feed-forward neural network to improve the prediction of the onset of type 2 diabetes mellitus (T2DM).",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:0EnyYjriUFMC,,1
1451,Automatic skin lesion analysis towards melanoma detection,"Deep learning methods for image analysis have shown impressive performance in recent years. In this paper, we present deep learning based approaches to solve two problems in skin lesion analysis using a dermoscopic image containing skin tumor. In the first problem, we use a fully convolutional-deconvolutional architecture to automatically segment skin tumor from the surrounding skin. In the second problem, we use a simple convolutional neural network and VGG-16 architecture using transfer learning to address the two different tasks in skin tumor classification. The proposed models are trained and evaluated on standard benchmark datasets from the International Skin Imaging Collaboration (ISIC) 2017 Challenge, which consists of 2000 training samples and 600 testing samples. The result shows that the proposed methods achieve promising performances. In the first problem, the average value of Jaccard …",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:2osOgNQ5qMEC,,1
1452,iEnhancer-ECNN: identifying enhancers and their strength using ensembles of convolutional neural networks,"Enhancers are non-coding DNA fragments which are crucial in gene regulation (e.g. transcription and translation). Having high locational variation and free scattering in 98% of non-encoding genomes, enhancer identification is, therefore, more complicated than other genetic factors. To address this biological issue, several in silico studies have been done to identify and classify enhancer sequences among a myriad of DNA sequences using computational advances. Although recent studies have come up with improved performance, shortfalls in these learning models still remain. To overcome limitations of existing learning models, we introduce iEnhancer-ECNN, an efficient prediction framework using one-hot encoding and k-mers for data transformation and ensembles of convolutional neural networks for model construction, to identify enhancers and classify their strength. The benchmark dataset from Liu et al.’s …",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:3fE2CSJIrl8C,,1
1453,Deep learning models for tuberculosis detection from chest X-ray images,"This paper explores the usefulness of transfer learning on medical imaging for tuberculosis detection. We show an improved method for transfer learning over the regular method of using ImageNet weights. We also discover that the low-level features from ImageNet weights are not useful for imaging tasks for modalities like X-rays and also propose a new method for obtaining low level features by training the models in a multiclass multilabel scenario. This results in an improved performance in the classification of tuberculosis as opposed to training from a randomly initialized settings. In other words, we have proposed a better way for training in a data constrained setting such as the healthcare sector.",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:7PzlFSSx8tAC,,1
1454,Diabetic retinopathy detection using deep learning,"Diabetic Retinopathy (DR) is an eye disease associated with chronic diabetes. DR is the leading cause of blindness among working aged adults around the world and estimated it may affect more than 93 million people. Progression to vision impairment can be slowed or controlled if DR is detected in time, however this can be difficult as the disease often shows few symptoms until it is too late to provide effective treatment.",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:9ZlFYXVOiuMC,,1
1455,iProDNA-CapsNet: identifying protein-DNA binding residues using capsule neural networks,"Since protein-DNA interactions are highly essential to diverse biological events, accurately positioning the location of the DNA-binding residues is necessary. This biological issue, however, is currently a challenging task in the age of post-genomic where data on protein sequences have expanded very fast. In this study, we propose iProDNA-CapsNet – a new prediction model identifying protein-DNA binding residues using an ensemble of capsule neural networks (CapsNets) on position specific scoring matrix (PSMM) profiles. The use of CapsNets promises an innovative approach to determine the location of DNA-binding residues. In this study, the benchmark datasets introduced by Hu et al. (2017), i.e., PDNA-543 and PDNA-TEST, were used to train and evaluate the model, respectively. To fairly assess the model performance, comparative analysis between iProDNA-CapsNet and existing state-of-the-art methods …",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:kNdYIx-mwKoC,,1
1456,Automatic speech recognition for Vietnamese using HTK system,"This paper presents our research on Vietnamese speech recognition using HTK1 system. Our method is the adaptation of Vietnamese data into HTK system: text corpus was collected from Web and then bigram language model was built by the CMU SLM toolkit. The experiments were carried out with different acoustic models. Further more, the experiment using speaker adaptation technique was implemented by MLLR algorithm. For our results, WA (Word Accuracy) in the case of speaker independent recognition test is 71.37% and 75.96% with the test of speaker adaptation.",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:WF5omc3nYNoC,,1
1457,Tone recognition of Vietnamese continuous speech using Hidden Markov Model,"This paper presents our study on context independent tone recognition of Vietnamese continuous speech. Each of the six Vietnamese tones is represented by a hidden Markov model (HMM for short) and we used VNSPEECHCORPUS to learn these models in terms of fundamental frequency, F 0 , and short-time energy. We focus on evaluating the influence of different factors on the tone recognition. The experimental results show that the best method to learn F 0  and energy is to use a logarithmic transformation function and then normalization with mean and mean deviation. In addition, we show that using 8 forms of tones and the discrimination between male and female speakers increase the accuracy of the Vietnamese tone recognition system.",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:Y0pCki6q_DkC,,1
1458,"A novel approach in continuous speech recognition for Vietnamese, an isolating tonal language","This paper proposes a new approach for the integration of the Vietnamese language characteristics into a Large Vocabulary Continuous Speech Recognition System (LVCSR) which was built for some European languages. Firstly, a new module of tone recognition using Hidden Markov model was constructed. Secondly, several methods were applied to transform a text corpus of monosyllabic words into text corpus of polysyllabic words and a statistical language model of polysyllabic words was built by using the new text corpus. Finally, all the knowledge has been included in the LVCSR system so that this system can be adapted for Vietnamese. Experiments are made on the VNSPEECHCORPUS. The results show that the accuracy of Vietnamese recognition system was increased, 46% of relative reduction of the word error rate is obtained by using Vietnamese language characteristics.",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:YsMSGLbcyi4C,,1
1459,Multimodal detection of Parkinson disease based on vocal and improved spiral test,"Previous studies have used data related to either voice or spiral drawing to detect Parkinson disease (PD). However, different people experience different symptoms and different levels of severity of PD. This paper proposes a multimodal approach combining voice and image test to enhance the reliability of detecting PD patients. To substantiate this idea, we have evaluated both voice and spiral test data using various machine learning models. The results based on the two types of dataset demonstrate an excellent level of accuracy for PD identification. Pairwise correlation and k-means clustering techniques are used to extract features from the vocal dataset. In this classification problem, the highest accuracy of 95.89% is obtained using an ensemble of 3 classification models. The Pearson's correlation is used to extract features from the image dataset. The best accuracy of 99.6% is achieved using the k-Nearest …",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:qxL8FJ1GzNcC,,1
1460,Breast cancer prediction using feature selection and ensemble voting,"Breast cancer is the most common cause of cancer among women worldwide. This paper analyses the performance of supervised and unsupervised models for breast cancer classification. Data from Wisconsin Breast Cancer Dataset is used in this paper. Feature selection is processed through scaling and principal component analysis. Final results indicate that Ensemble Voting approach is ideal as a predictive model for breast cancer. The raw data has 569 cases of breast cancer. The data is split into training and testing sets in the ration 70:30, respectively. The benchmark model is then created using Random Forest method. Various models are trained and tested on the data after Feature Scaling and Principle Component Analysis. Cross-validation is performed which showed that our model is stable. Among all the evaluated models, only four models, i.e., Ensemble - Voting Classifier, Logistics Regression, SVM …",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:ULOm3_A8WrAC,,1
1461,Stacking segment-based CNN with SVM for recognition of atrial fibrillation from single-lead ECG recordings,"Atrial fibrillation (AF) is the most common form of cardiac rhythm disorder. Early detection of AF can result in a lower risk of stroke, heart failure, systemic thromboembolism, and coronary artery disease. AF detection however is challenging due to the need for specialised equipment and professional technicians. Hand-held electrocardiogram (ECG) devices, including wearables, are now available and provide a potential mechanism for detecting AF. We wished to identify AF from short single-lead ECG recordings using a machine learning method.",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:GnPB-g6toBAC,,1
1462,Using tone information for Vietnamese continuous speech recognition,"This paper presents our study on the use of tone information in a large vocabulary for a Vietnamese continuous speech recognition system. Firstly, a new module of tone recognition using Hidden Markov model is presented. Then, a new methodology for integrating this module into the Speeral system is given. The experiments were implemented on VNSpeechCorpus. The results showed that the direct use of tone score in the Speeral system would increase the performance of the system, e.g., 28.6% relative reduction in word error rate.",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:eQOLeE2rZwMC,,1
1463,Music genre classification using residual attention network,"This paper proposes a new method in music genre classification by using the Residual Attention Network (RAN). Through the integration of attention mechanism and stacking attention modules, RAN has shown efficiency in the field of image processing. Residual blocks and attention modules are the most important considerations in RAN. First, each audio file is converted into a set of spectral images. Next, RAN is used for the classification of these spectral images, where the optimal stochastic gradient descent algorithm is used as the training model. The method is evaluated on the music data set from the Zalo AI “Music Genre Classification” challenge. This data set includes recordings of 10 different musical genres in Vietnam. Data augmentation combined with error analysis were implemented on the validation set. The results obtained had an accuracy of 71.7% on the test set. This highlights the potential of the …",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:_kc_bZDykSQC,,1
1464,Lesion segmentation and automated melanoma detection using deep convolutional neural networks and XGBoost,"Melanoma is a lethal form of skin cancer. It is one of the leading causes of deaths related to skin cancer. Most clinical studies show that early diagnosis of melanoma can improve patient outcomes. Currently, the most effective mean of analyzing lesions involves the use of dermoscopic imaging. However, this process is highly Subjective as it is dependent on the proficiency levels and experience of the specialist. Our research aims at minimizing the level of uncertainty and Subjectivity in human assessment. We propose a deep learning approach to model lesion patterns with the goal of performing automated melanoma detection and lesion segmentation from skin images. We use an ensemble of deep learning models to combine multiple hypothesis into a single decision point. This mimics the real world approach where the specialist will typically consult other specialists to cross reference and double check their …",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:M3ejUd6NZC8C,,1
1465,Predicting hospital readmission patterns of diabetic patients using ensemble model and cluster analysis,"Diabetes is a chronic illness that affects around 425 million people globally in 2017, and this is predicted to increase to 629 million by the end of 2045. The ability to analyze and predict the readmission patterns of diabetic patients would allow the optimization of hospital resources and assessment of treatment effectiveness. This paper proposes an ensemble model to predict hospital readmission by choosing from a pool of 15 models, made up of variants of Logistic Regression, Decision Trees (DT), Neural Network (NN) and Augmented Naïve Bayes (NB) networks. The final ensemble model was assembled using the five best models, determined based on individual model accuracy and the Jaccard distance between them, to maximize overall accuracy and sensitivity. The final ensemble contained DT (CHAID), Tree Augmented Naïve Bayes network, DT (CHAID with boosting), Neural Network with bagging and DT …",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:4TOpqqG69KYC,,1
1466,"Large vocabulary continuous speech recognition for Vietnamese, an under-resourced language",This paper proposes a method to build a Vietnamese Large Vocabulary Continuous Speech Recognition system (Vietnamese LVCSR system). The difference between Vietnamese and European languages is analyzed and used to adapt a LVCSR system for European languages to Vietnamese. Experiments are implemented on the VNSPEECHCORPUS. The results show that the accuracy of Vietnamese recognition system is increased by using Vietnamese language characteristics.,89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:W7OEmFMy1HYC,,1
1467,iPseU-NCP: Identifying RNA pseudouridine sites using random forest and NCP-encoded features,"Pseudouridine modification is most commonly found among various kinds of RNA modification occurred in both prokaryotes and eukaryotes. This biochemical event has been proved to occur in multiple types of RNAs, including rRNA, mRNA, tRNA, and nuclear/nucleolar RNA. Hence, gaining a holistic understanding of pseudouridine modification can contribute to the development of drug discovery and gene therapies. Although some laboratory techniques have come up with moderately good outcomes in pseudouridine identification, they are costly and required skilled work experience. We propose iPseU-NCP – an efficient computational framework to predict pseudouridine sites using the Random Forest (RF) algorithm combined with nucleotide chemical properties (NCP) generated from RNA sequences. The benchmark dataset collected from Chen et al. (2016) was used to develop iPseU-NCP and fairly compare its performances with other methods. Under the same experimental settings, comparing with three state-of-the-art methods including iPseU-CNN, PseUI, and iRNA-PseU, the Matthew’s correlation coefficient (MCC) of our model increased by about 20.0%, 55.0%, and 109.0% when tested on the H. sapiens (H_200) dataset and by about 6.5%, 35.0%, and 150.0% when tested on the S. cerevisiae (S_200) dataset, respectively. This significant growth in MCC is very important since it ensures the stability and performance of our model. With those two independent test datasets, our model also presented higher accuracy with a success rate boosted by 7.0%, 13.0%, and 20.0% and 2.0%, 9.5%, and 25.0% when compared to iPseU-CNN …",89,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=rZHE4jcAAAAJ&citation_for_view=rZHE4jcAAAAJ:KlAtU1dfN6UC,,1
1468,"Hedge algebras, linguistic-value logic and their application to fuzzy reasoning","People use natural languages to think, to reason, to deduce conclusions, and to make decisions. Fuzzy set theory introduced by L. A. Zadeh has been intensively developed and founded a computational foundation for modeling human reasoning processs. The contribution of this theory both in the theoretical and the applied aspects is well recognized. However, the traditional fuzzy set theory cannot handle linguistic terms directly. In our approach, we have constructed algebraic structures to model linguistic domains, and developed a method of linguistic reasoning, which directly manipulates linguistic terms, In particular, our approach can be applied to fuzzy control problems.",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:u5HHmVD_uO8C,,1
1469,Estimation of air temperature using smartphones in different contexts,"Measuring air temperature at a high spatial resolution is very important for many applications including detection of urban heat islands. However, air temperature is currently measured by weather stations those are very sparse spatially. In this paper, we propose a new approach to estimate air temperature using smartphones in different contexts. Most of the smartphones are not equipped with air temperature sensors but they are all equipped with battery temperature sensors. When a smartphone is in idle state, its battery temperature is stable and correlated with ambient air temperature. Furthermore, it is often carried close to human body, e.g. in pockets of coats, trousers and in hand. Therefore we developed a new approach of using two linear regression models to estimate air temperature from the idle smartphones battery temperature given their in-pocket or out-of-pocket positions. Lab test results show that the …",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:KxtntwgDAa4C,,1
1470,Using cross-layer heuristic and network coding to improve throughput in multicast wireless mesh networks,"Wireless mesh networks (WMNs) receive much research interests because of their reliability, scalability and low cost. Obtaining high-throughput for multicast applications (e.g. video streaming broadcast) in WMN is challenging due to the interference and the change of channel quality. Cross-layer design and network coding are approaches which have been recently received considerable attention for high-throughput problem in wireless networks. In this paper, we propose an approach namely CLNC (Cross-Layer Network Coding) which is a combination of the above approaches to improve throughput in multicast wireless mesh networks. Our simulation results show that when the number of receivers is high CLNC's throughput is higher at least 30% than that of known methods such as AODV, DSDV and DSR and higher than that of MAODV. Moreover, PDR (Packet Delivery Ration) of CLNC is higher than that of …",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:_xSYboBqXhAC,,1
1471,Implementation of fast multipole algorithm on special-purpose computer MDGRAPE-2,"N-body simulation is a time consuming task in which force calculation part is most dominant part. The simplest and most accurate algorithm for force calculation is direct summation which has time complexity O (N2). It is not practically suitable for large-scale simulations on most general-purpose computers. To cut down cost of force calculation one applies fast algorithms or performs force calculation on specialpurpose hardware. GRAPE is a special-purpose computer designed for force calculation between pointcharge or point-mass particles. It performs force calculation much faster than general-purpose computers of similar cost. However the time complexity of direct force calculation on GRAPE is still O (N2). In this paper, we deal with the implementation of fast multipole algorithm whose time complexity is O (N) on specialpurpose computer MDGRAPE-2. We present our experimental results for up to four millions particles system. Performance and accuracy of FMM on GRAPE is presented. Comparison of FMM with treecode and direct summation on GRAPE is also given.",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:Tyk-4Ss8FVUC,,1
1472,Acceleration of fast multipole method using special-purpose computer GRAPE,"We have implemented the fast multipole method (FMM) on a special-purpose computer                 GRAPE (GRAvity piPE). The FMM is one of the fastest approximate algorithms to                 calculate forces among particles. Its calculation cost scales as                 O(N), while the naive algorithm scales as                     O(N2). Here,  N is the number of particles                 in the system. GRAPE is hardware dedicated to the calculation of Coulombic or                 gravitational forces among particles. GRAPE's calculation speed is                 100—1000 times faster than that of conventional computers of the same                 price, though it cannot handle anything but force calculation. We can expect                 significant speedup by the combination of the fast algorithm and the fast hardware.                 However, a straightforward implementation of the algorithm actually runs on GRAPE at                 rather modest speed. This is because of …",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:Y0pCki6q_DkC,,1
1473,Parallelization of the fast multipole method for molecular dynamics simulations on multicore computers,"We have parallelized the fast multipole method (FMM) on multicore computers using OpenMP programming model. The FMM is the one of the fastest approximate force calculation algorithms for molecular dynamics simulations. Its computational complexity is linear. Parallelization of FMM on multicore computers using OpenMP has been reported since the multicore processors become increasingly popular. However the number of those FMM implementations is not large. The main reason is that those FMM implementations have moderate or low parallel efficiency for high expansion orders due to sophisticated formulae of the FMM. In addition, parallel efficiency of those implementations for high expansion orders rapidly drops to 40% or lower as the number of threads increases to 8 or higher. Our FMM implementation on multicore computers using a combination approach as well as a newly developed …",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:YsMSGLbcyi4C,,1
1474,A new formulation for fast calculation of far field force in molecular dynamics simulations,We have developed a new formulation for fast calculation of far-field force of fast multipole method (FMM) in molecular dynamics simulations. FMM is a linear algorithm to calculate force for molecular dynamics simulations. GRAPE is a special-purpose computer dedicated to Coulombic force calculation. It runs 100-1000 times faster than normal computer at the same price. However FMM cannot be implemented directly on GRAPE. We have succeeded to implement FMM on GRAPE and developed a new formulation for far-field force calculation. Numerical tests show that the performance of FMM using our new formulation on GRAPE is approximately 2-5 times faster than that of FMM using conventional far field formulation.,100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:f2IySw72cVMC,,1
1475,A New Approach to Estimate Urban Air Temperature Using Smartphones,"Measuring urban air temperature at a high spatial resolution is very important for many applications including detection of urban heat islands. However air temperature is currently measured by professional weather stations those are very sparse at 50 km spatial resolution or more. In this paper, we propose a new approach to estimate air temperature from smartphones. Most of the smartphones are not equipped with air temperature sensors. However they are all equipped with battery temperature sensors. When a smartphone is in idle state, its battery temperature is stable and correlated with air temperature around the smartphone. Therefore we have developed a linear regression model to estimate air temperature from the idle smartphones battery temperature. Experiment results show that the new approach is statistically comparable to an existing one using mean error, mean absolute error and …",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:P5F9QuxV20EC,,1
1476,"Using local weather and geographical information to predict cholera outbreaks in Hanoi, Vietnam","In 2007, repeated outbreaks of cholera in Hanoi have raised the need to have up-to-date evidence on the impact of factors on cholera epidemic, which is essential for developing an early warning system. We have successfully built models to predict cholera outbreaks in Hanoi from 2001 to 2012 using Random Forests method. We found that geographical factors—the number of cholera cases of a district of interest and its neighbours—are very important to predict accurately cholera cases besides the weather factors. Among weather factors, temperature and relative humidity are the most important. We also found that prediction accuracy of our models, measured in adjusted coefficient of determination, will decrease by 0.0076 if prediction length increases by one day.",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:ufrVoPGSRksC,,1
1477,An efficient parallel algorithm for computing the closeness centrality in social networks,"Closeness centrality is an substantial metric used in large-scale network analysis, in particular social networks. Determining closeness centrality from a vertex to all other vertices in the graph is a high complexity problem. Prior work has a strong focuses on the algorithmic aspect of the problem, and little attention has been paid to the definition of the data structure supporting the implementation of the algorithm. Thus, we present in this paper an efficient algorithm to compute the closeness centrality of all nodes in a social network. Our algorithm is based on (i) an appropriate data structure for increasing the cache hit rate, and then reducing amount of time accessing the main memory for the graph data, and (ii) an efficient and parallel complete BFS search to reduce the execution time. We tested performance of our algorithm, namely BigGraph, with five different real-world social networks and compare the performance …",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:dshw04ExmUIC,,1
1478,An experimental study on cholera modeling in Hanoi,"Cholera modeling for prediction Cholera state is the research topic is specially considered by researchers. An experimental frame for Cholera modeling based on using data mining techniques is showed in this paper. In the Data Preparation Phase, two versions of data presentation and a solution for feature selection are considered. Moreover, both cases of two-value Cholera state and three-value Cholera state also investigated. Experimental results show that the global presentation is better than the local presentation, F1 measures are between 0.79 and 0.86, and the target variable has correlation with climate condition variables in some cases.",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:IjCSPb-OGe4C,,1
1479,Special-purpose computer accelerated fast multipole method,"We have implemented fast multipole method (FMM) on a special-purpose computer GRAPE. The FMM is one of the fastest approximate algorithms to calculate forces among particles. Its calculation cost scales as O (N), while naive algorithm scales as O (N2) where N is number of particles in the system. GRAPE is hardware dedicated to the calculation of Coulombic or gravitational force among particles. Its calculation speed is 100-1000 times faster than that of conventional computers of the same price, though it cannot handle anything but force calculation. We can expect significant speed up by combination of the fast algorithm and the fast hardware. However, a straight forward implementation of the algorithm actually runs on GRAPE at rather modest speed. This is because of the limited function of the hardware. Since GRAPE can handle particle force only, just a small fraction of the calculation procedure can be put on it. The rest part must be performed on a conventional computer connected to GRAPE. In order to take full advantage of the dedicated hardware, we modified the FMM using Pseudoparticle Multipole Method and Anderson’s method. In our modified algorithm, multipole and local expansions are expressed by distribution of a small number of imaginary particles (pseudoparticles), and thus they can be evaluated by GRAPE. Results of numerical experiments show that GRAPE accelerates the FMM by a factor of 3-60 depending on the accuracy. Its performance exceeds that of Barnes-Hut treecode on GRAPE at high accuracy (root-mean-square relative force error∼ 10− 5), in the case of close-to-uniform distribution of particles.",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:eQOLeE2rZwMC,,1
1480,Phase Prediction of Multi-principal Element Alloys Using Support Vector Machine and Bayesian Optimization,"Designing new materials with desired properties is a complex and time-consuming process. One of the challenging factors of the design process is the huge search space of possible materials. Machine learning methods such as k-nearest neighbours, support vector machine (SVM) and artificial neural network (ANN) can contribute to this process by predicting materials properties accurately. Properties of multi-principal element alloys (MPEAs) highly depend on alloys’ phase. Thus, accurate prediction of the alloy’s phase is important to narrow down the search space. In this paper, we propose a solution of employing support vector machine method with hyperparameters tuning and the use of weight values for prediction of the alloy’s phase. Using the dataset consisting of the experimental results of 118 MPEAs, our solution achieves the cross-validation accuracy of 90.2%. We confirm the superiority of this …",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:UxriW0iASnsC,,1
1481,"Enhancing Cholera Outbreaks Prediction Performance in Hanoi, Vietnam Using Solar Terms and Resampling Data","A solar term is an ancient Chinese concept to indicate a point of season change in lunisolar calendars. Solar terms are currently in use in China and nearby countries including Vietnam. In this paper we propose a new solution to increase performance of cholera outbreaks prediction in Hanoi, Vietnam. The new solution is a combination of solar terms, training data resampling and classification methods. Experimental results show that using solar terms in combination with ROSE resampling and random forests method delivers high area under the Receiver Operating Characteristic curve (AUC), balanced sensitivity and specificity. Without interaction effects the solar terms help increasing mean of AUC by 12.66%. The most important predictor in the solution is Sun’s ecliptical longitude corresponding to solar terms. Among the solar terms, frost descent and start of summer are the most important.",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:abG-DnoFyZgC,,1
1482,"Enhancing Cholera Outbreaks Prediction Performance in Hanoi, Vietnam Using Solar Terms and Resampling Data","A solar term is an ancient Chinese concept to indicate a point of season change in lunisolar calendars. Solar terms are currently in use in China and nearby countries including Vietnam. In this paper we propose a new solution to increase performance of cholera outbreaks prediction in Hanoi, Vietnam. The new solution is a combination of solar terms, training data resampling and classification methods. Experimental results show that using solar terms in combination with ROSE resampling and random forests method delivers high area under the Receiver Operating Characteristic curve (AUC), balanced sensitivity and specificity. Without interaction effects the solar terms help increasing mean of AUC by 12.66%. The most important predictor in the solution is Sun’s ecliptical longitude corresponding to solar terms. Among the solar terms, frost descent and start of summer are the most important.",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:bFI3QPDXJZMC,,1
1483,Building the Local Spatial Data Infrastructure for the Northeast and Northwest of Vietnam,"National Spatial Data Infrastructure (NSDI) is the backbone of the development of e-government for a country. Since 2011, Vietnam began to pay attention to NSDI. Till now, the is no significant progress in implementation of NSDI except a common research between World Bank and Ministry of Natural Resources and Environment on strategic planning of NSDI and another research on making policy for building NSDI. In this paper, we present our research on the construction of the Local SDI (LSDI) for the Northeast and the Northwest of Vietnam. Our hypothesis is that the success of this LSDI will be a good lesson for building NSDI in the future. The reason why we selected the Northeast and Northwest because it is a large area with a total area of approximately 115,700km2 that is nearly 1/3 of Vietnam. Moreover, the Northeast and the Northwest comprehensively contain of topographical and morphological characteristics of nature, economics and society of Vietnam. NSDI is considered consisting of seven components: (i) spatial data; (ii) standards; (iii) Access to data; (iv) technology; (v) policy – legislation; (vi) organization – institution; (vii) partnership. In this research we only focused on four technical components which are spatial data, standards, access to data and technology. We collected spatial data from three sources: (i) existing spatial data and framework spatial data from Government and Local Government; (ii) existing remote sensing data from the NASA, USGS; (iii) Update remote sensing data from MODIS/NPP ground station at our university. We used open source platforms for building LSDI. Regarding to the access to data, we have …",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:EUQCXRtRnyEC,,1
1486,Air pollution monitoring and warning system,"Nowaday, remote sensing images have provided a large dataset with geospatial information at global scale at different resolutions, which is widely using in various domains. The usage of satellite technology for air pollution monitoring applications has been recently increasing especially to provide global-to-local distribution of aerosol and its properties for deriving Particulate Matter concentration (PM). The paper investigates aerosol retrieval for multi- resolution satellite images. After that, PM is estimated from aerosol products and meteorological parameters in order to provide dust observations at different spatial scales. A geographic information system for air pollution monitoring and warning is developed based on these research results.",100,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=72osj9YAAAAJ&citation_for_view=72osj9YAAAAJ:UeHWp8X0CEIC,,1
1488,Learning to classify short and sparse text & web with hidden topics from large-scale data collections,"This paper presents a general framework for building classifiers that deal with short and sparse text & Web segments by making the most of hidden topics discovered from large-scale data collections. The main motivation of this work is that many classification tasks working with short segments of text & Web, such as search snippets, forum & chat messages, blog & news feeds, product reviews, and book & movie summaries, fail to achieve high accuracy due to the data sparseness. We, therefore, come up with an idea of gaining external knowledge to make the data more related as well as expand the coverage of classifiers to handle future data better. The underlying idea of the framework is that for each classification task, we collect a large-scale external data collection called"" universal dataset"", and then build a classifier on both a (small) set of labeled training data and a rich set of hidden topics discovered from that …",102,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=9HELB78AAAAJ&citation_for_view=9HELB78AAAAJ:u5HHmVD_uO8C,,1
1489,A hidden topic-based framework toward building applications with short web documents,"This paper introduces a hidden topic-based framework for processing short and sparse documents (e.g., search result snippets, product descriptions, book/movie summaries, and advertising messages) on the Web. The framework focuses on solving two main challenges posed by these kinds of documents: 1) data sparseness and 2) synonyms/homonyms. The former leads to the lack of shared words and contexts among documents while the latter are big linguistic obstacles in natural language processing (NLP) and information retrieval (IR). The underlying idea of the framework is that common hidden topics discovered from large external data sets (universal data sets), when included, can make short documents less sparse and more topic-oriented. Furthermore, hidden topics from universal data sets help handle unseen data better. The proposed framework can also be applied for different natural languages and …",102,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=9HELB78AAAAJ&citation_for_view=9HELB78AAAAJ:2osOgNQ5qMEC,,1
1491,Vietnamese word segmentation with CRFs and SVMs: An investigation,"Word segmentation for Vietnamese, like for most Asian languages, is an important task which has a significant impact on higher language processing levels. However, it has received little attention of the community due to the lack of a common annotated corpus for evaluation and comparison. Also, most previous studies focused on unsupervised-statistical approaches or combined too many techniques. Consequently, their accuracies are not as high as expected. This paper reports a careful investigation of using conditional random fields (CRFs) and support vector machines (SVMs)-two of the most successful statistical learning methods in NLP and pattern recognition-for solving the task. We first build a moderate annotated corpus using different sources of materials. For a careful evaluation, different CRF and SVM models using different feature settings were trained and their results are compared and contrasted with each other. In addition, we discuss several important points about the accuracy, computational cost, corpus size and other aspects that might influence the overall quality of Vietnamese word segmentation.",102,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=9HELB78AAAAJ&citation_for_view=9HELB78AAAAJ:u-x6o8ySG0sC,,1
1496,Web search clustering and labeling with hidden topics,Web search clustering is a solution to reorganize search results (also called “snippets”) in a more convenient way for browsing. There are three key requirements for such post-retrieval clustering systems: (1) the clustering algorithm should group similar documents together; (2) clusters should be labeled with descriptive phrases; and (3) the clustering system should provide high-quality clustering without downloading the whole Web page.,102,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=9HELB78AAAAJ&citation_for_view=9HELB78AAAAJ:qjMakFHDy7sC,,1
1497,Named entity recognition in vietnamese free-text and web documents using conditional random fields,"Named entity recognition (NER) is the process of identifying different entity types (eg person, location, organization, or date/time), mentioned in natural language documents. It is an important task in information extraction and is a necessary precursor to higher processing and understanding natural language such as text mining, text summarization, question-answering, and machine translation. Further, the automated recognition of named entities is essential for the increasing need of searching, extracting, and tracking relevant information on the web environment, and especially for building the emerging semantic web technology.",102,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=9HELB78AAAAJ&citation_for_view=9HELB78AAAAJ:IjCSPb-OGe4C,,1
1498,Using Dependency Analysis to Improve Question Classification,"Question classification is a first necessary task of automatic question answering systems. Linguistic features play an important role in developing an accurate question classifier. This paper proposes to use typed dependencies which are extracted automatically from dependency parses of questions to improve accuracy of classification. Experiment results show that with only surface typed dependencies, one can improve the accuracy of a discriminative question classifier by over 8.0% on two benchmark datasets.",102,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=9HELB78AAAAJ&citation_for_view=9HELB78AAAAJ:bEWYMUwI8FkC,,1
1499,Personal name resolution crossover documents by a semantics-based approach,"Cross-document personal name resolution is the process of identifying whether or not a common personal name mentioned in different documents refers to the same individual. Most previous approaches usually rely on lexical matching such as the occurrence of common words surrounding the entity name to measure the similarity between documents, and then clusters the documents according to their referents. In spite of certain successes, measuring similarity based on lexical comparison sometimes ignores important linguistic phenomena at the semantic level such as synonym or paraphrase. This paper presents a semantics-based approach to the resolution of personal name crossover documents that can make the most of both lexical evidences and semantic clues. In our method, the similarity values between documents are determined by estimating the semantic relatedness between words. Further, the …",102,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=9HELB78AAAAJ&citation_for_view=9HELB78AAAAJ:2P1L_qKh6hAC,,1
1500,Semantic parsing with structured SVM ensemble classification models,We present a learning framework for structured support vector models in which boosting and bagging methods are used to construct ensemble models. We also propose a selection method which is based on a switching model among a set of outputs of individual classifiers when dealing with natural language parsing problems. The switching model uses subtrees mined from the corpus and a boosting-based algorithm to select the most appropriate output. The application of the proposed framework on the domain of semantic parsing shows advantages in comparison with the original large margin methods.,102,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=9HELB78AAAAJ&citation_for_view=9HELB78AAAAJ:zYLM7Y9cAGgC,,1
1501,Performance Prediction for Students: A Multi-Strategy Approach,"This paper presents a study on Predicting Student Performance (PSP) in academic systems. In order to solve the task, we have proposed and investigated different strategies. Specifically, we consider this task as a regression problem and a rating prediction problem in recommender systems. To improve the performance of the former, we proposed the use of additional features based on course-related skills. Moreover, to effectively utilize the outputs of these two strategies, we also proposed a combination of the two methods to enhance the prediction performance. We evaluated the proposed methods on a dataset which was built using the mark data of students in information technology at Vietnam National University, Hanoi (VNU). The experimental results have demonstrated that unlike the PSP in e-Learning systems, the regression-based approach should give better performance than the recommender system-based approach. The integration of the proposed features also helps to enhance the performance of the regression-based systems. Overall, the proposed hybrid method achieved the best RMSE score of 1.668. These promising results are expected to provide students early feedbacks about their (predicted) performance on their future courses, and therefore saving times of students and their tutors in determining which courses are appropriate for students’ ability.",102,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=9HELB78AAAAJ&citation_for_view=9HELB78AAAAJ:vV6vV6tmYwMC,,1
1502,Automated data extraction from the web with conditional models,"Extracting data on the Web is an important information extraction task. Most existing approaches rely on wrappers which require human knowledge and user interaction during extraction. This paper proposes the use of conditional models as an alternative solution to this task. Deriving the strength of conditional models like maximum entropy and maximum entropy Markov models, our method offers three major advantages: the full automation, the ability to incorporate various non-independent, overlapping features of different hypertext representations, and the ability to deal with missing and disordered data fields. The experimental results on a wide range of e-commercial websites with different layouts show that our method can achieve a satisfactory trade-off between automation and accuracy, and also provide a practical application of automated data extraction from the Web.",102,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=9HELB78AAAAJ&citation_for_view=9HELB78AAAAJ:W7OEmFMy1HYC,,1
1504,A feature-word-topic model for image annotation and retrieval,"Image annotation is a process of finding appropriate semantic labels for images in order to obtain a more convenient way for indexing and searching images on the Web. This article proposes a novel method for image annotation based on combining feature-word distributions, which map from visual space to word space, and word-topic distributions, which form a structure to capture label relationships for annotation. We refer to this type of model as Feature-Word-Topic models. The introduction of topics allows us to efficiently take word associations, such as {ocean, fish, coral} or {desert, sand, cactus}, into account for image annotation. Unlike previous topic-based methods, we do not consider topics as joint distributions of words and visual features, but as distributions of words only. Feature-word distributions are utilized to define weights in computation of topic distributions for annotation. By doing so, topic models in …",102,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=9HELB78AAAAJ&citation_for_view=9HELB78AAAAJ:_Qo2XoVZTnwC,,1
1505,An efficient feature selection using hidden topic in text categorization,"Text categorization is an important research area in information retrieval. In order to save the storage space and get better accuracy, efficient and effective feature selection methods for reducing the data before analysis are highly desired. Usually, researches on feature selection use only a proper measurement such as information gain. In this paper, we propose a new feature selection method by adopting an attractive hidden topic analysis and entropy-based feature ranking. Experiments dealing with the well-known Reuters-21578 and Ohsumed datasets show that our method can achieve a better classification accuracy while reducing the feature dimension dramatically.",102,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=9HELB78AAAAJ&citation_for_view=9HELB78AAAAJ:ufrVoPGSRksC,,1
1506,Maturation of resistance to lead encephalopathy: cellular and subcellular mechanisms.,"The rat pup fed inorganic lead has been studied extensively as an animal model of human lead encephalopathy. As in man, the sensitivity of the brain to lead toxicity is age-dependent. Pups given daily lead feedings for one week beginning in the first week of life show pathologic changes (ie, hemorrhage, edema, and neuronal necrosis) throughout the brain including the cerebral cortex and cerebellum. Pups begun on daily lead feedings for two weeks between 10-18 days of age show similar pathologic changes almost entirely confined to the cerebellum. Pups receiving very large quantities of lead for two weeks beginning at 20 or 24 days of age develop only minimal edema or no changes by light microscopy. We have proposed that the effects of lead on cellular aerobic energy metabolism are important in the pathogenesis of the encephalopathy in the developing brain. Early in the course of lead feedings begun at 14 days of age, isolated cerebellar mitochondria show a loss of respiratory control. During the second week of lead feedings, respiration with NAD-linked substrates is inhibited in cerebellar mitochondria, but not in cerebral mitochondria, from these animals. Cerebral mitochondrial respiration in pups fed lead from birth also is inhibited while both cerebral and cerebellar mitochondrial respiration in lead-fed adults is not affected. Isolated brain mitochondria exposed to lead in vitro show similar changes; an initial respiratory stimulation (probably reflecting an energy-coupled uptake of lead) and a secondary inhibition of dehydrogenases located in the mitochondrial matrix. Lead also may compete with calcium for brain mitochondrial carrier …",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:9pM33mqn1YgC,,1
1507,Epidemiology of new-onset atrial fibrillation following coronary artery bypass graft surgery,"Postoperative atrial fibrillation (AF) following coronary artery bypass graft surgery (CABG) is significantly associated with reduced survival, but poor characterisation and inconsistent definitions present barriers to developing effective prophylaxis and management. We sought to address this knowledge gap.",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:4fGpz3EwCPoC,,1
1508,Covid-19 misinformation and the social (media) amplification of risk: A Vietnamese perspective,"The amplification of Coronavirus risk on social media sees Vietnam falling volatile to a chaotic sphere of mis/disinformation and incivility, which instigates a movement to counter its effects on public anxiety and fear. Benign or malign, these civil forces generate a huge public pressure to keep the one-party system on toes, forcing it to be unusually transparent in responding to public concerns.",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:bz8QjSJIRt4C,,1
1509,"Effects of extracts of lupine seed on blood glucose levels in glucose resistant mice: antihyperglycemic effects of Lupinus albus (white lupine, Egypt) and Lupinus caudatus …","Lupine is a medicinal food plant with potential value in the management of diabetes. In white mice, extracts of seeds of the white lupine [Lupinus albus (L. termis L.)] were associated with increased tolerance to an oral glucose bolus. Antihyperglycemic activity was present in extracts of the whole seed but not extracts of the seed coat, and was not detected when glucose was administered intraperitoneally rather than orally. However, in contrast to results seen with the prescription drug, acarbose, lupine extract did not appear to increase the bulk or carbohydrate content of the feces. Antihyperglycemic activity was also seen in extracts of the tailcup lupine (L. caudatus) found in the Four Corners Region of the United States.",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:i2xiXl-TujoC,,1
1510,Detecting attacks on web applications using autoencoder,"Web attacks have become a real threat to the Internet. This paper proposes the use of autoencoder to detect malicious pattern in the HTTP/HTTPS requests. The autoencoder is able to operate on the raw data and thus, does not require the hand-crafted features to be extracted. We evaluate the original autoencoder and its variants and end up with the Regularized Deep Autoencoder, which can achieve an F1-score of 0.9463 on the CSIC 2010 dataset. It also produces a better performance with respect to OWASP Core Rule Set and other one-class methods, reported in the literature. The Regularized Deep Autoencoder is then combined with Modsecurity in order to protect a website in real time. This algorithm proves to be comparable to the original Modsecurity in terms of computation time and is ready to be deployed in practice.",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:fQNAKQ3IYiAC,,1
1511,Prevalence of comorbidities and their impact on hospital management and short-term outcomes in Vietnamese patients hospitalized with a first acute myocardial infarction,"Background Cardiovascular disease is one of the leading causes of morbidity and mortality in Vietnam. We conducted a pilot study of Hanoi residents hospitalized with a first acute myocardial infarction (AMI) at the Vietnam National Heart Institute in Hanoi for purposes of describing the prevalence of cardiovascular (CVD) and non-CVD comorbidities and their impact on hospital management, in-hospital clinical complications, and short-term mortality in these patients.  Methods The study population consisted of 302 Hanoi residents hospitalized with a first AMI at the largest tertiary care medical center in Hanoi in 2010.  Results The average age of study patients was 66 years and one third were women. The proportions of patients with none, any 1, and ≥ 2 CVD comorbidities were 34%, 42%, and 24%, respectively. Among the CVD comorbidities, hypertension was the most commonly reported (59%). There were decreasing trends in the proportion of patients who were treated with effective cardiac medications and coronary interventions as the number of CVD comorbidities increased. Patients with multiple CVD comorbidities tended to develop acute clinical complications and die at higher rates during hospitalization compared with patients with no CVD comorbidities (Odds Ratio: 1.40; 95% Confidence Interval: 0.40–4.84).  Conclusions Our data suggest that patients with multiple cardiac comorbidities tended to experience high in-hospital death rates in the setting of AMI. Full-scale surveillance of Hanoi residents hospitalized with AMI at all Hanoi hospitals is needed to confirm these findings. Effective strategies to manage Vietnamese patients …",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:Mojj43d5GZwC,,1
1512,Synergic activity against MCF-7 breast cancer cell growth of nanocurcumin-encapsulated and cisplatin-complexed nanogels,"Nanogel-based systems loaded with single anticancer drugs display miscellaneous effectiveness in tumor remission, gradually circumventing mutation and resistance in chemotherapy. Hence, the existence of dual-drug delivered nano-sized systems has been contemporaneous with drug development and preceded the conventional-dose chemotherapy. Among outstanding synergistic drug nanoplatforms, thermosensitive copolymer heparin-Pluronic F127 (Hep-F127) co-delivering cisplatin (CDDP) and curcumins (Cur) (Hep-F127/CDDP/Cur) has emerged as a notable candidate for temperature-responsive drug delivery. The procedure was based on the entrapment of curcumin into the hydrophobic core of bio-degradable co-polymer Hep-F127 while the hydrophilic drug CDDP subsequently conjugated to the backbone heparin to form the core-shell structure. The copolymer was characterized by Fourier transform infrared (FT-IR) spectrophotometry, Transmission Electron Microscopy (TEM), and Dynamic Light Scattering (DLS), to corroborate the successful synthesis and via HPLC along with AES-ICP to evaluate the high drug loading along with a controllable release from the nano-gels. A well-defined nano-shell with size in the 129.3 ± 3.8 nm size range could enhance higher the efficacy of the conjugated-CDDP to Hep-F127 than that of single doses. Moreover, the considerable amount of dual-drug released from thermosensitive nanogels between different conditions (pH = 7.4 and pH = 5.5) in comparison to CDDP from Hep-F127 partially indicated the significantly anti-proliferative ability of Hep-F127/CDDP/Cur to the MCF-7 cell line …",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:5Ul4iDaHHb8C,,1
1513,"Sex differences in clinical characteristics, hospital management practices, and in-hospital outcomes in patients hospitalized in a Vietnamese hospital with a first acute …","Background Cardiovascular disease is one of the leading causes of morbidity and mortality in Vietnam. We conducted a pilot study of Hanoi residents hospitalized with acute myocardial infarction (AMI) at the Vietnam National Heart Institute in Hanoi. The objectives of this observational study were to examine sex differences in clinical characteristics, hospital management, in-hospital clinical complications, and mortality in patients hospitalized with an initial AMI.  Methods The study population consisted of 302 Hanoi residents hospitalized with a first AMI at the largest tertiary care medical center in Hanoi in 2010.  Results The average age of study patients was 66 years and one third were women. Women were older (70 vs. 64 years) and were more likely than men to have had hyperlipidemia previously diagnosed (10% vs. 2%). During hospitalization, women were less likely to have undergone percutaneous coronary intervention (PCI) compared with men (57% vs. 74%), and women were more likely to have developed heart failure compared with men (19% vs. 10%). Women experienced higher in-hospital case-fatality rates (CFRs) than men (13% vs. 4%) and these differences were attenuated after adjustment for age and history of hyperlipidemia (OR: 2.64; 95% CI: 1.01, 6.89), and receipt of PCI during hospitalization (OR: 2.09; 95% CI: 0.77, 5.09).  Conclusions Our pilot data suggest that among patients hospitalized with a first AMI in Hanoi, women experienced higher in-hospital CFRs than men. Full-scale surveillance of all Hanoi residents hospitalized with AMI at all Hanoi medical centers is needed to confirm these findings. More targeted and …",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:olpn-zPbct0C,,1
1514,A parallel architecture for the ICA algorithm: DSP plane of a 3-D heterogeneous sensor,"A 3D heterogeneous sensor using a stacked chip has recently been proposed. While the sensors are located on one of the planes, the other planes provide for analog processing, digital signal processing, and wireless communication. This paper focuses on its DSP plane, in particular on the implementation of the ICA (independent component analysis) algorithm in the DSP plane. ICA is a recently proposed method for solving the blind source separation problem. The objective is to recover the unobserved source signals from the observed mixtures without the knowledge of the mixing coefficients. We present a parallel architecture utilizing the reconfigurable J-platform, which employs coarse-gain VLSI cells. These include a universal nonlinear (UNL) cell, an extended multiply accumulate (MA PLUS) cell, and a data-fabric (DF) cell. The coarse-grain approach has the distinct advantages of reduced external …",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:BwyfMAYsbu0C,,1
1515,Dialogue homme-machine: modélisation de multisession,"Cette thèse porte essentiellement sur la modélisation de dialogues multisessions, et sur la négociation, sous l'angle de la résolution d'un conflit entre plusieurs utilisateurs. Un dialogue multisession est un dialogue qui engage plusieurs utilisateurs et le système de dialogue. Il est défini comme un ensemble successif de sous-dialogues, chacun d'entre eux représentant une session spécifique entre un seul utilisateur et le système. L'enchaînement d'un dialogue multisession est divisé en trois phases : émergence, processus de négociation et notification. En se fondant sur le modèle stratégique de dialogue et sur la théorie des jeux, une gestion générique de dialogue, qui offre plusieurs avantages par une réduction maximale de la dépendance entre le modèle du dialogue et le modèle de la tâche, a été proposée. Cette gestion n'utilise que trois éléments principaux : acte de dialogue, but de dialogue et stratégie de dialogue, afin de diviser un dialogue en des thèmes différents, déterminés dans un agenda de dialogue. L'incompréhension est également traitée dans cette gestion en utilisant des marqueurs dialogiques d'incompréhension. La modélisation, ainsi que la gestion de dialogue multisession, sont développées en se fondant sur cette gestion. L'expérimentation, effectuée avec le système de dialogue Mélina, dédié au service d'organisation d'une réunion, a permis de valider et de prouver notre approche.",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:IWHjjKOFINEC,,1
1516,A solution for automatically malicious Web shell and Web application vulnerability detection,"According to Internet Live Stats, it is evident that organizations and developers are underestimating security issues on their system. In this paper, we propose a protective and extensible solution for automatically detecting both the Web application vulnerabilities and malicious Web shells. Based on the original THAPS, we proposed E-THAPS that has a new detecting mechanism, improved SQLi, XSS and vulnerable functions detecting capabilities. For malicious Web shell detection, taint analysis and pattern matching methods are selected as the main approach. The broad experiment that we performed showed our outstanding results in comparison with other solutions for detecting the Web application vulnerabilities and malicious Web shells.",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:bEWYMUwI8FkC,,1
1517,Generic manager for spoken dialogue systems,"Based on three principal elements: dialogue acts, strategy, and dialogue goal, this paper presents a management model of human-machine spoken dialogue.",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:M3ejUd6NZC8C,,1
1519,Gestion de buts de dialogue,"La gestion du but de dialogue est une tâche délicate pour le contrôleur de dialogue, car bien souvent il est en concurrence avec le gestionnaire de tâches avec lequel on le confond parfois dans certains systèmes. Dans cet article, nous présentons une stratégie dynamique de gestion de buts qui permet au contrôleur de dialogue de réduire sa dépendance au gestionnaire de tâche et lui apporte une meilleure réutilisabilité. Nous expérimentons le système dans le cadre du projet PVE (Portail Vocal d’Entreprise) dans lequel le dialogue peut se dérouler en plusieurs sessions et avec des interlocuteurs différents.",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:aqlVkmm33-oC,,1
1520,Ultra-deep massively parallel sequencing with unique molecular identifier tagging achieves comparable performance to droplet digital PCR for detection and quantification of …,"The identification and quantification of actionable mutations are of critical importance for effective genotype-directed therapies, prognosis and drug response monitoring in patients with non-small-cell lung cancer (NSCLC). Although tumor tissue biopsy remains the gold standard for diagnosis of NSCLC, the analysis of circulating tumor DNA (ctDNA) in plasma, known as liquid biopsy, has recently emerged as an alternative and noninvasive approach for exploring tumor genetic constitution. In this study, we developed a protocol for liquid biopsy using ultra-deep massively parallel sequencing (MPS) with unique molecular identifier tagging and evaluated its performance for the identification and quantification of tumor-derived mutations from plasma of patients with advanced NSCLC. Paired plasma and tumor tissue samples were used to evaluate mutation profiles detected by ultra-deep MPS, which showed 87.5% concordance. Cross-platform comparison with droplet digital PCR demonstrated comparable detection performance (91.4% concordance, Cohen’s kappa coefficient of 0.85 with 95% CI = 0.72–0.97) and great reliability in quantification of mutation allele frequency (Intraclass correlation coefficient of 0.96 with 95% CI = 0.90–0.98). Our results highlight the potential application of liquid biopsy using ultra-deep MPS as a routine assay in clinical practice for both detection and quantification of actionable mutation landscape in NSCLC patients.",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:4MWp96NkSFoC,,1
1521,GuruWS: A hybrid platform for detecting malicious web shells and web application vulnerabilities,"Web application/service is now omnipresent but its security risks, such as malware and vulnerabilities, are indeed underestimated. In this paper, we propose a protective, extensible and hybrid platform, named GuruWS, for automatically detecting both web application vulnerabilities and malicious web shells. Based on the original PHP vulnerability scanner THAPS, we propose E-THAPS which implements a novel detection mechanism, an improved SQL injection, Cross-site Scripting and vulnerability detection capabilities. For malicious web shell detection, taint analysis and pattern matching methods are chosen to be implemented in GuruWS. A number of extensive experiments are carried out to prove the outstanding performance of our proposed platform in comparison with several existing solutions in detecting either web application vulnerabilities or malicious web shells.",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:sSrBHYA8nusC,,1
1522,"Kaposi sarcoma risk in HIV-infected children and adolescents on combination antiretroviral therapy from sub-Saharan Africa, Europe, and Asia",  Background.  The burden of Kaposi sarcoma (KS) in human immunodeficiency virus (HIV)–infected children and adolescents on combination antiretroviral therapy (cART) has not been compared globally.,103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:foquWX3nUaYC,,1
1523,Toward a deep learning approach for detecting php webshell,"The most efficient way of securing Web applications is searching and eliminating threats therein (from both malwares and vulnerabilities). In case of having Web application source codes, Web security can be improved by performing the task to detecting malicious codes, such as Web shells. In this paper, we proposed a model using a deep learning approach to detect and identify the malicious codes inside PHP source files. Our method relies on (i) pattern matching techniques by applying Yara rules to build a malicious and benign datasets,(ii) converting the PHP source codes to a numerical sequence of PHP opcodes and (iii) applying the Convolutional Neural Network model to predict a PHP file whether embedding a malicious code such as a webshell. Thus, we validate our approach with different webshell collections from reliable source published in Github. The experiment results show that the proposed …",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:AvfA0Oy_GE0C,,1
1524,Vers une architecture générique de système de dialogue oral homme-machine,"Cet article présente une architecture générique de système de dialogue oral homme-machine. Premièrement, nous abordons quelques problèmes soulevés par la généricité des systèmes de dialogue homme-machine. Nous décrivons ensuite dans ce cadre quelques systèmes récents et typiques. Nous présentons finalement une architecture générique pour concevoir/construire des systèmes de dialogue oral homme-machine.",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:qxL8FJ1GzNcC,,1
1525,"Diabetes, mortality and glucose monitoring rates in the TREAT Asia HIV Observational Database Low Intensity Transfer (TAHOD‐LITE) study","Diabetes is a growing cause of morbidity and mortality in people living with HIV (PLHIV) receiving antiretroviral therapy (ART). We investigated the association between fasting plasma glucose (FPG) levels and mortality, and factors associated with FPG monitoring rates in Asia.",103,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=sUQ6eEsAAAAJ&citation_for_view=sUQ6eEsAAAAJ:xtoqd-5pKcoC,,1
1526,Modular conformance testing and assume-guarantee verification for evolving component-based software,"This paper proposes a framework for modular verification of evolving component-based software. This framework includes two stages: modular conformance testing for updating inaccurate models of the evolved components and modular verification for evolving component-based software. When a component is evolved after adapting some refinements, the proposed framework focuses on this component and its model in order to update the model and recheck the whole evolved system. The framework also reuses the previous verification results and the previous models of the evolved components to reduce the number of steps required in the model update and modular verification processes. An implementation and some experimental results are presented.",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:IjCSPb-OGe4C,,1
1527,An assume-guarantee model checker for component-based systems,"This paper introduces an assume-guarantee model checker, named AGMC, for verifying correctness of designs of component-based systems. Given UML 2.0 sequence diagrams that describe behaviors of the system components and a required property, AGMC generates accurate models of the components represented by labeled transition systems (LTSs) automatically. AGMC then model checks that whether the system satisfies the property by using the assume-guarantee verification method. AGMC has been implemented and tested by applying some typical component-based systems. The implemented AGMC is not only useful to verify component-based systems in practice but also has a potential to solve the state space explosion problem in model checking.",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:7PzlFSSx8tAC,,1
1528,A minimized assumption generation method for component-based software verification,"An assume-guarantee verification method has been recognized as a promising approach to verify component-based software with model checking. The method is not only fitted to component-based software but also has a potential to solve the state space explosion problem in model checking. This method allows us to decompose a verification target into components so that we can model check each of them separately. In this method, assumptions which are environments of the components are generated. The number of states of the assumptions should be minimized because the computational cost of model checking is influenced by that number. Thus, we propose a method for generating minimal assumptions for the assume-guarantee verification of component-based software. The key idea of this method is finding the minimal assumptions in the search spaces of the candidate assumptions. These …",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:Se3iqnhoufwC,,1
1529,A method for automated test data generation from sequence diagrams and object constraint language,"This paper proposes an automated test data generation method from the information embedded in model elements such as Unified Modeling Language (UML) sequence diagrams, class diagrams, and Object Constraint Language (OCL). The method supports UML 2.0 sequence diagrams including eight kinds of combined fragments describing control flow of systems. Comparing with some approaches by using depth first search (DFS) or breadth first search (BFS) algorithms, the proposed method generates all possible test scenarios with the higher error uncover capability. Test data for testing loop fragment is also generated. Therefore, it helps to detect errors in testing loops and the concurrency errors such as safety and liveness property of the systems.",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:8k81kl-MbHgC,,1
1530,On optimization of minimized assumption generation method for component-based software verification,"The minimized assumption generation has been recognized as an important improvement of the assume-guarantee verification method in order to generate minimal assumptions. The generated minimal assumptions can be used to recheck the whole component-based software at a lower computational cost. The method is not only fitted to component-based software but also has a potential to solve the state space explosion problem in model checking. However, the computational cost for generating the minimal assumption is very high so the method is difficult to be applied in practice. This paper presents an optimization as a continuous work of the minimized assumption generation method in order to reduce the complexity of the method. The key idea of this method is to find a smaller assumption in a sub-tree of the search tree containing the candidate assumptions using the depth-limited search strategy. With this …",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:e5wmG9Sq2KIC,,1
1531,A method for automated unit testing of C programs,"This research proposes an automated test case generation method for C functions. In this method, the source code is transformed into a control flow graph corresponding to the given coverage criterion. After that, a list of feasible test paths are discovered by traversing the control flow graph using backtracking algorithm, symbolic execution, and Z3 solver. We also generate test cases for functions containing one loop or two-nested loop. A tool supporting the proposed method has been developed and applied to test on some C functions. The experimental results show the high coverage with the minimum number of test cases, the ability to improve the total time of the test case generation with a specified coverage criterion, and the increasing precision of checking the feasibility of test paths if comparing with the random technique. The experimental results display the potential usefulness of this tool for automated test …",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:-f6ydRqryjwC,,1
1534,A Method for Generating Models of Black-box Components,"The model-based approaches are difficult to be applied in practice due to the lack of formal models describing behaviors of systems. This paper proposes a method for generating accurate models of components in order to solve this issue in the context of the component-based systems. The key idea of this method is to generate minimal deterministic finite automata as the accurate models of the corresponding components. For this purpose, the proposed method first computes a set of traces as a regular language of a given component by executing all possible experiments over the alphabet of the component. This method then constructs a regular expression to represent this set. After that, a minimal deterministic finite automaton as an accurate model of the component is generated by applying the Thompson algorithm and some optimized activities. The generated models are useful for the existing model-based …",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:qxL8FJ1GzNcC,,1
1535,Assume-guarantee tools for component-based software verification,"This paper presents a minimized assumption generation method and its associated tools for L*-based assume-guarantee verification of component-based software by model checking. The method is not only fitted to component-based software but also has a potential to solve the state space explosion problem in model checking. In the proposed method, a verification target is decomposed into components so that we can model check each of them separately. The key idea of this method is finding the minimal assumptions in the search spaces of the candidate assumptions. The minimal assumptions generated by the proposed method can be used to recheck the whole system at much lower computational cost. Our experience so far indicates that the implemented tools are potential for verifying practical component-based software.",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:ULOm3_A8WrAC,,1
1536,An effective framework for assume-guarantee verification of evolving component-based software,"This paper proposes an effective framework for the assume-guarantee verification of component-based software in the context of component evolution at system design level. In this framework, when a component is evolved after adapting some refinements, we do not recheck all components in the evolved system, but only the evolved component. For this purpose, if the design model of the evolved component satisfies the current assumption of the system before evolving, the evolved system still satisfies the required property. Otherwise, if the assumption is too strong to be satisfied by the model, a new assumption must be generated again. We propose two methods for regenerating the new assumption called assumption regeneration and minimized assumption regeneration. The methods reuse the assumption to reduce a large number of required membership queries and candidate assumptions which are needed …",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:3fE2CSJIrl8C,,1
1537,On improvement of assume-guarantee verification method for timed component-based software,"The Assume-guarantee verification using TL* algorithm implemented in Learner is an important method to alleviate the state space explosion problem in model checking of timed systems, thanks to its divide and conquer strategy. This paper presents an improvement for TL* learning algorithm of Learner by removing the first phase from the verification process and starting learning assumption from λ. This improvement reduces the time complexity and covers a number of cases where assumptions exist while the original TL* algorithm cannot. Besides, this paper adds a kind of bound to the candidate query answering algorithm of Teacher for it to return “don't know” result and prevent Learner from running endlessly. We give some discussions related to both of original algorithms and the improved ones in the paper.",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:WF5omc3nYNoC,,1
1538,A minimized assumption generation method for component-based software verification,"An assume-guarantee verification method has been recognized as a promising approach to verify component-based software by model checking. This method is not only fitted to component-based software but also has a potential to solve the state space explosion problem in model checking. The method allows us to decompose a verification target into components so that we can model check each of them separately. In this method, assumptions are seen as the environments needed for the components to satisfy a property and for the rest of the system to be satisfied. The number of states of the assumptions should be minimized because the computational cost of model checking is influenced by that number. Thus, we propose a method for generating minimal assumptions for the assume-guarantee verification of component-based software. The key idea of this method is finding the minimal assumptions in the …",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:5nxA0vEk-isC,,1
1539,On implementation of the improved assume-guarantee verification method for timed systems,"The two-phase assume-guarantee verification method for timed systems using TL algorithm implemented in the learner has been known as a potential method to solve the problem of state space explosion in model checking thanks to its divide and conquer strategy. This paper presents three improvements to the verification method. First, we remove the untimed verification phase from the verification process. This removal reduces the time complexity of the verification process because of the great time complexity of this phase. Second, we introduce a maxbound to the equivalence queries answering algorithm implemented in the teacher which acts as a method for the teacher to return"" don't know"" results to the learner to prevent the verification process from many endless scenarios. Finally, we introduce a technique to analyze the counterexample received from the teacher and another one implemented in the …",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:_FxGoFyzp5QC,,1
1540,Improvements of Directed Automated Random Testing in Test Data Generation for C++ Projects,"This paper improves the breadth-first search strategy in directed automated random testing (DART) to generate a fewer number of test data while gaining higher branch coverage, namely Static DART or SDART for short. In addition, the paper extends the test data compilation mechanism in DART, which currently only supports the projects written in C, to generate test data for C++ projects. The main idea of SDART is when it is less likely to increase code coverage with the current path selection strategies, the static test data generation will be applied with the expectation that more branches are covered earlier. Furthermore, in order to extend the test data compilation of DART for C++ context, the paper suggests a general test driver technique for C++ which supports various types of parameters including basic types, arrays, pointers, and derived types. Currently, an experimental tool has been implemented based on the …",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:9ZlFYXVOiuMC,,1
1541,On Implementation of the Assumption Generation Method for Component-Based Software Verification,"The assume-guarantee verification has been recognized as a promising method for solving the state space explosion in modular model checking of component-based software. However, the counterexample analysis technique used in this method has huge complexity and the computational cost for generating assumptions is very high. As a result, the method is difficult to be applied in practice. Therefore, this paper presents two improvements of the assume-guarantee verification method in order to solve the above problems. The first one is a counterexample analysis method that is simple to implement but effective enough to prevent the verification process from infinite loops when considering the last action of counterexample as suffix in implementation. This is done by finding a suffix that can make the observation table not closed when being added to the suffix set of the table and use that suffix for the …",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:aqlVkmm33-oC,,1
1542,An Efficient Method for Automated Generating Models of Component-Based Software,This paper proposes an efficient method for automated generating models of component-based software. This method accepts regular expressions that describe behaviors of software components. The proposed method uses the CNNFA algorithm to parse the regular expressions in order to generate corresponding models. This method can generate minimal accurate models of the software components. A tool is implemented and applied for some typical systems to show the efficiency of this method. The obtained experimental results show that this method is faster than existing methods. The generated models play an important role in making such model-based approaches as model checking and model-based testing more feasible in practice.,106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:4TOpqqG69KYC,,1
1543,A method for automated user interface testing of windows-based applications,"This paper proposes a method for automated user interface testing of Windows-based applications to increase the accuracy in identifying the target widgets or executing several interactions. The key idea of this method is to generate new test scenarios from widgets and test specification where widgets are extracted during the execution of the application and test specification is generated by combining the interactions of widgets. Furthermore, the paper contributes some techniques to detect hidden widgets which considering as one of the most challenging problems in user interface testing. Currently, a supporting tool has been implemented and tested with several industrial projects. The details of the experimental results will be presented and discussed.",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:Zph67rFs4hoC,,1
1544,JCIA: A tool for change impact analysis of java EE applications,"This paper presents a novel tool for change impact analysis of Java EE applications named JCIA. Analyzing the source code of the Java EE applications is a big challenge because of the complexity and large scale of the applications. Moreover, components in Java EE applications are not only in Java language but also in different languages such as XHTML, XML, JSP. This tool analyzes source code of Java EE applications for building the dependency graphs (called JDG). The main idea for generating JDG is based on developing the source code analyzers for the typical technologies of Java EE such as JavaServer Faces, Context and Dependency Injection, Web services. Based on the obtained JDG and the given change sets, JCIA calculates the corresponding impact sets by applying the change impact analysis (CIA) based on change types and Wave-CIA method. The calculated impact sets help …",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:mB3voiENLucC,,1
1545,A test data generation method for C/C++ projects,"This research proposes an automated test data generation method for C/C++ projects to generate the lower number of test data while gaining higher code coverage in comparison with KLEE, CAUT, PathCrawler, and CREST. In order to do that, the proposed method contributes an algorithm named loop depth first search by combining both static testing and concolic testing together. Besides, the paper also provides an improvement symbolic execution for avoiding the initial test data problem in the concolic testing. Currently, a tool supporting the proposed method has been developed and applied to test on different C/C++ projects in several software companies. The experimental results show the higher coverage with the lower number of test data compared with the existing methods. The experimental results display the effectiveness and practical usefulness of the proposed method for automated test data generation …",106,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7gFdUb4AAAAJ&citation_for_view=7gFdUb4AAAAJ:RHpTSmoSYBkC,,1
1546,Building high-accuracy thermal simulation for evaluation of thermal comfort in real houses,"Thermal comfort is an essential aspect for the control and verification of many smart home services. In this research, we design and implement simulation which models thermal environment of a smart house testbed. Our simulation can be used to evaluate thermal comfort in various conditions of home environment. In order to increase the accuracy of the simulation, we measure thermal-related parameters of the house such as temperature, humidity, solar radiation by the use of sensors and perform parameter identification to estimate uncertain parameters in our thermal model. We also implement a communication interface which allows our simulator to communicate with other external simulators. Experimental result showed that our simulation can achieve high accuracy when compared with actual measurement data.",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:WF5omc3nYNoC,,1
1547,A low-delay push-pull based application layer multicast for p2p live video streaming,"In this paper, we present our low-delay push-pull based application layer multicast for live video streaming on P2P networks. The main goal of our work is to optimize content delivery on P2P networks to guarantee the time constraints of live video streaming. We achieve this goal by constructing multiple balanced sub-trees for pushing data and optimizing pulling connections between nodes in different sub-trees to reduce the time gap between arrival times of pushing data and pulling data. As the result, our mechanism can reduce buffering time at each node. Our mechanism also includes a tit-for-tat method to promote node contribution. The effectiveness of our method is proved by simulation results.",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:Tyk-4Ss8FVUC,,1
1548,"Building a low-latency, proximity-aware DHT-based P2P network","DHT-based P2P networks have a problem of topology mismatch which causes high latency for message routing. This paper focuses on improving the latency of routing process for chord, which is a typical DHT-based P2P network protocol. We propose a simple method to build a chord network based on the proximity of nodes in the underlying physical network. The idea of our method is the combination of two techniques: identifier selection in node joining phase and neighbor selection in network stabilization phase, both are performed based on physical network latency. We have evaluated our proposed solution by simulations. In a simulation network with 4096 nodes, the average latency of routing process in our method can reduces 30% comparing with the conventional chord routing method.",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:ufrVoPGSRksC,,1
1549,Personal mesh: A design of flexible and seamless Internet access for personal area network,"With the proliferation of various types of computing and networking resources in ubiquitous computing environments, an architecture allowing mobile users to flexibly access these resources is desirable. We have focused our attention on the access link resources of devices surrounding users. Our framework named Personal Mesh allows personal devices to seamlessly access the Internet via appropriate access links available in a personal area network. The Personal Mesh deals with two technical issues: access link selection management and a PAN mobility support mechanism. In this paper, we describe the design and implementation of Personal Mesh and show the effectiveness of our system by experiment.",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:roLk4NBRz8UC,,1
1550,Implementation and evaluation of thermal simulator for houses, Home network services are to provide users with comfortable living environment Air conditioner systems consume electricity to keep room temperature and humidity at a comfortable level,107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:7PzlFSSx8tAC,,1
1551,Short-term prediction of energy consumption of air conditioners based on weather forecast,"In residential houses, air conditioners consume a lot of electrical energy. In order to improve energy efficiency for residential houses, short-term prediction of energy consumption of air conditioners is required. In this paper, we propose the use of our thermal simulation to simulate the change of room temperature based on weather forecast information and predict the energy consumption of an air conditioner in a residential house. In order to calculate solar radiation heat flux, which contributes a lot to the change of room temperature, we utilize a neural network model to predict global solar radiation using training data obtained from weather stations. We also utilize a PID control model to simulate the operation of air conditioners. The accuracy of our simulation is verified by experiments carried out at a real testbed house.",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:3fE2CSJIrl8C,,1
1552,Black box modelling the thermal behaviour of iHouse using auto regressive and moving average (ARMA) model,"Modelling and simulation of the dynamic thermal behaviour of a building is important to test any proposed thermal comfort control system and strategy in the building. A simulation model can be obtained by using either the white box, grey box or black box modelling method. This research focuses on the usage of auto regressive and moving average (ARMA) model, a type of black box model that represents the dynamic thermal behaviour of iHouse testbed and uses real recorded data from the testbed and limited knowledge regarding the physical characteristics of the testbed. The performance of the ARMA model developed in this research is compared with the performance of House Thermal Simulator, a previously developed model, based on grey box modelling. Results obtained shows that ARMA model works better than House Thermal Simulator in some aspects. Â Â",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:ULOm3_A8WrAC,,1
1553,Adaptive Smart Lighting Control based on Genetic Algorithm,"Smart lighting system plays an important role in smart homes because of its convenience comparing with boring and tedious on/off switches. With conventional smart lighting systems, users can easily change the lighting level of a room with a smart phone or voice control device. However, design an automated lighting system with energy saving is still a challenge. In this paper, we propose an adaptive smart lighting system using a control algorithm based on Genetic Algorithm (GA). Our system turns on and off lights that match the user's desire brightness and utilizes nature illuminance efficiently to save as much electric energy as possible. The control algorithm based on GA provides the optimized light turning pattern to the system and can adapt with the change of lighting environment and user requirement. Our evaluation results showed that the execution time of our algorithm is short enough to be used in real …",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:LkGwnXOMwfcC,,1
1554,Simulation-based short-term model predictive control for hvac systems of residential houses,"In this paper, we propose a simple model predictive control (MPC) scheme for Heating, ventilation, and air conditioning (HVAC) systems in residential houses. Our control scheme utilizes a fitted thermal simulation model for each house to achieve precise prediction of room temperature and energy consumption in each prediction period. The set points for each control step of HVAC systems are selected to minimize the amount of energy consumption while maintaining room temperature within a desirable range to satisfy user comfort. Our control system is simple enough to implement in residential houses and is more efficient comparing with rule-based control methods.",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:zYLM7Y9cAGgC,,1
1555,Simplifying the auto regressive and moving average (arma) model representing the dynamic thermal behaviour of ihouse based on theoretical knowledge,"Modelling and simulation is an alternative way of testing the dynamic behaviour of a real system – in some situation, testing the real system are expensive, time consuming, not comfortable, and dangerous. Mathematical model describing the dynamic behaviour of a system can be represented by using white, black, or grey box model. This study focuses on developing a simplified Auto Regressive Moving Average (ARMA) model (a type of linear black model) to represent the dynamic thermal behaviour of iHouse – simplification is done based on the theoretical knowledge of the building. The performance of the simplified ARMA model developed in this study is compared with the performance of the models developed in previous studies, which are: (1) House Thermal Simulator; (2) and ARMA model. Result shows that the simplified ARMA model developed in this study consists of simpler set of mathematical …",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:KlAtU1dfN6UC,,1
1556,A cluster-based file replication scheme for DHT-based file backup systems,"DHT-based file backup systems can provide efficient mechanisms for resource placement and lookup. However, these systems must deal with irregular and frequent join/leave of nodes and the problem of load unbalancing between nodes. This paper presents an efficient file replication scheme based on DHT identifier space clustering in order to guarantee data availability and support load balancing for DHT-based file backup systems. The main idea of our method is to divide a DHT network into a number of clusters, each of which locally stores and maintains data chunks of data files to guarantee the data availability for backup data files even in the case of node churn occurred in the network. Further, backup nodes are selected among high-capacity nodes in clusters to achieve a good degree of load balancing. Simulation results demonstrate the effectiveness of our proposed scheme in comparison with other file …",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:YOwf2qJgpHMC,,1
1557,SMAV: A solution for multiple-attribute search on DHT-based P2P network,"DHT-based P2P networks such as Chord, CAN, Pastry, etc can achieve exact query with characteristic of scalability, efficiency and fault-tolerate. However, in the case of complex queries such as range query or multiple-attribute query, pure DHT is not efficient since a lot of query messages must be sent. In this paper we focus our intentions on multiple-attribute query on DHT-based P2P network systems. Conventional researches meet a problem arising from the load unbalance among nodes due to the appearance of common attribute/value pairs (AV pairs) in content names (i.e. AV pairs those appear in a lot of content names). We propose a hierarchical key generation approach for storing and querying multiple-attribute contents. The main idea of our proposed system is to limit the number of information contents distributed by one key to a node by the use of sub-keys created from multiple common AV pairs. Sub …",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:8k81kl-MbHgC,,1
1558,Personal Mesh: A Design of Flexible Internet Access for Personal Area Network,"We have designed and implemented a personal area network named Personal Mesh to provide mobile user in pervasive environment with flexible access to various computing and networking resources. We have worked on several technical issues such as access link resource management, end-to-end mobility support mechanism using session information, context-sensitive dynamic access link handoff mechanism. In this paper, we describe the design and our implementation of Personal Mesh and show the effectiveness of our system by experiment.",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:9ZlFYXVOiuMC,,1
1559,A dynamic-clustering backup scheme for high-availability distributed File sharing Systems,"SUMMARY DHT routing algorithms can provide efficient mechanisms for resource placement and lookup for distributed file sharing systems. However, we must still deal with irregular and frequent join/leave of nodes and the problem of load unbalancing between nodes in DHT-based file sharing systems. This paper presents an efficient file backup scheme based on dynamic DHT key space clustering in order to guarantee data availability and support load balancing. The main idea of our method is to dynamically divide the DHT network into a number of clusters, each of which locally stores and maintains data chunks of data files to guarantee the data availability of user data files even when node churn occurs. Further, highcapacity nodes in clusters are selected as backup nodes to achieve adequate load balancing. Simulation results demonstrate the superior effectiveness of the proposed scheme over other file replication schemes.",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:qxL8FJ1GzNcC,,1
1560,BAM-Chord: DHT-Based Bandwidth Adaptive Multicast System,"In recent years, there has been considerable interest in applying Distributed Hash Tables (DHTs) to application-level multicast since DHTs have many advantages that are good for multicast applications: decentralization, scalability, fault tolerance, load balancing, and good routing performances. However, an effective bandwidth utilization method is required for DHT-based multicast systems because of a number of technical issues such as heterogeneous node capacity and dynamic membership. In this paper, the authors propose their BAM-Chord (ie, Bandwidth Adaptive Multicast over Chord), a DHT-based multicast system that focuses on host heterogeneity, network scalability and effective bandwidth utilization. In the authors’ system, when a node joins into the system, it will find out an appropriate position (ie, node identifier) on a BAM-Chord ring and create links to neighbor nodes based on node’s bandwidth …",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:IjCSPb-OGe4C,,1
1561,Building distributed attribute-value trees on dht-based networks,"Today, when a huge number of information resources appear in the Internet, one problem arising is how to search and exploit these information resources efficiently and flexibly with high scalability. We believe that the realization of multiple-attribute searching in a distributed manner is an important solution for this problem. In this paper, we propose a solution of building a distributed attribute-value tree (AV tree) on DHT-based networks to realize multiple-attribute searching. Each resource is named by an AV tree and DHT keys are created from each AV branch of a resource name. Resource information is distributed to a DHT-based network by the use of these DHT keys. Our solution keeps the locality of resource names in the key space of DHT networks such that AV branches that share a number of attribute/value pairs (AV pairs) will be mapped to a continuous portion of the key space. Therefore, multiple-attribute …",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:Se3iqnhoufwC,,1
1562,Topology Optimization for DHT-based Application Layer Multicast,"In recent years, Distributed Hash Tables (DHTs) attract a lot of attentions since they have many advantages such as good routing performance, scalability, fault tolerance, load balancing. Those properties make DHTs are suitable for deploying multicast services at application layer. However, early DHT-based multicast systems are insufficient in addressing a number of technical issues such as heterogeneous node capacity, large-scale multicast and dynamic membership. In this paper, we propose BAM-Chord: Bandwidth Adaptive Multicast over Chord, a DHT-based multicast system that focuses on host heterogeneity, scalability and fault tolerance. In our system, when a node joins into the system, it will And out an appropriate position (i.e. node identifier) on a BAM-Chord ring and create links to neighbor nodes based on node's bandwidth capacity such that the multicast tree can be built efficiently and balanced …",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:4DMP91E08xMC,,1
1563,Sens: A scalable and expressive naming system using can routing algorithm,"We design a scalable and expressive naming system called SENS, which can retrieve information of computing and content resources distributed widely on the Internet by exact queries and multi-attribute range queries over resource names. Our system utilizes a descriptive naming scheme to name resources and a multi-dimensional resource ID space for message routing through the overlay network of name servers (NSs). The resource ID space is constructed on the overlay network based on CAN routing algorithm. We propose a novel mapping scheme between resource names and resource IDs, which can preserve the locality of resource IDs while still achieving a good degree of load balancing regarding resource information distribution. We also propose a multicast routing algorithm to deliver resource information and a broadcast routing algorithm to route query messages to corresponding NSs at minimum …",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:MXK_kJrjxJIC,,1
1564,A scalable multistage packet switch for terabit IP router based on deflection routing and shortest path routing,"According to the growth of the Internet, an IP (Internet protocol) router with capacity of terabit/s is required in the near future. To achieve high capacity, the switch fabric should handle many pairs of input ports and output ports. For this purpose, a multistage network switch is required. This paper presents a new scalable multistage packet switch using deflection routing and shortest path routing multistage network. Deflection routing multistage networks have the advantage of hardware simplicity since the switch element has no buffer memory, and variable length packet switching can be easily handled. Furthermore, in the proposed new interconnection method between switch elements, the required amount of hardware is reduced compared with the conventional switch based on the deflection routing principle. A circuit of 8 /spl times/ 8 variable length packet switch element is designed on FPGA, and the required …",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:dhFuZR0502QC,,1
1565,ECHONET Lite-based IoT Platform for Smart Homes,"In this paper, we introduce an ECHONET Lite based IoT platform which allows households to set up smart home environment with ease as well as allows service providers to deploy their IoT applications easily and securely. Our proposed platform is fully implemented with three main features: a simple process for integrating new IoT devices into smart home environment, a seamless data transfer mechanism between IoT devices and service providers, and a RESTful API for verifying and supporting service providers to deploy their IoT applications into our platform. The experimental results verified the practicability of our proposed IoT platform.",107,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=qemeoesAAAAJ&citation_for_view=qemeoesAAAAJ:Y0pCki6q_DkC,,1
1566,Satellites may underestimate rice residue and associated burning emissions in Vietnam,"In this study, we estimate rice residue, associated burning emissions, and compare results with existing emissions inventories employing a bottom-up approach. We first estimated field-level post-harvest rice residues, including separate fuel-loading factors for rice straw and rice stubble. Results suggested fuel-loading factors of 0.27 kg m− 2 (±0.033), 0.61 kg m− 2 (±0.076), and 0.88 kg m− 2 (±0.083) for rice straw, stubble, and total post-harvest biomass, respectively. Using these factors, we quantified potential emissions from rice residue burning and compared our estimates with other studies. Our results suggest total rice residue burning emissions as 2.24 Gg PM 2.5, 36.54 Gg CO and 567.79 Gg CO 2 for Hanoi Province, which are significantly higher than earlier studies. We attribute our higher emission estimates to improved fuel-loading factors; moreover, we infer that some earlier studies relying on residue-to …",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:Wp0gIr-vW9MC,,1
1567,Vegetation fires and air pollution in Vietnam,"Forest fires are a significant source of air pollution in Asia. In this study, we integrate satellite remote sensing data and ground-based measurements to infer fire–air pollution relationships in selected regions of Vietnam. We first characterized the active fires and burnt areas at a regional scale from MODIS satellite data. We then used satellite-derived active fire data to correlate the resulting atmospheric pollution. Further, we analyzed the relationship between satellite atmospheric variables and ground-based air pollutant parameters. Our results show peak fire activity during March in Vietnam, with hotspots in the Northwest and Central Highlands. Active fires were significantly correlated with UV Aerosol Index (UVAI), aerosol extinction absorption optical depth (AAOD), and Carbon Monoxide. The use of satellite aerosol optical thickness improved the prediction of Particulate Matter (PM) concentration significantly.",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:2osOgNQ5qMEC,,1
1568,Improvement of land-cover classification over frequently cloud-covered areas using Landsat 8 time-series composites and an ensemble of supervised classifiers,"Recent abundance of moderate-to-high spatial resolution satellite imagery has facilitated land-cover map production. However, in cloud-prone areas, building high-resolution land-cover maps is still challenging due to infrequent satellite revisits and lack of cloud-free data. We propose a classification method for cloud-persistent areas with high temporal dynamics of land-cover types. First, compositing techniques are employed to create dense time-series composite images from all available Landsat 8 images. Then, spectral–temporal features are extracted to train an ensemble of five supervised classifiers. The resulting composite images are clear with at least 99.78% cloud-free pixels and are 20.47% better than their original images on average. We classify seven land classes, including paddy rice, cropland, grass/shrub, trees, bare land, impervious area, and waterbody over Hanoi, Vietnam, in 2016. Using a time …",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:QIV2ME_5wuYC,,1
1569,Particulate matter concentration mapping from MODIS satellite data: a Vietnamese case study,"Particulate Matter (PM) pollution is one of the most important air quality concerns in Vietnam. In this study, we integrate ground-based measurements, meteorological and satellite data to map temporal PM concentrations at a 10× 10 km grid for the entire of Vietnam. We specifically used MODIS Aqua and Terra data and developed statistically-significant regression models to map and extend the ground-based PM concentrations. We validated our models over diverse geographic provinces ie, North East, Red River Delta, North Central Coast and South Central Coast in Vietnam. Validation suggested good results for satellite-derived PM2. 5 data compared to ground-based PM2. 5 (n= 285, r2= 0.411, RMSE= 20.299 μg m− 3 and RE= 39.789%). Further, validation of satellitederived PM2. 5 on two independent datasets for North East and South Central Coast suggested similar results (n= 40, r2= 0 …",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:UebtZRa9Y70C,,1
1570,Exposure to air pollution and risk of hospitalization for cardiovascular diseases amongst Vietnamese adults: Case-crossover study,"Associations between hospital admissions and ambient air pollutants in the Vietnamese population have been reported in previous studies. However, most studies were conducted in Hanoi or Hochiminh city. We used hospital records of seven hospitals in Northern Vietnam to investigate short-term associations between ambient air pollutants and hospital admissions due to cardiovascular conditions.",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:CdxZDUztZiMC,,1
1571,Particulate matter concentration estimation from satellite aerosol and meteorological parameters: data-driven approaches,"Estimation of Particulate Matter concentration (PM1, PM2.5 and PM10) from aerosol product derived from satellite images and meteorological parameters brings a great advantage in air pollution monitoring since observation range is no longer limited around ground stations and estimation accuracy will be increased significantly. In this article, we investigate the application of Multiple Linear Regression (MLR) and Support Vector Regression (SVR) to make empirical data models for PM1/2.5/10 estimation from satellite- and ground-based data. Experiments, which are carried out on data recorded in two year over Hanoi - Vietnam, not only indicate a case study of regional modeling but also present comparison of performance between a widely used technique (MLR) and an advanced method (SVR).",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:u-x6o8ySG0sC,,1
1572,Spatial Interpolation of Meteorologic Variables in Vietnam using the Kriging Method.,"This paper presents the applications of Kriging spatial interpolation methods for meteorologic variables, including temperature and relative humidity, in regions of Vietnam. Three types of interpolation methods are used, which are as follows: Ordinary Kriging, Universal Kriging, and Universal Kriging plus Digital Elevation model correction. The input meteorologic data was collected from 98 ground weather stations throughout Vietnam and the outputs were interpolated temperature and relative humidity gridded fields, along with their error maps. The experimental results showed that Universal Kriging plus the digital elevation model correction method outperformed the two other methods when applied to temperature. The interpolation effectiveness of Ordinary Kriging and Universal Kriging were almost the same when applied to both temperature and relative humidity.",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:zYLM7Y9cAGgC,,1
1573,Spatiotemporal analysis of ground and satellite-based aerosol for air quality assessment in the Southeast Asia region,"Satellite observations for regional air quality assessment rely on comprehensive spatial coverage, and daily monitoring with reliable, cloud-free data quality. We investigated spatiotemporal variation and data quality of two global satellite Aerosol Optical Depth (AOD) products derived from MODIS and VIIRS imagery. AOD is considered an essential atmospheric parameter strongly related to ground Particulate Matter (PM) in Southeast Asia (SEA). We analyze seasonal variation, urban/rural area influence, and biomass burning effects on atmospheric pollution. Validation indicated a strong relationship between AERONET ground AOD and both MODIS AOD (R2 = 0.81) and VIIRS AOD (R2 = 0.68). The monthly variation of satellite AOD and AERONET AOD reflects two seasonal trends of air quality separately for mainland countries including Myanmar, Laos, Cambodia, Thailand, Vietnam, and Taiwan, Hong Kong …",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:evX43VCCuoAC,,1
1574,Rapid Assessment of Flood Inundation and Damaged Rice Area in Red River Delta from Sentinel 1A Imagery,"The Red River Delta (RRD), including 11 provinces, is one of the four largest rice-growing areas in Vietnam. Tropical storms often occur and cause serious flooding from May to October annually in the RRD, which strongly affects the productivity of the summer–autumn rice, one of two main rice crops. Therefore, the rapid assessment of damaged rice area by flooding inundation is critical for farmers and the government. In this study, we proposed a methodology for quick estimation of rice areas damaged by flooding using Sentinel 1A (S1A) imagery. Firstly, the latest rice map was produced. Then, a Near Real-Time (NRT) flood map, which is estimated from S1A images at the closest time to a flooding event, was generated by excluding the yearly permanent map from the temporal water map. Our experiment was conducted for the assessment of damaged rice area by flooding from the tropical storm named Son-Tinh, which happened on 19–21 July 2018. A Support Vector Machine (SVM) classifier was applied on time-series of S1A VV with VH data (VVVH) to obtain a rice map for the winter-spring season of 2018 with 90.5% Overall Accuracy (OA) and 2.37% difference (12,544 ha) from the General Statistics Office (GSO) of Vietnam’s reports for the whole region. Then, the Otsu thresholding method was applied for permanent water surface extraction and NRT flood mapping. The estimated damaged area was compared to available provincial and communal statistics for validation and further analysis. Right after the Son-Tinh storm, the estimation of inundated rice was approximately 50% of the total rice area in the RRD (271,092 ha). As a result, rice …",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:6bLC7aUMtPcC,,1
1575,Current status of PM2. 5 pollution and its mitigation in Vietnam,"Vietnam is experiencing serious fine particulate matter (PM2. 5) pollution as a result of local activities and long-range transport (LRT) pollutants. In this article, we summarize and analyze PM2. 5 data from ground stations and manual measurements showing PM2. 5 status, characteristics and emission sources in the period from 1996 to 2017 in Vietnam. In addition, we provide a brief impact assessment of PM2. 5 pollution on public health regarding diseases and deaths. Conscious of PM2. 5ʼs harmful effects, Vietnam has been taking steps to mitigate PM2. 5 pollution in various forms through efforts by the government, nongovernmental organizations, media, communities and individuals, and has obtained initial results. This article presents a comprehensive review of current PM2. 5 pollution and its mitigation in Vietnam.",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:NhqRSupF_l8C,,1
1576,Estimation of aerosol and air quality fields with PM MAPPER–an optical multispectral data processing package,"In the frame of the Remote Sensing applications applied to MODIS data collected by the polar orbiting satellites Terra and Aqua, operated by the NASA, we present PM MAPPER, a novel data processing system developed for air pollution monitoring. Our system has derived from the MODIS data an updated set of information consisting of AOT, PM2. 5/10, AQI, and surface information with increased spatial resolutions up to 3x3 km2. With the fine spatial resolution and augmented background information, the software is effective in monitoring air pollution at local scale, especially over small urban areas with complicated topography. We carried out a validation on the data set covering Italy in a period of six months to evaluate the system’s performance. The validation outcomes show that our results have good quality in comparison with MODIS standard products and a higher capacity in retrieving AOT information over land areas, especially coastlines where are nearly empty in the MODIS products. Besides, integrated surface information could be useful for further improvements of aerosol derivation.",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:d1gkVwhDpl0C,,1
1577,Validation of PM MAPPER aerosol optical thickness retrievals at 1×1 km2of spatial resolution,"The polar-orbiting MODerate resolution Imaging Spectrora-diometer (MODIS) on-board Terra and Aqua satellites is a key instrument for the daily monitoring of global aerosol properties over a large spectral range. Its aerosol retrieval algorithm is set to a size of 10×10 km 2  of spatial resolution, and hence may not be adequate for detailed analysis at local scale. PM MAPPER is a software system capable of handling the multispectral data acquired by the MODIS sensors. It produces maps of Aerosol Optical Thickness (AOT) at increased spatial resolution up to 1×1 km 2 , which are then available online in a GIS environment. This article shows the validation results of these products, obtained by comparison with AOT measurements of several ground-based radiometers of the A Erosol RObotic NETwork (AERONET) over Europe, for a period of 3 years (2007-2009). They show a good correlation between satellite …",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:9yKSN-GCB0IC,,1
1578,Automated posture segmentation in continuous finger spelling recognition,"Recognizing continuous finger spelling plays an important role in understanding sign language. There are two major phases in recognizing continuous finger spelling, which are posture segmentation and posture recognition. In the former, a continuous gesture sequence is decomposed into segments, which are then used for the latter to identify corresponding characters. Among all the segments, beside valid postures corresponding to characters, there are also many movement expentheses, which appear between pairs of postures to move the hands from the end of one posture to the beginning of the next. In this paper, we propose a framework to split a continuous movement sequence into segments as well as to identify valid postures and movement epentheses. By using the velocity and signing rate based filter, we can obtain very good result with both high recall and precision rate.",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:M3ejUd6NZC8C,,1
1579,Comparison of various image fusion methods for impervious surface classification from VNREDSat-1,"Impervious surface is an important indicator for urban development monitoring. Accurate urban impervious surfaces mapping with VNREDSat-1 remains challenging due to their spectral diversity not captured by individual PAN image. In this artical, five multi-resolution image fusion techniques were compared for classification task of urban impervious surface. The result shows that for VNREDSat-1 dataset, UNB and Wavelet tranform methods are the best techniques reserving spatial and spectral information of original MS image, respectively. However, the UNB technique gives best results when it comes to impervious surface classification especially in the case of shadow area included in non-impervious surface group.",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:MXK_kJrjxJIC,,1
1580,Satellite aerosol optical depth over Vietnam-An analysis from VIIRS and CALIOP aerosol products,"Satellite aerosol products are useful to address a variety of questions relating to the atmosphere, climate change, air pollution, and human health. Thus, their evaluation followed by validation in different regions of the world can help in refining the products. In this study, VIIRS (2012–2015) and CALIPSO (2006–2015) aerosol products are analyzed and compared for seasonal trend and aerosol subtypes at Nghia Do, Nha Trang, and Bac Lieu AERONET stations located in the north, central, and southern regions of Vietnam, respectively. At Nghia Do station, VIIRS AOD captured the northern seasonal trends well with low errors, and high correlation coefficients. CALIPSO aerosol subtypes have shown polluted dust, biomass burning, polluted continental, clean continental, and desert dust coinciding with the northern climate conditions, agricultural burning, and long-range transport. At Nha Trang station, VIIRS …",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:R3hNpaxXUhUC,,1
1581,Aerosol optical thickness retrieval from satellite observation using support vector regression,"Processing of data recorded by the MODIS sensors on board the Terra and Aqua satellites has provided AOT maps that in some cases show low correlations with ground-based data recorded by the AERONET. Application of SVR techniques to MODIS data is a promising, though yet poorly explored, method of enhancing the correlations between satellite data and ground measurements. The article explains how satellite data recorded over three years on central Europe are correlated in space and time with ground based data and then shows results of the application of the SVR technique which somewhat improves previously computed correlations. Hints about future work in testing different SVR variants and methodologies are inferred from the analysis of the results thus far obtained.",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:qjMakFHDy7sC,,1
1582,Paddy rice mapping in red river delta region using landsat 8 images: Preliminary results,"Paddy rice, an important crop in Vietnam, need to be monitored and managed for food security, water resource management and Greenhouse gas (GHG) emission. In this study, we present preliminary results of rice mapping in Red River Delta (RRD) region, one of two largest rice planting area in Vietnam. We employ all available Landsat 8 acquired over RRD, regardless of cloud coverage, to establish yearly rice maps from 2013 to 2016. A time-series classification scheme is proposed which includes two main stages. At first, all Landsat 8 images in classification year are used to create monthly composite images. Then, time-series features are extracted to feed an eXtreme Gradient Boosting (XGBoost) classifier which is able to work with sparse features. Validation against test data show promising results with overall accuracy of 89.42-91.53%, kappa of 0.76-0.79 and F1 score of 0.90-0.92. Map validation against …",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:HDshCWvjkbEC,,1
1583,"Air pollution monitoring network using low-cost sensors, a case study in Hanoi, Vietnam","Air pollution is a serious problem in Vietnam, especially in urban areas with high pressures of population, traffic, construction, and industrial development. Besides high accurate measurements from automatic and continuous monitoring ground stations and high-cost sensor devices, low-cost sensors have recently utilized to extent air pollution monitoring networks although their data quality is still argumentative. In this paper, we present a low-cost device, named FAirKit, which measured 6 basic air pollutants including PM2.5, PM10, CO, O3, NO2, and SO2, and temperature and relative humidity. The sensors are calibrated with standard devices to improve their data quality. FAirKits are installed and transferred data in real-time to servers where an information system based on Sensor Web Enablement (SWE) standard of Open Geospatial Consortium (OGC) has been developed to store, process, and visualize real …",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:4fKUyHm3Qg0C,,1
1584,Spatial Interpolation and Assimilation Methods for Satellite and Ground Meteorological Data in Vietnam.,"This paper presents the applications of spatial interpolation and assimilation methods for satellite and ground meteorological data, including temperature, relative humidity, and precipitation in regions of Vietnam. In this work, Universal Kriging is used for spatially interpolating ground data and its interpolated results are assimilated with corresponding satellite data to anticipate better gridded data. The input meteorological data was collected from 98 ground weather stations located all over Vietnam; whereas, the satellite data consists of the MODIS Atmospheric Profiles product (MOD07), the ASTER Global Digital Elevation Map (ASTER DEM), and the Tropical Rainfall Measuring Mission (TRMM) in six years. The outputs are gridded fields of temperature, relative humidity, and precipitation. The empirical results were evaluated by using the Root mean square error (RMSE) and the mean percent error (MPE), which illustrate that Universal Kriging interpolation obtains higher accuracy than other forms of Kriging; whereas, the assimilation for precipitation gradually reduces RMSE and significantly MPE. It also reveals that the accuracy of temperature and humidity when employing assimilation that is not significantly improved because of low MODIS retrieval due to cloud contamination.",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:W7OEmFMy1HYC,,1
1585,Validation of Support Vector Regression in deriving aerosol optical thickness maps at 1 km2spatial resolution from satellite observations,"As a result of great improvements in satellite technologies, satellite-based observations have provided possibilities to monitor air pollution at the global scale with moderate quality in comparison with ground truth measurement. In tradition, the inversion process that derives atmospheric parameters from satellite-based data is replied on simulated physics models of matter interactions. Recently, the usage of machine learning techniques in this field has been investigated and presented competitive results to the physical approach. In this paper, we present validation of Support Vector Regression (SVR) technique in estimating Aerosol Optical Thickness (AOT), one of the most important atmospheric variables, from satellite observations at 1×1 km 2  of spatial resolution. Validation by different European countries is carried out on a large amount of datasets collected in three years, which aims at investigating prediction …",108,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dkzooDwAAAAJ&citation_for_view=dkzooDwAAAAJ:UeHWp8X0CEIC,,1
1586,A hidden topic-based framework toward building applications with short web documents,"This paper introduces a hidden topic-based framework for processing short and sparse documents (e.g., search result snippets, product descriptions, book/movie summaries, and advertising messages) on the Web. The framework focuses on solving two main challenges posed by these kinds of documents: 1) data sparseness and 2) synonyms/homonyms. The former leads to the lack of shared words and contexts among documents while the latter are big linguistic obstacles in natural language processing (NLP) and information retrieval (IR). The underlying idea of the framework is that common hidden topics discovered from large external data sets (universal data sets), when included, can make short documents less sparse and more topic-oriented. Furthermore, hidden topics from universal data sets help handle unseen data better. The proposed framework can also be applied for different natural languages and …",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:u5HHmVD_uO8C,,1
1587,An experiment in integrating sentiment features for tech stock prediction in twitter,"Economic analysis indicates a relationship between consumer sentiment and stock price movements. In this study we harness features from Twitter messages to capture public mood related to four Tech companies for predicting the daily up and down price movements of these companies’ NASDAQ stocks. We propose a novel model combining features namely positive and negative sentiment, consumer confidence in the product with respect to ‘bullish’or ‘bearish’lexicon and three previous stock market movement days. The features are employed in a Decision Tree classifier using cross-fold validation to yield accuracies of 82.93%, 80.49%, 75.61% and 75.00% in predicting the daily up and down changes of Apple (AAPL), Google (GOOG), Microsoft (MSFT) and Amazon (AMZN) stocks respectively in a 41 market day sample.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:9yKSN-GCB0IC,,1
1588,Vietnamese word segmentation with CRFs and SVMs: An investigation,"Word segmentation for Vietnamese, like for most Asian languages, is an important task which has a significant impact on higher language processing levels. However, it has received little attention of the community due to the lack of a common annotated corpus for evaluation and comparison. Also, most previous studies focused on unsupervised-statistical approaches or combined too many techniques. Consequently, their accuracies are not as high as expected. This paper reports a careful investigation of using conditional random fields (CRFs) and support vector machines (SVMs)-two of the most successful statistical learning methods in NLP and pattern recognition-for solving the task. We first build a moderate annotated corpus using different sources of materials. For a careful evaluation, different CRF and SVM models using different feature settings were trained and their results are compared and contrasted with each other. In addition, we discuss several important points about the accuracy, computational cost, corpus size and other aspects that might influence the overall quality of Vietnamese word segmentation.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:u-x6o8ySG0sC,,1
1589,Web search clustering and labeling with hidden topics,Web search clustering is a solution to reorganize search results (also called “snippets”) in a more convenient way for browsing. There are three key requirements for such post-retrieval clustering systems: (1) the clustering algorithm should group similar documents together; (2) clusters should be labeled with descriptive phrases; and (3) the clustering system should provide high-quality clustering without downloading the whole Web page.,110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:d1gkVwhDpl0C,,1
1590,Named entity recognition in Vietnamese free-text and web documents using conditional random fields,"Named entity recognition (NER) is the process of identifying different entity types (eg person, location, organization, or date/time), mentioned in natural language documents. It is an important task in information extraction and is a necessary precursor to higher processing and understanding natural language such as text mining, text summarization, question-answering, and machine translation. Further, the automated recognition of named entities is essential for the increasing need of searching, extracting, and tracking relevant information on the web environment, and especially for building the emerging semantic web technology.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:-f6ydRqryjwC,,1
1591,Bisimulation-based concept learning in description logics,"Concept learning in description logics (DLs) is similar to binary classification in traditional machine learning. The difference is that in DLs objects are described not only by attributes but also by binary relationships between objects. In this paper, we develop the first bisimulation-based method of concept learning in DLs for the following setting: given a knowledge base KB in a DL, a set of objects standing for positive examples and a set of objects standing for negative examples, learn a concept C in that DL such that the positive examples are instances of C wrt KB, while the negative examples are not instances of C wrt KB. We also prove soundness of our method and investigate its C-learnability.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:Y0pCki6q_DkC,,1
1592,An experimental study on Vietnamese POS tagging,"In Natural Language Processing (NLP), Part-of-speech tagging is one of the important tasks. It, however, has not drawn much attention of Vietnamese researchers all over the world. In this paper, we present an experimental study on Vietnamese POS tagging. Motivated from Chinese research and Vietnamese characteristics, we present a new kind of features based on the idea of word composition. We call it morpheme based features. To verify the effectiveness of these features, we use three powerful machine learning techniques - MEM, CRF and SVM. In addition, we also built a Vietnamese POS-tagged corpus with approximately 8000 sentences of different genres to conduct experiments. Experimental results showed that morpheme-based features always give higher precision in comparison with previous approaches - usually word-based features. We achieved the precision of 91.64% by using these morpheme …",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:mVmsd5A6BfQC,,1
1593,An upgrading feature-based opinion mining model on Vietnamese product reviews,"Feature-based opinion mining and summarizing (FOMS) of reviews is an interesting issue in the opinion mining field. SentiWordNet is an useful lexical resource for opinion mining, especially for FOMS. In this paper, an upgrading FOMS model on Vietnamese reviews on mobile phone products is described. Feature words and opinion words were extracted based on some Vietnamese syntactic rules. Extracted feature words were grouped by using HAC clustering and semi-supervised SVM-kNN classification. Customers’ opinion orientation and summarization on features was determined by using a VietSentiWordNet, which had been extended from an initial VietSentiWordNet. Experiments on feature extraction and opinion summarization on features are showed.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:WF5omc3nYNoC,,1
1594,A feature-based opinion mining model on product reviews in Vietnamese,"Feature-based opinion mining and summarizing (FOMS) of reviews is an interesting issue in opinion mining field. In this paper, we propose an opinion mining model on Vietnamese reviews on mobile phone products. Explicit/Implicit feature-words and opinion-words were extracted by using Vietnamese syntax rules as same as synonym feature words were grouped into a feature, which belongs to the feature dictionary. Customers’ opinion orientations and summarization on features were determined by using VietSentiWordNet and suitable formulas.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:zYLM7Y9cAGgC,,1
1595,Improving vietnamese word segmentation and pos tagging using mem with various kinds of resources,"Word segmentation and POS tagging are two important problems included in many NLP tasks. They, however, have not drawn much attention of Vietnamese researchers all over the world. In this paper, we focus on the integration of advantages from several resourses to improve the accuracy of Vietnamese word segmentation as well as POS tagging task. For word segmentation, we propose a solution in which we try to utilize multiple knowledge resources including dictionary-based model, N-gram model, and named entity recognition model and then integrate them into a Maximum Entropy model. The result of experiments on a public corpus has shown its effectiveness in comparison with the best current models. We got 95.30% F1 measure. For POS tagging, motivated from Chinese research and Vietnamese characteristics, we present a new kind of features based on the idea of word composition. We call it morpheme-based features. Our experiments based on two POS-tagged corpora showed that morpheme-based features always give promising results. In the best case, we got 89.64% precision on a Vietnamese POS-tagged corpus when using Maximum Entropy model.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:QIV2ME_5wuYC,,1
1596,Learning to recognize phenotype candidates in the auto-immune literature using SVM re-ranking,"The identification of phenotype descriptions in the scientific literature, case reports and patient records is a rewarding task for bio-medical text mining. Any progress will support knowledge discovery and linkage to other resources. However because of their wide variation a number of challenges still remain in terms of their identification and semantic normalisation before they can be fully exploited for research purposes. This paper presents novel techniques for identifying potential complex phenotype mentions by exploiting a hybrid model based on machine learning, rules and dictionary matching. A systematic study is made of how to combine sequence labels from these modules as well as the merits of various ontological resources. We evaluated our approach on a subset of Medline abstracts cited by the Online Mendelian Inheritance of Man database related to auto-immune diseases. Using partial matching the best micro-averaged F-score for phenotypes and five other entity classes was 79.9%. A best performance of 75.3% was achieved for phenotype candidates using all semantics resources. We observed the advantage of using SVM-based learn-to-rank for sequence label combination over maximum entropy and a priority list approach. The results indicate that the identification of simple entity types such as chemicals and genes are robustly supported by single semantic resources, whereas phenotypes require combinations. Altogether we conclude that our approach coped well with the compositional structure of phenotypes in the auto-immune domain.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:qjMakFHDy7sC,,1
1597,A bisimulation-based method of concept learning for knowledge bases in description logics,"We develop the first bisimulation-based method of concept learning, called BBCL, for knowledge bases in description logics (DLs). Our method is formulated for a large class of useful DLs, with well-known DLs like ALC, SHIQ, SHOIQ, SROIQ. As bisimulation is the notion for characterizing indis-cernibility of objects in DLs, our method is natural and very promising.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:UeHWp8X0CEIC,,1
1598,Sentiment analysis and user similarity for social recommender system: An experimental study,"Social recommender system has become an emerging research topic due to the prevalence of online social networking services during the past few years. A social recommender model can be considered the combination of a recommender model and a social information model. Many approaches have been proposed to exploit the social interaction or connections among users to overcome the defect of traditional recommender systems assuming that all the users are independent and identically distributed. In this paper, we propose a social recommender system using memory based collaborative filtering models with user-oriented methods as basic models, in which we conduct an analysis on the correlations between social relations and user interest similarities. We also combine techniques of sentiment analysis to get dataset of users with their favorite products; this dataset is the input for the social …",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:8k81kl-MbHgC,,1
1599,An experimental study of vietnamese question answering system,"The development of world wide web calls for how to efficiently exploit the information. Mostly, current search engines return a set of related documents which contain keywords. However, users expect the exact and concrete answer for each question. Therefore, it is necessary to build an automatic question answering system (QA). In this paper, we focus on building a QA for Vietnamese. This task especially becomes more and more difficult because of the lack of available tools for processing Vietnamese text. Based on previous research for English, this paper proposed an implementation for Vietnamese question answering system by combining SnowBall system and semantic relation extraction using search engines. The experimental results on travelling domain proved that this proposed method is sufficient for Vietnamese question answering system. We achieved 89.7% precision and 91.4% ability to give the …",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:pqnbT2bcN3wC,,1
1600,Measuring the influence of bloggers in their community based on the H-index family,"Nowadays, people in social networks can have impact on the actual society, e.g. a post on a person’s space can lead to real actions of other people in many areas of life. This is called social influence and the task of evaluating the influence is called social influence analysis which can be exploited in many fields, such as typical marketing (object oriented advertising), recommender systems, social network analysis, event detection, expert finding, link prediction, ranking, etc. The h-index, proposed by Hirsch in 2005, is now a widely used index for measuring both the productivity and impact of the published work of a scientist or scholar. This paper proposes to use h-index to measure the blogger influence in a social community. We also propose to enhance information for h-index (as well as its variants) calculation, and our experimental results are very promising.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:YsMSGLbcyi4C,,1
1601,A richer-but-smarter shortest dependency path with attentive augmentation for relation extraction,"To extract the relationship between two entities in a sentence, two common approaches are (1) using their shortest dependency path (SDP) and (2) using an attention model to capture a context-based representation of the sentence. Each approach suffers from its own disadvantage of either missing or redundant information. In this work, we propose a novel model that combines the advantages of these two approaches. This is based on the basic information in the SDP enhanced with information selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but- Smarter SDP). To exploit the representation behind the RbSP structure effectively, we develop a combined deep neural model with a LSTM network on word sequences and a CNN on RbSP. Experimental results on the SemEval-2010 dataset demonstrate improved performance over competitive baselines. The data and source code are available at https: //github.com/catcd/RbSP.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:dfsIfKJdRG4C,,1
1602,A trace clustering solution based on using the distance graph model,"Process discovery is the most important task in the process mining. Because of the complexity of event logs (i.e. activities of several different processes are written into the same log), the discovered process models may be diffuse and unintelligible. That is why the input event logs should be clustered into simpler event sub-logs. This work provides a trace clustering solution based on the idea of using the distance graph model for trace representation. Experimental results proved the effect of the proposed solution on two measures of Fitness and Precision, especially the effect on the Precision measure.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:_Qo2XoVZTnwC,,1
1603,Concept learning for description logic-based information systems,"The work [1] by Nguyen and Szalas is a pioneering one that uses bisimulation for machine learning in the context of description logics. In this paper we generalize and extend their concept learning method [1] for description logic-based information systems. We take attributes as basic elements of the language. Each attribute may be discrete or numeric. A Boolean attribute is treated as a concept name. This approach is more general and much more suitable for practical information systems based on description logic than the one of [1]. As further extensions we allow also data roles and the concept constructors ""functionality"" and ""unquantified number restrictions"". We formulate and prove an important theorem on basic selectors. We also provide new examples to illustrate our approach.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:2osOgNQ5qMEC,,1
1604,Matching and ranking with hidden topics towards online contextual advertising,"In online contextual advertising, ad messages are displayed related to the content of the target Web page. It leads to the problem in information retrieval community: how to select the most relevant ad messages given the content of a page. To deal with this problem, we propose a framework that takes advantage of large scale external datasets. This framework provides a mechanism to discover the semantic relations between Web pages and ad messages by analyzing topics for them. This helps overcome the problem of mismatch due to unimportant words and the difference in vocabularies between Web pages and ad messages. The framework has been evaluated through a number of experiments. It shows a significant improvement in accuracy over word/lexicon-based matching and ranking methods.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:IjCSPb-OGe4C,,1
1605,A maximum entropy model for text classification,"Text categorization or document classification is one of the major tasks in text data mining and information retrieval. With the explosive development of the Web, the automated and online classification of web document content becomes significant for searching, filtering, anti-spam activities, etc. and thus makes the Web more accessible to end users.",110,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=grsLl2wAAAAJ&citation_for_view=grsLl2wAAAAJ:dhFuZR0502QC,,1
1606,UFBoot2: improving the ultrafast bootstrap approximation,"The standard bootstrap (SBS), despite being computationally intensive, is widely used in maximum likelihood phylogenetic analyses. We recently proposed the ultrafast bootstrap approximation (UFBoot) to reduce computing time while achieving more unbiased branch supports than SBS under mild model violations. UFBoot has been steadily adopted as an efficient alternative to SBS and other bootstrap approaches. Here, we present UFBoot2, which substantially accelerates UFBoot and reduces the risk of overestimating branch supports due to polytomies or severe model violations. Additionally, UFBoot2 provides suitable bootstrap resampling strategies for phylogenomic data. UFBoot2 is 778 times (median) faster than SBS and 8.4 times (median) faster than RAxML rapid bootstrap on tested data sets. UFBoot2 is implemented in the IQ-TREE software package version 1.6 and freely available at http://www …",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:_kc_bZDykSQC,,1
1607,POY version 4: phylogenetic analysis using dynamic homologies,"We present POY version 4, an open source program for the phylogenetic analysis of morphological, prealigned sequence, unaligned sequence, and genomic data. POY allows phylogenetic inference when not only substitutions, but insertions, deletions, and rearrangement events are allowed (computed using the breakpoint or inversion distance). Compared with previous versions, POY 4 provides greater flexibility, a larger number of supported parameter sets, numerous execution time improvements, a vastly improved user interface, greater quality control, and extensive documentation. We introduce POY’s basic features, and present a simple example illustrating the performance improvements over previous versions of the application.",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:Se3iqnhoufwC,,1
1608,IQPNNI: moving fast through tree space and stopping in time,"An efficient tree reconstruction method (IQPNNI) is introduced to reconstruct a phylogenetic tree based on DNA or amino acid sequence data. Our approach combines various fast algorithms to generate a list of potential candidate trees. The key ingredient is the definition of so-called important quartets (IQs), which allow the computation of an intermediate tree in O(n2) time for n sequences. The resulting tree is then further optimized by applying the nearest neighbor interchange (NNI) operation. Subsequently a random fraction of the sequences is deleted from the best tree found so far. The deleted sequences are then re-inserted in the smaller tree using the important quartet puzzling (IQP) algorithm. These steps are repeated several times and the best tree, with respect to the likelihood criterion, is considered as the inferred phylogenetic tree. Moreover, we suggest a rule which indicates when to stop the search …",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:zYLM7Y9cAGgC,,1
1609,pIQPNNI: parallel reconstruction of large maximum likelihood phylogenies, Summary: IQPNNI is a program to infer maximum-likelihood phylogenetic trees from DNA or protein data with a large number of sequences. We present an improved and MPI-parallel implementation showing very good scaling and speedup behavior.,111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:IjCSPb-OGe4C,,1
1610,MPBoot: fast phylogenetic maximum parsimony tree inference and bootstrap approximation,"The nonparametric bootstrap is widely used to measure the branch support of phylogenetic trees. However, bootstrapping is computationally expensive and remains a bottleneck in phylogenetic analyses. Recently, an ultrafast bootstrap approximation (UFBoot) approach was proposed for maximum likelihood analyses. However, such an approach is still missing for maximum parsimony. To close this gap we present MPBoot, an adaptation and extension of UFBoot to compute branch supports under the maximum parsimony principle. MPBoot works for both uniform and non-uniform cost matrices. Our analyses on biological DNA and protein showed that under uniform cost matrices, MPBoot runs on average 4.7 (DNA) to 7 times (protein data) (range: 1.2–20.7) faster than the standard parsimony bootstrap implemented in PAUP*; but 1.6 (DNA) to 4.1 times (protein data) slower than the standard bootstrap with a fast search routine in TNT (fast-TNT). However, for non-uniform cost matrices MPBoot is 5 (DNA) to 13 times (protein data) (range:0.3–63.9) faster than fast-TNT. We note that MPBoot achieves better scores more frequently than PAUP* and fast-TNT. However, this effect is less pronounced if an intensive but slower search in TNT is invoked. Moreover, experiments on large-scale simulated data show that while both PAUP* and TNT bootstrap estimates are too conservative, MPBoot bootstrap estimates appear more unbiased. MPBoot provides an efficient alternative to the standard maximum parsimony bootstrap procedure. It shows favorable performance in terms of run time, the capability of finding a maximum parsimony tree, and high bootstrap …",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:R3hNpaxXUhUC,,1
1611,"FLU, an amino acid substitution model for influenza proteins","The amino acid substitution model is the core component of many protein analysis systems such as sequence similarity search, sequence alignment, and phylogenetic inference. Although several general amino acid substitution models have been estimated from large and diverse protein databases, they remain inappropriate for analyzing specific species, e.g., viruses. Emerging epidemics of influenza viruses raise the need for comprehensive studies of these dangerous viruses. We propose an influenza-specific amino acid substitution model to enhance the understanding of the evolution of influenza viruses. A maximum likelihood approach was applied to estimate an amino acid substitution model (FLU) from ~113, 000 influenza protein sequences, consisting of ~20 million residues. FLU outperforms 14 widely used models in constructing maximum likelihood phylogenetic trees for the majority of influenza protein alignments. On average, FLU gains ~42 log likelihood points with an alignment of 300 sites. Moreover, topologies of trees constructed using FLU and other models are frequently different. FLU does indeed have an impact on likelihood improvement as well as tree topologies. It was implemented in PhyML and can be downloaded from                    ftp://ftp.sanger.ac.uk/pub/1000genomes/lsq/FLU                                     or included in PhyML 3.0 server at                    http://www.atgc-montpellier.fr/phyml/                                    . FLU should be useful for any influenza protein analysis system which requires an accurate description of amino acid substitutions.",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:QIV2ME_5wuYC,,1
1612,A Vietnamese human genetic variation database,"Large scale human genome projects have created tremendous human genome databases for some well‐studied populations. Vietnam has about 95 million people (the 14th largest country by population in the world) of which more than 86% are Kinh people. To date, genetic studies for Vietnamese people mostly rely on genetic information from other populations. Building a Vietnamese human genetic variation database is a must for properly interpreting Vietnamese genetic variants. To this end, we sequenced 105 whole genomes and 200 whole exomes of 305 unrelated Kinh Vietnamese (KHV) people. We also included 101 other previously published KHV genomes to build a Vietnamese human genetic variation database of 406 KHV people. The KHV database contains 24.81 million variants (22.47 million single nucleotide polymorphisms (SNPs) and 2.34 million indels) of which 0.71 million variants are novel. It …",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:maZDTaKrznsC,,1
1613,Mechanisms of sex determination and transmission ratio distortion in Aedes aegypti,"More effective mosquito control strategies are urgently required due to the increasing prevalence of insecticide resistance. The sterile insect technique (SIT) and the release of insects carrying a dominant lethal allele (RIDL) are two proposed methods for environmentally-friendly, species-targeted population control. These methods may be more suitable for developing countries if producers reduce the cost of rearing insects. The cost of control programs could be reduced by producing all-male mosquito populations to circumvent the isolation of females before release without reducing male mating competitiveness caused by transgenes. An RNAi construct targeting the RNA recognition motif of the Aedes aegypti transformer-2 (tra-2) gene does not trigger female-to-male sex conversion as commonly observed among dipterous insects. Instead, homozygous insects show greater mortality among m-chromosome-bearing sperm and mm zygotes, yielding up to 100 % males in the subsequent generations. The performance of transgenic males was not significantly different to wild-type males in narrow-cage competitive mating experiments. Our data provide preliminary evidence that the knockdown of Ae. aegypti tra-2 gene expression causes segregation distortion acting at the level of gametic function, which is reinforced by sex-specific zygotic lethality. This finding could promote the development of new synthetic sex distorter systems for the production of genetic sexing mosquito strains.",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:JV2RwH3_ST0C,,1
1614,Shortest triplet clustering: reconstructing large phylogenies using representative sets,"Understanding the evolutionary relationships among species based on their genetic information is one of the primary objectives in phylogenetic analysis. Reconstructing phylogenies for large data sets is still a challenging task in Bioinformatics. We propose a new distance-based clustering method, the shortest triplet clustering algorithm (STC), to reconstruct phylogenies. The main idea is the introduction of a natural definition of so-called k-representative sets. Based on k-representative sets, shortest triplets are reconstructed and serve as building blocks for the STC algorithm to agglomerate sequences for tree reconstruction in O(n2) time for n sequences. Simulations show that STC gives better topological accuracy than other tested methods that also build a first starting tree. STC appears as a very good method to start the tree reconstruction. However, all tested methods give similar results if balanced nearest …",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:MXK_kJrjxJIC,,1
1615,Improved mitochondrial amino acid substitution models for metazoan evolutionary studies,"Amino acid substitution models play an essential role in inferring phylogenies from mitochondrial protein data. However, only few empirical models have been estimated from restricted mitochondrial protein data of a hundred species. The existing models are unlikely to represent appropriately the amino acid substitutions from hundred thousands metazoan mitochondrial protein sequences. We selected 125,935 mitochondrial protein sequences from 34,448 species in the metazoan kingdom to estimate new amino acid substitution models targeting metazoa, vertebrates and invertebrate groups. The new models help to find significantly better likelihood phylogenies in comparison with the existing models. We noted remarkable distances from phylogenies with the existing models to the maximum likelihood phylogenies that indicate a considerable number of incorrect bipartitions in phylogenies with the existing models …",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:dhFuZR0502QC,,1
1616,Large maximum likelihood trees,"We introduce the underlying principles of phylogenetic reconstruction, avoiding all technicalities. Since phylogenetic reconstruction based on DNA-sequence data is a computational expensive undertaking, efficient algorithms are required to suggest reasonable solutions with respect to an objective function. However, even efficient programs like IQPNNI cannot cope in reasonable time with extremely large data sets. Thus, we summarize our results on implementing a hybird parallelization scheme for IQPNNI.",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:8k81kl-MbHgC,,1
1617,ReplacementMatrix: a web server for maximum-likelihood estimation of amino acid replacement rate matrices," Summary: Amino acid replacement rate matrices are an essential basis of protein studies (e.g. in phylogenetics and alignment). A number of general purpose matrices have been proposed (e.g. JTT, WAG, LG) since the seminal work of Margaret Dayhoff and co-workers. However, it has been shown that matrices specific to certain protein groups (e.g. mitochondrial) or life domains (e.g. viruses) differ significantly from general average matrices, and thus perform better when applied to the data to which they are dedicated. This Web server implements the maximum-likelihood estimation procedure that was used to estimate LG, and provides a number of tools and facilities. Users upload a set of multiple protein alignments from their domain of interest and receive the resulting matrix by email, along with statistics and comparisons with other matrices. A non-parametric bootstrap is performed optionally to assess …",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:ZeXyd9-uunAC,,1
1618,A mutation in GABRB3 associated with Dravet syndrome,"Dravet syndrome is a rare and severe type of epilepsy in infants. Approximately, 70–80% of patients with Dravet syndrome have mutations in SCN1A, the gene encoding the alpha‐1 subunit of the sodium channel, while some simplex patients have variants in one of several other genes, including but not limited to GABRA1, SCN2A, STXBP1, GABRG2, and SCN1B. In this study, we performed exome sequencing in six patients with SCN1A‐negative Dravet syndrome to identify other genes related to this disorder. In one affected individual, we detected a novel de novo heterozygous missense variant, c.695G>A, p.(Arg232Gln), in GABRB3, the gene encoding the β3‐subunit of the gamma‐aminobutyric acid type A (GABAA) receptor, which mediates inhibitory signaling within the central nervous system. In summary, the data in this study identify GABRB3 as a candidate gene for Dravet syndrome.",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:isC4tDSrTZIC,,1
1619,Loss of matKRNA editing in seed plant chloroplasts,"RNA editing in chloroplasts of angiosperms proceeds by C-to-U conversions at specific sites. Nuclear-encoded factors are required for the recognition of cis-elements located immediately upstream of editing sites. The ensemble of editing sites in a chloroplast genome differs widely between species, and editing sites are thought to evolve rapidly. However, large-scale analyses of the evolution of individual editing sites have not yet been undertaken. Here, we analyzed the evolution of two chloroplast editing sites, matK-2 and matK-3, for which DNA sequences from thousands of angiosperm species are available. Both sites are found in most major taxa, including deep-branching families such as the nymphaeaceae. However, 36 isolated taxa scattered across the entire tree lack a C at one of the two matK editing sites. Tests of several exemplary species from this in silico analysis of matK processing unexpectedly …",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:aqlVkmm33-oC,,1
1620,PhyNav: A novel approach to reconstruct large phylogenies,"A novel method, PhyNav, is introduced to reconstruct the evolutionary relationship among contemporary species based on their genetic data. The key idea is the definition of the so-called minimal κ-distance subset which contains most of the relevant phylogenetic information from the whole dataset. For this reduced subset the subtree is created faster and serves as a scaffold to construct the full tree. Because many minimal subsets exist the procedure is repeated several times and the best tree with respect to some optimality criterion is considered as the inferred phylogenetic tree. PhyNav gives encouraging results compared to other programs on both simulated and real datasets. A program to reconstruct phylogenetic trees based on DNA or amino acid based is available (http://www. bi. uni-duesseldorf. de/software/phynav/).",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:u5HHmVD_uO8C,,1
1621,"FastMG: a simple, fast, and accurate maximum likelihood procedure to estimate amino acid replacement rate matrices from large data sets","Amino acid replacement rate matrices are a crucial component of many protein analysis systems such as sequence similarity search, sequence alignment, and phylogenetic inference. Ideally, the rate matrix reflects the mutational behavior of the actual data under study; however, estimating amino acid replacement rate matrices requires large protein alignments and is computationally expensive and complex. As a compromise, sub-optimal pre-calculated generic matrices are typically used for protein-based phylogeny. Sequence availability has now grown to a point where problem-specific rate matrices can often be calculated if the computational cost can be controlled. The most time consuming step in estimating rate matrices by maximum likelihood is building maximum likelihood phylogenetic trees from protein alignments. We propose a new procedure, called FastMG, to overcome this obstacle. The key innovation is the alignment-splitting algorithm that splits alignments with many sequences into non-overlapping sub-alignments prior to estimating amino acid replacement rates. Experiments with different large data sets showed that the FastMG procedure was an order of magnitude faster than without splitting. Importantly, there was no apparent loss in matrix quality if an appropriate splitting procedure is used. FastMG is a simple, fast and accurate procedure to estimate amino acid replacement rate matrices from large data sets. It enables researchers to study the evolutionary relationships for specific groups of proteins or taxa with optimized, data-specific amino acid replacement rate matrices. The programs, data sets, and the new mammalian …",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:L8Ckcad2t8MC,,1
1622,Pairwise alignment with rearrangements,"The increase of available genomes poses new optimization problems in genome comparisons. A genome can be considered as a sequence of characters (loci) which are genes or segments of nucleotides. Genomes are subject to both nucleotide transformation and character order rearrangement processes. In this context, we define a problem of so-called pairwise alignment with rearrangements (PAR) between two genomes. The PAR generalizes the ordinary pairwise alignment by allowing the rearrangement of character order. The objective is to find the optimal PAR that minimizes the total cost which is composed of three factors: the edit cost between characters, the deletion/insertion cost of characters, and the rearrangement cost between character orders. To this end, we propose simple and effective heuristic methods: character moving and simultaneous character swapping. The efficiency of the methods is tested on Metazoa mitochondrial genomes. Experiments show that, pairwise alignments with rearrangements give better performance than ordinary pairwise alignments without rearrangements. The best proposed method, simultaneous character swapping, is implemented as an essential subroutine in our software POY version 4.0 to reconstruct genome-based phylogenies.",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:LkGwnXOMwfcC,,1
1623,QMaker: Fast and accurate method to estimate empirical models of protein evolution,"Amino acid substitution models play a crucial role in phylogenetic analyses. Maximum likelihood (ML) methods have been proposed to estimate amino acid substitution models; however, they are typically complicated and slow. In this article, we propose QMaker, a new ML method to estimate a general time-reversible  matrix from a large protein data set consisting of multiple sequence alignments. QMaker combines an efficient ML tree search algorithm, a model selection for handling the model heterogeneity among alignments, and the consideration of rate mixture models among sites. We provide QMaker as a user-friendly function in the IQ-TREE software package (http://www.iqtree.org) supporting the use of multiple CPU cores so that biologists can easily estimate amino acid substitution models from their own protein alignments. We used QMaker to estimate new empirical general amino acid substitution …",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:pqnbT2bcN3wC,,1
1624,Random tree-puzzle leads to the Yule–Harding distribution,"Approaches to reconstruct phylogenies abound and are widely used in the study of molecular evolution. Partially through extensive simulations, we are beginning to understand the potential pitfalls as well as the advantages of different methods. However, little work has been done on possible biases introduced by the methods if the input data are random and do not carry any phylogenetic signal. Although Tree-Puzzle (Strimmer K, von Haeseler A. 1996. Quartet puzzling: a quartet maximum-likelihood method for reconstructing tree topologies. Mol Biol Evol. 13:964–969; Schmidt HA, Strimmer K, Vingron M, von Haeseler A. 2002. Tree-Puzzle: maximum likelihood phylogenetic analysis using quartets and parallel computing. Bioinformatics 18:502–504) has become common in phylogenetics, the resulting distribution of labeled unrooted bifurcating trees when data do not carry any phylogenetic signal has not …",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:0EnyYjriUFMC,,1
1625,Genetic landscape of autism spectrum disorder in Vietnamese children,"Autism spectrum disorder (ASD) is a complex disorder with an unclear aetiology and an estimated global prevalence of 1%. However, studies of ASD in the Vietnamese population are limited. Here, we first conducted whole exome sequencing (WES) of 100 children with ASD and their unaffected parents. Our stringent analysis pipeline was able to detect 18 unique variants (8 de novo and 10×-linked, all validated), including 12 newly discovered variants. Interestingly, a notable number of X-linked variants were detected (56%), and all of them were found in affected males but not in affected females. We uncovered 17 genes from our ASD cohort in which CHD8, DYRK1A, GRIN2B, SCN2A, OFD1 and MDB5 have been previously identified as ASD risk genes, suggesting the universal aetiology of ASD for these genes. In addition, we identified six genes that have not been previously reported in any autism database …",111,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=40kW2KsAAAAJ&citation_for_view=40kW2KsAAAAJ:RYcK_YlVTxYC,,1
1627,Cross-cultural multimedia computing with impression-based semantic spaces,"Over the past decade, the rapid progress of multimedia data management technology has realized the large scale of media data transfer and resource-accumulation in the world. The multimedia computing technology has also been creating new information provision environments in the world-wide scope. Innovative integrations of large scale multimedia data management and computing technology will lead to a new information society.",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:9yKSN-GCB0IC,,1
1628,Cross-cultural image database system with impression-based color arrangement,"Colors and color combinations are tightly associated with human impressions for image data, and color- oriented impression varies between cultures. Though the shape and structure of images are important feature for retrieving similar images in the field of Content-based Image Retrieval (CBIR), in this paper, we focus on color feature of images because the purpose of our research is to create a cross-cultural and dynamic media designing environment using culture-dependent color-impressions for cultural property's images, especially, images of patterns used in cultural products. The main feature of our cross- cultural image database system for cultural-product design is characterized as the following three functions: (1) Impression-based color arrangement, (2) Culture-based color-space transformation among culture-dependent color collections, and (3) Attribute- based data selection and mapping. First, in our …",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:5ugPr518TE4C,,1
1630,PoB: Toward reasoning patterns of beauty in image data,"Aiming to develop of computational grammar system for visual information, we design a 4-tier framework that consists of four levels of 'visual grammar of images.' As a first step of realization, we propose a new dataset, named the PoB dataset, in which each image is annotated with multiple labels of armature patterns that compose the pictorial scene. The PoB dataset includes of a 10,000-painting dataset for art and a 4,959-image dataset for photography. In this paper, we discuss the consistency analysis of our dataset and its applicability. We also demonstrate how the armature patterns in the PoB dataset are useful in assessing aesthetic quality of images, and how well a deep learning algorithm can recognize these patterns. This paper seeks to set a new direction in image understanding with a more holistic approach beyond discrete objects and in aesthetic reasoning with a more interpretative way.",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:eflP2zaiRacC,,1
1631,Maintaining Ad-Hoc Communication Network in Area Protection Scenarios with Adversarial Agents,"We address a problem of area protection in graph-based scenarios with multiple mobile agents where connectivity is maintained among agents to ensure they can communicate. The problem consists of two adversarial teams of agents that move in an undirected graph shared by both teams. Agents are placed in vertices of the graph; at most one agent can occupy a vertex; and they can move into adjacent vertices in a conflict free way. Teams have asymmetric goals: the aim of one team - attackers - is to invade into given area while the aim of the opponent team - defenders - is to protect the area from being entered by attackers by occupying selected vertices. The team of defenders need to maintain connectivity of vertices occupied by its own agents in a visibility graph. The visibility graph models possibility of communication between pairs of vertices. We study strategies for allocating vertices to be occupied by the team of defenders to block attacking agents where connectivity is maintained at the same time. To do this we reserve a subset of defending agents that do not try to block the attackers but instead are placed to support connectivity of the team. The performance of strategies is tested in multiple benchmarks. The success of a strategy is heavily dependent on the type of the instance, and so one of the contributions of this work is that we identify suitable strategies for diverse instance types.",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:q3oQSFYPqjQC,,1
1632,An imagination-based query creation method for image retrieval,"Our imagination-based query creation method is a new approach to image-recall functions reflecting color-based imaginations in human brains. This method dynamically represents user's imaginations and creates queries for image retrieval. The important aim of this method is to express user's abstract intentions in a computable objects and to recall those imaginations from image databases with color-analytical image retrieval. The main features of this method are:(1) a hierarchical model for color indexing to express image contexts,(2) color zooming for context-dependent color histogram generation by utilizing color names as contextual words,(3) integrating several histograms by using five histogram-combining operations and dynamic semantic weighting, and (4) threshold controlling for semantic correlation. This method exploits color information to retrieve intended images expressing a user's imagination. This …",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:qjMakFHDy7sC,,1
1633,Imagination-based image search system with dynamic query creation and its application,"An imagination-based image search system is a new environment to acquire unknown but desired images by queries, which reflects user’s dynamic imagination process. It leads to a new computation environment for searching images data resources in a contextual way. This paper presents a dynamic image-query creation method for imagination-based image search system and its application for travel information associated with scenery images. The main feature of our system is to extend analytical functions for image search, not only in retrieval processing, but also in query manipulation, according to the color-based combination of images with common features. A query creation method in this system is a novel approach to represent a user’s imagination process. This method enables a user to create dynamically a query, which reflects the user’s intention, impression and memory as his/her own context existing only his/her mind by color-based combinations of existent images in the real world. The proposed method consists of five operations for creating image-query vector from combinations of images, which are “plus”,“intersection”,“accumulation”, as local operations and “minus” and “difference” as global operations. Using the imagination-based image search system for travel information associated with scenery images, users can easily discover images and information of places where they never been. The system performs a series of qualitative and quantitative experiments to examine the feasibility, effectiveness of proposed method and scalability of the system.",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:wbdj-CoPYUoC,,1
1634,An Adaptive Search Path Traverse for Large-scale Video Frame Retrieval.,"Multimedia retrieval task is faced with increasingly large datasets and variously changing preferences of users in every query. We realize that the high dimensional representation of physical data which previously challenges search algorithms now brings chances to cope with dynamic contexts. In this paper, we introduce a method of building a large-scale video frame retrieval environment with a fast search algorithm that handles user’s dynamic contexts of querying by imagination and controlling response time. The search algorithm quickly finds an initial candidate, which has highest-match possibility, and then iteratively traverses along feature indexes to find other neighbor candidates until the input time bound is elapsed. The experimental studies based on the video frame retrieval system show the feasibility and effectiveness of our proposed search algorithm that can return results in a fraction of a second with a high success rate and small deviation to the expected ones. Moreover, its potential is clear that it can scale to large dataset while preserving its search performance.",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:mvPsJ3kp5DgC,,1
1635,Music retrieval and adjustment technique to support and motivate ergotherapy and daily exercises,"To maintain and uplift the motivation for doing exercise which is tend to be too simple in rehabilitation and ergotherapy, doing exercise with music is one of the good solutions. In this paper, we confirm that doing exercise with music is fun. Next, we design five types of musical features which relate to exercises. These features are evaluated by doing five types of exercises with listening to the tunes. Based on the results obtained from the experiments, we found many new things, for example, tempo has the adjustment limitation based on how well the user knows the tune. In this paper, we report the latest results from the experiments and explain the way to progress of this research.",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:kRWSkSYxWN8C,,1
1636,WEE: A mashup system using social networks for collaborative environmental monitoring education with spatiotemporal analysis and visualization,"The aim of this research is to construct a new collaborative mashup system, named Web-based Environmental Education (WEE), for global environmental monitoring research and education in which all participants could associatively organize multimedia data about physical environments and visually monitor various environmental topics based on timeseries multi-geographical views. The main features of this system are:(1) collaborative multimedia database creation using “social sensors”,(2) information aggregation from Web resources,(3) content-based image and document retrieval, and (4) big-picture visualization with spatiotemporal view controllers and content-based image, document retrieval. In this paper, a new designed usage of social network services as an educational tool and a new method for gathering environmental information from Web resources are presented. We also present three promising …",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:u5HHmVD_uO8C,,1
1637,Paper VI,We address a problem of area protection in graph-based scenarios with multiple mobile agents where connectivity is maintained among agents to ensure they can communicate. The problem consists of two adversarial teams of agents that move in an undirected graph shared by both teams. Agents are placed in vertices of the graph; at most one agent can occupy a vertex; and they can move into adjacent vertices in a conflict free way. Teams have asymmetric goals: the aim of one team-attackers-is to invade into given area while the aim of the opponent team-defenders-is to protect the area from being entered by attackers by occupying selected vertices. The team of defenders need to maintain connectivity of vertices occupied by its own agents in a visibility graph. The visibility graph models possibility of communication between pairs of vertices.,116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:XiVPGOgt02cC,,1
1638,Multicontext-adaptive indexing and search for large-scale video navigation,"Many multimedia retrieval tasks are faced with increasingly large-scale datasets and variously changing preferences of users in each query. There are at least three distinctive contextual aspects comprised to form a set of preferences of a user at each query time: content, intention, and response time. A content preference refers to the low-level or semantic representations of the data that a user is interested in. An intention preference refers to how the content should be regarded as relevant. And a response time preference refers to the ability to control a reasonable wait time. This paper features the dynamic adaptability of a multimedia search system to the contexts of its users and proposes a multicontext-adaptive indexing and search system for video data. The main contribution is the integration of context-based query creation functions with high-performance search algorithms into a unified search system …",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:BrmTIyaxlBUC,,1
1639,A bigraph representation model and directional search mechanism for debates,"The beauty of thinking resides not only in finding answers but also in challenging already existing ones and creating intelligible networks of thoughts and ideas of many different perspectives. Understanding that the essence of critical thinking presents in the flows of questions and answers and aiming at a tool of augmenting it, we propose a new knowledge representation model based on bipartite graphs and a set of functions aided to explore those graphs. A bipartite graph has two disjoint vertex sets that are question and answer sets. Every edge connects a question to an answer holds an evidence for the answer and every edge connects an answer to a question holds an argument raised from the answer. The directional search mechanism is particularly designed to reuse reasoning flows in the debate graphs by a projecting function between the question and answer sets. We also introduce a framework placing …",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:1qzjygNMrQYC,,1
1640,A regional food's features extraction algorithm and its application,"Automatically detecting food's taste is a non-trivial part. However, we realize that the taste of food can be extracted by directly analyzing recipes by the ingredients and the amount of them in the recipes. In this paper, we present a food analysis system to discover the taste of foods and to better understand the featured ingredients in each specific geographical region. The main features of this system are (1) to extract dominant ingredients and tastes in a region by analyzing the ingredients' frequency and its uniqueness, and (2) to transform user's existing materials or original recipe to a new recipe according to a targeted taste. To examine the feasibility and applicability of the algorithm, we have developed a web-based application with a recipe database collected from approximately 200 recipes in over 7 regions of Japan: Hokkaido-Tohoku, Kanto, Kansai, Shikoku, Tyubu, Kyusyu-Okinawa and Tyugoku.",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:V3AGJWp-ZtQC,,1
1642,5D WORLD PICMAP: IMAGINATION-BASED IMAGE SEARCH SYSTEM WITH SPATIOTEMPORAL ANALYZERS,"An imagination-based image search system with spatiotemporal analyzers,“5D World PicMap,” is a new computation environment for discovering knowledge from image data by using the spatiotemporal information and user’s imagination. The main feature of this system is to dynamically create various context-dependent patterns of pictorial stories on spatiotemporal spaces according to a user’s viewpoints and imagination processes using color-combinations of images. 5D World PicMap system provides users with 5-dimensional information overview: spatiotemporal 4 dimensions plus the degenerated color dimension. The system consists of two kinds of main functions:(1) color-based image retrieval functions with dynamic query creation method, and (2) spatiotemporal visualization functions for image data with timeseries multi-geographical views. This system enables general users to retrieve scenery images and information of unknown places according to their imagination; and researchers to analyze the synchronism and the time-series variation of image data with spatiotemporal information in the research fields. In this paper, we present several experimental results to examine the feasibility and applicability of our system to several fields such as geoscience, environmental analysis, cross-cultural art comparison, and so on.",116,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=afw-EfoAAAAJ&citation_for_view=afw-EfoAAAAJ:bnK-pcrLprsC,,1
1643,A study on perception of emotional states in multiple languages on valence-activation approach,"Human beings can judge emotional states of a voice only by listening, no matter thay understand the language or not. Investigating the commonalities and differences of emotional states perception among multiple languages is important to understand how human beings perceive emotional states among multiple languages, and to build a human perception model independent to used languages. This paper investigates commonalities and differences among multiple languages in human perception of emotional states based on a dimensional approach. The results suggest that human beings can perceive emotional states regardless of languages. Moreover, the results can be used to build a human perception model regardless of languages only by control the deviations between neutral voices and other emotional states.",117,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0VjG8eIAAAAJ&citation_for_view=0VjG8eIAAAAJ:IjCSPb-OGe4C,,1
1644,A Spatial-temporal 3D Human Pose Reconstruction Framework,"3D human pose reconstruction from single-view camera is a difficult and challenging topic. Many approaches have been proposed, but almost focusing on frame-by-frame independently while inter-frames are highly correlated in a pose sequence. In contrast, we introduce a novel spatial-temporal 3D reconstruction framework that leverages both intra and inter frame relationships in consecutive 2D pose sequences. Orthogonal Matching Pursuit (OMP) algorithm, pre-trained Pose-angle Limits and Temporal Models have been implemented. We quantitatively compare our framework versus recent works on CMU motion capture dataset and Vietnamese traditional dance sequences. Our method outperforms others with 10 percent lower of Euclidean reconstruction error and robustness against Gaussian noise. Additionally, it is also important to mention that our reconstructed 3D pose sequences are smoother and more natural than others.",117,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0VjG8eIAAAAJ&citation_for_view=0VjG8eIAAAAJ:u5HHmVD_uO8C,,1
1645,Improving simulation of continuous emotional facial expressions by analyzing videos of human facial activities,"Conversational agents are receiving significant attention from multi-agent and human computer interaction research societies. In order to make conversational agents more believable and friendly, giving them the ability to express emotions is one of research fields which have drawn a lot of attention lately. In this paper, we propose a work on analysis of how emotional facial activities happen temporally. Our goal is to find the temporal patterns of facial activity of six basic emotions in order to improve the simulation of continuous emotional facial expressions on a 3D face of an embodied agent. Using facial expression recognition techniques, we first analyze a spontaneous video database in order to consider how facial activities are related to six basic emotions temporally. From there, we bring out the general temporal patterns for facial expressions of the six basic emotions. Then, based on the temporal …",117,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0VjG8eIAAAAJ&citation_for_view=0VjG8eIAAAAJ:Y0pCki6q_DkC,,1
1646,A Vietnamese 3D taking face for embodied conversational agents,"Conversational agents are receiving significant attention from multi-agent and human computer interaction research societies. Many techniques have been developed to enable these agents to behave in a human-like manner. In order to do so, they are simulated with similar communicative channels as humans. Moreover, they are also simulated with emotion and personality. In this work, we focus on issue of expressing emotions for embodied-agents. We present a three dimensional face with ability to speak emotional Vietnamese speech and naturally express emotions while talking. Our face can represent lip movements during emotionally pronouncing Vietnamese words, and at the same time it can show emotional facial expressions while speaking. The face's architecture consists of three parts: Vietnamese Emotional Speech Synthesis module, Emotions to Facial Expressions module, and Combination module …",117,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0VjG8eIAAAAJ&citation_for_view=0VjG8eIAAAAJ:eQOLeE2rZwMC,,1
1647,Toward a rule-based synthesis of vietnamese emotional speech,"This paper presents a framework used to simulate four basic emotional styles of Vietnamese speech, by means of acoustic feature transplantation techniques applied to neutral utterances. First, it describes some analyses of acoustic features of Vietnamese emotional speech, accomplished to find the relations between prosodic, voice quality variations and emotional states in Vietnamese speech. Then the target pitch profiles together with duration, energy and spectrum constraints were obtained by applying rules which were inferred from the analysis results and based on the idea that when some emotional speech is synthesized from neutral speech, acoustic features are modified more in some syllables, instead of uniformly modified in all syllables. From there, neutral speech were morphed to produced synthesized speech with emotions. Results of perceptual tests show that emotional styles were well …",117,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0VjG8eIAAAAJ&citation_for_view=0VjG8eIAAAAJ:Tyk-4Ss8FVUC,,1
1648,A study on prosody of vietnamese emotional speech,"This paper describes the analyses of the prosody of Vietnamese emotional speech, accomplished to find the relations between prosodic variations and emotional states in Vietnamese speech. These relations were obtained by investigating the variations of prosodic features in Vietnamese emotional speech in comparison with prosodic features of neutral speech. The analyses were performed on a multi-style emotional speech database which consisted of Vietnamese sentences uttered in different styles. Specifically, four emotional styles were considered: happiness, sadness, cold anger, and hot anger. Speech data in the neutral style were also collected, and prosodic differences of each style with respect to this neutral baseline were quantified. The acoustic features related to prosody which were investigated were fundamental frequency, power, and duration. According to the analysis results, for each speaker of …",117,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0VjG8eIAAAAJ&citation_for_view=0VjG8eIAAAAJ:u-x6o8ySG0sC,,1
1649,When and how to smile: Emotional expression for 3D conversational agents,"Conversational agents have become more and more common in the multimedia worlds of films, educative applications, e - business, computer games. Many techniques have been developed to enable these agents to behave in a human-like manner. In order to do so, conversational agents are simulated with emotion and personality as well as communicative channels such as voice, head and eye movement, manipulator and facial expression. Up to now, creating facial expression from emotions has received much attention. However, most of the work concentrates on producing static facial expressions from emotions. In this paper, we propose a scheme for displaying continuous emotional states of a conversational agent on a 3D face. The main idea behind the scheme is that an emotional facial expression happens for a few seconds only when there is a significant change in the emotional states. This …",117,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0VjG8eIAAAAJ&citation_for_view=0VjG8eIAAAAJ:UeHWp8X0CEIC,,1
1652,An Ontology Web Application-based Annotation Tool for Intangible Culture Heritage Dance Videos,"Collecting dance videos, preserving and promoting them after enriching the collected data has been significant actions in preserving Intangible culture heritage in South-East Asia. Whereas techniques for the conceptual modeling of the expressive semantics of dance videos are very complex, they are crucial to exploit effectively the video semantics. This paper proposes an ontology web-based dance video annotation system for representing the semantics of dance videos at different granularity levels. Especially, the system incorporates both syntactic and semantic features of pre-built dance ontology system in order to not only use the available semantic web system but also to create unity for users when annotating videos to minimize conflicts.",117,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0VjG8eIAAAAJ&citation_for_view=0VjG8eIAAAAJ:YsMSGLbcyi4C,,1
1655,Feature Extraction for Non-frontal Faces,"One of the most challenge tasks in building a face recognition system is how to represent and extract good quality features from face images. The difficulties come from variations in head poses, illumination conditions, and facial expression. Although many researches have been done, most were carried on under constrained environments. Most researches concentrated on dealing with frontal faces. Processing non-frontal faces encounters more challenge because some features on faces become occluded dramatically. In this paper, we propose two models to extract features from non-frontal faces in the range of 30o to 90o. First, we use the Viola-Jones detection method to identify the pose of face images. Then, we use Active Appearance Model (AAM) to interpret face images. Lastly, the models are trained to know how to fit new images. To improve the efficiency of fitting, we apply a nonlinear parameter update method. Experimental results show that using nonlinear fitting for non-frontal can increase the accuracy of the AAM fitting, compared with some previous methods.",117,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0VjG8eIAAAAJ&citation_for_view=0VjG8eIAAAAJ:W7OEmFMy1HYC,,1
1656,Emotional Facial Expression Analysis in the Time Domain,"Emotions have been studied for a long time and results show that they play an important role in human cognitive functions. In fact, emotions play an extremely important role during the communication between people. And the human face is the most communicative part of the body for expressing emotions; it is recognized that a link exists between facial activity and emotional states. In order to make computer applications more believable and friendly, giving them the ability to recognize and/or express emotions are research fields which have been much focused on. Being able to perform these tasks, firstly, we need to have knowledge about the relationship between emotion and facial activity. Up to now, there have been proposed researches on this relationship. However, almost all these researches focused on analyzing the relationship without taking into account time factors. They analyzed the relationship …",117,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0VjG8eIAAAAJ&citation_for_view=0VjG8eIAAAAJ:d1gkVwhDpl0C,,1
1665,Estimating searching cost of regular path queries on large graphs by exploiting unit-subqueries.,"Regular path queries (RPQs) are widely used on a graph whose answer is a set of tuples of nodes connected by paths corresponding to a given regular expression. Traditional automata-based approach for evaluating RPQs is restricted in the explosion of graph size, which makes graph searching take high cost (i.e. memory space and response time). Recently, a cost-based optimization technique using rare labels has been proved to be effective when it is applied to large graph. However, there is still a room for improvement, because the rare labels in the graph and/or the query are coarse information which could not guarantee the minimum searching cost all the time. This is our motivation to find a new approach using fine-grained information to estimate correctly the searching cost, which helps improving the performance of RPQs evaluation. For example, by using estimated searching cost, we can decompose an RPQ into small subqueries or separate multiple RPQs into small batch of queries in an efficient way for parallelism evaluation. In this paper, we present a novel approach for estimating the searching cost of RPQs on large graphs with cost functions based on the combinations of the searching cost of unit-subqueries (i.e. every smallest possible query). We extensively evaluated our method on real-world datasets including Alibaba, Yago, Freebase as well as synthetic datasets. Experimental results show that our estimation method obtains high accuracy which is approximately 87% on average. Moreover, two comparisons with automata-based and rare label based approaches demonstrate that our approach outperforms traditional ones.",85,https://doi.org/10.1007/s10732-018-9402-0,1,
1666,New non-homogeneous Poisson process software reliability model based on a 3-parameter S-shaped function.,"Software reliability modelling is the mathematical technique used to evaluate the reliability of a software system. The non-homogeneous Poisson process is a prominent approach in this field. More than half of the models in this group are based on S-shaped functions, primarily the 2-parameter S-shaped function. This paper proposes a new model based on the 3-parameter S-shaped function, which is an expanded form of the 2-parameter S-shaped function obtained by adding a growth rate controller. Real data from industrial software development projects are used to verify the usability of the proposed model. The proposed model is shown to perform better than the existing models, especially with respect to the predictive performance. Furthermore, the rate of convergence of the proposed model is acceptable, with a rate of 76.47%.",85,https://doi.org/10.1049/sfw2.12055,1,
1667,Toward the Analysis of Graph Neural Networks.,"Graph Neural Networks (GNNs) have recently emerged as a robust framework for graph-structured data. They have been applied to many problems such as knowledge graph analysis, social networks recommendation, and even Covid19 detection and vaccine developments. However, unlike other deep neural networks such as Feed Forward Neural Networks (FFNNs), few analyses such as verification and property inferences exist, potentially due to dynamic behaviors of GNNs, which can take arbitrary graphs as input, whereas FFNNs which only take fixed size numerical vectors as inputs.
This paper proposes an approach to analyze GNNs by converting them into FFNNs and reusing existing FFNNs analyses. We discuss various designs to ensure the scalability and accuracy of the conversions. We illustrate our method on a study case of node classification. We believe that our approach opens new research directions for understanding and analyzing GNNs.",85,https://arxiv.org/abs/2201.00115,1,
1668,Usability and Aesthetics: Better Together for Automated Repair of Web Pages.,"With the recent explosive growth of mobile devices such as smartphones or tablets, guaranteeing consistent web appearance across all environments has become a significant problem. This happens simply because it is hard to keep track of the web appearance on different sizes and types of devices that render the web pages. Therefore, fixing the inconsistent appearance of web pages can be difficult, and the cost incurred can be huge, e.g., poor user experience and financial loss due to it. Recently, automated web repair techniques have been proposed to automatically resolve inconsistent web page appearance, focusing on improving usability. However, generated patches tend to disrupt the webpage's layout, rendering the repaired webpage aesthetically unpleasing, e.g., distorted images or misalignment of components.
In this paper, we propose an automated repair approach for web pages based on meta-heuristic algorithms that can assure both usability and aesthetics. The key novelty that empowers our approach is a novel fitness function that allows us to optimistically evolve buggy web pages to find the best solution that optimizes both usability and aesthetics at the same time. Empirical evaluations show that our approach is able to successfully resolve mobile-friendly problems in 94% of the evaluation subjects, significantly outperforming state-of-the-art baseline techniques in terms of both usability and aesthetics.",85,https://arxiv.org/abs/2201.00117,1,
1669,Link-Intensive Alignment for Incomplete Knowledge Graphs.,"Knowledge graph (KG) alignment - the task of recognizing entities referring to the same thing in different KGs - is recognized as one of the most important operations in the field of KG construction and completion. However, existing alignment techniques often assume that the input KGs are complete and isomorphic, which is not true due to the real-world heterogeneity in the domain, size, and sparsity. In this work, we address the problem of aligning incomplete KGs with representation learning. Our KG embedding framework exploits two feature channels: transitivity-based and proximity-based. The former captures the consistency constraints between entities via translation paths, while the latter captures the neighbourhood structure of KGs via attention guided relation-aware graph neural network. The two feature channels are jointly learned to exchange important features between the input KGs while enforcing the output representations of the input KGs in the same embedding space. Also, we develop a missing links detector that discovers and recovers the missing links in the input KGs during the training process, which helps mitigate the incompleteness issue and thus improve the compatibility of the learned representations. The embeddings then are fused to generate the alignment result, and the high-confidence matched node pairs are updated to the pre-aligned supervision data to improve the embeddings gradually. Empirical results show that our model is more accurate than the SOTA and is robust against different levels of incompleteness.",85,https://arxiv.org/abs/2112.09266,1,
1670,Probabilistic Method for Managing Common Risks in Software Project Scheduling Based on Program Evaluation Review Technique.,"Project management and project scheduling are crucial to help development teams keep track of timing as well as resource allocation. In order to manage software projects, project managers need to anticipate, analyze the risk factors that may occur as well as their impacts on the progress of the project, and assess and adapt the project resource allocation. This paper concentrates on a quantitative approach for risk analysis in software project scheduling by taking advantage of Bayesian networks capacity (including related mathematical calculations) in modeling and assessing uncertainty and incorporates them in software project scheduling with program evaluation and review technique (PERT). Common risk factors in project scheduling are also examined, and a Bayesian networks model of 19 common risk factors and their causal relationships is proposed and confirmed. The research also borrows and implements categories and levels of risk from construction projects into software projects. A tool was built to experiment and validate the proposed model.",85,https://doi.org/10.4018/IJITPM.2020070105,1,
1671,Automated Test Input Generation via Model Inference Based on User Story and Acceptance Criteria for Mobile Application Development.,"There has been observed explosive growth in the development of mobile applications (apps) for Android and iOS operating systems, which has led to the direct impact towards mobile app development. In order to design and propose quality-oriented apps, it is the primary responsibility of developers to devote time and sufficient efforts towards testing to make the apps bug-free and operational in the hands of end-users without any hiccup. Manual testing procedures take a prolonged amount of time in writing test cases, and in some cases, the full testing requirements are not met. Besides, the insufficient knowledge of tester also impacts the overall quality and bug-free apps. To overcome the obstacles of testing, we propose a new testing methodology cum tool called “AgileUATM” which works primarily towards white-box and black-box testing. To evaluate the validity of the proposed tool, we put the tool in a real-time operational environment concerning mobile test apps. By using this tool, all the acceptance criteria are determined via user stories. The testers/developers specify requirements with formal specifications based on programs properties, predicates, invariants, and constraints. The results show that the proposed tool generated effective and accurate test cases, test input. Meanwhile, expected output was also generated in a unified fashion from the user stories to meet acceptance criteria. The proposed solution also reduced the development time to identify test data as compared to manual Behavior-Driven Development (BDD) methodologies. This tool can support the developers to get a better idea about the required tests and able to translate the customer’s natural languages to computer languages as well. This paper fulfills an approach to suitably test mobile application development.",85,https://doi.org/10.1142/s0218194020500163,1,
1672,An Effective Approach for Context Driven Testing in Practice - A Case Study.,"Software testing is a continuous process during the software development stages to ensure quality software products. Researchers, experts and software engineers keep going on studying new techniques, methods and approaches of testing to accommodate changes in software development because of the flexible requirement along with the changing of technology. So, developers and testers need to have effective methods, tools and approaches to create a high-quality product at an efficient cost. This paper provides an effective approach for context-driven testing (CDT) in an agile software development process. CDT is a testing approach that supports the tester to choose their testing techniques and test objectives based on specific contexts. The aim of this paper is to propose an effective approach for implementing the CDT in practice, called CDTiP. Through an analysis of two case studies using an agile development process with different contexts, we validate the effectiveness of the approach in terms of test coverage, detect errors, test effort. The empirical results show that CDTiP is suitable for the agile development process that can help the tester to detect defects faster at minimum cost. The results of this method have been applied at Enclave, an ODC Software Engineering company, on real projects.",85,https://doi.org/10.1142/S0218194020500333,1,
1673,Monitoring Rice Crop with Dense Segmentation on Satellite Images: A case study in Vietnamese Mekong Delta.,"Rice is the major crop in Vietnam with the crucial roles in national food security and export. While the demand of this crop is increasing overtime, the land use for this purpose is restricted due to various factors such as urban expansion and seawater intrusion. Therefore, a sustainable and effective rice paddy area monitoring is highly desirable, as the traditional methods are costly, inaccurate and requires heavy human-related workload. In this paper, we propose a novel rice monitoring framework using the satellite image, thanks to its high quality and free-to-access. Our framework first preprocesses the raw satellite image to alleviate the effect of adversarial factors such as geometric discrepancy and solar radiation. Then we design a deep learning model inspiring from u-net architecture to exploit simultaneously the spatial, spectral and temporal nature of satellite images. The empirical result conducted over data collected in Vietnamese Mekong Delta shows that our framework outperforms others state-of-the-art baselines and achieves the accuracy of 92.7% without the need of any handcraft feature engineering or expert knowledge.",85,https://doi.org/10.1109/KSE50997.2020.9287368,1,
1674,A Method for Improvement the Parameter Estimation of Non-linear Regression in Growth Model to Predict Project Cost at Completion.,"In this paper, we propose a comparison between existing parameter estimation methods of a nonlinear regression-based growth model to forecast project duration as well as its cost at completion. To analyze and perform some experiments, we used the Gompertz growth model and the dataset comes from a number of similar previous studies. Two other nonlinear models were also applied to compare the results with the Gompertz model in terms of fitness-function score. The performance of the proposed methods is also the key for further studies in fitting the S- curve and predicting the Estimate-to-Complete and the Estimate-at-Completion of certain projects.",85,https://doi.org/10.1109/RIVF48685.2020.9140765,1,
1675,Generating Test Data for Blackbox Testing from UML-Based Web Engineering Content and Presentation Models.,"Software testing is a process that produces and consumes huge amounts of data. Thus, the test data is usually either gathered manually by the testers or randomly generated by tools. The manual method consumes lot of time and highly depends on the testers’ experience while the random approach faces the problem of redundant test data caused by identical use cases. By leveraging the concept of Model-based testing, this paper provides a novel method of testing to save the cost of manual testing and to increase the reliability of the testing processes. In Model-based testing, test cases and test data can be derived from different models. In this paper, we present a technique to generate test data from UML-based Web Engineering (UWE) presentation model for web application testing by using formal specification and Z3 SMT solver. We also build a model-based testing Eclipse Plug-in tool called TESTGER-UWE that generates test data based on the model of UWE for the web application. We evaluate the proposed methods by applying them to generate test data for an Address Book project of UWE. Experimental results show that our proposed methods can reduce the time significantly when generating test data for automation test tools such as Selenium, Katalon, Unit test, etc.",85,https://doi.org/10.1007/978-3-030-30149-1_17,1,
1676,Some Improvements of Using the NSGA-II Algorithm for the Problem of Resource Allocation and Scheduling and Its Applying to Inventory Management Strategies.,"Vendor-managed inventory (VMI) is an approach to prevent undesired stocking inventories and hence can lead to a cost reduction of the whole supply chain. One of the main objectives of this approach is to optimize the inventory buffer as safety stock and to optimize the scheduling of inventory and delivery. Such optimization could be considered as a problem of the project's resource scheduling and allocation. In this paper, we present some experimentations for solving this problem by implementing two different algorithms: (i) the Nondominated Sorting Genetic Algorithm (NSGA-II), and (ii) the multi-objective optimization algorithm provided by the MOEA framework. Based on the experimented results, we propose some improvements in using NSGA-II to define an optimized VMI strategy. Such a strategy is implemented and demonstrated through the data collected from a real VMI project.",85,https://doi.org/10.1109/KSE.2019.8919492,1,
1677,A Combinatorial Technique for Mobile Applications Software Testing.,"In this article, we propose a combinatorial testing technique for mobile application, thereby suggesting a combination of test-driven data generation techniques as well as developing a support tool named CTGen which using the IPO algorithm to generate test data with input model files, the tool also supports code generation for JUnit testing. The experiment results were compared in terms of test coverage, effort to build a test case with the PICT tool for positive results. Depending on the purpose of developer or tester, testing effort or reliability of application to use appropriate approaches and tools.",85,https://doi.org/10.1109/KSE.2019.8919456,1,
1678,A Scalable Approach for Dynamic Evacuation Routing in Large Smart Buildings.,"This paper considers the problem of dynamic evacuation routing in large smart buildings. We investigate a scalable routing approach which not only generates effective routes for evacuees but also quickly updates routes as the disaster status and building conditions could change during the evacuation time. We first design a flexible and scalable evacuation system for large smart buildings with multiple levels of computational support. Given such a system, we develop a novel distributed algorithm for finding effective evacuation routes dynamically by using an LCDT (Length-Capacity-Density-Trustiness) weighted graph model, which is built upon the current disaster information and building conditions. Finally, we propose a caching strategy which expedites dynamic route generation with the current effective route part(s) in order to improve the performance of dynamic evacuation in large buildings. To validate our approach, we test the proposed algorithm with our implementation of an evacuation simulator and compare the results with other approaches. Experimental results show that our approach outperforms other ones in the aspect of the evacuation time reduction and the maximum number of people being evacuated in each time span.",85,https://doi.org/10.1109/SMARTCOMP.2019.00065,1,
1679,A Genetic Algorithm for Large Graph Partitioning Problem.,"This paper considers the problem of partitioning large graphs. We propose a genetic algorithm to distribute large-scale graphs for parallel computations including individual representation, fitness function, and genetic operators. We first propose individual representation with the ability to change the number of clusters of the graph as well as the ability to flexibly change the cluster of each node. We then increase the diversity of the population by a random hybrid algorithm. Finally, we optimize the solution with the heuristic hybrid and the heuristic mutation algorithms. To validate our approach, we test the proposed algorithm and compare the results with other algorithms such as Greedy algorithm and Bulk Swap algorithm on both synthetic and real-world datasets. Experimental results show that our algorithm outperforms other ones in the aspect of the number of cross edges reduction among graph partitions.",85,https://doi.org/10.1145/3368926.3369724,1,
1680,Auto-Updating Portable Application Model of Multi-Cloud Marketplace Through Bidirectional Transformations System.,"At present, the cloud marketplace becomes more and more widely used for delivering cloud applications to consumers. The diversity of IaaS and PaaS services from many cloud providers gives customers many choices that benefit them the most. If a customer is not satisfied with his existing cloud resource service (IaaS or PaaS), he is going to stop using the service in use and consider other cloud service providers. And he also wants his cloud software bought on the market to be hosted on new cloud platforms. However, changing cloud resource services for multi-cloud application is not trivial. In this paper, we propose an approach that uses a Composable Application Model (CAM) to construct the topology of a multi-cloud application in a Blueprint. Thereby, all the changes of cloud platform services are reflected to the Blueprint. In this way, the cloud application is managed. Thus, cloud application operation is guaranteed after one or several of its software components are re-deployed on new cloud platform services and re-established application interconnections so that the operation of the cloud application is as an initial state. For updating the Blueprint, we built a bidirectional transformation system where the core is a bidirectional transformation program. We show how the Blueprint which is described by TOSCA-based specification is automatically correctly auto-updated.",85,https://doi.org/10.3233/FAIA190035,1,
1681,Applying a Unified Game-Based Model in a Payment Scheduling Problem and Design of Experiments Using MOEA Framework.,"This paper aims to put a new approach in the picture to the payment scheduling problem, which looks for a schedule that maximizes the benefit of all parties in a project. In a project, both sponsor and contractor seek to have a good payment strategy on their own. The timing of payments and the completion times of activities in projects are determined simultaneously in order to achieve an equitable schedule among the sponsor and the development team. In previous research, we developed a Unified Game-Based Model for conflicts in project management. In this paper, we applied this model to this problem, implemented in an open sourced evolutionary computation library named MOEA framework. The use of a Unified Game-Based Model enables us to figure out a suitable schedule for the problem, and in the tool, we conducted an experimental test of the model by the used of several multi-objective optimization algorithms. The experimental results demonstrated that the presented approach is effective and promising so that both parties could use this model to choose the proper tactics for each of them in scheduling payment",85,https://doi.org/10.3233/FAIA190038,1,
1682,Development of Rules and Algorithms for Model-Driven Code Generator with UWE Approach.,"UML-based Web Engineering (UWE) is an object-driven method for model-driven Web application development based on the UML modeling language. Focusing on the principles of the Model-Driven Engineering (MDE) paradigm, UWE helps to develop Web applications speedily and efficiently. This approach provides four separated models for representing different web-application concerns, i.e., content, navigation, processing and presentation models. In this paper, we exploit UWE to propose rules and algorithms in order to automatically generate code from different models. We also develop a model-driven code generator tool named CODEGER-UWE which is embedded with the proposed rules and algorithms. CODEGER-UWE has been experimented to semi-automatically generate code for a typical example, i.e., a web-based address book application. We then evaluate the performance of CODEGER-UWE with respect to some metrics including the number of generated source codes, the generation speed and the completion level.",85,https://doi.org/10.3233/FAIA190078,1,
1683,Formal Transformation from UML Sequence Diagrams to Queueing Petri Nets.,"In this paper, a formal strongly consistent transformation from UML Sequence diagrams (SDs) to Queueing Petri Nets (QPNs) is defined. Sequence diagrams are charts used to identify the sequence of event occurrences of a certain audience. QPNs are graphical formalisms, at a lower level of abstraction, for which efficient and mature simulation-based solution techniques are available. We show how the language of sequence diagrams is mapped onto an equivalent language of QPNs through formal transformation rules. We develop 12 general rules for formal SD-to-QPN transformation. We also present applying the proposed rules for a typical case study of a cloud service system. Experimental results are also provided.",85,https://doi.org/10.3233/FAIA190082,1,
1684,Shinobi: A Novel Approach for Context-Driven Testing (CDT) Using Heuristics and Machine Learning for Web Applications.,"Context-Driven Testing is widely used in the Agile World. It optimizes the testing value and provides an effective way to detect unexpected bugs. Context-driven testing requires the testing team to leverage the full knowledge and skills to solve the problem or to make a decision. In this paper, we propose an approach for Context-Driven Testing using Heuristics and Machine Learning for web applications with a framework called Shinobi. The framework can detect web controls, suggest a set of heuristic values, recognize the meaningful input data, and detect changes of application to recommend test ideas. In the context of improvising the testing performance, Shinobi is considered as Test Assistant for context-driven testers. Shinobi is a PoC to prove the idea of using Machine Learning to develop a Virtual Tester to improve the test quality and train junior testers as responsible testers. The framework is well integrated into all eCommerce projects at MeU Solutions which is a value-added advantage for testing.",85,https://doi.org/10.1007/978-3-030-05873-9_8,1,
1685,Development of the Rules for Transformation of UML Sequence Diagrams into Queueing Petri Nets.,"Sequence diagrams (SDs) are an abstraction of communication modeling between different entities, objects or classes. SDs are used to describe an execution trace of a particular system at a particular point in time. Queueing Petri Nets (QPNs) are graphical formalisms, at a lower level of abstraction, for which efficient and mature simulation-based solution techniques are available. This paper defines and explains the relationship between sequence diagrams and QPNs. Our approach can be used to transform sequence diagrams into QPNs. We presented the development of the model transformation solution to translate UML Sequence Diagrams (SDs) into equivalent QPNs. A case study of a new account opening for banking management system is used to illustrate the transformation rules.",85,https://doi.org/10.1007/978-3-030-05873-9_11,1,
1686,Risk Management in Agile Software Project Iteration Scheduling Using Bayesian Networks.,"Agile Software Development methods have been largely adopted in the last ten years since they have certain advantages over the traditional approaches. However, industrial software development processes are getting more and more complex and dynamic. As a consequence, optimization of software project scheduling has always been big challenges in both practice and academia, even with Agile methods. There is always uncertainty as well as a need for a probabilistic method that better model and predict uncertainty in software projects. This paper proposes Bayesian Networks to model risk factors in Agile software projects as well as managing risks in Agile iteration scheduling. The paper also addresses 19 common risk factors that affect iteration scheduling. Based on the method, a software was developed as a support tool for managers to control their project schedules as it can assess the possibility of each schedule.",85,https://doi.org/10.3233/978-1-61499-900-3-596,1,
1687,One2Explore - Graph Builder for Exploratory Testing from a Novel Approach.,"It is undeniable that visual aid is the most useful option to capture the objects than any methods by using text, log. With current trends of software development methods, instead of documenting requirements in hundreds of pages or planning projects, developers are using Scrum-board and Mind-map as tools to visualize their plans, project progress, and project requirements. To the given challenges of Exploratory Testing, one of the solutions is to track in detail all of what has been done by testers and visualize them in any form which easies to capture information. By using a case study, this study investigates the role of One2Explore as a visual tool for testing in MeU Company. The findings of this study indicate that using graphs as a method to display test execution and relevant information is an approach that can be beneficial in achieving effective testing.",85,https://doi.org/10.3233/978-1-61499-900-3-637,1,
1688,Research on Genetic Algorithm and Nash Equilibrium in Multi-Round Procurement.,"Recently, research papers which are related to artificial intelligence topic, especially decision support methodologies have received continuous concentration and achieved remarkable developments. Many articles have shown the important application of those methods in several aspects along with their effectiveness in most fields of research. In this paper we focus on approach using genetic algorithm and Nash equilibrium to solve the problem choosing appropriate bidders in multi-round procurement, which is currently considered an unsolved problem to many procuring entities. Instead of using manual and subjective consideration from procuring entities, a scientific methodology on decision-making support has been studied and identified equilibrium points in multiple-round procurements, which is the most beneficial to both investors and selected tenderers. These results can be a scientific promising solution for choosing bidders in multi-round procurement and ensure win-win relationship for all parties in procurement process.",85,https://doi.org/10.3233/978-1-61499-800-6-51,1,
1690,Applying PNZ Model in Reliability Prediction of Component-Based Systems and Fault Tolerance Structures Technique.,"Reliability is the chief quality that one wishes for in anything. Reliability is also the main issue with computer systems. One of the purposes of system reliability analysis is to identify the weakness in a system and to quantify the impact of component failures. However, existing reliability prediction approaches for component-based software systems are limited in their applicability because they either neglect or do not support modeling explicitly several factors like error propagation, software fault tolerance mechanisms. In this paper, we evaluate reliability prediction of component-based system and fault tolerance structures technique by applying Pham Nordmann Zhang (PNZ) model, one of the best models based on non homogeneous Poisson process. Our approach uses a reliability modeling schema whose models are automatically transformed by a reliability prediction tool into PNZ models for reliability predictions and sensitivity analyses. Via these our case studies, we demonstrate its applicability and introduce how much reliability of software system can be improved by using fault tolerance structures technique.",85,https://doi.org/10.1007/978-3-319-29236-6_27,1,
1692,New NHPP SRM Based on Generalized S-shaped Fault-Detection Rate Function.,"Software reliability modelling (SRM) is a mathematics technique to estimate some measures of computer system that relate to software reliability. One group of existing models is using non-homogeneous Poisson process (NHPP) whose fault-number and failure-rate are constant or time-dependent functions. A few studies have been manipulated S-shaped curve to construct their models. However, those works remain some limitations. In this study, we introduce a new model that is based on a generalised S-shaped curve and evaluate it by real data set. After installing it in real code of Matlab and using MLE method to estimate parameter with a range of initial solution, we prove that our model converge to the most basic model of NHPP group, Goel-Okumoto model.",85,https://doi.org/10.1007/978-3-319-15392-6_21,1,
1693,Developing Method for Optimizing Cost of Software Quality Assurance Based on Regression-Based Model.,"In this paper we present a method for Optimizing Cost of Software Quality Assurance base on Regression-based Model proposed by Omar AlShathry [1, 2]. Based on the regression-based model, regression analysis to estimate the number of defects in software, we propose an optimal method for software quality assurance based on the constraint conditions using linear programming techniques. The results of a detailed analysis of the theoretical and empirical models are presented and evaluated.",85,https://doi.org/10.1007/978-3-319-15392-6_24,1,
1694,"Proceedings of the Fifth Symposium on Information and Communication Technology, SoICT '14, Hanoi, Vietnam, December 4-5, 2014.","The Fifth Symposium on Information and Communication Technology (SoICT 2014) was held on December 4-5, 2014, in Hanoi, Vietnam. As the preceding editions, SoICT 2014 is a scientific forum which aims at bringing together researchers and practitioners for technical discussions and interactions on major computing topics. SoICT 2014 is organized by ACM Vietnam Chapter and School of Information and Communication Technology - Hanoi University of Science and Technology. This year, SoICT 2014 is organized in conjunction with iiWAS 2014 (The 16th International Conference on Information Integration and Web-based Applications & Services).",85,https://dl.acm.org/doi/proceedings/10.1145/2676585,1,
1697,Analysis of effectiveness of communication overheads in the parallel computing system using the closed product form queueing network.,"Assessment, computation and establishment of methods for decreasing communication overheads in parallel computing systems using multi-processor architectures are important problems. In the current research, the communication overhead was assessed using a new method that was executed based on components such as: effectiveness of the interconnecting network architecture, delay of nodes, initial communication time and transferring time of one data. The effectiveness of the communication overhead in the parallel computing system was also analyzed using the closed product form queueing network.",85,https://doi.org/10.1109/RIVF.2013.6719881,1,
1698,Modeling and debugging numerical constraints of cyber-physical systems design.,"To design and analyze Cyber-Physical Systems (CPSs), engineers should consider computation and physical processes at the same time. Engineers require a design description which is easy to understand. SysML provides diagrammatic notations which are independent of specific disciplines. To express numerical constraints of SysML diagrams, we extend OCL with real arithmetic. Both SysML and extended-OCL require an executable design language to check inconsistencies of constraints. To achieve the above idea, we bridge a gap between SysML and VDM-RT. We propose a framework to embed SysML/extended-OCL into VDM-RT, where we prepare some libraries for extended-OCL. It is unnecessary for engineers to know VDM-RT. Using a unified CPS design description, our approach automatically debugs inconsistencies in whole CPS design. This paper illustrates the proposed method with a car race system.",85,https://doi.org/10.1145/2542050.2542068,1,
1700,Development of an OPC UA SDK Based WCF Technology and Its Deployment for Environmental Monitoring Applications.,"This paper focuses on a technological innovation that has been applied in many countries and still has continuously been researched to enhance its effectiveness – OPC (Openness, Productivity, and Connectivity) Unified Architecture. OPC Unified Architecture (OPC UA) is new specification used for the connection of accessories, based on open form, and independent of any technological systems. OPC UA gains access to data base and events following its real time or are stored through safe, trustworthy and totally separate connection. The research presented in this paper attempts to introduce the development of an OPC UA SDK based WCF technology and used for monitoring and control systems. The deployment of the proposed OPC UA SDK for an environmental monitoring application (EMA) is also presented as an illustration of the successful development of the proposed OPC UA SDK. This SDK in turn makes system architects and developers easy to design and implement applications in terms of environmental monitoring. In addition, this also reduces the development time and cost for such applications.",85,https://doi.org/10.1007/978-3-642-36642-0_34,1,
1703,A general model for digitalizing human being's thought: MiMaS.,"A General Model For Digitalizing Human Being's Thought MIMAS proposes a general model including both hardware and software to determine and recognize some states of thinking of human. The hardware is a device that receives, pre-processes, and digitalizes brain wave signals which are then sent to the software to be processed. The software using some algorithms Independent Component Analysis, Expectation Maximization, and Gaussian mixture model classifier to process data, extract features, total up data and storage digitalized thinking of human. Our model has been experimented with over 50 persons and our accuracy is 87%.",85,https://doi.org/10.1145/1852611.1852615,1,
1704,Research and development of OPC client-server architectures for manufacturing and process automation.,"The OPC (Openness, Productivity, and Collaboration; formerly ""OLE for Process Control"") standards today have been accepted as industrial standards for designing and developing industrial applications in manufacturing and process automation. However, the development of real industrial applications using the OPC standards at universities is limited due to the complexity and difficulty. In this paper, not only the current state of the OPC standards is provided, but also the current problems and how they can be managed and overcome are discussed. The general integration architecture based on the new OPC Unified Architecture (UA) specifications is then proposed and presented together with the initial results from the performed simulation. The potential areas of the proposed architecture and the redundancy strategies to increase the efficiency and reliability of the system are also introduced.",85,https://doi.org/10.1145/1852611.1852642,1,
1706,A Basic Framework to Build a Test Collection for the Vietnamese Text Catergorization.,"The aim of this paper is to present a basic
framework to build a test collection for a
Vietnamese text categorization. The presented content includes our evaluations of
some popular text categorization test collections, our researches on the requirements, the proposed model and the techniques to build the BKTexts - test collection for a Vietnamese text categorization.
The XML specification of both text and
metadata of Vietnamese documents in the
BKTexts also is presented. Our BKTexts
test collection is built with the XML specification and currently has more than 17100
Vietnamese text documents collected from
e-newspapers.",85,https://aclanthology.org/I08-7019/,1,
1709,A Consensus-Based Load-Balancing Algorithm for Sharded Blockchains.,"Public blockchains are decentralized networks where each participating node executes the same decision-making process. This form of decentralization does not scale well because the same data are stored on each network node, and because all nodes must validate each transaction prior to their confirmation. One solution approach decomposes the nodes of a blockchain network into subsets called “shards”, each shard processing and storing disjoint sets of transactions in parallel. To fully benefit from the parallelism of sharded blockchains, the processing load of shards must be evenly distributed. However, the problem of computing balanced workloads is theoretically hard and further complicated in practice as transaction processing times are unknown prior to be assigned to shards. In this paper we introduce a dynamic workload-balancing algorithm where the allocation strategy of transactions to shards is periodically adapted based on the recent workload history of shards. Our algorithm is an adaptation to sharded blockchains of a consensus-based load-balancing algorithm. It is a fully distributed algorithm inline with network based applications such as blockchains. Some preliminary results are reported based on simulations that shard transactions of three well-known blockchain platforms.",86,https://doi.org/10.1007/978-3-030-91387-8_16,1,
1710,Lower-Bound Study for Function Computation in Distributed Networks via Vertex-Eccentricity.,"Distributed computing network-systems are modeled as graphs with vertices representing compute elements and adjacency-edges capturing their uni- or bi-directional communication. Distributed function computation has a wide spectrum of major applications in distributed systems. Distributed computation over a network-system proceeds in a sequence of time-steps in which vertices update and/or exchange their values based on the underlying algorithm constrained by the time-(in)variant network-topology. For finite convergence of distributed information dissemination and function computation in the model, we present a lower bound on the number of time-steps for vertices to receive (initial) vertex-values of all vertices regardless of underlying protocol or algorithmics in time-invariant networks via the notion of vertex-eccentricity in a graph-theoretic framework. We also address lower bounds on vertex-eccentricity and its maximum version in terms of common graph-parameters such as maximum degree, and order and size.",86,https://doi.org/10.1007/s42979-019-0002-3,1,
1711,"Designed Features for Improving Openness, Scalability and Programmability in the Fog Computing-Based IoT Systems.","Global-scale Internet of Things (IoT) applications commonly process a huge amount of data and quickly provide useful information (to users) and instructions to monitor or control the physical world; most of these services are required to be of low latency while satisfying resource constraints (computing, storage, networking). In order to reduce communication latency, network load, energy consumption and operational cost which are essential for real-world IoT applications, fog computing is a promising technology as it allows utilizing virtualized resources already available on the network edges which are close to IoT data generators (IoT devices) and consumers (users or actuators). However, there are still many challenges in realizing this computing paradigm as it is still in the infancy stage. This paper investigates and proposes designed features for the “openness,” “scalability” and “programmability” in fog computing to which IoT services/applications can be flexibly implemented in the fog landscape. We apply the proposed mechanism to a real-world application, namely the traffic light optimization problem, to validate its effectiveness and feasibility. The evaluation results reveal the effectiveness of the proposed approach.",86,https://doi.org/10.1007/s42979-020-00197-w,1,
1712,Extremal Problem with Network-Diameter and -Minimum-Degree for Distributed Function Computation.,"Distributed function computation has a wide spectrum of major applications in distributed systems. Distributed computation over a network-system proceeds in a sequence of time-steps in which vertices update and/or exchange their values based on the underlying algorithm constrained by the time-(in)variant network-topology. Distributed computing network-systems are modeled as directed/undirected graphs with vertices representing compute elements and adjacency-edges capturing their uni- or bi-directional communication. To quantify an intuitive tradeoff between two graph-parameters: minimum vertex-degree and diameter of the underlying graph, we formulate an extremal problem with the two parameters: for all positive integers n and d, the extremal value ∇(n,d) denotes the least minimum vertex-degree among all connected order-n graphs with diameters of at most d. We prove matching upper and lower bounds on the extremal values of ∇(n,d) for various combinations of n- and d-values.",86,https://doi.org/10.1007/s42979-020-00219-7,1,
1713,Message from the Program Chairs and Industry Panel Chairs.,"The International Conference on Advanced COMPuting and Applications (ACOMP) is an annual, multitrack international forum for academics, engineers, practitioners and research students to exchange their ideas, techniques, methods, and state-of-the-art applications for advanced computing. Initially formed as a scientific venue for high-performance computing & advanced applications, the conference kept expanding and had a pedigree of attracting international and Vietnamese participants who both are interested in advanced topics of computer science & engineering. The first occurrence of ACOMP was dated back to as early as 2007.",86,https://doi.org/10.1109/ACOMP50827.2020.00005,1,
1715,Invariant Properties and Bounds on a Finite Time Consensus Algorithm.,"Finite time consensus algorithms compute consensus values exactly and in a finite number of steps, contrasting with asymptotic consensus algorithms. In the literature, there exists few approaches deriving finite time convergence for discrete consensus algorithms. In this paper we focus on an analysis of finite time convergence based on the observability matrix for consensus networks. We introduce analytical results extending the applicability of network observability theory to consensus and other distributed algorithms. New analytical bounds on the number of steps to compute consensus are provided as well as counterexamples which are disproving a conjecture on the minimum of steps to compute consensus. A polynomial time algorithm is described to calculate empirically the exact number of steps to compute consensus values. We have implemented a consensus-based network intrusion detection system based on the observability matrix approach of consensus networks. This implementation validates empirically our analytical results. We also compare the performance of the finite time consensus with an implementation of the same intrusion detection system using asymptotic consensus. Although the finite time algorithm provides exact solutions, tests show that it needs less iterations to obtain a consensus solution.",86,https://doi.org/10.1007/978-3-662-58808-6_2,1,
1716,Lower Bound on Network Diameter for Distributed Function Computation.,"Parallel and distributed computing network-systems are modeled as graphs with vertices representing compute elements and adjacency-edges capturing their uni- or bi-directional communication. Distributed function computation covers a wide spectrum of major applications, such as quantized consensus and collaborative hypothesis testing, in distributed systems. Distributed computation over a network-system proceeds in a sequence of time-steps in which vertices update and/or exchange their values based on the underlying algorithm constrained by the time-(in)variant network-topology. For finite convergence of distributed information dissemination and function computation in the model, we study lower bounds on the number of time-steps for vertices to receive (initial) vertex-values of all vertices regardless of underlying protocol or algorithmics in time-invariant networks via the notion of vertex-eccentricity in a graph-theoretic framework. We prove a lower bound on the maximum vertex-eccentricity in terms of graph-order and -size in a strongly connected directed graph, and demonstrate its optimality via an explicitly constructed family of strongly connected directed graphs.",86,https://doi.org/10.1007/978-3-030-35653-8_16,1,
1717,Openness in Fog Computing for the Internet of Things.,"Fog computing is a promising technology for global-scale Internet of Things (IoT) applications as it allows moving resources (computing, storage, networking) close to IoT devices, thus helping to reduce communication latency, network load, energy consumption and operational cost. In order to realize these advantages, application and management services provided by fog landscape should be flexibly implemented. This paper investigates the “Openness” in fog computing and proposes a suitable scheme where the “Openness” can be conveniently implemented in fog nodes. We applied the proposed mechanism to a real-world application, namely the traffic light optimization (TLO) problem, to validate its effectiveness and feasibility. The evaluation results reveal the effectiveness of the proposed approach.",86,https://doi.org/10.1007/978-3-030-35653-8_23,1,
1718,Minimum Graph Partition with Basis.,"In this paper, we introduce the minimum graph partition problem with basis. A graph partition with basis is a partition of the node set into connected parts, each contains exactly one element of the basis set. The objective is to minimize the size of the largest part. We propose three different mixed-integer linear models to solve this problem, enhanced with a heuristic procedure. Experimental tests are conducted to explore the efficiency of the models on wireless sensor networks. These tests also show that the best model could solve some problem instances with up to 400 nodes in 30 minutes.",86,https://doi.org/10.1145/3368926.3369674,1,
1720,Scheduled service network design with resource acquisition and management.,"We present a new planning model for freight consolidation carriers, one that links strategic, resource acquisition, and allocation decisions with tactical, service network design-related decisions. Specifically, such as service network design models that recognize resource constraints, the model selects services and routes both commodities and the resources needed to support the services that transport them. In addition, the model recognizes that resources can be grouped into types that differ from one another with respect to capabilities, e.g., speeds, capacities, scheduling rules, etc. Ultimately, along with recognizing resource constraints, the model also makes strategic decisions such as how many resources of each type should be acquired, to what terminal new resources should be assigned, and which existing terminal-based resources should be reassigned. As such, the model can be used from a strategic planning, resource acquisition, mixing, and allocation perspective as it provides an estimate of the impact of such decisions on transportation costs. We extend a matheuristic for a service network design problem with a fixed set (both in number and allocation) of resources of a single type to one that can also make these acquisition and allocation decisions for multiple types of resource. Then, with an extensive computational study, we demonstrate the efficacy of the matheuristic and benchmark its performance against both a leading commercial solver and a column generation-based heuristic. Finally, we perform an extensive computational study to understand how the resource-related and service network design-related components of the model interact, including how freight volumes and cost structures impact how many resources should beacquired.",86,https://doi.org/10.1007/s13676-017-0103-x,1,
1721,Applicability and Resilience of a Linear Encoding Scheme for Computing Consensus.,Consensus algorithms have a set of network nodes converge asymptotically to a same state which depends on some function of their initial states. At the core of these algorithms is a linear iterative scheme where each node updates its current state based on its previous state and the state of its neighbors in the network. In this paper we review a proposal from control theory which uses linear iterative schemes of asymptotic consensus and observability theory to compute consensus states in a finite number of iterations. This proposal has low communication requirements which makes it attractive to address consensus problems in a limited resource environment such as edge computing. However it assumes static networks contrary to wireless edge computing networks which are often dynamic and prone to attacks. The main purpose of this paper is to assess the network conditions and attack scenarios where this algorithm can still be considered useful in practice to address consensus problems in ,86,https://doi.org/10.5220/0006806001730184,1,
1722,Multi-trip pickup and delivery problem with time windows and synchronization.,"In this paper, we consider two-tiered city logistics systems accounting for both the inbound and outbound traffic, that have not been taken into account in models and algorithms for vehicle routing research. The problem under study, called the Multi-trip Pickup and Delivery Problem with Time Windows and Synchronization, has two sets of intertwined decisions: the routing decisions which determine the sequence of customers visited by each vehicle route, the scheduling decisions which plan movements of vehicles between facilities within time synchronization restrictions. We propose a tabu search algorithm integrating multiple neighborhoods targeted to the decision sets of the problem. To assess the proposed algorithm, tests have been conducted on the first benchmark instances of the problem which have up to 72 facilities and 7200 customer demands. As no previous results are available in the literature for the problem, we also evaluate the performance of the method through comparisons with published results on two simplified problems: the Multi-zone multi-trip vehicle routing problem with separate delivery and collection, and the Vehicle routing problem with backhauls. The proposed algorithm is competitive with existing exact and meta-heuristic methods for these two problems.",86,https://doi.org/10.1007/s10479-015-2001-7,1,
1723,Defense Strategies against Byzantine Attacks in a Consensus-Based Network Intrusion Detection System.,"The purpose of a Network Intrusion Detection System (NIDS) is to monitor network trac such to detect malicious usages of network facilities. NIDSs can also be part of the a　ected network facilities and be the subject of attacks aiming at degrading their detection capabilities. The present paper investigates such vulnerabilities in a recent consensus-based NIDS proposal [1]. This system uses an average consensus algorithm to share information among the NIDS modules and to develop coordinated responses to network intrusions. It is known however that consensus algorithms are not resilient to compromised nodes sharing falsied information, i.e. they can be the target of Byzantine attacks. Our work proposes two di　erent strategies aiming at identifying compromised NIDS modules sharing falsied information. Also, a simple approach is proposed to isolate compromised modules, returning the NIDS into a non-compromised state. Validations of the defense strategies are provided through several simulations of Distributed Denial of Service attacks using the NSL-KDD data set. The eciency of the proposed methods at identifying compromised NIDS nodes and maintaining the accuracy of the NIDS is compared. The computational cost for protecting the consensus-based NIDS against Byzantine attacks is evaluated. Finally we analyze the behavior of the consensus-based NIDS once a compromised module has been isolated.",86,http://www.informatica.si/index.php/informatica/article/view/1665,1,
1724,Multihop Wireless Access Networks for Flood Mitigation Crowd-Sourcing Systems.,"Natural disasters can be mitigated or even anticipated if we have appropriate means, in terms of communications and data sharing models, to collect relevant data in advance or during disaster occurrences, which can be used for supporting disaster prevention and recovery processes. This work proposes a framework that encourages people to collect and share data about disaster, especially flood in Ho Chi Minh City, via on-site established multihop wireless access networks configured by the sharing of internet connectivity in users’ mobile devices. For connectivity sharing, on-the-fly establishment of multihop wireless access network (OEMAN) scheme is thoroughly analyzed and improved to resolve its inherent issue on traffic load imbalance due to its tree-based structure. More specifically, we propose a linear program for overload-aware routing optimization considering wireless interference. Evaluations implemented in Matlab show that the overload-aware routing improves load balancing among available virtual access points in OEMAN. By avoiding nodes with heavy load in the network, our solution improves network throughput compared to overload-unaware routing protocols.",86,https://doi.org/10.1007/978-3-662-56266-6_5,1,
1725,An Exact Consensus-Based Network Intrusion Detection System.,"In a recent work Toulouse et al. [1] introduced a fully distributed network intrusion detection system (NIDS) based on an average consensus algorithm. In this initial work, modules of the NIDS repeatedly average their state with the state of their neighbors to converge asymptotically to a same value, which in turn is used as measurement of some relevant state of the network wide monitored traffic. In the present work, local averaging is used to implement a finite convergence procedure for the consensus-based NIDS in [1]. We call this implementation exact consensus as local averaging computes exactly in a finite number of steps a function of the initial NIDS states. Furthermore, unlike asymptotic consensus which computed only the average sum function, this new distributed protocol can compute almost any function of the initial NIDS states. Tests are performed that compare the asymptotic consensus with this new exact consensus protocol. In particular, we compare the convergence speed of the two methods given a same pre-defined level of accuracy in the decisions computed by the intrusion detection system.",86,https://doi.org/10.1007/978-3-319-70004-5_25,1,
1726,Protecting Consensus Seeking NIDS Modules against Multiple Attackers.,"This work concerns distributed consensus algorithms and application to a network intrusion detection system (NIDS) [21]. We consider the problem of defending the system against multiple data falsification attacks (Byzantine attacks), a vulnerability of distributed peer-to-peer consensus algorithms that has not been widely addressed in its practicality. We consider both naive (independent) and colluding attackers. We test three defense strategy implementations, two classified as outlier detection methods and one reputation-based method. We have narrowed our attention to outlier and reputation-based methods because they are relatively light computationally speaking. We have left out control theoretic methods which are likely the most effective methods, however their computational cost increase rapidly with the number of attackers. We compare the efficiency of these three implementations for their computational cost, detection performance, convergence behavior and possible impacts on the intrusion detection accuracy of the NIDS. Tests are performed based on simulations of distributed denial of service attacks using the KSL-KDD data set.",86,https://doi.org/10.1145/3155133.3155185,1,
1727,Toward a Crowdsourcing-Based Urban Flood Mitigation Platform.,"Urban flood management and mitigation is one of the critical missions to maintain a stable city development. Most of the current available flood mitigation approaches are using data from fixed-site sensor systems which are not only costly but also limited in their coverage. Crowdsourcing is one of the potential approaches for addressing the coverage issue where flood data is collected and shared by crowds available everywhere. However, existing crowdsourcing-based approaches to flood mitigation systems lack thorough solutions on data validity. This paper proposes an appropriate crowdsourced data validity scheme which relies on both the scores evaluated by other users in the crowds and the accumulated reputation obtained by the user who shares the data. The paper also presents a practical mechanism to cluster reported data based on spatial and temporal information to improve the effectiveness of the data analytics. The proposed approaches have been implemented in an urban flood mitigation platform prototype running on both the IOS and the Android mobile devices. This prototype preliminarily shows the appropriateness and the usefulness of the proposed solutions.",86,https://doi.org/10.1145/3155133.3155153,1,
1728,Service Network Design with Resource Constraints.,"We first present a new service network design model for freight consolidation carriers, one that selects services and routes both commodities and resources needed to support the services that transport them, while explicitly recognizing that there are limits on how many resources are available at each terminal. We next present a solution approach that combines column generation, meta-heuristic, and exact optimization techniques to produce high-quality solutions. We demonstrate the efficacy of the approach with an extensive computational study and benchmark its performance against both a leading commercial solver and a column generation-based heuristic.",86,https://doi.org/10.1287/trsc.2014.0525,1,
1729,Routing Optimization Model in Multihop Wireless Access Networks for Disaster Recovery.,"On-the-fly establishment of multihop wireless access networks (OEMAN) is a new wireless communication approach to quickly establish temporary Internet connectivity in a disaster region. OEMAN creates virtual access points on mobile devices of disaster victims to reach still-alive access points of the Internet so that rescuers can be informed about their status and position. The simple routing strategy in OEMAN is based on a tree like topology raising some traffic load balancing issues at nodes closed to the root. In this paper, we propose a linear program for overload-aware routing. Moreover, we consider wireless interference and integrate it into our routing optimization model. Our evaluations implemented in Matlab show that the overload-aware routing improves load balancing among available virtual access points in OEMAN. By avoiding nodes with heavy load in the network, our solution improves network throughput compared to overload-unaware routing.",86,https://doi.org/10.1109/ACOMP.2016.028,1,
1730,Robust consensus-based network intrusion detection in presence of Byzantine attacks.,"Consensus algorithms provide strategies to solve problems in a distributed system with the added constraint that data can only be shared between adjacent computing nodes. We find these algorithms in applications for wireless and sensor networks, spectrum sensing for cognitive radio, even for some IoT services. However, consensus-based applications are not resilient to compromised nodes sending falsified data to their neighbors, i.e. they can be the target of Byzantine attacks. Several solutions have been proposed in the literature inspired from reputation based systems, outlier detection or model-based fault detection techniques in process control. We have reviewed some of these solutions, and propose two mitigation techniques to protect the consensus-based Network Intrusion Detection System in [1]. We analyze several implementation issues such as computational overhead, fine tuning of the solution parameters, impacts on the convergence of the consensus phase, accuracy of the intrusion detection system.",86,https://doi.org/10.1145/3011077.3011121,1,
1731,A hybrid generational genetic algorithm for the periodic vehicle routing problem with time windows.,"We propose a new population-based hybrid meta-heuristic for the periodic vehicle routing problem with time windows. This meta-heuristic is a generational genetic algorithm that uses two neighborhood-based meta-heuristics to optimize offspring. Local search methods have previously been proposed to enhance the fitness of offspring generated by crossover operators. In the proposed method, neighborhood-based meta-heuristics are used for their capacity to escape local optima, and deliver optimized and diversified solutions to the population of the next generation. Furthermore, the search performed by the neighborhood-based meta-heuristics repairs most of the constraint violations that naturally occur after the application of the crossover operators. The genetic algorithm we propose introduces two new crossover operators addressing the periodic vehicle routing problem with time windows. The two crossover operators are seeking the diversification of the exploration in the solution space from solution recombination, while simultaneously aiming not to destroy information about routes in the population as computing routes is NP-hard. Extensive numerical experiments and comparisons with all methods proposed in the literature show that the proposed methodology is highly competitive, providing new best solutions for a number of large instances.",86,https://doi.org/10.1007/s10732-014-9244-3,1,
1732,A tabu search for Time-dependent Multi-zone Multi-trip Vehicle Routing Problem with Time Windows.,"We propose a tabu search meta-heuristic for the Time-dependent Multi-zone Multi-trip Vehicle Routing Problem with Time Windows. Two types of neighborhoods, corresponding to the two sets of decisions of the problem, together with a strategy controlling the selection of the neighborhood type for particular phases of the search, provide the means to set up and combine exploration and exploitation capabilities for the search. A diversification strategy, guided by an elite solution set and a frequency-based memory, is also used to drive the search to potentially unexplored good regions and, hopefully, enhance the solution quality. Extensive numerical experiments and comparisons with the literature show that the proposed tabu search yields very high quality solutions, improving those currently published.",86,https://doi.org/10.1016/j.ejor.2013.05.026,1,
1733,A three-phase matheuristic for capacitated multi-commodity fixed-cost network design with design-balance constraints.,"This paper proposes a three-phase matheuristic solution strategy for the capacitated multi-commodity fixed-cost network design problem with design-balance constraints. The proposed matheuristic combines exact and neighbourhood-based methods. Tabu search and restricted path relinking meta-heuristics cooperate to generate as many feasible solutions as possible. The two meta-heuristics incorporate new neighbourhoods, and computationally efficient exploration procedures. The feasible solutions generated by the two procedures are then used to identify an appropriate part of the solution space where an exact solver intensifies the search. Computational experiments on benchmark instances show that the proposed algorithm finds good solutions to large-scale problems in a reasonable amount of time.",86,https://doi.org/10.1007/s10732-013-9225-y,1,
1734,A distributed p-cycle protection scheme in multi-domain optical networks.,"Providing protection in multi-domain optical networks amounts to ensuring protection for the inter-domain connections. Due to scalability issues, almost all previous studies focused on heuristics. In this study, we propose a large scale optimization ILP model, which allows the exact solution of quite large instances, under the assumption of a distributed network management. The model relies on p-cycles in order to protect the inter-domain links, while FIPP p-cycles are used for the protection in each individual domain, and for the virtual links in between the inter-domain links. Experiments were successfully conducted on a multi-domain network with 5 domains. They include a comparison of bandwidth requirements between the proposed distributed scheme and a centralized scheme.",86,https://doi.org/10.1109/GLOCOM.2012.6503577,1,
1735,A multilevel tabu search algorithm for the feature selection problem in biomedical data.,"The automated analysis of patients’ biomedical data can be used to derive diagnostic and prognostic inferences about the observed patients. Many noninvasive techniques for acquiring biomedical samples generate data that are characterized by a large number of distinct attributes (i.e., features) and a small number of observed patients (i.e., samples). Using these biomedical data to derive reliable inferences, such as classifying a given patient as either cancerous or noncancerous, requires that the ratio  of the number of samples to the number of features be within the range . To satisfy this requirement, the original set of features in the biomedical data has to be reduced to an ‘optimal’ subset of features that most enhances the classification of the observed patients.

In this paper, we propose a new feature selection technique (multilevel feature selection) that seeks the ‘optimal’ feature subset in biomedical data using a multilevel search algorithm. This algorithm combines a hierarchical search framework with a tabu search method. The framework consists of increasingly coarse forms (i.e., search subspaces) of the original feature space that are strategically and progressively explored by the tabu search method. The result of the search at any given coarse subspace is used to initialize the search at the previous less coarse subspace.

We evaluate the performance of the proposed technique in terms of the solution quality, using experiments that compare the classification inferences derived from the solution found, with those derived from other feature selection techniques such as the sequential forward selection, random feature selection and tabu search feature selection. An equivalent amount of computational resource is allocated to the evaluated techniques to provide a common basis for comparison. The empirical results show that the multilevel feature selection technique finds ‘optimal’ subsets that enable more accurate and stable classification than those obtained by using the other feature selection techniques.",86,https://doi.org/10.1016/j.camwa.2006.12.098,1,
1736,Explicit and Emergent Cooperation Schemes for Search Algorithms.,"Cooperation as problem-solving and algorithm-design strategy is widely used to build methods addressing complex discrete optimization problems. In most cooperative-search algorithms, the explicit cooperation scheme yields a dynamic process not deliberately controlled by the algorithm design but inflecting the global behaviour of the cooperative solution strategy. The paper presents an overview of explicit cooperation mechanisms and describes issues related to the associated dynamic processes and the emergent computation they often generate. It also identifies a number of research directions into cooperation mechanisms, strategies for dynamic learning, automatic guidance, and self-adjustment, and the associated emergent computation processes.",86,https://doi.org/10.1007/978-3-540-92695-5_8,1,
1738,A first multilevel cooperative algorithm for capacitated multicommodity network design.,"We describe the first multilevel cooperative tabu search for the capacitated multicommodity network design problem. Main design challenges are associated to the specification of the problem instance addressed at each level in cooperation, as well as to the definition of the cooperation operators. The paper proposes a first approach to address these challenges and tests it on a set of well-known benchmark problems. The proposed method appears competitive, particularly when difficult problems with many commodities are considered. Directions and challenges for future research are identified and discussed.",86,https://doi.org/10.1016/j.cor.2005.07.015,1,
1740,Variations of the maximum leaf spanning tree problem for bipartite graphs.,"The maximum leaf spanning tree problem is known to be NP-complete. In [M.S. Rahman, M. Kaykobad, Complexities of some interesting problems on spanning trees, Inform. Process. Lett. 94 (2005) 93–97], a variation on this problem was posed. This variation restricts the problem to bipartite graphs and asks, for a fixed integer K, whether or not the graph contains a spanning tree with at least K leaves in one of the partite sets. We show not only that this problem is NP-complete, but that it remains NP-complete for planar bipartite graphs of maximum degree 4. We also consider a generalization of a related decision problem, which is known to be polynomial-time solvable. We show the problem is still polynomial-time solvable when generalized to weighted graphs.",86,https://doi.org/10.1016/j.ipl.2005.10.011,1,
1742,A Cooperative Multilevel Tabu Search Algorithm for the Covering Design Problem.,"This work describes an adaptation of multilevel search to the covering design problem. The search engine is a tabu search algorithm which explores several levels of overlapping search spaces of a t–(v,k,λ) covering design problem. Tabu search finds “good” approximations of covering designs in each search space. Blocks from those approximate solutions are transferred to other levels, redefining the corresponding search spaces. The dynamics of cooperation among levels tends to regroup good approximate solutions into small search spaces. Tabu search has been quite effective at finding re-combinations of blocks in small search spaces which provide successful search directions in larger search spaces.",86,https://doi.org/10.1007/11740698_11,1,
1743,Visualizing Time Dependent Semantics: An Application to Quantum Algorithms.,We have developed a visual syntax for representing concepts that are contingent on temporal properties (time-dependent semantics). A within-group (N=24) experiment was conducted to identify the representations that conveyed best a given semantic. We then applied our representations to the visualization of algorithms in quantum computing and carried out a second experiment (N=16) on subjects unfamiliar with the semantic concepts that were tested. The results show that our representations are intuitive and facilitate a good level of understanding of the algorithms.,86,https://doi.org/10.1109/IV.2005.154,1,
1744,Systemic behavior of cooperative search algorithms.,Distributed computer systems may be used in various ways to find good solutions to difficult combinatorial optimization problems. An interesting approach consists in executing concurrently different search methods that exchange information gathered in previously explored regions of the solution space. This cooperation mechanism strongly impacts how the solution space is explored. We introduce a formal representation of such cooperative procedures based on discrete-time dynamical systems. We describe how the search behavior of cooperative programs depends on systems of complex and correlated interactions. We derive the notion of systemic cooperation and illustrate through simulations the relevance of this notion to the understanding of the search behavior of cooperative procedures.,86,https://doi.org/10.1016/j.parco.2002.07.001,1,
1745,Parallel Strategies for Meta-Heuristics.,"We present a state-of-the-art survey of parallel meta-heuristic developments and results, discuss general design and implementation principles that apply to most meta-heuristic classes, instantiate these principles for the three meta-heuristic classes currently most extensively used—genetic methods, simulated annealing, and tabu search, and identify a number of trends and promising research directions.",86,https://doi.org/10.1007/0-306-48056-5_17,1,
1747,Multilevel cooperative search for the circuit/hypergraphpartitioning problem.,"The objectives in this paper are twofold: design an approach for the netlist partitioning problem using the cooperative multilevel search paradigm introduced by Toulouse et al. and study the effectiveness of this paradigm for solving combinatorial optimization problems, in particular, those arising in the very large scale integration (VLSI) computer-aided design (CAD) area. The authors present a cooperative multilevel search algorithm CoMHP and describe a parallel implementation on the SGI O2000 system. Experiments on ISPD98 benchmark suite of circuits show, for four-way and eight-way partitioning, a reduction of 3% to 15% in the size of hyperedge cuts compared to those obtained by hMETIS. Bisections of hypergraphs based on the algorithm also outperform hMETIS, although more modestly. The authors present experimental results to demonstrate that the cooperation scheme plays a key role in the performance of CoMHP. In fact, the improvement in the quality of the solutions produced by CoMHP is to a large extent independent of the partitioners used in the implementation of CoMHP. The experimental results also demonstrate the effectiveness of the cooperative multilevel search paradigm for solving the netlist partitioning problem.",86,https://doi.org/10.1109/TCAD.2002.1004312,1,
1748,Supporting Co-Allocation in an Auctioning-Based Resource Allocator for Grid Systems.,"we present the overall design for an auctioning based resource trading/acquiring system that can be deployed in wide-area computing systems such as Grid systems. Selecting the winning bids is one of the core issues in any system that utilizes the auctioning paradigm. We identify the unique aspects of our system that impact the winner selection process. More specifically, the necessity to acquire or trade resources as a bundle (i.e., perform co-allocation) presents a challenge to traditional bidding mechanisms. We present a new bidding mechanism called “co-bids” to address this problem. Two heuristics for winner selection with co-bids are proposed. The performance of the heuristics are examined via simulations.",86,https://doi.org/10.1109/IPDPS.2002.1015666,1,
1749,Global optimization properties of parallel cooperative search algorithms: A simulation study.,"Cooperative search is a parallelization strategy where parallelism is obtained by concurrently executing several search programs for the same optimization problem instance. The programs cooperate by exchanging information on previously explored regions of the solution space. When the sharing of information overlaps among several programs, changes in the search behavior of one program can propagate over time to several other programs; this is a process called diffusion in physical systems. The optimization properties of diffusion dynamics in cooperative algorithms have not been formally established. However, it is generally believed that when the selection of shared information is biased by the cost (objective) function, diffusion dynamics help to improve the search of cooperating programs. In this study, we simulate this aspect of cooperative algorithms using cellular automata (CAs) (these are artificial dynamical systems often used to simulate the dynamics of complex systems). Our results show that the sharing of information based on the cost function does not affect the diffusion dynamics and therefore does not seem to help the optimization strategy of cooperating programs. However, this study increases our understanding of the role played by diffusion processes in cooperative algorithms. We suggest new approaches that can help to subordinate diffusion dynamics to the optimization goals of the search programs.",86,https://doi.org/10.1016/S0167-8191(99)00097-6,1,
1751,Multi-level Cooperative Search: A New Paradigm for Combinatorial Optimization and an Application to Graph Partitioning.,Cooperative search is a parallelization strategy for search algorithms where parallelism is obtained by concurrently executing several search programs. The solution space is implicitly decomposed according to the search strategy of each program. The programs cooperate by exchanging information on previously explored regions of the solution space. In this paper we propose a new design for cooperative search algorithms which is also a new parallel problem solving paradigm for combinatorial optimization problems. Our new design is based on an innovative approach to decompose the solution space which is inspired from the modeling of cooperative algorithms based on dynamical systems theory. Our design also gives a new purpose to the sharing of information among cooperating tasks based on principles borrowed from scatter search evolutionary algorithms. We have applied this paradigm to the graph partitioning problem. We describe the parallel implementation of this algorithm on a cluster of workstations and compare our results with other well known graph partitioning methods.,86,https://doi.org/10.1007/3-540-48311-X_75,1,
1752,Self-organization in cooperative tabu search algorithms.,"The search history of memory based heuristics like tabu search can be used to design a category of parallel algorithms, called cooperative search. These algorithms execute in parallel several search programs on the same optimization problem instance. At run time, the data gathered in the memory by one sequential search program need not be used only by this program, but it can be recycled and shared with other concurrently executing tabu search programs for the same purpose. In this paper we compare the global behavior of cooperating and non-cooperating tabu search programs. We show that cooperating programs tend to have a search pattern which is less diversified than non-cooperating programs. Our findings also indicate that this second order impact of the sharing of gathered data on the search behaviors of cooperating programs is not related to the optimization properties of the individual tabu programs.",86,https://doi.org/10.1109/ICSMC.1998.725012,1,
1753,Toward a Taxonomy of Parallel Tabu Search Heuristics.,"In this paper we present a classification of parallel tabu search metaheuristics based, on the one hand, on the control and communication strategies used in the design of the parallel tabu search procedures, and on the other hand, on how the search space is partitioned. These criteria are then used to review the parallel tabu search implementations described in the literature. The taxonomy is further illustrated by the results of several parallelization implementations of a tabu search procedure for multicommodity location-allocation problems with balancing requirements.",86,https://doi.org/10.1287/ijoc.9.1.61,1,
1754,Parallel asynchronous tabu search for multicommodity location-allocation with balancing requirements.,"We study and compare asynchronous parallelization strategies for tabu search, and evaluate the impact on performance and solution quality of some important algorithmic design parameters: number of processors, handling of exchanged information, etc. Parallelization approaches are implemented and compared by using a tabu search algorithm for multicommodity location-allocation problems with balancing requirements.",86,https://doi.org/10.1007/BF02125458,1,
1755,CLAP: an object-oriented programming system for distributed memory parallel machines.,"The Agha and Hewitt Actor model [1,3,4] is a natural extension of the object paradigm as we know it to the field of parallel programming. The model respects the primary principles of object-oriented programming, i.e. abstraction of data and encapsulation, making object-oriented programming a good tool for the programming of parallel computers. We describes one Actor model implementation on a distributed memory parallel machine: the CLAP actor language. We explain why the Actor model can express, on distributed memory parallel machines, the natural parallelism of applications and offer other properties useful for programming this type of machine. We then discuss the implementation, currently in its final phase, of the Actor model on a distributed memory parallel machine.",86,https://doi.org/10.1145/182078.182088,1,
1756,An Actor Based Programming System for Heterogeneous Processing.,"épidaure, an Actor based programming environment, is presented. The Actor programming approach is combined with the distributed shared memory (DSM) abstraction. Rather than using processes as compounding structure, memory structures are used. The defined actor segment is a programming abstraction that goes across process and processor boundaries and shields applications from several problems related to heterogeneous programming.",86,https://doi.org/10.1007/3-540-58184-7_149,1,
1759,Proceedings of the Sixth International Symposium on Information and Communication Technology.,"The Sixth Symposium on Information and Communication Technology (SoICT 2015) was held on December 3-4, 2015, in Hue, Vietnam. SoICT 2015 is an international academic forum for researchers and graduate students to share their latest research findings and to identify future challenges in computer science. In 2015, the symposium was organized by ACM Vietnam Chapter, the School of Information and Communication Technology - Hanoi University of Science and Technology, Hue University's College Education and Duy Tan University.",87,https://dl.acm.org/doi/proceedings/10.1145/2833258,1,
1760,UIT at VBS 2022: An Unified and Interactive Video Retrieval System with Temporal Search.,"This paper introduces our multimedia retrieval system for the Video Browser Showdown 2022 competition. The system was built for interactive retrieval task in a large video collection by focusing on four fundamental methods. First, we allow users to search by object features such as position and color. Secondly, our system also supports searching by text instances appearing in video segments. Next, we support searching by visual-textual association. And finally, the system can also search for videos containing a specific audio category. Moreover, we extend our framework to support temporal queries for all of the mentioned features.",89,https://doi.org/10.1007/978-3-030-98355-0_54,1,
1761,Flocking control for two-dimensional multiple agents with limited communication ranges.,"In all existed researches on flocking control for multi-agent systems, each agent is modelled as a single point or covered by an circle, ellipse or rectangle; however, in case of limited space traffic systems, many complex vehicles cannot be fitted in these agent shapes. This paper proposes a distributed control law for two-dimensional multiple agents considered as arbitrarily polygonal shapes under limited communication ranges. The proposed control laws guarantee all flocking properties: each agent stays close to its nearby neighbours, no collision occurs in group of agents and velocity of each agent converges to a common vector. The designed control law based on a new anti-collision condition and potential function approach aims at dealing with the issue of collisions between agents. Further, the control signals are bounded and differentiable despite of agent's limited communication ranges and non-smooth boundary in agent shapes. Numerical simulations are implemented to demonstrate the performance of proposed control law for multiple agents with the same and different shapes.",89,https://doi.org/10.1080/00207179.2019.1707878,1,
1762,Multiband Ambient RF Energy Harvester with High Gain Wideband Circularly Polarized Antenna toward Self-Powered Wireless Sensors.,"In this work toward a sustainable operation of a self-powered wireless sensor, we investigated a multiband Wi-Fi/3G/4G/5G energy harvester based on a novel wideband circularly polarized antenna, a quadplexer, and rectifiers at four corresponding bands. This proposed antenna consisted of four sequentially rotated dual-dipoles, fed by a hybrid feeding network with equal amplitude and an incremental 90° phase delay. The feeding network was composed of three Wilkinson power dividers and Schiffman phase shifters. Based on the sequential rotation method, the antenna obtained a −10 dB reflection coefficient bandwidth of 71.2% from 1.4 GHz to 2.95 GHz and a 3 dB axial ratio (AR) bandwidth of 63.6%, from 1.5 GHz to 2.9 GHz. In addition, this antenna gain was higher than 6 dBi in a wide bandwidth from 1.65 GHz to 2.8 GHz, whereas the peak gain was 9.9 dBi. The quad-band rectifier yielded the maximum AC–DC conversion efficiency of 1.8 GHz and was 60% at −1 dBm input power, 2.1 GHz was 55% at 0 dBm, 2.45 GHz was 55% at −1 dBm, and 2.6 GHz was 54% at 0.5 dBm, respectively. The maximum RF–DC conversion efficiency using the wideband circularly polarized antenna was 27%, 26%, 25.5%, and 27.5% at −6 dBm of input power, respectively.",89,https://doi.org/10.3390/s21217411,1,
1763,"Multi-Decadal Changes in Mangrove Extent, Age and Species in the Red River Estuaries of Viet Nam.","This research investigated the performance of four different machine learning supervised image classifiers: artificial neural network (ANN), decision tree (DT), random forest (RF), and support vector machine (SVM) using SPOT-7 and Sentinel-1 images to classify mangrove age and species in 2019 in a Red River estuary, typical of others found in northern Viet Nam. The four classifiers were chosen because they are considered to have high accuracy, however, their use in mangrove age and species classifications has thus far been limited. A time-series of Landsat images from 1975 to 2019 was used to map mangrove extent changes using the unsupervised classification method of iterative self-organizing data analysis technique (ISODATA) and a comparison with accuracy of K-means classification, which found that mangrove extent has increased, despite a fall in the 1980s, indicating the success of mangrove plantation and forest protection efforts by local people in the study area. To evaluate the supervised image classifiers, 183 in situ training plots were assessed, 70% of them were used to train the supervised algorithms, with 30% of them employed to validate the results. In order to improve mangrove species separations, Gram–Schmidt and principal component analysis image fusion techniques were applied to generate better quality images. All supervised and unsupervised (2019) results of mangrove age, species, and extent were mapped and accuracy was evaluated. Confusion matrices were calculated showing that the classified layers agreed with the ground-truth data where most producer and user accuracies were greater than 80%. The overall accuracy and Kappa coefficients (around 0.9) indicated that the image classifications were very good. The test showed that SVM was the most accurate, followed by DT, ANN, and RF in this case study. The changes in mangrove extent identified in this study and the methods tested for using remotely sensed data will be valuable to monitoring and evaluation assessments of mangrove plantation projects.",89,https://doi.org/10.3390/rs12142289,1,
1764,Ontology-based heuristic patent search.,"Large collections of patent documents disclosing novel, non-obvious technologies are publicly available and beneficial to academia and industries. To maximally exploit its potential, searching these patent documents has increasingly become an important topic. Although much research has processed a large size of collections, a few studies have attempted to integrate both patent classifications and specifications for analyzing user queries. Consequently, the queries are often insufficiently analyzed for improving the accuracy of search results. This paper aims to address such limitation by exploiting semantic relationships between patent contents and their classification",89,https://doi.org/10.1108/IJWIS-06-2018-0053,1,
1765,Vibration Suppression Control of a Flexible Gantry Crane System with Varying Rope Length.,The paper presents a control approach to a flexible gantry crane system. From Hamilton’s extended principle the equations of motion that characterized coupled transverse-transverse motions with varying rope length of the gantry is obtained. The equations of motion consist of a system of ordinary and partial differential equations. Lyapunov’s direct method is used to derive the control located at the trolley end that can precisely position the gantry payload and minimize vibrations. The designed control is verified through extensive numerical simulations.,89,https://doi.org/10.1155/2019/9640814,1,
1766,GeTCo: an ontology-based approach for patent classification search.,"The main contribution of this paper is a method for creating a Graph-Embedded-Tree-based ontology, which utilizes domain knowledge from a patent classification scheme, for a patent classification process. Our contribution is twofold. First, we propose a novel definition of GeTCo ontology, which consists of four types of concept: Class, Document, Phrase, and Term. Depending on relationships of each pair of concepts, we further define their semantic information to give our classifier better reasoning capability whenever the semantic ambiguation occurs. Second, we propose a novel method to construct our ontology based on the United State Patent Classification Scheme (USPC) without relying on a rule-based method for concept extraction and thus, it can negate intensive-manual efforts in traditional ontology construction. We developed a prototype application on top of Rocchio classifier, called the GeTCo-enabled Rocchio classifier, to evaluate our proposed ontology. Our experiments with filtered 9703 single-class patents showed that the GeTCo-enabled Rocchio classifier, backed by our proposed directed-graph ontology, yields higher F1-score (i.e., +7%) than original Rocchio classifier without GeTCo supports.",89,https://doi.org/10.1145/3011141.3011205,1,
1767,GeTFIRST: ontology-based keyword search towards semantic disambiguation.,"This paper aims to improve the semantic-disambiguation capability of an information-retrieval system by taking advantages of a well-crafted classification tree. The unstructured nature and sheer volume of information accessible over networks have made it drastically difficult for users to seek relevant information. Many information-retrieval methods have been developed to address this problem, and keyword-based approach is amongst the most common approach. Such an approach is often inadequate to cope with the conceptualization associated with user needs and contents. This brings about the problem of semantic ambiguation that refers to the disagreement in meaning of terms between involving parties of a communication due to polysemy, leading to increased complexity and lesser accuracy in information integration, migration, retrieval and other related activities.",89,https://doi.org/10.1108/IJWIS-06-2015-0019,1,
1768,Multi-layered semantic search for open innovation needs.,"Under open-innovation paradigm, firms may gain beneficial from cost reductions and external-workforce collaborations. These advantages have increasingly drawn researchers' attentions to the ways that technology solutions could be retrieved to address firms' technology needs. However, prior studies were either (i) manual and effort-intensive or (ii) focused on data analytics instead of providing a technology solution for a technology need. In this paper, we propose a novel multi-layered semantic search to tackle the above problems. First, we propose a multi-layered search approach in which each layer encompasses a semantic capability for acquiring relevant technology solutions. Second, we propose a novel multi-layered similarity-measurement strategy, called NSk, to estimate the relatedness between a technology <u>n</u>eed and a technology <u>s</u>olution, with respect to the layer k. Our prototype OIG (short for <u>O</u>pen <u>I</u>nnovation <u>G</u>ate) is evaluated through a case study of finding technology solutions for a real-world technology need requiring ""facial recognition"". The study conveyed the feasibility of applying our proposed approach to search for technology solutions under open-innovation paradigm.",89,https://doi.org/10.1145/2837185.2837214,1,
1769,MobiPSE: A scenario-based mobile-application development for end-user developers.,"This paper aims to solve the stated limitations through (i) exploring usages of scenarios and (ii) proposing a lightweight iterative requirement exploration process that utilizes scenarios to transform the vague ideas of the developers into concrete application’s vision. The explosive growth in pervasiveness, programmability and computational capability of smartphones and tablets unlocks the possibility for many uses of technology, including end-user development (EUD) where end-users create applications as pragmatic tools for their work and hobbies. An extensive amount of research effort has been done to solve EUD challenges; however, the focus of such research lies mainly on designing and implementing software applications.",89,https://doi.org/10.1108/IJPCC-03-2014-0024,1,
1770,Optimizing the Stretch for Virtual Screening Application on Pilot-agent Platforms on Grid/Cloud by using Multi-level Queue-based Scheduling.,"Virtual screening has proven very effective on grid infrastructures. We focus on finding platform scheduling policy for pilot-agent platform shared by many virtual screening users. They need a suitable scheduling algorithm at platform level to ensure a certain fairness between users. Optimal criterion used in our research is the stretch, a measure for user experience on the platform. From our latest research (Quang et al., 2013), simulation result and experimentation on real pilot agent platform showed that SPT policy is the best policy in 4 different existing scheduling policies (FIFO, SPT, LPT and Round Robin) for optimizing the stretch. However, research on real grid workload (Medernach, 2005) showed that there are two types of grid user: normal users who submit frequently little jobs to grid and data challenge users who submit occasionally large number of jobs to grid. And SPT policy, in particularly, is not appropriate for data challenge user because they have to wait always nor ( ",89,https://doi.org/10.5220/0004962801990204,1,
1771,GeT-based Ontology Construction for Semantic Disambiguation.,"Semantic Ambiguation refers to the disagreement in meaning of terms used by parties in communication due to polysemy, leading to increased complexity and lesser accuracy in information integration, migration, retrieval and related activities. Semantic Disambiguation can be performed through establishing and reconciling relative positions of parties in communication by matching their terms' meanings to a common domain-specific ontology -- a knowledge representation showing concepts, organized into domains, and relationships between them. In this paper, based on the wide existence and limitations of established hierarchical ontology in a form of Classification Schemes, we proposed a novel ontology structure combining the structured nature of hierarchy with expressive capability of graphs, called Graph-embedded Tree (GeT), and a novel approach to construct a GeT-based Ontology. Evaluation was performed on United States Patent Classification System (USPC); the results showed that the information retrieval backed by GeT-based ontology yields better disambiguation capability than other typical patent search methods.",89,https://doi.org/10.1145/2684200.2684320,1,
1773,Structured content-aware discovery for improving XML data consistency.,"With the explosive growth of heterogeneous XML sources, data inconsistency has become a serious problem that leads to ineffective business operations and poor decision-making. To address such inconsistency, XML functional dependencies (XFDs) have been proposed to constrain the data integrity of a source. Unfortunately, existing approaches to XFDs have insufficiently addressed data inconsistency arising from both semantic and structural inconsistencies inherent in heterogeneous XML data sources. This paper proposes a novel approach, called SCAD, to discover anomalies from a given source, which is essential to address prevalent inconsistencies in XML data. Our contribution is twofold. First, we introduce a new type of path and value-based data constraint, called XML Conditional Structural Dependency (XCSD), whereby (i) the paths in XCSD approximately represent groups of similar paths in sources to express constraints on objects with diverse structures; while (ii) the values bound to particular elements express constraints with conditional semantics. XCSD can capture data inconsistency disregarded by XFDs.

Second, our proposed SCAD is used to discover XCSDs from a given source. Our approach exploits the semantics of data structures to detect similar paths from the sources, from which a data summary is constructed as an input for the discovery process. This aims to avoid returning redundant data rules due to structural inconsistencies. During the discovery process, SCAD employs semantics hidden in the data values to discover XCSDs. To evaluate our proposed approach, experiments and case studies were conducted on synthetic datasets which contain structural diversity causing XML data inconsistency. The experimental results show that SCAD can discover more dependencies and the dependencies found convey more meaningful semantics than those of the existing XFDs.",89,https://doi.org/10.1016/j.ins.2013.06.050,1,
1774,Scheduling of virtual screening application on multi-user pilot-agent platform on grid/cloud to optimize the stretch.,"In this paper, we present our research on the scheduling of virtual screening platform on grid/cloud which is shared by many users. We find the scheduling policy to ensure the fairness between users. We evaluate two policies in existing platform (FIFO and Round Robin) and two candidate policies from literature (SPT and LPT) by our simulator. Simulation result showed that SPT improve performance of scheduling policies in existing platform.",89,https://doi.org/10.1145/2506583.2512369,1,
1775,Influenza-A Circulation in Vietnam through Data Analysis of Hemagglutinin Entires.,"Hemagglutinin is a glycoprotein locating on surface of influenza virus. Influenza A human hemagglutinin protein H1, H2, H3, H5, H7, H9 sequences on NCBI database from Vietnam were retrieved and analyzed to find out the pattern of viral circulation within the country in the last 12 years. 1060 sequences were collected and analyzed. The results revealed five main influenza A virus including H5N1, H3N2, H1N1, H6N2, and H9N2 circulating in Vietnam with high level of genetic compatibility, identified as reassortants between a wide variety viral genotypes and share common mechanisms of spread. The most two popular host are human and poultry. The two recent pandemics avian H5N1 in 2005 and Swine H1N1 in 2009 did show significant effects on the viral circulation in Vietnam. H5N1 and H1N1 subtype remain pathogenic and require close surveillance.",89,https://doi.org/10.1109/ICCSA.2013.42,1,
1776,MobiPDA: A Systematic Approach to Mobile-Application Development.,"The advent of mobile-computing platforms and profitable application-distributing channels has increasingly attracted a large number of non-professional people to mobile application development. They typically start with a simple idea about the applications of their interest; and to complete these applications, they often face two key problems: (i) the lack of a systematic method for exploring ideas and (ii) the lack of a method for organizing activities to keep balance between learning, designing and coding. Unfortunately, existing approaches have inadequately address these problems. In this paper, we propose a novel approach, named MobiPDA, to mobile application development targeting non-professional developers. Our systematic approach provides (i) conceptual tools for exploring an initial idea from different perspectives and (ii) an activities-organization process to apply these tools in software projects. We performed a comparative analysis with Mobile-D and Mobia, and evaluated MobiPDA on two pilot software projects.",89,https://doi.org/10.1007/978-3-642-39640-3_17,1,
1777,A grid-based multistage algorithm for parameter simulation-optimization of complex system.,"Evolutionary algorithms (EA) are recently used to explore the parameter space of complex system simulations as the methodology sees models as black boxes. The first advantage is that these algorithms become independent from what kind of simulation has to be explored. The task is finding the parameter settings to optimize a given objective function. This optimization process evaluates the performance of possible parameter sets and converges towards the best alternatives. The evaluation needs to launch hundreds of thousands of simulation runs. This procedure copes with the combinatorial explosion of computation time and requires considerable computational resources. Furthermore, the original algorithms cannot guarantee the exploration in the search space uniformly and equally because the search is probabilistic. This work elaborates a multistage optimization process in a grid-enabled modeling and simulation platform. We propose a hybrid integration of various continuous optimization algorithms and optimize them for running with different Distributed Resource Management (DRM) systems. The performance of algorithm is compared to original algorithm in the optimization of Ants model.",89,https://doi.org/10.1109/RIVF.2013.6719897,1,
1779,Mining Hierarchical Negative Association Rules.,"The negative association between items in databases is as important and interesting as the positive one. But, it has not been studied as much. We consider negative association in a hierarchical setting, in which we are able to generate negative association rules at different hierarchy levels. It allows to impose restrictions when we proceed to the next level and discover only most interesting negative association rules among the vast number of possible negative association rules. In this paper, we propose two algorithms for mining negative association rules by considering that items are organized in a hierarchy, and this hierarchy is reflected on the association rules we produce. In this way, we can mine for both general and specialized rules of negative association between items.",89,https://doi.org/10.1080/18756891.2012.696905,1,
1780,FPGA-based fuzzy sliding mode control for sensorless PMSM drive.,"This paper presents an observer-based fuzzy sliding mode controller for sensorless Permanent Magnet Synchronous Motor (PMSM) drive based on the Field Programmable Gate Array (FPGA) technology. For enhancement of robustness, a sliding mode observer (SMO) is proposed to estimate first the current and back electromotive force (EMF), then to derive the flux angle. These estimated values together with the computed rotor speed of the motor are fed back for the control purpose in both the current loop and the speed loop. To cope with dynamic uncertainty and external load, a fuzzy sliding mode control (FSMC) is designed by incorporating a fuzzy inference mechanism into the proposed sliding mode control scheme to tune the discontinunous gain in the speed control loop. An FPGA chip is designed for implementing the vector-controlled current loop as well as the speed control loop. The very high speed integrated circuit-hardware description language (VHDL) is adopted to describe advantageous behaviors of the proposed control system. By integrating advantages of the sensorless and fuzzy sliding mode control techniques into the speed controller of a PMSM drive, its performance can be substantially enhanced while improving cost-effectiveness and reliability. The validity of the proposed approach is verified through results based on the VDHL Modelsim and Simulink co-simulation method.",89,https://doi.org/10.1109/CoASE.2012.6386449,1,
1781,Double-layered schema integration of heterogeneous XML sources.,"Schema integration aims to create a mediated schema as a unified representation of existing heterogeneous sources sharing a common application domain. These sources have been increasingly written in XML due to its versatility and expressive power. Unfortunately, these sources often use different elements and structures to express the same concepts and relations, thus causing substantial semantic and structural conflicts. Such a challenge impedes the creation of high-quality mediated schemas and has not been adequately addressed by existing integration methods. In this paper, we propose a novel method, named XINTOR, for automating the integration of heterogeneous schemas. Given a set of XML sources and a set of correspondences between the source schemas, our method aims to create a complete and minimal mediated schema: it completely captures all of the concepts and relations in the sources without duplication, provided that the concepts do not overlap. Our contributions are fourfold. First, we resolve structural conflicts inherent in the source schemas. Second, we introduce a new statistics-based measure, called path cohesion, for selecting concepts and relations to be a part of the mediated schema. The path cohesion is statistically computed based on multiple path quality dimensions such as average path length and path frequency. Third, we resolve semantic conflicts by augmenting the semantics of similar concepts with context-dependent information. Finally, we propose a novel double-layered mediated schema to retain a wider range of concepts and relations than existing mediated schemas, which are at best either complete or minimal, but not both. Performed on both real and synthetic datasets, our experimental results show that XINTOR outperforms existing methods with respect to (i) the mediated-schema quality using precision, recall, F-measure, and schema minimality; and (ii) the execution performance based on execution time and scale-up performance.",89,https://doi.org/10.1016/j.jss.2010.07.055,1,
1782,Improving Retake Detection by Adding Motion Feature.,"Retake detection is useful for many applications of video summarization. It is a challenging task since different takes of the same scene are usually of different lengths; or have been recorded under different environment conditions. A general approach to solve this problem is to decompose the input video sequence into sub-sequences and then group these sub-sequences into clusters. By combining with temporal information, the clustering result is used to find take and scene boundaries. One of the most difficult steps in this approach is to cluster sub-sequences. Most of previous approaches only use one keyframe for representing one sub-sequence and extract features such as color and texture from this keyframe for clustering. We propose another approach to improve the performance by combining the motion feature extracted from each sub-sequence and the features extracted from each representing keyframe. Experiments evaluated on the standard benchmark dataset of TRECVID BBC Rushes 2007 show the effectiveness of the proposed method.",89,https://doi.org/10.1007/978-3-642-24088-1_16,1,
1785,Grid-based International Network for Flu Observation (g-INFO).,"The 2009 H1N1 outbreak has demonstrated that continuing vigilance, planning, and strong public health research capability are essential defenses against emerging health threats. Molecular epidemiology of influenza virus strains provides scientists with clues about the temporal and geographic evolution of the virus. In the present paper, researchers from France and Vietnam are proposing a global surveillance network based on grid technology: the goal is to federate influenza data servers and deploy automatically molecular epidemiology studies. A first prototype based on AMGA and the WISDOM Production Environment extracts daily from NCBI influenza H1N1 sequence data which are processed through a phylogenetic analysis pipeline deployed on EGEE and AuverGrid e-infrastructures. The analysis results are displayed on a web portal (http://g-info.healthgrid.org) for epidemiologists to monitor H1N1 pandemics.",89,https://doi.org/10.3233/978-1-60750-583-9-215,1,
1786,Schema Mediation for Heterogeneous XML Schema Sources.,"Due to the increasingly widespread use of XML, many XML- related applications require the service of schema mediation, which is to find semantically similar elements from two or more schema sources. Current approaches to schema mediation require much improvement. First, they do not cope well with a large number of schemas since most involve repeating pairwise integration. Second, they do not provide sufficiently flexible solution because most mediated schemas commit to some particular choice regarding the representation of relationships between elements. This paper proposes a novel schema mediation approach, called XMediator, to address these problems. It exploits the structural properties of XML schemas, and generates a mediated schema from multiple schema sources. Experimental evaluations on both synthetic and real datasets show the effectiveness of XMediator.",89,https://doi.org/10.1109/WAINA.2009.111,1,
1787,Mediation-Based XML Query Answerability.,"This paper presents a novel mediation-based query answering approach which allows users (1) to reuse their own predefined queries to retrieve information properly from their local data source, and (2) to reformulate those queries in terms of remote data sources in order to obtain additional relevant information. The problem of structural diversity in XML design (e.g. nesting discrepancy and backward paths) makes it difficult to reformulate the queries. Therefore, we highlight the importance of precise query rewriting using composite concepts and relations of the mediated schema. Our experimental evaluations on real application datasets show that our approach effectively obtains correct answers over a broad diversity of schemas.",89,https://doi.org/10.1007/978-3-540-88873-4_43,1,
1788,XMiner: Mining XML Mediated Schemas.,"This paper presents a novel schema mediation approach, called XMiner, for mining mediated schemas from a set of XML schemas. XMiner addresses three main problems resulting from the heterogeneous source schemas: nesting discrepancy, backward paths and schema discrepancy. XMiner discovers frequent substructures using frequent subtree mining algorithms, and then constructs a mediated schemas. XMiner aims to preserve the hierarchical structure as the best as possible while avoiding information loss. XMiner exploits structural context, forward/backward paths, and label semantics for matching, mapping and merging frequent substructures. Experiments on real and synthetic datasets are reported to show that XMiner offers acceptable performance and quality for large-scale application scenarios.",89,https://doi.org/10.1109/WIIAT.2008.301,1,
1789,XML Schema Design Approach.,"This paper presents a systematic XML Schema design approach which conceptually captures semantics of the problem domain at conceptual level and represents such semantics in XML Schema at schema level. At the conceptual level, objects, their inter‐relationships and constraints are semantically powered by object‐oriented models. At the schema level, these conceptual semantics are comprehensively represented in textbased representation of XML Schema using various schema components and design styles, each of which offers different quality characteristics. Two primary design styles in use are nesting and linking. The nesting design styles are developed based on the choice of schema components and their definition/declaration scopes (global vs. local), whereas the linking design styles use referencing facilities provided by XML Schema and other XML technologies such as XLink and XPointer. With an in‐depth analysis of outstanding problems of existing approaches, the proposed design approach is motivated to help improve the quality and robustness of the XML documents in large‐scale XML‐based applications.",89,https://doi.org/10.1108/17440080580000091,1,
1791,Integrating QoS management in a micro-kernel based UNIX operating system.,"We present a prototype that supports distributed applications handling continuous media such as digital audio and video with Quality of Service requirements. It provides Quality of Service guarantees, rather than a best effort behaviour. Our prototype is based on a modular UNIX operating system in which system resources (such as CPU, network, and file system) are managed by individual components, that can be enhanced to supply the QoS. The global QoS is managed by a component that coordinates the behaviour of the managers and that interfaces the applications. We show how this model has been implemented in a distributed operating system, namely Chorus/MiX, for supporting both applications handling continuous media and standard applications.",89,https://doi.org/10.1109/EURMIC.1997.617317,1,
1793,Experience with Chorus.,"This paper summarizes works done at I.N.T. with Chorus Operating System. It briefly describes Chorus' concepts and abstractions useful to understand our work.

Then it focuses on Chorus micro-kernel capabilities and explains how these capabilities have been used to make an experiment allowing the cohabitation between the Chorus micro-kernel and the Macintosh Operating system.

Then, it describes how a new subsystem can be built over the micro-kernel. As an example, it shows how a subsystem which emulates the Macintosh Operating System has been built.

Then, it explains how new capabilities can be integrated into an existing subsystem. To illustrate this point, it gives two examples of work we are currently doing on Chorus/MiX running on PCs. The first project adds “Quality of Service” support for distributed multimedia applications; the second one allows “Fault Tolerant” aspects to be taken into account in distributed applications.

Finally, we discuss some limitations of Chorus, especially in supporting large networks, and how the system should be extended to address this new feature.",89,https://doi.org/10.1007/3-540-60609-2_13,1,
1794,An ensemble multi-objective particle swarm optimization approach for exchange rates forecasting problem.,"In this paper, the authors propose an ensemble multi-objective particle swarm optimisation approach (named EMPSO) for forecasting the currency exchange rate chain. The proposed algorithm consists of two main phases. The first phase uses a multi-objective particle swarm optimisation algorithm to find a set of the best optimal particles (named leaders). The second phase then uses these leaders to jointly calculate the final results by using the soft voting ensemble method. The two objective functions used here are predictive error and particle diversity. The empirical data used in this study are six different sets of currency exchange rates. Through comparison results with other evolutionary algorithms and other multi-objective PSO algorithms, the proposed algorithm shows that it can achieve better as well as more stability results on experimental data sets.",92,https://doi.org/10.1145/3380688.3380717,1,
1795,A novel evolutionary multi-objective ensemble learning approach for forecasting currency exchange rates.,"Due to the potential impact of the (currency) exchange rate risk in the financial market, forecasting exchange rate (FET) has become a hot topic in both academic and practical worlds. For many years, the various methods have been proposed and used for FET problems including the method of the artificial neural network (ANN). However, in many cases of FET, there is the limitation of using separate methods since they are not able to fully capture financial characteristics. Recently, more researchers have been beginning to pay attention to FET based on an ensemble of forecasting models (in other words, the combination of individual methods). Previous studies of ensemble methods have shown that the performance of an ensemble depends on two key elements (1) The individual performance and (2) diversity degree of base learners. The main idea behind this paper comes from these key elements, the authors use ANNs as the base method (or weak learners), and weights of these ANNs will be optimized by using multi-objective evolutionary algorithms (MOEAs) including the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) and the Non-Dominated Sorting Differential Evolution (NSDE) using directional information. To assist MOEAs, a number of diversity-preservation mechanisms are used to generate diverse sets of base classifiers and finally we propose to use modified Adaboost algorithms to combine the results of weak learners for overall forecasts. The results show that the proposed novel ensemble learning approach can achieve higher forecasting performance than those of individual ones.",92,https://doi.org/10.1016/j.datak.2017.07.001,1,
1796,A multi-objective ensemble learning approach based on the non-dominated sorting differential evolution for forecasting currency exchange rates.,"Currency exchange rates forecasting is paid a considerable attention of the researchers in the field of forecasting. The neural network is a well-known tool in machine learning. However, two issues are always interested by the scientists: getting toward to global convergence of extreme solutions and determining the optimal weight of the network. This paper proposes the multi-objective method of ensemble learning techniques based on the non-dominated sorting differential evolution (NSDE, a kind of direction-based methods) for training neural networks and application in Foreign Exchange forecasting problems. Two objectives of the selected model are defined based on the Mean Squared Errors and Diversity respectively, in which we used the concept of fitness-sharing based diversity. We experimented the model on four data sets of currency and compared with some of the others that the research community has announced. Through the performance forecasting indicators to show that our new method gives outstanding forecasting results.",92,https://doi.org/10.1109/KSE.2016.7758036,1,
1797,Transfer learning in Genetic Programming.,"Transfer learning is a process in which a system can apply knowledge and skills learned in previous tasks to novel tasks. This technique has emerged as a new framework to enhance the performance of learning methods in machine learning. Surprisingly, transfer learning has not deservedly received the attention from the Genetic Programming research community. In this paper, we propose several transfer learning methods for Genetic Programming (GP). These methods were implemented by transferring a number of good individuals or sub-individuals from the source to the target problem. They were tested on two families of symbolic regression problems. The experimental results showed that transfer learning methods help GP to achieve better training errors. Importantly, the performance of GP on unseen data when implemented with transfer learning was also considerably improved. Furthermore, the impact of transfer learning to GP code bloat was examined that showed that limiting the size of transferred individuals helps to reduce the code growth problem in GP.",92,https://doi.org/10.1109/CEC.2015.7257018,1,
1798,Saltwater Intrusion Forecast of the Pleistocene Aquifer Caused by Groundwater Exploiting in the Nam Dinh Coastal Zone.,"The hydrogeological model is applied and carried out for the Pleistocene confined aquifer systems in the Nam Dinh coastal zone, Vietnam. A 3D groundwater flow and matters spreading numerical model forecast about decreasing of the groundwater level and saltwater intrusion on the coastal areas Hydraulic conductivity K, specific storage µ*, specific yield µ and effective porosity n0 and boundary conditions are calibrated in this model. The simulated results are significant with low errors corresponding the available data. Calibrated model is utilized to forecast about decreasing of the groundwater level and movement of saltwater intrusion boundary into freshwater areas. The simulated results are indicated that freshwater areas are intrused proportional to exploiting reserves. According to currently freshwater exploiting recharge per year in 2014, freshwater areas until 2025 would be falled 4.17% than in 2005. Moreover, with currently freshwater exploiting recharge per year in 2014 for consumption and population growth and pumping-wells along main river setting, freshwater areas are intrused more strongly than the above method, and would be decreased 6.16%.",94,https://doi.org/10.1007/978-3-319-65687-8_8,1,
1799,On the accuracy of a smooth mathematical model for electric circuits with diode current converters.,"The mathematical model of electric circuits with diode current converters is a system with diode nonlinearity represented as a system of ordinary differential equations with a discontinuous right-hand side that has a special form. In this work, we construct and study a similar system with continuous right-hand side and large parameter K; we prove a theorem that states that solutions of the corresponding initial problems of order",94,https://doi.org/10.1134/S0005117916100088,1,
1801,Where Should We Stop? An Investigation on Early Stopping for GP Learning.,"We investigate the impact of early stopping on the speed and accuracy of Genetic Programming (GP) learning from noisy data. Early stopping, using a popular stopping criterion, maintains the generalisation capacity of GP while significantly reducing its training time.",94,https://doi.org/10.1007/978-3-642-34859-4_39,1,
1802,A study on Genetic Programming with layered learning and incremental sampling.,"In this paper, we investigate the impact of a layered learning approach with incremental sampling on Genetic Programming (GP). The new system, called GPLL, is tested and compared with standard GP on twelve symbolic regression problems. While GPLL does not differ from standard GP on univariate target functions, it has better training efficiency on problems with bivariate targets. This indicates the potential usefulness of layered learning with incremental sampling in improving the efficiency of GP evolutionary learning.",94,https://doi.org/10.1109/CEC.2011.5949750,1,
1803,Improving the Generalisation Ability of Genetic Programming with Semantic Similarity based Crossover.,"This paper examines the impact of semantic control on the ability of Genetic Programming (GP) to generalise via a semantic based crossover operator (Semantic Similarity based Crossover - SSC). The use of validation sets is also investigated for both standard crossover and SSC. All GP systems are tested on a number of real-valued symbolic regression problems. The experimental results show that while using validation sets barely improve generalisation ability of GP, by using semantics, the performance of Genetic Programming is enhanced both on training and testing data. Further recorded statistics shows that the size of the evolved solutions by using SSC are often smaller than ones obtained from GP systems that do not use semantics. This can be seen as one of the reasons for the success of SSC in improving the generalisation ability of GP",94,https://doi.org/10.1007/978-3-642-12148-7_16,1,
1804,ORDERTREE: a new test problem for genetic programming.,"In this paper, we describe a new test problem for genetic programming (GP), ORDERTREE. We argue that it is a natural analogue of ONEMAX, a popular GA test problem, and that it also avoids some of the known weaknesses of other benchmark problems for Genetic Programming. Through experiments, we show that the difficulty of the problem can be tuned not only by increasing the size of the problem, but also by increasing the non-linearity in the fitness structure.",94,https://doi.org/10.1145/1143997.1144141,1,
1805,On Integration Linguistic Factors to Fuzzy Similarity Measures and Intuitionistic Fuzzy Similarity Measures.,"The article is concerned with integrating linguistic elements into fuzzy similarity measures and intuitionistic fuzzy similarity measure. Some new concepts are proposed: a fuzzy linguistic value (FLv), a fuzzy linguistic vector (FLV), an intuitionistic fuzzy linguistic vector (ILV) and similarity measures. The proposed measures are used to build classification algorithms. As predicted theoretically, experiments show that with the same type of similarity measures, the linguistic-aggregated similarity measures produce better results in classification problems.",97,https://doi.org/10.4018/IJSE.2019010101,1,
1806,Investigation of Micro-volume Viscosity with Janus Microbeads Based on Rotational Brownian Motion.,"Viscosity is an important property of liquids. A viscosity change of aqueous substances that deviates from their normal levels usually implies a compromise in quality due to degradation or microorganism proliferation. Monitoring of macro-scale viscosity can be simply realized by various conventional tools, such as rotational viscometers, capillary tubes, falling bodies, and so forth. Nevertheless, today, micro-volume viscosity measurement remains a challenging endeavor, resulting in rare, expensive, or difficult-to-obtain samples not very well studied. For this reason, a novel technique for micro-viscosity based on rotational Brownian motion is presented in this paper. Janus microbeads were made by coating fluorescent polystyrene beads with gold film. Taking advantage of the bead configuration of half gold/half fluorescence, the rotational Brownian signal was expressed in terms of blinking fluorescent intensity. The characteristic correlation time was derived from the blinking intensity of trace amounts of a selected medium over a certain time period, and results were correlated with viscosity. Given a volume of only 2 μL for each measurement, calibration of a series of glycerol–water mixtures (100%–1% (v/v) water content) yielded good agreement with the expected viscosity predictions over the range of 0.8–574.8 cP. Five common oil products, including lubricant oil, baby oil, food oil, olive oil, and motor oil, were further investigated to demonstrate the feasibility and practicability of the proposed technique. Data measured by the rotational Brownian motion-based diffusometer were comparable with those measured by a commercial rotational viscometer. The method also explicitly showed viscosity degradation after the oils were heated at a high temperature of over 100 °C for 10 min. Evaluation proved the proposed Janus microbead-enabled rotational diffusometric technique to be a promising approach for rapid and micro-scale viscosity measurement. ",97,https://doi.org/10.3390/s19051217,1,
1807,Linguistic Vector Similarity Measures and Applications to Linguistic Information Classification.,"In this paper, we generalize the similarity degree for linguistic labels to the so-called the linguistic similarity measure. Linguistic vector, whichcan be used to represent objects whose attributes are given in terms of linguistic labels, is defined. Some mathematical properties are stated and proved. The linguistic vector similarity measure is developed and applied to linguistic information classification. Experimental results on real data confirm the effectiveness of the proposed method.",97,https://doi.org/10.1002/int.21830,1,
1808,On the performance evaluation of intuitionistic vector similarity measures for medical diagnosis.,"Intuitionistic fuzzy recommender system (IFRS), which has been recently presented based on the theories of intuitionistic fuzzy sets and recommender systems, is an efficient tool for medical diagnosis. IFRS used the intuitionistic fuzzy similarity degree (IFSD) regarded as the generalization of the hard user-based, item-based and the rating-based similarity degrees in recommender systems to calculate the analogousness between patients in the system. In this paper, we firstly extend IFRS by using a new term - the intuitionistic fuzzy vector (IFV) instead of the existing intuitionistic fuzzy matrix (IFM) in IFRS. Then, the intuitionistic value similarity measure (IvSM) and the intuitionistic vector similarity measure (IVSM) are defined on the basis of the intuitionistic fuzzy vector. Some mathematical properties of these new terms are examined, and several IVSM functions are proposed. The performances of these IVSM functions for medical diagnosis are experimentally validated and compared with the existing similarity degrees of IFRS. The suggestion and recommendation of this paper involve the most efficient IVSM function(s) that should be used for medical diagnosis.",97,https://doi.org/10.3233/JIFS-151654,1,
1809,Linguistic approach in medical diagnosis.,"Motivated by Sanchez's approach for medical diagnosis and fuzzy linguistic approach, this paper introduces a novel medical diagnosis model based on linguistic representation. Analogous to Sanchez's notion of “Medical Knowledge” we define “Linguistic Medical Knowledge” as a fuzzy linguistic relation (FLR) from the set of symptoms to the set of diagnoses. In order to compose two FLRs, some fuzzy linguistic composition techniques are given: fuzzy linguistic max-min, fuzzy linguistic max-prod and fuzzy linguistic max-τ compositions. The rationality of the proposed method is examined in both theoretical and practical aspects.",97,https://doi.org/10.1109/KSE.2016.7758026,1,
1810,New Composition of Intuitionistic Fuzzy Relations.,"Fuzzy relations have applications in fields such as psychology, medicine, economics, and sociology. Burillo and Bustince introduced the concepts of intuitionistic fuzzy relation and a composition of intuitionistic fuzzy relations using four triangular norms or conorms α, β, λ, ρ (we abbreviate to α, β, λ, ρ-composition). In this paper, we define a new composition of intuitionistic fuzzy relations using two intuitionistic fuzzy triangular norms or intuitionistic fuzzy triangular conorms (Φ, Ψ-composition for short). It is shown that α, β, λ, ρ-composition is special case of Φ, Ψ-composition. Many properties of Φ, Ψ-composition are stated and proved.",97,https://doi.org/10.1007/978-3-319-02741-8_12,1,
1811,Aligning Multi Sequences on GPUs.,"Implementing Multi Sequence Alignment (MSA) problem using the method of progressive alignment is not feasible on common computing systems; it takes several hours or even days for aligning thousands of sequences if we use sequential versions of the most popular MSA algorithm - Clustal. In this paper, we present our parallel algorithm called CUDAClustal, a MSA parallel program. We have paralleled the first stage of the algorithm Clustal and achieved a significant speedup when compared to the sequential program running on a computer of Pentium 4 3.0 GHz processor. Our tests were performed on one GPU Geforce GTX 295 and they gave a great computing performance: the running time of CUDAClustal is smaller approximately 30 times than Clustal for the first stage. This shows the large benefit of GPU for solving the MSA problem and its high applicability in bioinformatics.",97,https://doi.org/10.1007/978-3-642-36642-0_30,1,
1812,Password recovery for encrypted ZIP archives using GPUs.,"Protecting data by passwords in documents such as DOC, PDF or RAR, ZIP archives has been demonstrated to be weak under dictionary attacks. Time for recovering the passwords of such documents mainly depends on two factors: the size of the password search space and the computing power of the underline system. In this paper, we present an approach using modern multi-core graphic processing units (GPUs) as computing devices for finding lost passwords of ZIP archives. The combination of GPU's extremely high computing power and the state-of-the-art password structure analysis methods would bring us a feasible solution for recovering ZIP file password. We first apply password generation rules[9] in generating a reasonable password space, and then use GPUs for exhaustively verifying every password in the space. The experimental results have shown that the password verification speed increases about from 48 to 170 times (depends on the number of GPUs) compared to sequential execution on the Intel Core 2 Quad Q8400 2.66 Ghz. These results have demonstrated the potential applicability of GPUs in this cryptanalysis field.",97,https://doi.org/10.1145/1852611.1852617,1,
1814,"Analysis of CALIPSO satellite imagery for air pollution source identification in Hanoi, Vietnam.","Identification of air pollution is a significant task for environmental control, manage, and policy decision. In traditional approach, chemical composition analysis is very costly to be applied frequently and largely, especially in developing countries. This paper proposes the use of CALIPSO satellite image to analyze the aerosol sources, highly linking with particulate matter sources, in Hanoi in the periods from 2016 to 2019. Other datasets including Hanoi land-cover map and the monthly average wind direction from MERRA-2 reanalysis were utilized to explain the spatial distribution of aerosol sources. The result shows that polluted continental/smoke accounted for the largest proportion with 40%, followed by polluted dust, smoke, dust and clean continental with a percentage of 35%, 14%, 6% and 5%, respectively. The monthly variation of the aerosol type shown a high frequency of elevated smoke in March, April and October meanwhile polluted continental/smoke was a peak in the dry season (November to March) and lower in the rainy season (May to September). The aerosol types were observed mostly at high attitude including polluted dust, polluted continental and elevated smoke could be related to long-range transport from other places to Hanoi. This study highlights the potentials of using CALIPSO products for identification of air pollution sources in Vietnam.",99,https://doi.org/10.1109/KSE50997.2020.9287409,1,
1815,Retinal Vessels Segmentation by Improving Salient Region Combined with Sobel Operator Condition.,"Medical images contribute greatly to help physicians identify abnormalities in the patient’s body in today’s health care. Retinal vessels are one of the effective methods for diagnosing diseases, such as: age-related macular degeneration, diabetes, hypertension, arteriosclerosis. However, manual analysis for retinal images is time-consuming and costly for ophthalmologists. In this paper, we proposed an approach for segmentation in retinal vessels by improving salient region map combined with Sobel mask. The algorithm includes two steps: superpixel detection and segmentation based on salient region map. The result of proposed method is better than the other methods.",99,https://doi.org/10.1007/978-3-030-35653-8_39,1,
1817,A Comparison of SimpSVM and RVM for Sign Language Recognition.,"Sign language recognition is a rather new field and many challenges, especially when motion capture devices become more popular. In this paper, we study the feasibility and effectiveness of two classification methods, namely Simplification of Support Vector Machine (SimpSVM) and Relevance Vector Machine (RVM), and also give some comparative results of them for the sign language recognition problem. The experimental results on the Auslan data set and ASLID data set show that SimpSVM and RVM could achieve good predictive performances and SimpSVM is better as compared to RVM on sign recognition. They also pointed out that prediction behaviors of them are similar in terms of the prediction accuracy when the amount of data or the number of feature changed and sign discrimination. However, SimpSVM requires fewer training time than RVM in training phase.",99,https://doi.org/10.1145/3036290.3036322,1,
1818,Paddy rice mapping in red river delta region using landsat 8 images: Preliminary results.,"Paddy rice, an important crop in Vietnam, need to be monitored and managed for food security, water resource management and Greenhouse gas (GHG) emission. In this study, we present preliminary results of rice mapping in Red River Delta (RRD) region, one of two largest rice planting area in Vietnam. We employ all available Landsat 8 acquired over RRD, regardless of cloud coverage, to establish yearly rice maps from 2013 to 2016. A time-series classification scheme is proposed which includes two main stages. At first, all Landsat 8 images in classification year are used to create monthly composite images. Then, time-series features are extracted to feed an eXtreme Gradient Boosting (XGBoost) classifier which is able to work with sparse features. Validation against test data show promising results with overall accuracy of 89.42-91.53%, kappa of 0.76-0.79 and F1 score of 0.90-0.92. Map validation against statistical data at provincial levels shows high correlations with R2 ranging from 0.96-0.98 and mapped rice areas difference from statistical rice areas is from 7.06%-15.42%. The results highlighted effect of our approach in which Landsat 8 images are only used but rice maps are still created with fine spatial resolution, high accuracy and large coverage over cloud-prone areas.",99,https://doi.org/10.1109/KSE.2017.8119460,1,
1819,Semantic based crossovers in Tree-Adjoining Grammar Guided Genetic Programming.,"In this paper we propose an approach to implementing new semantic based crossover operators in Tree-Adjoining Grammar Guided Genetic Programming (TAG3P). The design of the new crossover operators is based on the non-fixed arity (also called feasibility) property of TAG-based representation in TAG3P. The new operators are then tested on a family of benchmark symbolic regression problems and compared with standard Genetic programming (GP), GP with Semantic Similarity based Crossover (SSC), and TAG3P. The results show that TAG3P with the new operators significantly outperforms GP, GP with SSC, and TAG3P.",99,https://doi.org/10.1109/RIVF.2013.6719883,1,
1820,Sender - Receiver cooperation and buffer observation for fine-grained adaptive transmission of video over mobile ad hoc networks.,"Deployment of video communication services over mobile ad hoc networks faces numerical obstacles since the networks are characterized by limited and unpredictable bandwidth. Following the tenet of service-orientation, this study proposes a cross-layer design for fine-grained adaptive transmission of real-time video over time-varying channels. Specifically, the sender accurately estimates the current effective bandwidth by observing its transmission buffer and negative acknowledgments (NACKs) from the receiver. Information on route quality provided by routing protocols may also be exploited to control the real-time stream efficiently. At the same time, the sender handles NACK messages to predict route goodness and orders retransmissions for improving video quality. We have evaluated the performance of the proposed design by both simulations and experiments of ad hoc nodes delivering H264/AVC, which consistently demonstrated the soundness and feasibility of the framework with respect to video quality and resource utilization.",99,https://doi.org/10.1109/PCCC.2010.5682333,1,
1821,Password recovery for encrypted ZIP archives using GPUs.,"Protecting data by passwords in documents such as DOC, PDF or RAR, ZIP archives has been demonstrated to be weak under dictionary attacks. Time for recovering the passwords of such documents mainly depends on two factors: the size of the password search space and the computing power of the underline system. In this paper, we present an approach using modern multi-core graphic processing units (GPUs) as computing devices for finding lost passwords of ZIP archives. The combination of GPU's extremely high computing power and the state-of-the-art password structure analysis methods would bring us a feasible solution for recovering ZIP file password. We first apply password generation rules[9] in generating a reasonable password space, and then use GPUs for exhaustively verifying every password in the space. The experimental results have shown that the password verification speed increases about from 48 to 170 times (depends on the number of GPUs) compared to sequential execution on the Intel Core 2 Quad Q8400 2.66 Ghz. These results have demonstrated the potential applicability of GPUs in this cryptanalysis field.",99,https://doi.org/10.1145/1852611.1852617,1,
1823,Some Preliminary Results on the Stableness of Extended F-rule Systems.,"In this paper we shall investigate an extended version of F-rule systems, in which each F-rule can include an arbitrary combination of disjunctions and conjunctions of atoms in the premise. The first main result here is a way to determine values assigned to these extended facts, based on two basic operators ⊕ and x;, which are shown to be equivalent to external probabilistic reasoning by resolving linear programming problem. Based on this, a definition on mixed inference operator for extended F-rule systems is discussed. We have shown that an extended F-rule system with the defined reasoning operator is stable iff its corresponding F-rule system is stable. This proposition allows us to apply all our available research results on F-rule systems to extended F-rule systems.",99,https://doi.org/10.20965/jaciii.2003.p0252,1,
1824,End-to-End Image Patch Quality Assessment for Image/Video With Compression Artifacts.,"In this paper, we present an experimental image quality assessment (IQA) method for image/video patches with compression artifacts. Using the High Efficiency Video Coding (HEVC) standard, we create a new database of image patches with compression artifacts. Then, we conduct a completed subjective testing process to obtain the `ground truth' quality scores for the mentioned database. Finally, we employ an end-to-end learning method to estimate the IQA model for the patches with HEVC compression artifacts. In such proposed method, a modified convolutional neural network (CNN) architecture is exploited for feature extraction while an adaptive moment estimation optimizer solution is used to perform the training process. Experimental results show that the proposed end-to-end IQA method significantly outperforms the relevant IQA benchmarks, especially when the compression artifacts are strongly realized in image/video patches. The proposed IQA method is expected to drive a new set of image/video compression solutions in future image/video coding and transmissions.",101,https://doi.org/10.1109/ACCESS.2020.3040416,1,
1825,"Studying on the Accuracy Improvement of GM (1, 1) Model.","In order to expand the application of GM (1, 1) in the condition of fluctuation data and incomplete information, this paper proposed the new systematic optimization based on three steps as follows. First step, we used parameters c1 to transform any sequence data into the non-negative sequence data. The second, we used moving average operation method on the new sequence data to smooth the sequence data aim to satisfy the quasi-exponential condition and quasi-smooth condition. The final, we adopt Fourier series to modify residual error of model a grey sequence. To demonstrate the superiority of the proposed model, the numerical example in the research of Wang and Hsu and the raw data sequence are used. The simulation outcomes show that the proposed approach provides a better forecast results than several different kinds of grey forecasting models with the lowest average of MAPE for in and out-of-samples in two cases. For future direction, the author will applied different methodologies to improve the performance of GM (1, 1) or use proposed model to analyse the issues with high fluctuation data.",101,https://doi.org/10.1007/978-3-030-63119-2_10,1,
1826,Compression Artifacts Image Patch database for Perceptual Quality Assessment.,"Ground truth is one of the most important component for training, testing, and benchmarking algorithms for objective quality assessment In this paper, we propose an image patch quality database with compression artifacts. We create a new database of image patches with High Efficiency Video Coding (HEVC) compression artifacts. Then, the subjective test is conducted in a controlled environment to obtain the ground truth of image patch quality, where we collect differential mean opinion scores (DMOS) from a larger amount of observers. Finally, the rank order correlation factors between DMOS and a set of popular image quality metrics are calculated and presented. The proposed database is expected for learning patch based IQA model for block size in video rate-distortion optimization.",101,https://doi.org/10.1109/KSE50997.2020.9287704,1,
1827,On the Detection of Video's Ethnic Vietnamese Thai Dance Movements.,"The problem addressed in this paper is the one of classifying Vietnamese dances' videos. In particular, we focus on an automatic detection of movements in the Ethnic Vietnamese Thai dances (ETVD). We first propose an ontology-based description of ETVD movements in terms of main movements' steps. We then associate with each movement step a profile containing typical features that characterize a movement step. The automatic detection of ETVD movements is based on a correlation method that matches movements' steps profiles with concepts present in frames of dances' videos. The last part of the paper contain experimental studies that show the good classification rate of our ETVD movement detection method.",101,https://doi.org/10.1109/SITIS.2019.00064,1,
1828,Annotating Movement Phrases in Vietnamese Folk Dance Videos.,"This paper aims at the annotation of movement phrases in Vietnamese folk dance videos that were mainly gathered, stored and used in teaching at art schools and in preserving cultural intangible heritages (performed by different famous folk dance masters). We propose a framework of automatic movement phrase annotation, in which the motion vectors are used as movement phrase features. Movement phrase classification can be carried out, based on dancer’s trajectories. A deep investigation of Vietnamese folk dance gives an idea of using optical flow as movement phrase features in movement phrase detection and classification. For the richness and usefulness in annotation of Vietnamese folk dance, a lookup table of movement phrase descriptions is defined. In initial experiments, a sample movement phrase dataset is built up to train k-NN classification model. Experiments have shown the effectiveness of the proposed framework of automatic movement phrase annotation with classification accuracy at least 88%.",101,https://doi.org/10.1007/978-3-319-60045-1_1,1,
1829,A New Histogram Modification Based Reversible Data Hiding Algorithm Considering the Human Visual System.,"In this letter, we propose an improved histogram modification based reversible data hiding technique. In the proposed algorithm, unlike the conventional reversible techniques, a data embedding level is adaptively adjusted for each pixel with a consideration of the human visual system (HVS) characteristics. To this end, an edge and the just noticeable difference (JND) values are estimated for every pixel, and the estimated values are used to determine the embedding level. This pixel level adjustment can effectively reduce the distortion caused by data embedding. The experimental results and performance comparison with other reversible data hiding algorithms are presented to demonstrate the validity of the proposed algorithm.",101,https://doi.org/10.1109/LSP.2010.2095498,1,
1830,Motion Vector Refinement Algorithm for the High Definition Video Coding.,"In H.264/AVC, simplified rate distortion optimisation (RDO) is performed to estimate the motion vector (MV). As a result, the estimated MV is often suboptimal since the residual bitrate is not considered, which results in the deterioration of the coding efficiency. In this paper, we propose an MV refinement algorithm in which the final MV is selected from a set of candidate MVs by using the RDO including the residual bitrate. To further improve the performance of optimal MV selection, the RD calculation method in the RDO is modified using the deblocking filters. Experimental results show that the proposed algorithm outperforms the conventional H.264 in terms of coding efficiency.

",101,https://doi.org/10.1007/978-3-642-26010-0_43,1,
1831,An Improved Method to CABAC in the H.264/AVC Video Compression Standard.,"Context-based Adaptive Binary Arithmetic Coding (CA-BAC) is adopted as an entropy coding tool for main profile of the video coding standard H.264/AVC. CABAC achieves higher degree of redundancy reduction by estimating the conditional probability of each binary symbol which is the input to the arithmetic coder. This paper presents an entropy coding method based on CABAC. In the proposed method, the binary symbol is coded using more precisely estimated conditional probability, thereby leading to performance improvement. We apply our method to the standard and evaluate its performance for different video sources and various quantization parameters (QP). Experiment results show that our method outperforms the original CABAC in term of coding efficiency, and the average bit-rate savings are up to 1.2%.",101,https://doi.org/10.1587/transfun.E92.A.3355,1,
1832,Person identification system for future digital tv with intelligence.,"Intelligent digital TV (iDTV) is a future digital TV with intelligence which can automatically provide user-personalized services for each audience. For the user-personalized services, the iDTV should recognize audiences in real-time. In this paper, we define a novel structure of the iDTV and propose a real-time person identification system in the iDTV that analyzes captured images and recognizes audiences. The proposed system consists of three processing units: preprocessing for reducing computational costs of the proposed system, face detection using a statistical approach, and face recognition using support vector machines (SVMs). Experimental results show that the proposed system achieves efficient performance with high recognition accuracy of 90% or higher at the speed of 15~20 fps, which is suitable for the iDTV",101,https://doi.org/10.1109/TCE.2007.339528,1,
1833,Adaptive GOP Bit Allocation to Provide Seamless Video Streaming in Vertical Handoff.,"Vertical handoff is required to offer users the capability of achieving anywhere and anytime internet access with the full service support from interoperability between third generation (3G) and wireless LAN (WLAN) network. However, video data can be lost due to latency caused by vertical handoff. To solve this problem, in this paper, we propose an efficient GOP bit allocation method to provide seamless video streaming in vertical handoff. In the proposed method, the streaming server first predicts the network status by using the wireless channel modeling and then adaptively performs bit allocation for a group of pictures (GOP) to adapt video delivery to vertical handoff scenario. Experimental results show that the proposed method can provide significant improvement to video streaming performance, in terms of users-perceive video quality, and robustness against the latency and packet loss caused by vertical handoff.",101,https://doi.org/10.1007/11949534_79,1,
1834,Online Structured Learning for Semantic Parsing with Synchronous and lambda-Synchronous Context Free Grammars.,"We formulate semantic parsing as a parsing problem on a synchronous context free grammar (SCFG) which is automatically built on the corpus of natural language sentences and the representation of semantic outputs. We then present an online learning framework for estimating the synchronous SCFG grammar. In addition, our online learning methods for semantic parsing problems are also extended to deal with the case, in which the semantic representation could be represented under lambda-calculus. Experimental results in the domain of semantic parsing show advantages in comparison with previous works.",102,https://doi.org/10.1109/ICTAI.2008.96,1,
1835,Matching and Ranking with Hidden Topics towards Online Contextual Advertising.,"In online contextual advertising, ad messages are displayed related to the content of the target Web page. It leads to the problem in information retrieval community: how to select the most relevant ad messages given the content of a page. To deal with this problem, we propose a framework that takes advantage of large scale external datasets. This framework provides a mechanism to discover the semantic relations between Web pages and ad messages by analyzing topics for them. This helps overcome the problem of mismatch due to unimportant words and the difference in vocabularies between Web pages and ad messages. The framework has been evaluated through a number of experiments. It shows a significant improvement in accuracy over word/lexicon-based matching and ranking methods.",102,https://doi.org/10.1109/WIIAT.2008.180,1,
1837,A New Sentence Reduction Technique Based on a Decision Tree Model.,"This paper addresses a novel sentence reduction algorithm based on a decision tree model in which semantic information is used to enhance the accuracy of sentence reduction. The proposed algorithm is able to deal with the changeable order problem in sentence reduction. Furthermore, the use of decision list to solve the fragment problem in sentence reduction based decision tree model is also discussed. Our experimental results show an improvement when compared with earlier methods.",102,https://doi.org/10.1142/S0218213007003242,1,
1838,A Multilingual Dependency Analysis System Using Online Passive-Aggressive Learning.,"This paper presents an online algorithm for
dependency parsing problems. We propose
an adaptation of the passive and aggressive
online learning algorithm to the dependency
parsing domain. We evaluate the proposed
algorithms on the 2007 CONLL Shared
Task, and report errors analysis. Experimental results show that the system score is better than the average score among the participating systems.",102,https://aclanthology.org/D07-1126/,1,
1839,Improving discriminative sequential learning by discovering important association of statistics.,"Discriminative sequential learning models like Conditional Random Fields (CRFs) have achieved significant success in several areas such as natural language processing or information extraction. Their key advantage is the ability to capture various nonindependent and overlapping features of inputs. However, several unexpected pitfalls have a negative influence on the model's performance; these mainly come from a high imbalance among classes, irregular phenomena, and potential ambiguity in the training data. This article presents a data-driven approach that can deal with such difficult data instances by discovering and emphasizing important conjunctions or associations of statistics hidden in the training data. Discovered associations are then incorporated into these models to deal with difficult data instances. Experimental results of phrase-chunking and named entity recognition using CRFs show a significant improvement in accuracy. In addition to the technical perspective, our approach also highlights a potential connection between association mining and statistical learning by offering an alternative strategy to enhance learning performance with interesting and useful patterns discovered from large datasets.",102,https://doi.org/10.1145/1236181.1236187,1,
1840,A Maximum Entropy Model for Transforming Sentences to Logical Form.,We formulate the problem of transformation natural language sentences as the determination of sequence of actions that transforms an input sentence to its logical form. The model to determine a sequence of actions for a corresponding sentence is automatically estimated from a corpus of sentences and their logical forms with a MEM framework. Experimental results show that the MEM framework are suitable for the transformation problem and archived a comparable result in comparison with other methods.,102,https://doi.org/10.1007/11589990_84,1,
1841,Classification with Maximum Entropy Modeling of Predictive Association Rules.,"This paper presents a new classification model in which a classifier is built upon predictive association rules (PARs) and the maximum entropy principle (maxent). In this model, PARs can be seen as confident statistical patterns discovered from training data with strong dependencies and correlations among data items. Maxent, on the other hand, is an approach to build an estimated distribution having maximum entropy while obeying a potentially large number of useful features observed in empirical data. The underlying idea of our model is that PARs have suitable characteristics to serve as features for maxent. As a result, our classifier can take advantage of both the useful correlation and confidence of PARs as well as the strong statistical modeling capability of maxent. The experimental results show that our model can achieve significantly higher accuracy in comparison with the previous methods.",102,https://doi.org/10.1007/11564096_69,1,
1842,Improving discriminative sequential learning with rare--but--important associations.,otential connection between association mining and statistical learning by offering an alternative strategy to enhance learning performance with interesting and useful patterns discovered from large dataset.,102,https://doi.org/10.1145/1081870.1081906,1,
1843,A Structured SVM Semantic Parser Augmented by Semantic Tagging with Conditional Random Field.,"This paper presents a novel method of semantic parsing that maps a natural language (NL)
sentence to a logical form. We propose a semantic parsing method by conducting separately two
steps as follows;
 1) The first step is to predict semantic tags for a given input sentence.
2) The second step is to build a semantic representation structure for the sentence using the
sequence of semantic tags.
We formulate the problem of semantic tagging as a sequence learning using a conditional random
field models (CRFs). We then represent a tree structure of a given sentence in which syntactic and
semantic information are integrated in that tree. The learning problem is to map a given input
sentence to a tree structure using a structure support vector model. Experimental results on the
CLANG corpus show that the semantic tagging performance achieved a sufficiently high result. In
addition, the precision and recall of mapping NL sentences to logical forms i.e. the meaning
representation in CLANG show an improvement in comparison with the previous work. ",102,https://aclanthology.org/Y05-1015/,1,
1845,Modified Space Vector Modulation for Cascaded H-Bridge Multilevel Inverter with Open-Circuit Power Cells.,"In this research, a new space vector modulation control algorithm is proposed to increase the reliability of the cascaded H-bridge multilevel inverters in case of faulty situations, where one or several power cells do not function. Methods to detect faults ensure finding open-circuit module exactly, which is fast and easy to program. By giving a detailed analysis of the impact of the faulty power cells, optimal redundant level states are chosen such that highest possible output voltage can be achieved, while the balance of the three-phase line-to-line voltage is maintained and common-mode voltage is reduced. The proposed algorithm is generalized so that it can be applied to H-bridge inverters of any level. The validity of the method is verified by numerical simulations and experiment results with an 11-level cascaded H-bridge inverter.",104,https://doi.org/10.1155/2021/6643589,1,
1846,Modeling and Specification of Real-Time Interfaces with UTP.,"Interface modeling and specification are central issues of component-based software engineering. How a component will be used is specified in its interface. Real-time interfaces are interfaces with timing constraints relating the time of outputs with the time of inputs. The timing constraint of an interface may depend on the resource availability for the component. In this paper, we propose a general model for real-time interfaces. At a time during execution, an interface behaves according to a contract made with environment about its functionality as well as execution time to fulfill the contract. This contract is specified as a timed design using the UTP notations, and depends on the computation histories of the interface. We model this dependence as a partial function from computation histories of the interface to real-time contracts. How interfaces are composed to form new interfaces, how interfaces are refined, and how to represent interfaces finitely are also considered in this paper. We show that checking the consistency between an environment and an interface and checking the refinement between two interfaces when they are represented by an automaton can be done effectively.",104,https://doi.org/10.1007/978-3-642-39698-4_9,1,
1847,An Effective Solution to Regression Problem by RBF Neuron Network.,"Radial Basis Function (RBF) neuron network is being applied widely in multivariate function regression. However, selection of neuron number for hidden layer and definition of suitable centre in order to produce a good regression network are still open problems which have been researched by many people. This article proposes to apply grid equally space nodes as the centre of hidden layer. Then, the authors use k-nearest neighbour method to define the value of regression function at the center and an interpolation RBF network training algorithm with equally spaced nodes to train the network. The experiments show the outstanding efficiency of regression function when the training data has Gauss white noise.",105,https://doi.org/10.4018/IJORIS.2015100104,1,
1848,GA_SVM: A Genetic Algorithm for Improving Gene Regulatory Activity Prediction.,"Gene regulatory activity prediction problem is one of the important steps to understand the significant factors for gene regulation in biology. The advents of recent sequencing technologies allow us to deal with this task efficiently. Amongst these, Support Vector Machine (SVM) has been applied successfully up to more than 80% accuracy in the case of predicting gene regulatory activity in Drosophila embryonic development. In this paper, we introduce a metaheuristic based on genetic algorithm (GA) to select the best parameters for regulatory prediction from transcriptional factor binding profiles. Our approach helps to improve more than 10% accuracy compared to the traditional grid search. The improvements are also significantly supported by biological experimental data. Thus, the proposed method helps boosting not only the prediction performance but also the potentially biological insights.",105,https://doi.org/10.1109/rivf.2012.6169861,1,
1849,An Efficient Two-Phase Ant Colony Optimization Algorithm for the Closest String Problem.,"Given a finite set S of strings of length m, the task of finding a string t that minimizes the Hamming distance from t to S, has wide applications. This paper presents a two-phase Ant Colony Optimization (ACO) algorithm for the problem. The first phase uses the Smooth Max-Min (SMMAS) rule to update pheromone trails. The second phase is a memetic algorithm that uses ACO method to generate a population of solutions in each iteration, and a local search technique on the two best solutions. The efficiency of our algorithm has been evaluated by comparing to the Ant-CSP algorithm.",105,https://doi.org/10.1007/978-3-642-34859-4_19,1,
1850,Efficient Algorithm for Training Interpolation RBF Networks With Equally Spaced Nodes.,"This brief paper proposes a new algorithm to train interpolation Gaussian radial basis function (RBF) networks in order to solve the problem of interpolating multivariate functions with equally spaced nodes. Based on an efficient two-phase algorithm recently proposed by the authors, Euclidean norm associated to Gaussian RBF is now replaced by a conveniently chosen Mahalanobis norm, that allows for directly computing the width parameters of Gaussian radial basis functions. The weighting parameters are then determined by a simple iterative method. The original two-phase algorithm becomes a one-phase one. Simulation results show that the generality of networks trained by this new algorithm is sensibly improved and the running time significantly reduced, especially when the number of nodes is large.",105,https://doi.org/10.1109/TNN.2011.2120619,1,
1851,ACOPHY: A Simple and General Ant Colony Optimization Approach for Phylogenetic Tree Reconstruction.,"We introduce ACOPHY, a novel framework to apply Ant Colony Optimization (ACO) for phylogenetic reconstruction. ACOPHY overcomes a main drawback of other attempts to reconstruct phylogenies by defining a compact ACO graph that is nicely coupled with the tree space. The proposed graph allows the ants to walk globally through the tree space. Thus, ACOPHY can be generally applied to all well-known optimality criteria in phylogenetics. We compared ACOPHY with the traditional phylogenetic method PHYLIP and obtained slightly better results. This is promising since our current implementation of ACOPHY is still at the proof of concept stage. We list a number of points where ACOPHY can be improved. Once the improvements are integrated, we hope for competitive performance against other recent phylogenetic inference methods.",105,https://doi.org/10.1007/978-3-642-15461-4_32,1,
1852,Multivariate interpolation using radial basis function networks.,"There is, hitherto, no efficient method to interpolate multivariate functions, for especially dynamic problems in which new training data are often added in real-time. In order to construct an efficient method, this paper considers local interpolation RBF networks, where artificial neural network approach and instance-based learning are combined. In these networks, training data are clustered into relatively small sub-clusters and on each sub-cluster, an interpolation RBF network is trained by using a new algorithm recently proposed by the authors; it is a two-phase algorithm for training interpolation RBF networks using Gaussian basis functions and it has the complexity O(N²), where N is the number of nodes. The training time of this new architecture is effectively short and its generality is superior to global RBF networks. Furthermore its universal approximation property is proven. Especially, this new architecture can be efficiently used for dynamic training.",105,https://doi.org/10.1504/IJDMMM.2009.027287,1,
1853,A novel efficient two-phase algorithm for training interpolation radial basis function networks.,"Interpolation radial basis function (RBF) networks have been widely used in various applications. The output layer weights are usually determined by minimizing the sum-of-squares error or by directly solving interpolation equations. When the number of interpolation nodes is large, these methods are time consuming, difficult to control the balance between the convergence rate and the generality, and difficult to reach a high accuracy. In this paper, we propose a two-phase algorithm for training interpolation RBF networks with bell-shaped basis functions. In the first phase, the width parameters of basis functions are determined by taking into account the tradeoff between the error and the convergence rate. Then, the output layer weights are determined by finding the fixed point of a given contraction transformation. The running time of this new algorithm is relatively short and the balance between the convergence rate and the generality is easily controlled by adjusting the involved parameters, while the error is made as small as desired. Also, its running time can be further enhanced thanks to the possibility to parallelize the proposed algorithm. Finally, its efficiency is illustrated by simulations.",105,https://doi.org/10.1016/j.sigpro.2007.05.001,1,
1855,Appliance classification on low-cost smart power outlets based on frequency feature of electric current.,"Smart power outlets equipped with appliance classification function can play an important role in smart homes. With the help of smart power outlets, users can know exactly the appliance plugged into an outlet and monitor or turn on/turn off the appliance. In this paper, we propose the design of low-cost smart power outlets, which can detect appliance-on state for triggering the sampling of electric current and extract frequency feature of electric current based on Fast Fourier Transform (FFT). The frequency feature of appliances is then used to classify appliances with the help of Feedforward neural networks (FNN). As results, our proposed method can achieve high classification accuracy with small traffic cost and hardware cost.",107,https://doi.org/10.1109/APCC49754.2021.9609819,1,
1856,Simulation of precision feeding systems for swine.,"Precision livestock farming has become an inevitable trend for agricultural industry in the world. In that field, precision feeding is widely acknowledged because of its potential to reduce feed costs, environmental footprint and to improve animal health and welfare. Precision feeding involves modern multidisciplinary technologies such as information technology, mechanics, electronics, automation, etc. Such a system consists of automatic troughs linked to a computer system to exploit data collected from the individual animals (e.g. body weight, feed intake and feeding behaviour), and/or from ambient sensors. Data is processed and analysed based on mathematical models to make predictions, warnings for farmers or to formulate diets that fit requirements of each individual animal at each production period. However, implementing such a system often requires high investment, which may go beyond the capabilities of smallholders and small/medium laboratories. Furthermore, the risk of implementing by design but not conforming to reality is very high. To avoid this problem, we introduce an agent-based modelling approach to simulate precision feeding systems for swine. Two simulation experiments were conducted to provide predictions about the growth of individual pigs and the usefulness of precision feeding systems over classic feeding models.",107,https://doi.org/10.1109/KSE53942.2021.9648760,1,
1857,D-AVTree: DHT-Based Search System to Support Scalable Multi-Attribute Queries.,"Abstract
At present, vast numbers of information resources are available on the Internet. However, one emerging issue is how to search and exploit these information resources in an efficient and flexible manner with high scalability. In this study, we focused our attention on the design of a distributed hash table (DHT)-based search system that supports efficient scalable multi-attribute queries of information resources in a distributed manner. Our proposed system, named D-AVTree, is built on top of a ring-based DHT, which partitions a one-dimensional key space across nodes in the system. It utilizes a descriptive naming scheme, which names each resource using an attribute-value (AV) tree, and the resource names are mapped to d-bit keys in order to distribute the resource information to responsible nodes based on a DHT routing algorithm. Our mapping scheme maps each AV branch of a resource name to a d-bit key where AV branches that share a subsequence of AV pairs are mapped to a continuous portion of the key space. Therefore, our mapping scheme ensures that the number of resources distributed to a node is small and it facilitates efficient multi-attribute queries by querying only a small number of nodes. Further, the scheme has good compatibility with key-based load balancing algorithms for DHT-based networks. Our system can achieve both efficiency and a good degree of load balancing even when the distribution of AV pairs in the resource names is skewed. Our simulation results demonstrated the efficiency of our solution in terms of the distribution cost, query hit ratio, and the degree of load balancing compared with conventional approaches.",107,https://doi.org/10.1587/transcom.E97.B.1898,1,
1858,Exploiting Social Networks as a Live Mass Media Channel During Disasters for Reactions.,"Social networks, e.g. Twitter, have been proved to be almost real-time systems for spreading information, that provide a valuable information channel in emergencies, e.g. disasters. This paper presents a framework designed to distill actionable tweets. The framework tackles the diversity, large volume, and noise of tweets for providing users live information for quick responses. To do that, our framework first retrieves a large number of tweets to ensure the diversity. It next removes irrelevant and indirect tweets for reducing the volume, divides informative tweets into predefined classes for quick navigation, and groups tweets in a class into topics to preserve the diversity. Finally, it ranks tweets in each topic to extract important tweets for the user’s quick scan. For ranking, the framework utilizes event extraction to enrich the semantics and reduce the noise of tweets. After that, the framework builds event graphs for ranking to find out important tweets. To validate the efficiency of our framework, we took Twitter as a case study. Experimental results on five disaster datasets show that our framework achieves promising results compared to strong methods in disaster scenarios.",109,https://doi.org/10.1142/S021821302150024X,1,
1859,Solving Reduction Problems in Cover Lattice Based Decision Tables.,"Covering based rough set is an important extension of Pawlak's traditional rough set. Reduction is a typical application of rough sets, including traditional, covering based and other rough set extensions. Although this task has several proposals, it is still an open problem to decision systems (tables) in covering based rough set. This paper focuses on the reduction problem for the condition lattice, and fitting problem for the decision lattice in the decision table based on cover lattice. A corresponding algorithm is proposed for each problem. Two examples to illustrate a covering based decision table and two related problems show the applications of these concepts and problems.",109,https://doi.org/10.1007/978-981-16-1685-3_5,1,
1860,Exploiting CBOW and LSTM Models to Generate Trace Representation for Process Mining.,"In the field of process mining, one of the challenges of the trace representation problem is to exploit a lot of potentially useful information within the traces while keeping a low dimension of the corresponding vector space. Motivated by the initial results of applying the deep neural networks for producing trace representation, in this paper, we continue to study and apply two more advanced models of deep learning, i.e., Continuous Bag of Words and Long short-term memory, for generating the trace representation. The experimental results have achieved significant improvement, i.e., not only showing the close relationship between the activities in a trace but also helping to reduce the dimension of trace representation.",109,https://doi.org/10.1007/978-981-15-3380-8_4,1,
1861,A Lifelong Sentiment Classification Framework Based on a Close Domain Lifelong Topic Modeling Method.,"In lifelong machine learning, the determination of the hypotheses related to the current task is very meaningful thanks to the reduction of the space to look for the knowledge patterns supporting for solving the current task. However, there are few studies for this problem. In this paper, we propose the definitions for measuring the “close domains to the current domain”, and a lifelong sentiment classification method based on using the close domains for topic modeling the current domain. Experimental results on sentiment datasets of product reviews from Amazon.com show the promising performance of system and the effectiveness of our approach.",109,https://doi.org/10.1007/978-3-030-41964-6_50,1,
1862,A Targeted Topic Model based Multi-Label Deep Learning Classification Framework for Aspect-based Opinion Mining.,"Recently, deep Convolutional Neural Network (CNN) model has achieved remarkable results in Natural Language Processing (NLP) tasks, such as information retrieval, relation classification, semantic parsing, sentence modeling and other traditional NLP tasks, etc. On the other hand, topic modeling method has been proved to be effective by exploiting hidden knowledge in a corpus of documents. Motivated from these successes, we propose a framework that takes the advantages of closure domain measure to get enriched knowledge from close domains to the training dataset to improve the CNN model, and apply a Targeted Topic Model to take more detailed exploration on each labeled aspect of an opinion. Experimental results on different scenarios show the effectiveness of the proposed framework for multi-label classification task in comparison to other related models on the same Hotel review dataset.",109,https://doi.org/10.1109/KSE50997.2020.9287397,1,
1863,Improving Named Entity Recognition in Vietnamese Texts by a Character-Level Deep Lifelong Learning Model.,"Named entity recognition (NER) is a fundamental task which affects the performance of its dependent task, e.g. machine translation. Lifelong machine learning (LML) is a continuous learning process, in which the knowledge base accumulated from previous tasks will be used to improve future learning tasks having few samples. Since there are a few studies on LML based on deep neural networks for NER, especially in Vietnamese, we propose a lifelong learning model based on deep learning with a CRFs layer, named DeepLML–NER, for NER in Vietnamese texts. DeepLML–NER includes an algorithm to extract the knowledge of “prefix-features” of named entities in previous domains. Then the model uses the knowledge in the knowledge base to solve the current NER task. Preprocessing and model parameter tuning are also investigated to improve the performance. The effect of the model was demonstrated by in-domain and cross-domain experiments, achieving promising results.",109,https://doi.org/10.1142/S219688881950026X,1,
1864,An Adversarial Learning and Canonical Correlation Analysis Based Cross-Modal Retrieval Model.,"The key of cross-modal retrieval approaches is to find a maximally correlated subspace among multiple datasets. This paper introduces a novel Adversarial Learning and Canonical Correlation Analysis based Cross-Modal Retrieval (ALCCA-CMR) model. For each modality, the ALCCA phase finds an effective common subspace and calculates the similarity by canonical correlation analysis embedding for cross-modal retrieval. We demonstrate an application of ALCCA-CMR model implemented for the dataset of two modalities. Experimental results on real music data show the efficacy of the proposed method in comparison with other existing ones.",109,https://doi.org/10.1007/978-3-030-14799-0_13,1,
1865,A Probability-Based Close Domain Metric in Lifelong Learning for Multi-label Classification.,"Lifelong machine learning has recently become a hot topic attracting the researchers all over the world by its effectiveness in dealing with current problem by exploiting the past knowledge. The combination of topic modeling on previous domain knowledge (such as topic modeling with Automatically generated Must-links and Cannot-links, which exploits must-link and cannot-link of two terms), and lifelong topic modeling (which employs the modeling of previous tasks) is widely used to produce better topics. This paper proposes a close domain metric based on probability to choose valuable knowledge learnt from the past to produce more associated topics on the current domain. This knowledge is, then, used to enrich features for multi-label classifier. Several experiments performed on review dataset of hotel show that the proposed approach leads to an improvement in performance over the baseline.",109,https://doi.org/10.1007/978-3-030-38364-0_13,1,
1866,A Compact Trace Representation Using Deep Neural Networks for Process Mining.,"In process mining, trace representation has a significant effect on the process discovery problem. The challenge is to get highly informative but low-dimensional vector space from event logs. This is required to improve the quality of the trace clustering problem for generating the process models clear enough to inspect. Though traditional trace representation methods have specific advantages, their vector space often has a big number of dimensions. In this paper, we address this problem by proposing a new trace representation method based on the deep neural networks. Experimental results prove our proposal not only is better than the alternatives, but also significantly helps to reduce the dimension of trace representation.",109,https://doi.org/10.1109/KSE.2019.8919355,1,
1867,Speeding up and enhancing a large-scale fingerprint identification system on GPU.,"Fingerprint identification is one of the most common biometric feature problems which is used in many applications. Although state-of-the-art algorithms are very accurate, the need for fast processing a big database of millions of fingerprints is highly demanding. In this paper, we propose to adapt the fingerprint matching algorithm based on Minutia Cylinder-Code on Graphics Processing Units to speed up the matching. Another contribution of this paper is to add a consolidation stage after matching to enhance the precision. The experimental results on a GTX-680 and K40 tesla devices with standard data-sets prove that the proposed algorithm can be comparable with the state-of-the-art approach, and is suitable for a real-time identification application.",109,https://doi.org/10.1080/24751839.2017.1404712,1,
1868,A New Lifelong Topic Modeling Method and Its Application to Vietnamese Text Multi-label Classification.,"Lifelong machine learning is emerging in recent years thanks to its ability to use past knowledge for current problem. Lifelong topic modeling algorithms, such as LTM and AMC, are proposed and they are very useful. However, these algorithms focus on learning bias on the topic level not the domain level. This paper proposes a lifelong topic modeling method, which focuses on learning bias on the domain level based on a proposed domain closeness measure, and an application framework for multi-label classification on Vietnamese texts. Experimental results on three previously solved Vietnamese texts, and five different current Vietnamese text datasets in combination with different topic set sizes showed that our proposed method is better than AMC method for all cases.",109,https://doi.org/10.1007/978-3-319-75417-8_19,1,
1869,A New Text Semi-supervised Multi-label Learning Model Based on Using the Label-Feature Relations.,"Multi-label learning has become popular and omnipresent in many real-world problems, especially in text classification applications, in which an instance could belong to different classes simultaneously. Due to these label constraints, there are some challenges occurring in building multi-label data. Semi-supervised learning is one possible approach to exploit abundantly unlabeled data for enhancing the classification performance with a small labeled dataset. In this paper, we propose a solution to select the most influential label based on using the relations among the labels and features to a semi-supervised multi-label classification algorithm on texts. Experiments on two datasets of Vietnamese reviews and English emails of Enron show the positive effects of the proposal.",109,https://doi.org/10.1007/978-3-319-98443-8_37,1,
1870,A New Trace Clustering Algorithm Based on Context in Process Mining.,"In process mining, trace clustering is an important technique that at-tracts the attention of researchers to solve the large and complex volume of event logs. Traditional trace clustering often uses available data mining algorithms which do not exploit the characteristic of processes. In this study, we propose a new trace clustering algorithm, especially for the process mining, based on the using trace context. The proposed clustering algorithm can automatic detects the number of clusters, and it does not need a convergence iteration like traditional ones like K-means. The algorithm takes two loops over the input to generate the clusters, thus the complexity is greatly reduced. Experimental results show that our method also has good results when compared to traditional methods.",109,https://doi.org/10.1007/978-3-319-99368-3_50,1,
1871,A semi-supervised multi-label classification framework with feature reduction and enrichment.,"Multi-label classification (MLC) has drawn much attention, thanks to its usefulness and omnipresence in real-world applications in which objects may be characterized by more than one label as in the traditional approach. Getting multi-label examples is costly and time-consuming; therefore, semi-supervised learning approach should be considered to take advantages of both labelled and unlabelled data. In this work, we propose a semi-supervised MLC algorithm exploiting the specific features of the prominent class label(s) chosen by a greedy approach as an extension of the LIFT algorithm, and unlabelled data consumption mechanism from the TExt classification using Semi-supervised Clustering algorithm. We also make a semi-supervised MLC application framework for Vietnamese texts with several feature enrichment steps including (a) a stage of enriching features by adding hidden topic features and (b) a stage of dimensional reduction for subtracting irrelevant features. Experimental results on a data set of hotel reviews (for tourism) indicate that a reasonable amount of unlabelled data helps to increase the F1 score. Interestingly, with a small amount of labelled data, our algorithm can reach a comparative performance to the case of using a larger amount of labelled data.",109,https://doi.org/10.1080/24751839.2017.1323486,1,
1872,MASS: A Semi-supervised Multi-label Classification Algorithm with Specific Features.,"Multi-label Classification (MLC), which recently has attracted numerous attentions, aims at building classification models for objects assigned with multiple class labels simultaneously. Existing approaches for MLC mainly focus on improving supervised learning which needs a relatively large amount of labeled data for training. In this work, we propose a semi-supervised MLC algorithm to exploit unlabeled data for enhancing the performance. In the training process, our algorithm exploits the specific features per prominent class label chosen by a greedy approach as an extension of LIFT algorithm, and unlabeled data consumption mechanism from TESC. In classification, the 1-Nearest-Neighbor (1NN) is applied to select appropriate class labels for a new data instance. Our experimental results on a data set of hotel (for tourism) reviews indicate that a reasonable amount of unlabeled data helps to increase the F1 score. Interestingly, with a small amount of labeled data, our algorithm can reach comparative performance to a larger amount of labeled data.",109,https://doi.org/10.1007/978-3-319-56660-3_4,1,
1873,Automatic Detection of Singular Points in Fingerprint Images Using Convolution Neural Networks.,"Minutiae based matching, the most popular approach used in fingerprint matching algorithms, is to calculate the similarity by finding the maximum number of matched minutiae pairs in two given fingerprints. With no prior knowledge about anchor/clue to match, this becomes a combinatorial problem. Global features of the fingerprints (e.g., singular core and delta points) can be used as the anchor to speed up the matching process. Most approaches use the conventional Poincare Index method with additional techniques to improve the detection of the core and delta points. Our approach uses Convolution Neural Networks which gained state-of-the-art results in many computer vision tasks to automatically detect those points. With the experimental results on FVC2002 database, we achieved the accuracy and false alarm of (96%, 7.5%) and (90%, 6%) for detecting core, and delta points, correspondingly. These results are comparative to those of the detection algorithms with human knowledge.",109,https://doi.org/10.1007/978-3-319-54430-4_20,1,
1874,Exploiting Distance Graph and Hidden Topic Models for Multi-label Text Classification.,"Hidden topic models, the method to automatically detect the topics which are (hidden in a text) represented by words, have been successfully in many text mining tasks including text classification. They help to get the semantics of text by abstracting the words in text into topics. Another new method for text representation is distance graph model, which has the ability of preserving the local order of words in text, thus, enhancing the text semantics. This paper proposes a method to combine both hidden topic and distance graph models for opinion mining in hotel review domain using multi-label classification approach. Experiments show the efficiency of the proposed model provides a better performance of 4% than that of the baseline.",109,https://doi.org/10.1007/978-3-319-56660-3_28,1,
1875,Exploiting GPU for Large Scale Fingerprint Identification.,"Fingerprints are the most used biometrics features for identification. Although state-of-the-art algorithms are very accurate, but the need for fast processing speed for databases containing millions fingerprints is highly demanding. GPU devices are widely used in parallel computing tasks for its efficiency and low-cost. In this paper, we propose to adapt minutia cylinder-code (MCC) matching algorithm, an efficient algorithm in term of accuracy to GPU. The proposed method fits well with the architecture of the GPU that makes it easy to implement. The results of our experiments with a GTX- 680 device show that the proposed algorithm can perform 8.5 millions matches in a second that is suitable for real time identification systems having databases containing millions of fingerprints.",109,https://doi.org/10.1007/978-3-662-49381-6_66,1,
1876,A Trace Clustering Solution Based on Using the Distance Graph Model.,"Process discovery is the most important task in the process mining. Because of the complexity of event logs (i.e. activities of several different processes are written into the same log), the discovered process models may be diffuse and unintelligible. That is why the input event logs should be clustered into simpler event sub-logs. This work provides a trace clustering solution based on the idea of using the distance graph model for trace representation. Experimental results proved the effect of the proposed solution on two measures of Fitness and Precision, especially the effect on the Precision measure.",109,https://doi.org/10.1007/978-3-319-45243-2_29,1,
1877,DESRM: a disease extraction system for real-time monitoring.,"In this paper, we proposed a method that combines semantic rules and machine learning to extract infectious disease events in Vietnamese electronic news for a real-time monitoring system of spreading status. Our method includes two important steps: detecting disease events from unstructured text and extracting information of the disease event. The detection phrase uses semantic rules and machine learning to detect a disease event; in the later step, named entity recognition (NER), rules, and dictionaries are utilised to capture the events information. The performance of the two steps has F-score of 77.33% (2.36% better than the baseline's) and 91.89% (4.31% better than the baseline's) correspondingly. The promising results from the comparisons showed that our method is suitable for extracting disease events in Vietnamese text.",109,https://doi.org/10.1504/IJCVR.2015.071341,1,
1878,Recognizing Textual Entailment in Vietnamese Text: An Experimental Study.,"This paper proposes a model which utilizes Support Vector Machines (SVMs) - a machine learning approach for recognizing textual entailment in Vietnamese text, including three steps: (1) feature extraction, (2) training and (3) judgement by voting. In the first step, many features (e.g., Euclidean distance, Cosine, if-idf, etc) were extracted to train three classification models for the second step. The final step judged whether there is an entailment relation between a text and a hypothesis (another text can be plausibly inferred from the original one) or not. To improve the recognition quality, a combination of classifiers was proposed under voting method as human judgement on Vietnamese version of RTE-3. By using voting, our approach obtained significant improvements (from 1.2% to 9.4% of F-score) in comparison with baselines and ensemble methods, e.g. AdaBoost and Bagging.",109,https://doi.org/10.1109/KSE.2015.23,1,
1879,TSum4act: A Framework for Retrieving and Summarizing Actionable Tweets During a Disaster for Reaction.,"Social networks (e.g. Twitter) have been proved to be an almost real-time mean of information spread, thus they can be exploited as a valuable channel of information for emergencies (e.g. disasters) during which people need updated information for suitable reactions. In this paper, we present TSum4act, a framework designed to tackle the challenges of tweets (e.g. diversity, large volume, and noise) for disaster responses. The objective of the framework is to retrieve actionable tweets (e.g. casualties, cautions, and donations) that were posted during disasters. For this purpose, the framework first identifies informative tweets to remove noise; then assigns informative tweets into topics to preserve the diversity; next summarizes the topics to be compact; and finally ranks the results for user’s faster scan. In order to improve the performance, we proposed to incorporate event extraction for enriching the semantics of tweets. TSum4act has been successfully tested on Joplin tornado dataset of 230.535 tweets and the completeness of 0.58 outperformed 17%, of the retweet baseline’s.",109,https://doi.org/10.1007/978-3-319-18032-8_6,1,
1880,Novel Operations for FP-Tree Data Structure and Their Applications.,"Frequent Pattern Tree (FP-tree) proposed by Han et al. is a data structure that is used for storing frequent patterns (or itemsets) in association rule mining. FP-tree helps to reduce the number of database (DB) scans to only two, and shrink down the number of candidates of frequent patterns. This paper proposes to define some operations on the FP-tree in order to empower its application. With the devised operations, we can: a) incrementally build the FP-tree when only a subset of a DB is ready at a time; b) construct FP-tree in parallel with low cost of communication; c) build local FP-trees independently based on local database and then use them to construct the global FP-tree in a distributed system; d) prune the FP-tree according to different values of minimum support threshold for frequent pattern mining.",109,https://doi.org/10.1007/978-3-319-06569-4_17,1,
1881,Measuring the Influence of Bloggers in Their Community Based on the H-index Family.,"Nowadays, people in social networks can have impact on the actual society, e.g. a post on a person’s space can lead to real actions of other people in many areas of life. This is called social influence and the task of evaluating the influence is called social influence analysis which can be exploited in many fields, such as typical marketing (object oriented advertising), recommender systems, social network analysis, event detection, expert finding, link prediction, ranking, etc. The h-index, proposed by Hirsch in 2005, is now a widely used index for measuring both the productivity and impact of the published work of a scientist or scholar. This paper proposes to use h-index to measure the blogger influence in a social community. We also propose to enhance information for h-index (as well as its variants) calculation, and our experimental results are very promising.",109,https://doi.org/10.1007/978-3-319-06569-4_23,1,
1882,A Compact FP-Tree for Fast Frequent Pattern Retrieval.,"Frequent patterns are useful in many data
mining problems including query
suggestion. Frequent patterns can be
mined through frequent pattern tree (FPtree) data structure which is used to store
the compact (or compressed)
representation of a transaction database
(Han, et al, 2000). In this paper, we
propose an algorithm to compress
frequent pattern set into a smaller one,
and store the set in a modified version of
FP-tree (called compact FP-tree) as an
inverted indexing of patterns for later
quick retrieval (for query suggestion).
With the compact FP-tree, we can also
restore the original frequent pattern set.
Our experiment results show that our
compact FP-tree has a very good
compression ratio, especially on sparse
dataset which is the nature of query log. ",109,https://aclanthology.org/Y13-1045/,1,
1883,Applying hidden topics in ranking social update streams on Twitter.,"As the number of users using Twitter 1 increases, an user may have a lot of friends whose tweet (posting) list (also called as “social update stream” [5, 8, 18]) may overwhelm his/her homepage. This can lead to the situation where important tweets (i.e. the tweets the user is interested in) are pushed down on the list, thus, it takes time to find them. Social update stream ranking is a possible solution that puts important tweets on the top of the page, so that the user can easily read it. In this paper, we propose to apply hidden topics [1, 15, 20] in the Combined Regression Ranking algorithm [2] to rank social update streams. The proposed system works like a content based recommendation system. The experimental results show a significant improvement proving that our proposal is a suitable direction.",109,https://doi.org/10.1109/RIVF.2013.6719890,1,
1884,Co-reference Resolution in Vietnamese Documents Based on Support Vector Machines.,"Co-reference resolution task still poses many challenges due to the complexity of the Vietnamese language, and the lack of standard Vietnamese linguistic resources. Based on the mention-pair model of Rahman and Ng. (2009) and the characteristics of Vietnamese, this paper proposes a model using support vector machines (SVM) to solve the co-reference in Vietnamese documents. The corpus used in experiments to evaluate the proposed model was constructed from 200 articles in cultural and social categories from vnexpress.net newspaper website. The results of the initial experiments of the proposed model achieved 76.51% accuracy in comparison with that of the baseline model of 73.79% with similar features.",109,https://doi.org/10.1109/IALP.2011.63,1,
1885,Acquisition of Named-Entity-Related Relations for Searching.,"Named entities (NEs) are important in many Natural Language Processing
(NLP) applications, and discovering NE-related relations in texts may be beneficial for
these applications. This paper proposes a method to extract the ISA relation between a
“named entity” and its category, and an IS-RELATED-TO relation between the category
and its related object. Based on the pattern extraction algorithm “Person Category
Extraction” (PCE), we extend it for solving our problem. Our experiments on Wall Street
Journal (WSJ) corpus show promising results. We also demonstrate a possible application
of these relations by utilizing them for semantic search. ",109,https://aclanthology.org/Y07-1036/,1,
1886,Solving Reduction Problems in Cover Lattice Based Decision Tables.,"Covering based rough set is an important extension of Pawlak's traditional rough set. Reduction is a typical application of rough sets, including traditional, covering based and other rough set extensions. Although this task has several proposals, it is still an open problem to decision systems (tables) in covering based rough set. This paper focuses on the reduction problem for the condition lattice, and fitting problem for the decision lattice in the decision table based on cover lattice. A corresponding algorithm is proposed for each problem. Two examples to illustrate a covering based decision table and two related problems show the applications of these concepts and problems.",110,https://doi.org/10.1007/978-981-16-1685-3_5,1,
1887,N-Tier Machine Learning-Based Architecture for DDoS Attack Detection.,"Distributed Denial of Service (DDoS) attack is a menace to network security that aims at exhausting the target networks with malicious traffic. With simple but powerful attack mechanisms, it introduces an immense threat to the current Internet community. In this paper, we propose a novel multi-tier architecture intrusion detection model based on a machine learning method that possibly detects DDoS attacks. We evaluate our model using the newly released dataset CICDDoS2019, which contains a comprehensive variety of DDoS attacks and address the gaps of the existing current datasets. Experimental results indicated that the proposed method is more efficient than other existing ones. The experiments demonstrated that the proposed model accurately recognize DDoS attacks outperforming the state-of-the-art by F1-score.",110,https://doi.org/10.1007/978-3-030-73280-6_30,1,
1888,Detection of Distributed Denial of Service Attacks Using Automatic Feature Selection with Enhancement for Imbalance Dataset.,"With the development of technology, the highly accessible internet service is the biggest demand for most people. Online network, however, has been suffering from malicious attempts to disrupt essential web technologies, resulting in service failures. In this work, we introduced a model to detect and classify Distributed Denial of Service attacks based on neural networks that take advantage of a proposed automatic feature selection component. The experimental results on CIC-DDoS 2019 dataset have demonstrated that our proposed model outperformed other machine learning-based model by large margin. We also investigated the effectiveness of weighted loss and hinge loss on handling the class imbalance problem.",110,https://doi.org/10.1007/978-3-030-73280-6_31,1,
1889,UETrice at MEDIQA 2021: A Prosper-thy-neighbour Extractive Multi-document Summarization Model.,"This paper describes a system developed to summarize multiple answers challenge in the MEDIQA 2021 shared task collocated with the BioNLP 2021 Workshop. We propose an extractive summarization architecture based on several scores and state-of-the-art techniques. We also present our novel prosper-thy-neighbour strategies to improve performance. Our model has been proven to be effective with the best ROUGE-1/ROUGE-L scores, being the shared task runner up by ROUGE-2 F1 score (over 13 participated teams).",110,https://doi.org/10.18653/v1/2021.bionlp-1.36,1,
1890,Models Distillation with Lifelong Deep Learning for Vietnamese Biomedical Named Entity Recognition.,"In realistic data, named entities may appear in a variety of rich contexts with unique characteristics and the performance of the named entity recognition (NER) task directly affects other NLP problems. Although both lifelong learning and deep learning have proven effective in many problems, including NER, the suitable combination of these two research directions is still limited. This paper describes a lifelong deep learning model for Vietnamese Biomedical NER based on model distillation mechanism. Our approach achieves potential results, helps to boost 2.16% compared to original deep learning model.",110,https://doi.org/10.1109/KSE53942.2021.9648790,1,
1891,Vietnamese Fake News Detection Based on Hybrid Transfer Learning Model and TF-IDF.,"There are a lot of studies about fake news detection on English social networks. However, Vietnamese fake news detection on social networks still limit. In this paper, we propose a new approach for Vietnamese Fake News Detection on Social Network Sites using a pre-train language model PhoBERT combine with Term Frequency - Inverse Document Frequency (TF-IDF) for word embedding and Convolutional Neural Network (CNN) for features extracting. Our proposed model is trained and evaluated on the dataset of Reliable Intelligence Identification on Vietnamese SNSs (ReINTEL) shared task. We process text data into two scenarios: raw data and processed data to elucidate the hypothesis of pre-processing data on social networks. In addition, we use the different extra features to improve the efficiency of model. We compare our proposed model with the baseline methods. The proposed model achieved outstanding results with 0.9538 AUC score on raw data.",110,https://doi.org/10.1109/KSE53942.2021.9648676,1,
1892,Bisimulation and bisimilarity for fuzzy description logics under the Gödel semantics.,"Description logics (DLs) are a suitable formalism for representing knowledge about domains in which objects are described not only by attributes but also by binary relations between objects. Fuzzy extensions of DLs can be used for such domains when data and knowledge about them are vague and imprecise. One of the possible ways to specify classes of objects in such domains is to use concepts in fuzzy DLs. As DLs are variants of modal logics, indiscernibility in DLs is characterized by bisimilarity. The bisimilarity relation of an interpretation is the largest auto-bisimulation of that interpretation. In DLs and their fuzzy extensions, such equivalence relations can be used for concept learning. In this paper, we define and study fuzzy bisimulation and bisimilarity for fuzzy DLs under the Gödel semantics, as well as crisp bisimulation and strong bisimilarity for such logics extended with involutive negation. The considered logics are fuzzy extensions of the DL  (a variant of PDL) with additional features among inverse roles, nominals, (qualified or unqualified) number restrictions, the universal role, local reflexivity of a role and involutive negation. We formulate and prove results on invariance of concepts under fuzzy (resp. crisp) bisimulation, conditional invariance of fuzzy TBoxes/ABoxes under bisimilarity (resp. strong bisimilarity), and the Hennessy-Milner property of fuzzy (resp. crisp) bisimulation for fuzzy DLs without (resp. with) involutive negation under the Gödel semantics. Apart from these fundamental results, we also provide results on using fuzzy bisimulation to separate the expressive powers of fuzzy DLs, as well as results on using strong bisimilarity to minimize fuzzy interpretations.",110,https://doi.org/10.1016/j.fss.2019.08.004,1,
1893,Exploiting CBOW and LSTM Models to Generate Trace Representation for Process Mining.,"In the field of process mining, one of the challenges of the trace representation problem is to exploit a lot of potentially useful information within the traces while keeping a low dimension of the corresponding vector space. Motivated by the initial results of applying the deep neural networks for producing trace representation, in this paper, we continue to study and apply two more advanced models of deep learning, i.e., Continuous Bag of Words and Long short-term memory, for generating the trace representation. The experimental results have achieved significant improvement, i.e., not only showing the close relationship between the activities in a trace but also helping to reduce the dimension of trace representation.",110,https://doi.org/10.1007/978-981-15-3380-8_4,1,
1894,A Lifelong Sentiment Classification Framework Based on a Close Domain Lifelong Topic Modeling Method.,"In lifelong machine learning, the determination of the hypotheses related to the current task is very meaningful thanks to the reduction of the space to look for the knowledge patterns supporting for solving the current task. However, there are few studies for this problem. In this paper, we propose the definitions for measuring the “close domains to the current domain”, and a lifelong sentiment classification method based on using the close domains for topic modeling the current domain. Experimental results on sentiment datasets of product reviews from Amazon.com show the promising performance of system and the effectiveness of our approach.",110,https://doi.org/10.1007/978-3-030-41964-6_50,1,
1895,A Targeted Topic Model based Multi-Label Deep Learning Classification Framework for Aspect-based Opinion Mining.,"Recently, deep Convolutional Neural Network (CNN) model has achieved remarkable results in Natural Language Processing (NLP) tasks, such as information retrieval, relation classification, semantic parsing, sentence modeling and other traditional NLP tasks, etc. On the other hand, topic modeling method has been proved to be effective by exploiting hidden knowledge in a corpus of documents. Motivated from these successes, we propose a framework that takes the advantages of closure domain measure to get enriched knowledge from close domains to the training dataset to improve the CNN model, and apply a Targeted Topic Model to take more detailed exploration on each labeled aspect of an opinion. Experimental results on different scenarios show the effectiveness of the proposed framework for multi-label classification task in comparison to other related models on the same Hotel review dataset.",110,https://doi.org/10.1109/KSE50997.2020.9287397,1,
1896,Improving Named Entity Recognition in Vietnamese Texts by a Character-Level Deep Lifelong Learning Model.,"Named entity recognition (NER) is a fundamental task which affects the performance of its dependent task, e.g. machine translation. Lifelong machine learning (LML) is a continuous learning process, in which the knowledge base accumulated from previous tasks will be used to improve future learning tasks having few samples. Since there are a few studies on LML based on deep neural networks for NER, especially in Vietnamese, we propose a lifelong learning model based on deep learning with a CRFs layer, named DeepLML–NER, for NER in Vietnamese texts. DeepLML–NER includes an algorithm to extract the knowledge of “prefix-features” of named entities in previous domains. Then the model uses the knowledge in the knowledge base to solve the current NER task. Preprocessing and model parameter tuning are also investigated to improve the performance. The effect of the model was demonstrated by in-domain and cross-domain experiments, achieving promising results.",110,https://doi.org/10.1142/S219688881950026X,1,
1897,Improving Semantic Relation Extraction System with Compositional Dependency Unit on Enriched Shortest Dependency Path.,"Experimental performance on the task of relation extraction/classification has generally improved using deep neural network architectures. In which, data representation has been proven to be one of the most influential factors to the model’s performance but still has many limitations. In this work, we take advantage of compressed information in the shortest dependency path (SDP) between two corresponding entities to classify the relation between them. We propose (i) a compositional embedding that combines several dominant linguistic as well as architectural features and (ii) dependency tree normalization techniques for generating rich representations for both words and dependency relations in the SDP. We also present a Convolutional Neural Network (CNN) model to process the proposed SDP enriched representation. Experimental results for both general and biomedical data demonstrate the effectiveness of compositional embedding, dependency tree normalization technique as well as the suitability of the CNN model.",110,https://doi.org/10.1007/978-3-030-14799-0_12,1,
1898,An Adversarial Learning and Canonical Correlation Analysis Based Cross-Modal Retrieval Model.,"The key of cross-modal retrieval approaches is to find a maximally correlated subspace among multiple datasets. This paper introduces a novel Adversarial Learning and Canonical Correlation Analysis based Cross-Modal Retrieval (ALCCA-CMR) model. For each modality, the ALCCA phase finds an effective common subspace and calculates the similarity by canonical correlation analysis embedding for cross-modal retrieval. We demonstrate an application of ALCCA-CMR model implemented for the dataset of two modalities. Experimental results on real music data show the efficacy of the proposed method in comparison with other existing ones.",110,https://doi.org/10.1007/978-3-030-14799-0_13,1,
1899,A Probability-Based Close Domain Metric in Lifelong Learning for Multi-label Classification.,"Lifelong machine learning has recently become a hot topic attracting the researchers all over the world by its effectiveness in dealing with current problem by exploiting the past knowledge. The combination of topic modeling on previous domain knowledge (such as topic modeling with Automatically generated Must-links and Cannot-links, which exploits must-link and cannot-link of two terms), and lifelong topic modeling (which employs the modeling of previous tasks) is widely used to produce better topics. This paper proposes a close domain metric based on probability to choose valuable knowledge learnt from the past to produce more associated topics on the current domain. This knowledge is, then, used to enrich features for multi-label classifier. Several experiments performed on review dataset of hotel show that the proposed approach leads to an improvement in performance over the baseline.",110,https://doi.org/10.1007/978-3-030-38364-0_13,1,
1900,QASA: Advanced Document Retriever for Open-Domain Question Answering by Learning to Rank Question-Aware Self-Attentive Document Representations.,"For information consumers, being able to obtain a short and accurate answer for a query is one of the most desirable features. This motivation, along with the rise of deep learning, has led to a boom in open-domain Question Answering (QA) research. While the problem of machine comprehension has received multiple success with the help of large training corpora and the emergence of attention mechanism, the development of document retrieval in open-domain QA is lagged behind. In this work, we propose a novel encoding method for learning question-aware self-attentive document representations. By applying pair-wise ranking approach to these encodings, we build a Document Retriever, called QASA, which is then integrated with a machine reader to form a complete open-domain QA system. Our system is thoroughly evaluated using QUASAR-T dataset and shows surpassing results compared to other state-of-the-art methods.",110,https://doi.org/10.1145/3310986.3310999,1,
1901,Ontology-Based Semantic Search for National Database of Natural Resources and Environment.,"Semantic search helps the user queries to be understandable for electric agents searching. In this way, ontology plays the main role to define the semantic and the relations between user queries. The national database of natural resources and environment is a very large database system. Therefore, building a search engine software for the system with high accuracy and fast speed is very important for sharing information in the field of natural resources and environment. However, the existing search engine software in the national database needs to be improved to better meet user’s needs. We proposed the architecture of ontology-based semantic search for the national database of natural resources and environment. Based on the proposed architecture, we have built semantic search software (VnNRESS) to demonstrate better results than the existing search software (NRESearch).",110,https://doi.org/10.1007/978-3-030-30149-1_13,1,
1902,A Compact Trace Representation Using Deep Neural Networks for Process Mining.,"In process mining, trace representation has a significant effect on the process discovery problem. The challenge is to get highly informative but low-dimensional vector space from event logs. This is required to improve the quality of the trace clustering problem for generating the process models clear enough to inspect. Though traditional trace representation methods have specific advantages, their vector space often has a big number of dimensions. In this paper, we address this problem by proposing a new trace representation method based on the deep neural networks. Experimental results prove our proposal not only is better than the alternatives, but also significantly helps to reduce the dimension of trace representation.",110,https://doi.org/10.1109/KSE.2019.8919355,1,
1903,On the possibility of correct concept learning in description logics.,"It is well known that any Boolean function in classical propositional calculus can be learned correctly if the training information system is good enough. In this paper, we extend that result for description logics. We prove that any concept in any description logic that extends \mathcal {ALC} with some features amongst I (inverse roles), Q_k (qualified number restrictions with numbers bounded by a constant k), and \mathsf {Self} (local reflexivity of a role) can be learned correctly if the training information system (specified as a finite interpretation) is good enough. That is, there exists a learning algorithm such that, for every concept C of those logics, there exists a training information system such that applying the learning algorithm to it results in a concept equivalent to C. For this result, we introduce universal interpretations and bounded bisimulation in description logics and develop an appropriate learning algorithm. We also generalize common types of queries for description logics, introduce interpretation queries, and present some consequences.",110,https://doi.org/10.1007/s40595-017-0094-4,1,
1904,A Positive-Unlabeled Learning Model for Extending a Vietnamese Petroleum Dictionary Based on Vietnamese Wikipedia Data.,"This study provides a positive-unlabeled learning model for extending a Vietnamese petroleum dictionary based on Vietnamese Wikipedia data. Machine learning algorithms with positive and unlabeled data together with separated and combined between Google similarity distance and Cosine similarity distance, used in this study. The data sources used to integrate are English - Vietnamese oil and gas dictionary and the Vietnamese Wikipedia. In the results, a novelty way for data integration with higher accuracy by using a combination of algorithms. The first Vietnamese oil and gas ontology was built in Vietnam. This ontology is a useful tool for staff in the oil and gas industry in training, research, search daily.",110,https://doi.org/10.1007/978-3-319-75417-8_18,1,
1905,A New Lifelong Topic Modeling Method and Its Application to Vietnamese Text Multi-label Classification.,"Lifelong machine learning is emerging in recent years thanks to its ability to use past knowledge for current problem. Lifelong topic modeling algorithms, such as LTM and AMC, are proposed and they are very useful. However, these algorithms focus on learning bias on the topic level not the domain level. This paper proposes a lifelong topic modeling method, which focuses on learning bias on the domain level based on a proposed domain closeness measure, and an application framework for multi-label classification on Vietnamese texts. Experimental results on three previously solved Vietnamese texts, and five different current Vietnamese text datasets in combination with different topic set sizes showed that our proposed method is better than AMC method for all cases.",110,https://doi.org/10.1007/978-3-319-75417-8_19,1,
1906,A Learning Method based on Bisimulation in Inconsistent Knowledge Systems.,"Inconsistencies may naturally occur in the considered application domains in Artificial Intelligence, for example as a result of data mining works in distributed sources. In order to solve inconsistent knowledge, several paraconsistent description logics have been proposed. In this paper, we face the problem of concept learning for an inconsistent knowledge base system based on bisimulation. This algorithm allows learning a concept from a training information system in a paraconsistent descriptive logic system with a set of positive items, negative items, and inconsistent items. Here, we present a system for learning concept in an inconsistent knowledge base and discuss preliminary experimental results obtained in the electronic application domain.",110,https://doi.org/10.1109/ICARCV.2018.8581062,1,
1907,A New Text Semi-supervised Multi-label Learning Model Based on Using the Label-Feature Relations.,"Multi-label learning has become popular and omnipresent in many real-world problems, especially in text classification applications, in which an instance could belong to different classes simultaneously. Due to these label constraints, there are some challenges occurring in building multi-label data. Semi-supervised learning is one possible approach to exploit abundantly unlabeled data for enhancing the classification performance with a small labeled dataset. In this paper, we propose a solution to select the most influential label based on using the relations among the labels and features to a semi-supervised multi-label classification algorithm on texts. Experiments on two datasets of Vietnamese reviews and English emails of Enron show the positive effects of the proposal.",110,https://doi.org/10.1007/978-3-319-98443-8_37,1,
1908,Fuzzy Bisimulations in Fuzzy Description Logics Under the Gödel Semantics.,"Book cover
International Joint Conference on Rough Sets

IJCRS 2018: Rough Sets pp 559–571Cite as

Fuzzy Bisimulations in Fuzzy Description Logics Under the Gödel Semantics
Quang-Thuy Ha, Linh Anh Nguyen, Thi Hong Khanh Nguyen & Thanh-Luong Tran 
Conference paper
First Online: 15 August 2018
809 Accesses

2 Citations

Part of the Lecture Notes in Computer Science book series (LNAI,volume 11103)

Abstract
Description logics (DLs) are a suitable formalism for representing knowledge about domains in which objects are described not only by attributes but also by binary relations between objects. Fuzzy DLs can be used for such domains when data and knowledge about them are vague. One of the possible ways to specify classes of objects in such domains is to use concepts in fuzzy DLs. As DLs are variants of modal logics, indiscernibility in DLs is characterized by bisimilarity. The bisimilarity relation of an interpretation is the largest auto-bisimulation of that interpretation. In (fuzzy) DLs, it can be used for concept learning. In this paper, for the first time, we define fuzzy bisimulation and (crisp) bisimilarity for fuzzy DLs under the Gödel semantics. The considered logics are fuzzy extensions of the DL ALCreg with additional features among inverse roles, nominals, qualified number restrictions, the universal role and local reflexivity of a role. We give results on invariance of concepts as well as conditional invariance of TBoxes and ABoxes for bisimilarity in fuzzy DLs under the Gödel semantics. We also provide a theorem on the Hennessy-Milner property for fuzzy bisimulations in fuzzy DLs under the Gödel semantics",110,https://doi.org/10.1007/978-3-319-99368-3_44,1,
1909,A New Trace Clustering Algorithm Based on Context in Process Mining.,"In process mining, trace clustering is an important technique that at-tracts the attention of researchers to solve the large and complex volume of event logs. Traditional trace clustering often uses available data mining algorithms which do not exploit the characteristic of processes. In this study, we propose a new trace clustering algorithm, especially for the process mining, based on the using trace context. The proposed clustering algorithm can automatic detects the number of clusters, and it does not need a convergence iteration like traditional ones like K-means. The algorithm takes two loops over the input to generate the clusters, thus the complexity is greatly reduced. Experimental results show that our method also has good results when compared to traditional methods.",110,https://doi.org/10.1007/978-3-319-99368-3_50,1,
1912,A semi-supervised multi-label classification framework with feature reduction and enrichment.,"Multi-label classification (MLC) has drawn much attention, thanks to its usefulness and omnipresence in real-world applications in which objects may be characterized by more than one label as in the traditional approach. Getting multi-label examples is costly and time-consuming; therefore, semi-supervised learning approach should be considered to take advantages of both labelled and unlabelled data. In this work, we propose a semi-supervised MLC algorithm exploiting the specific features of the prominent class label(s) chosen by a greedy approach as an extension of the LIFT algorithm, and unlabelled data consumption mechanism from the TExt classification using Semi-supervised Clustering algorithm. We also make a semi-supervised MLC application framework for Vietnamese texts with several feature enrichment steps including (a) a stage of enriching features by adding hidden topic features and (b) a stage of dimensional reduction for subtracting irrelevant features. Experimental results on a data set of hotel reviews (for tourism) indicate that a reasonable amount of unlabelled data helps to increase the F1 score. Interestingly, with a small amount of labelled data, our algorithm can reach a comparative performance to the case of using a larger amount of labelled data.",110,https://doi.org/10.1080/24751839.2017.1323486,1,
1913,Bisimilarity for paraconsistent description logics.,"We introduce comparisons w.r.t. information between interpretations in paraconsistent description logics and use them to define bisimilarity for such logics. This notion is useful for concept learning in description logics when inconsistencies occur. We give preservation results and the Hennessy-Milner property for comparisons w.r.t. information in paraconsistent description logics. As consequences, we obtain also invariance results and the Hennessy-Milner property for bisimilarity in paraconsistent description logics.",110,https://doi.org/10.3233/JIFS-169120,1,
1914,MASS: A Semi-supervised Multi-label Classification Algorithm with Specific Features.,"Multi-label Classification (MLC), which recently has attracted numerous attentions, aims at building classification models for objects assigned with multiple class labels simultaneously. Existing approaches for MLC mainly focus on improving supervised learning which needs a relatively large amount of labeled data for training. In this work, we propose a semi-supervised MLC algorithm to exploit unlabeled data for enhancing the performance. In the training process, our algorithm exploits the specific features per prominent class label chosen by a greedy approach as an extension of LIFT algorithm, and unlabeled data consumption mechanism from TESC. In classification, the 1-Nearest-Neighbor (1NN) is applied to select appropriate class labels for a new data instance. Our experimental results on a data set of hotel (for tourism) reviews indicate that a reasonable amount of unlabeled data helps to increase the F1 score. Interestingly, with a small amount of labeled data, our algorithm can reach comparative performance to a larger amount of labeled data.",110,https://doi.org/10.1007/978-3-319-56660-3_4,1,
1915,Exploiting Distance Graph and Hidden Topic Models for Multi-label Text Classification.,"Hidden topic models, the method to automatically detect the topics which are (hidden in a text) represented by words, have been successfully in many text mining tasks including text classification. They help to get the semantics of text by abstracting the words in text into topics. Another new method for text representation is distance graph model, which has the ability of preserving the local order of words in text, thus, enhancing the text semantics. This paper proposes a method to combine both hidden topic and distance graph models for opinion mining in hotel review domain using multi-label classification approach. Experiments show the efficiency of the proposed model provides a better performance of 4% than that of the baseline.",110,https://doi.org/10.1007/978-3-319-56660-3_28,1,
1916,Improving chemical-induced disease relation extraction with learned features based on convolutional neural network.,"There have been an increasing number of various machine learning-based models successfully proposed and applied for automatic chemical-induced disease (CID) relation extraction. They, however, usually require carefully handcrafted rich feature sets, which rely on expert knowledge, thus require expensive human labor but normally still cannot generalize data well enough. In this paper, we propose a CID relation extraction model that learns features automatically through a Convolutional Neural Network (CNN) instead of traditional handcrafted features. We exploit the shortest dependency path between a disease and a chemical for identifying their CID relation. Dependency relations, with and without their direction information, are further investigated. Experimental results on benchmark datasets (namely the BioCreative V dataset) are very potential, demonstrating the effectiveness of our proposed model for CID relation extraction.",110,https://doi.org/10.1109/KSE.2017.8119474,1,
1917,Sieve-based coreference resolution enhances semi-supervised learning model for chemical-induced disease relation extraction.,"The BioCreative V chemical-disease relation (CDR) track was proposed to accelerate the progress of text mining in facilitating integrative understanding of chemicals, diseases and their relations. In this article, we describe an extension of our system (namely UET-CAM) that participated in the BioCreative V CDR. The original UET-CAM system’s performance was ranked fourth among 18 participating systems by the BioCreative CDR track committee. In the Disease Named Entity Recognition and Normalization (DNER) phase, our system employed joint inference (decoding) with a perceptron-based named entity recognizer (NER) and a back-off model with Semantic Supervised Indexing and Skip-gram for named entity normalization. In the chemical-induced disease (CID) relation extraction phase, we proposed a pipeline that includes a coreference resolution module and a Support Vector Machine relation extraction model. The former module utilized a multi-pass sieve to extend entity recall. In this article, the UET-CAM system was improved by adding a ‘silver’ CID corpus to train the prediction model. This silver standard corpus of more than 50 thousand sentences was automatically built based on the Comparative Toxicogenomics Database (CTD) database. We evaluated our method on the CDR test set. Results showed that our system could reach the state of the art performance with F1 of 82.44 for the DNER task and 58.90 for the CID task. Analysis demonstrated substantial benefits of both the multi-pass sieve coreference resolution method (F1 + 4.13%) and the silver CID corpus (F1 +7.3%).",110,https://doi.org/10.1093/database/baw102,1,
1918,Argumentation Framework for Merging Stratified Belief Bases.,"This paper introduces a new approach for belief merging by using argumentation technique. The key idea is to organize each belief merging process as a game in which participating agents use argumentation technique to debate on their own belief bases to achieve consensus i.e. a common belief base. To this end, we introduce a framework for merging belief by argumentation in which an argumentation-based belief merging protocol is proposed and a set of intuitive and rational postulates to characterize the merging results is introduced. Several logical properties of the family of argumentation-based belief merging operators are also pointed out and discussed.",110,https://doi.org/10.1007/978-3-662-49381-6_5,1,
1919,An Experimental Study on Cholera Modeling in Hanoi.,"Cholera modeling for prediction Cholera state is the research topic is specially considered by researchers. An experimental frame for Cholera modeling based on using data mining techniques is showed in this paper. In the Data Preparation Phase, two versions of data presentation and a solution for feature selection are considered. Moreover, both cases of two-value Cholera state and three-value Cholera state also investigated. Experimental results show that the global presentation is better than the local presentation, F1 measures are between 0.79 and 0.86, and the target variable has correlation with climate condition variables in some cases.",110,https://doi.org/10.1007/978-3-662-49390-8_22,1,
1920,Recognizing Textual Entailment in Vietnamese Text: An Experimental Study.,"This paper proposes a model which utilizes Support Vector Machines (SVMs) - a machine learning approach for recognizing textual entailment in Vietnamese text, including three steps: (1) feature extraction, (2) training and (3) judgement by voting. In the first step, many features (e.g., Euclidean distance, Cosine, if-idf, etc) were extracted to train three classification models for the second step. The final step judged whether there is an entailment relation between a text and a hypothesis (another text can be plausibly inferred from the original one) or not. To improve the recognition quality, a combination of classifiers was proposed under voting method as human judgement on Vietnamese version of RTE-3. By using voting, our approach obtained significant improvements (from 1.2% to 9.4% of F-score) in comparison with baselines and ensemble methods, e.g. AdaBoost and Bagging.",110,https://doi.org/10.1109/KSE.2015.23,1,
1921,Novel Operations for FP-Tree Data Structure and Their Applications.,"Frequent Pattern Tree (FP-tree) proposed by Han et al. is a data structure that is used for storing frequent patterns (or itemsets) in association rule mining. FP-tree helps to reduce the number of database (DB) scans to only two, and shrink down the number of candidates of frequent patterns. This paper proposes to define some operations on the FP-tree in order to empower its application. With the devised operations, we can: a) incrementally build the FP-tree when only a subset of a DB is ready at a time; b) construct FP-tree in parallel with low cost of communication; c) build local FP-trees independently based on local database and then use them to construct the global FP-tree in a distributed system; d) prune the FP-tree according to different values of minimum support threshold for frequent pattern mining.",110,https://doi.org/10.1007/978-3-319-06569-4_17,1,
1922,Improving Text-Based Image Search with Textual and Visual Features Combination.,"With the huge number of available images on the web, an effective image retrieval system has been more and more needed. Improving the performance is one of crucial tasks in modern text-based image retrieval systems such as Google Image Search, Frickr, etc. In this paper, we propose a unified framework to cluster and re-rank returned images with respect to an input query. However, owning to a difference to previous methods of using only either textual or visual features of an image, we combine the textual and visual features to improve search performance. The experimental results show that our proposed model can significantly improve the performance of a text-based image search system (i.e. Flickr). Moreover, the performance of the system with the combination of textual and visual features outperforms the performance of both the textual-based system and the visual-based system.",110,https://doi.org/10.1007/978-3-319-11680-8_19,1,
1923,Hidden Topic Models for Multi-label Review Classification: An Experimental Study.,"In recent years, Multi-Label Classification (MLC) has become an important task in the field of Supervised Learning. The MLC tasks are omnipresent in real-world problems in which an instance could belong to different classes simultaneously. In this paper, we present a method for MLC using the hidden topic method to enrich data features and using mutual information for feature selection. Our experiments on classifying user reviews about one thousand Vietnamese hotels showed the efficiency of the proposed approach.",110,https://doi.org/10.1007/978-3-642-40495-5_60,1,
1924,Applying hidden topics in ranking social update streams on Twitter.,"As the number of users using Twitter 1 increases, an user may have a lot of friends whose tweet (posting) list (also called as “social update stream” [5, 8, 18]) may overwhelm his/her homepage. This can lead to the situation where important tweets (i.e. the tweets the user is interested in) are pushed down on the list, thus, it takes time to find them. Social update stream ranking is a possible solution that puts important tweets on the top of the page, so that the user can easily read it. In this paper, we propose to apply hidden topics [1, 15, 20] in the Combined Regression Ranking algorithm [2] to rank social update streams. The proposed system works like a content based recommendation system. The experimental results show a significant improvement proving that our proposal is a suitable direction.",110,https://doi.org/10.1109/RIVF.2013.6719890,1,
1925,Named Entity Recognition for Vietnamese Documents Using Semi-supervised Learning Method of CRFs with Generalized Expectation Criteria.,"Named Entity Recognition (NER) is an important, useful task in many natural language processing applications and much previous work in NER has been done in many other languages such as English, Japanese, Chinese However, Vietnamese NER task is still relatively new and challenge due to the characteristics of Vietnamese, the lack of a large annotated corpus This paper presents a new approach for Vietnamese NER -- a semi-supervised training method for Conditional random fields (CRFs) models using generalized expectation criteria to express a preference for parameter settings. We perform several experiments using different feature setting and different training data to show the high performance of this method and compare to the other method.",110,https://doi.org/10.1109/IALP.2012.54,1,
1926,On C-Learnability in Description Logics.,"We prove that any concept in any description logic that extends ALC with some features amongst I (inverse), Q k (quantified number restrictions with numbers bounded by a constant k), Self (local reflexivity of a role) can be learnt if the training information system is good enough. That is, there exists a learning algorithm such that, for every concept C of those logics, there exists a training information system consistent with C such that applying the learning algorithm to the system results in a concept equivalent to C.",110,https://doi.org/10.1007/978-3-642-34630-9_24,1,
1927,Refining the Judgment Threshold to Improve Recognizing Textual Entailment Using Similarity.,"In recent years, Recognizing Textual Entailment (RTE) catches strongly the attention of the Natural Language Processing (NLP) community. Using Similarity is an useful method for RTE, in which the Judgment Threshold plays an important role as the learning model. This paper proposes an RTE model based on using similarity. We describe clearly the solutions to determine and to refine the Judgment Threshold for Improvement RTE. The measure of the synonym similarity also is considered. Experiments on a Vietnamese version of the RTE3 corpus are showed.",110,https://doi.org/10.1007/978-3-642-34707-8_34,1,
1928,A Solution for Grouping Vietnamese Synonym Feature Words in Product Reviews.,"Feature-based opinion mining is an interesting opinion mining issue. For this problem, feature words/phrases are discovered at sentence level. However, customers usually use different words/phrases referring to the same feature in reviews. To produce a meaningful summary, synonym feature words/phrases in domain, need to be grouped under the same feature. This paper proposes a solution for grouping synonym features in Vietnamese customer reviews based on semi-supervised SVM-kNN classification and HAC clustering. Experimental results on reviews in mobile phone domain demonstrate that the proposed method is promising for the task. The Purity, Accuracy measures are 0.68 and 0.65 respectively.",110,https://doi.org/10.1109/APSCC.2011.48,1,
1929,Co-reference Resolution in Vietnamese Documents Based on Support Vector Machines.,"Co-reference resolution task still poses many challenges due to the complexity of the Vietnamese language, and the lack of standard Vietnamese linguistic resources. Based on the mention-pair model of Rahman and Ng. (2009) and the characteristics of Vietnamese, this paper proposes a model using support vector machines (SVM) to solve the co-reference in Vietnamese documents. The corpus used in experiments to evaluate the proposed model was constructed from 200 articles in cultural and social categories from vnexpress.net newspaper website. The results of the initial experiments of the proposed model achieved 76.51% accuracy in comparison with that of the baseline model of 73.79% with similar features.",110,https://doi.org/10.1109/IALP.2011.63,1,
1930,An Integrated Approach Using Conditional Random Fields for Named Entity Recognition and Person Property Extraction in Vietnamese Text.,"Personal names are among one of the most frequently searched items in web search engines and a person entity is always associated with numerous properties. In this paper, we propose an integrated model to recognize person entity and extract relevant values of a pre-defined set of properties related to this person simultaneously for Vietnamese. We also design a rich feature set by using various kind of knowledge resources and a apply famous machine learning method CRFs to improve the results. The obtained results show that our method is suitable for Vietnamese with the average result is 84 % of precision, 82.56% of recall and 83.39 % of F-measure. Moreover, performance time is pretty good, and the results also show the effectiveness of our feature set.",110,https://doi.org/10.1109/IALP.2011.37,1,
1931,XPath-Wrapper Induction for Data Extraction.,The Web contains an enormous amount of information which is formatted for human beings. This makes it difficult for computer to extract relevant content from various sources. This paper presents an XPath-wrapper induction algorithm which leverages user queries and template-based sites for extracting structured information. Our experiments show average accuracy of 94%.,110,https://doi.org/10.1109/IALP.2010.33,1,
1932,A pageranking based method for identifying characteristic genes of a disease.,"Ranking is an importance task in the research field of complex biological networks, including gene networks. In this article, we propose a method for ranking the genes causing a specific type of disease, special characteristics of samples under a particular condition. Genes with the highest scores may be considered as the key factors leading to the development of disease type in the studied samples. The experience results on the gene expression data of two subtypes of Leukemia cancer disease, i.e., ALB and ALT for B-cells and T-cells are respectively shown.",110,https://doi.org/10.1109/ICNSC.2008.4525457,1,
1933,Using Cross-layer Heuristic and Network Coding to Improve Throughput in Multicast Wireless Mesh Networks.,"Wireless mesh networks (WMNs) receive much research interests because of their reliability, scalability and low cost. Obtaining high-throughput for multicast applications (e.g. video streaming broadcast) in WMNs is challenging due to the interference and the change of channel quality. Cross-layer design and network coding are approaches which have been recently received considerable attention for high-throughput problem in wireless networks. In this paper, we propose an approach namely CLNC (Cross-Layer Network Coding) which is a combination of the above approaches to improve throughput in multicast wireless mesh networks. Our simulation results show that when the number of receivers is high CLNC's throughput is higher at least 30% than that of known methods such as AODV, DSDV and DSR and higher than that of MAODV. Moreover, PDR (Packet Delivery Ration) of CLNC is higher than that of MAODV and DSDV.",110,https://doi.org/10.1109/ICOIN.2008.4472771,1,
1935,A General Fuzzy-Based Framework for Text Representation and Its Application to Text Categorization.,"In this paper we develop the general framework for text representation based on fuzzy set theory. This work is extended from our original ideas [5],[4], in which a document is represented by a set of fuzzy concepts. The importance degree of these fuzzy concepts characterize the semantics of documents and can be calculated by a specified aggregation function of index terms. Based on this representation, a general framework is proposed and applied to text categorization problem. An algorithm is given in detail for choosing fuzzy concepts. Experiments on the real-world data set show that the proposed method is superior to the conventional method for text representation in text categorization.",110,https://doi.org/10.1007/11881599_73,1,
1937,fastTIGER: A rapid method for estimating evolutionary rates of sites from large datasets.,"The evolutionary processes vary among sites of an alignment, called rate heterogeneity, that must be properly handled when analyzing the evolutionary relationships among species based on their genomic data. To this end, methods have been proposed to estimate the relative evolutionary rates between sites. Tree Independent Generation of Evolutionary Rates (TIGER) is a popular method to estimate the evolutionary rates among sites. However, the TIGER method is computationally expensive to calculate the evolutionary rates for large datasets, especially for whole genome datasets. In this paper, we present a simplified, fast, and accurate method, called fastTIGER, to estimate evolutionary rates for large datasets. Experiments on several large real datasets show that the evolutionary rates from the fastTIGER method have a reasonable correlation with ones estimated from the TIGER method while the fastTIGER method is several orders of magnitudes faster than the TIGER method. Moreover, the site rates estimated by fastTIGER method are as good as the ones estimated from the TIGER method in partitioning alignments to build maximum likelihood trees. The fastTIGER method enhances us to study the evolutionary relationships among species using their genomic data.",111,https://doi.org/10.1109/KSE53942.2021.9648748,1,
1938,iK-means: an improvement of the iterative k-means partitioning algorithm.,"The evolutionary processes vary among sites of an alignment that strongly affect the accuracy of phylogenetic tree reconstruction. Partitioning an alignment into sub-alignments of sites such that the evolutionary processes at sites in the same sub-alignment are highly similar is a proper strategy. Gene features might be used as reasonable indicators to partition an alignment. However, the gene feature information is not always available or efficient Computational partitioning methods like iterative k-means has been proposed to automatically partition sites into groups based on the similarity of evolutionary rates of sites. Despite obtaining compelling results in terms of AICc and BIC measurements, the k-means method forms a group of all and only invariant sites that results in bias/wrong phylogenetic trees. In this paper, we improve the k-means algorithm by re-classifying invariant sites into different sub-alignments based on their likelihood values. Experimental results on simulated and empirical DNA datasets showed that the new method, called iK-means, overcame the pitfall of the K-means method, consequently, helps improve the quality of the partitioning sub-alignments. We recommend using the iK-means method to level up the accuracy in inferring phylogenetic trees.",111,https://doi.org/10.1109/KSE50997.2020.9287221,1,
1939,pQMaker: empirically estimating amino acid substitution models in a parallel environment.,"Amino acid substitution matrices are central to the model-based methods for reconstructing evolutionary trees from amino acid sequences. QMaker is an efficient method for estimating general time-reversible amino acid substitution matrices from a large biological dataset containing thousands of protein alignments using maximum likelihood principle. It allows researchers to build an amino acid substitution model on their own to best fit their subsequent phylogenetic analyses. In this work, we propose an approach to parallelize computation in QMaker, named pQMaker. Moreover, we provide an open-source message passing interface implementation for pQMaker (https://github.com/canhnd58/IQ-TREE/tree/pqmaker) built upon the latest IQ-TREE package. Experiments on benchmark data sets show that our implementation has significant speed gains compared with the original QMaker.",111,https://doi.org/10.1109/KSE50997.2020.9287569,1,
1940,A protein alignment partitioning method for protein phylogenetic inference.,"Phylogenetic trees inferred from protein sequences are strongly affected by amino acid evolutionary models. Choosing proper models are needed to account for the heterogeneity in evolutionary patterns across sites, especially when analyzing multiple genes or whole genome datasets. Partitioning is a prominent approach to combine sites undergone similar evolutionary processes into separated groups with proper models. The partitioning scheme can be defined by using structural features of the sequences, however, determining structural features of protein sequences is not always practical. Recently, methods have been proposed to automatically cluster sites into groups based on the rates of sites. The rate of sites is a good indicator; however, it is unable to properly reflex the complex evolutionary processes of sites along the protein sequence. In this paper, we present a new algorithm to automatically determine a partitioning scheme based on the best-fit model of sites, i.e., sites belong to the same model will be classified into the same group. Comparing our proposed method with current methods on a set of empirical protein datasets showed that our method helped to build better trees than other methods tested. Our method will significantly improve protein phylogenetic inference from multiple gene or whole genome datasets.",111,https://doi.org/10.1109/RIVF48685.2020.9140797,1,
1941,Building a Specific Amino Acid Substitution Model for Dengue Viruses.,"Phylogenetic trees inferred from protein sequences are strongly affected by amino acid substitution models. Although different amino acid substitution models have been proposed, only a few were estimated for specific species such as the FLU model for influenza viruses. Among the most dangerous viruses for human health, dengue is always on top and the cause of dengue fever up to 100 million people per year. In this study, we built a specific amino acid substitution model for dengue protein sequences, called DEN. The dengue protein sequences were obtained from the NCBI dengue database and the model was estimated using the maximum likelihood method. Experiments showed that the new model DEN helped to build better phylogenetic trees than other existing models. We strongly recommend researchers to use the DEN model for analyzing dengue protein data.",111,https://doi.org/10.1109/KSE.2018.8573341,1,
1942,Building Ancestral Recombination Graphs for Whole Genomes.,"We propose a heuristic algorithm, called ARG4WG, to build plausible ancestral recombination graphs (ARGs) from thousands of whole genome samples. By using the longest shared end for recombination inference, ARG4WG constructs ARGs with small numbers of recombination events that perform well in association mapping on genome-wide association studies.",111,https://doi.org/10.1109/TCBB.2016.2542801,1,
1943,Genomedics: Whole exome analysis system for clinical studies.,"Whole exome sequencing (WES) is a widely used technique in both medical studies and clinical practice. However, a number of studies show that the results produced by different WES analysis pipelines are not always homogeneous. To this end, we propose a method (called Genomedics) using a consensus approach to expand the list of variants by combining results called from six separate pipelines with sensitive options. To evaluate the performance of the proposed method, Gemomedics was compared to seven existing methods when they were tested on two datasets and F1-score was used as an indicator of accuracy. The results showed that Genomedics has the highest score among seven methods. We also applied Genomedics to analyze whole exomes from Multiple Myeloma and Dravet syndrome patients and found interesting results. The results demonstrate the promising applications of Genomedics in clinical studies.",111,https://doi.org/10.1109/KSE.2017.8119449,1,
1944,A new phylogenetic tree sampling method for maximum parsimony bootstrapping and proof-of-concept implementation.,"Reconstructing maximum parsimony (MP) trees along with bootstrap for large multiple sequence alignments is a computationally challenging task. In this paper, we present MPBoot, a new method for approximating MP bootstrap based on the idea of a previously introduced method for maximum likelihood bootstrap. MPBoot economizes the computations by only sampling trees on the original alignment and simultaneously calculating exact parsimony scores for all bootstrap alignments. Preliminary experiments on simulated data showed that MPBoot is about 2-3.5 times faster and yields better parsimony scores than a fast bootstrap procedure implemented in TNT. MPBoot also produced less biased support value than TNT for small simulated data sets (<;= 200 sequences). However, for large number of sequences (>=500) MPBoot overestimates the probability that a clade is true, i.e., results in likely high false positive rates. As MPBoot is currently a proof-of-concept implementation, more work is to understand the limitations of the method.",111,https://doi.org/10.1109/KSE.2016.7758020,1,
1945,Preface KSE 2016.,"Welcome to Hanoi, Vietnam and the 8th International Conference on Knowledge and System Engineering (KSE 2016). Following the success of the previous conferences in the series, KSE 2016 continues to provide a forum for researchers to discuss problems, exchange results, identify emerging issues and establish collaborations in related areas of Knowledge and Systems Engineering. KSE 2016 is organized by the Le Quy Don Technical University (LQDTU), Japan Advanced Institute of Science and Technology (JAIST) and University of Engineering and Technology (UET).",111,https://doi.org/10.1109/KSE.2016.7758016,1,
1947,Building Population-Specific Reference Genomes: A Case Study of Vietnamese Reference Genome.,"The human reference genome is an essential tool for studying human genomes. The standard reference genome is constructed from genomes of a few donors. The 1000 genomes project has revealed a huge amount of genetic differences between diverse populations. It is therefore naturally questioned whether the standard reference genome can work well for all human genome studies or population-specific reference genomes are needed accordingly. In this paper, we present a pipeline for constructing and evaluating a population-specific reference genome. The pipeline was examined on building the Vietnamese reference genome from 100 Kinh Vietnamese genomes obtained from the 1000 genomes project. Experiments showed that the resulting Vietnamese reference genome was better than the standard reference genome at analyzing Vietnamese genomic data. It helped improve the quality of short reads mapping and genotype calling for Vietnamese genomes. The pipeline is applicable for building and evaluating other population-specific reference genomes. For the first time the Vietnamese reference genome, which is now available for further Vietnamese genome studies, was successfully built.",111,https://doi.org/10.1109/KSE.2015.49,1,
1948,MVRM: A Hybrid Approach to Predict siRNA Efficacy.,"The discovery of RNA interference (RNAi) leads to design novel drugs for different diseases. Selecting short interfering RNAs (siRNAs) that can knockdown target genes efficiently is one of the key tasks in studying RNAi. A number of predictive models have been proposed to predict knockdown efficacy of siRNAs, however, their performance is still far from the expectation. This work aims to develop a predictive model to enhance siRNA knockdown efficacy prediction. The key idea is to combine both the rule -- based and the model -- based approaches. To this end, views of siRNAs that integrate available siRNA design rules are first learned using an adaptive Fuzzy C Means (FCM) algorithm. The learned views and other properties of siRNAs are combined to final representations of siRNAs. The elastic net regression method is employed to learn a predictive model from these final representations. Experiments on benchmark datasets showed that the proposed method achieved stable and accurate results in comparison with other methods.",111,https://doi.org/10.1109/KSE.2015.29,1,
1949,Detecting Bad SNPs from Illumina BeadChips Using Jeffreys Distance.,"Current microarray technologies are able to assay thousands of samples over million of SNPs simultaneously. Computational approaches have been developed to analyse a huge amount of data from microarray chips to understand sophisticated human genomes. The data from microarray chips might contain errors due to bad samples or bad SNPs. In this paper, we propose a method to detect bad SNPs from the probe intensities data of Illumina Beadchips. This approach measures the difference among results determined by three software Illuminus, GenoSNP and Gencall to detect the unstable SNPs. Experiment with SNP data in chromosome 20 of Kenyan people demonstrates the usefulness of our method. This approach reduces the number of SNPs that are needed to check manually. Furthermore, it has the ability in detecting bad SNPs that have not been recognized by other criteria.",111,https://doi.org/10.1109/KSE.2012.25,1,
1950,A Maximum Likelihood Method for Detecting Bad Samples from Illumina BeadChips Data.,"Genotype data provide crucial information to understand effects of genetic variation to human health. Current microarray technologies are able to generate raw genotype data from thousands of samples across million of SNP sites. These raw data are processed by computational methods, called genotype caller, to obtain genotypes. Genotype calls of different callers might not be consistent due to noise of bad samples or SNPs. This requires a manual quality control step conducted by experts to remove bad samples or bad SNP sites. In this paper, we propose a maximum likelihood method to detect bad samples to improve the reliability of the results. Experiments with real data demonstrate the usefulness of our method in the quality control process. Thus, our method has the ability to reduce the number of samples that are requested to manually check by experts",111,https://doi.org/10.1109/KSE.2012.24,1,
1951,A Fast and Efficient Method for Estimating Amino Acid Substitution Models.,"Amino acid substitution models (matrices) play important role for protein phylogenetics analysis and protein sequence alignment. Different approaches have been proposed to estimate amino acid substitution matrices since the time of Day Hoff in 1972. Currently, maximum likelihood approaches have been widely used to estimate popular matrices such as WAG, LG, FLU, etc. Although maximum likelihood approaches result in high quality matrices, they are slow and not applicable to very large datasets. The most time consuming step in estimating matrices is building phylogenetics trees from protein alignments. In this paper, we propose new methods to overcome the obstacle by splitting large alignments into small ones which still contain enough evolutionary information for estimating matrices. Experiments with both Pfam and FLU datasets showed that proposed methods were about three to nine times faster than the best current method while the quality of estimated matrices are nearly the same. Thus, our methods will enable researchers to estimate matrices from very large datasets.",111,https://doi.org/10.1109/KSE.2011.21,1,
1952,Protein Type Specific Amino Acid Substitution Models for Influenza Viruses.,"The amino acid substitution model (matrix) is a crucial part of protein sequence analysis systems. General amino acid substitution models have been estimated from large protein databases, however, they are not specific for influenza viruses. In previous study, we estimated the amino acid substitution model, FLU, for all influenza viruses. Experiments showed that FLU outperformed other models when analyzing influenza protein sequences. Influenza virus genomes consist of different protein types, which are different in both structures and evolutionary processes. Although FLU matrix is specific for influenza viruses, it is still not specific for influenza protein types. Since influenza viruses cause serious problems for both human health and social economics, it is worth to study them as specific as possible. In this paper, we used more than 27 million amino acids to estimate 11 protein type specific models for influenza viruses. Experiments showed that protein type specific models outperformed the FLU model, the best model for influenza viruses. These protein type specific models help researcher to conduct studies on influenza viruses more precisely.",111,https://doi.org/10.1109/KSE.2011.23,1,
1953,Association-Based Dissimilarity Measures for Categorical Data: Limitation and Improvement.,Measuring the similarity for categorical data is a challenging task in data mining due to the poor structure of categorical data. This paper presents a dissimilarity measure for categorical data based on the relations among attributes. This measure not only has the advantage of value variance but also overcomes the limitations of condition the probability-based measure when applied to databases whose attributes are independent. Experiments with 30 databases also showed that the proposed measure boosted the accuracy of Nearest Neighbor classification in comparison with other tested measures.,111,https://doi.org/10.1007/11731139_57,1,
1954,pQMaker: empirically estimating amino acid substitution models in a parallel environment.,"Amino acid substitution matrices are central to the model-based methods for reconstructing evolutionary trees from amino acid sequences. QMaker is an efficient method for estimating general time-reversible amino acid substitution matrices from a large biological dataset containing thousands of protein alignments using maximum likelihood principle. It allows researchers to build an amino acid substitution model on their own to best fit their subsequent phylogenetic analyses. In this work, we propose an approach to parallelize computation in QMaker, named pQMaker. Moreover, we provide an open-source message passing interface implementation for pQMaker (https://github.com/canhnd58/IQ-TREE/tree/pqmaker) built upon the latest IQ-TREE package. Experiments on benchmark data sets show that our implementation has significant speed gains compared with the original QMaker.",112,https://doi.org/10.1109/KSE50997.2020.9287569,1,
1955,Building a Specific Amino Acid Substitution Model for Dengue Viruses.,"Phylogenetic trees inferred from protein sequences are strongly affected by amino acid substitution models. Although different amino acid substitution models have been proposed, only a few were estimated for specific species such as the FLU model for influenza viruses. Among the most dangerous viruses for human health, dengue is always on top and the cause of dengue fever up to 100 million people per year. In this study, we built a specific amino acid substitution model for dengue protein sequences, called DEN. The dengue protein sequences were obtained from the NCBI dengue database and the model was estimated using the maximum likelihood method. Experiments showed that the new model DEN helped to build better phylogenetic trees than other existing models. We strongly recommend researchers to use the DEN model for analyzing dengue protein data.",112,https://doi.org/10.1109/KSE.2018.8573341,1,
1956,"FastMG: a simple, fast, and accurate maximum likelihood procedure to estimate amino acid replacement rate matrices from large data sets.","Amino acid replacement rate matrices are a crucial component of many protein analysis systems such as sequence similarity search, sequence alignment, and phylogenetic inference. Ideally, the rate matrix reflects the mutational behavior of the actual data under study; however, estimating amino acid replacement rate matrices requires large protein alignments and is computationally expensive and complex. As a compromise, sub-optimal pre-calculated generic matrices are typically used for protein-based phylogeny. Sequence availability has now grown to a point where problem-specific rate matrices can often be calculated if the computational cost can be controlled.",112,https://doi.org/10.1186/1471-2105-15-341,1,
1957,ReplacementMatrix: a web server for maximum-likelihood estimation of amino acid replacement rate matrices.,"Amino acid replacement rate matrices are an essential basis of protein studies (e.g. in phylogenetics and alignment). A number of general purpose matrices have been proposed (e.g. JTT, WAG, LG) since the seminal work of Margaret Dayhoff and co-workers. However, it has been shown that matrices specific to certain protein groups (e.g. mitochondrial) or life domains (e.g. viruses) differ significantly from general average matrices, and thus perform better when applied to the data to which they are dedicated. This Web server implements the maximum-likelihood estimation procedure that was used to estimate LG, and provides a number of tools and facilities. Users upload a set of multiple protein alignments from their domain of interest and receive the resulting matrix by email, along with statistics and comparisons with other matrices. A non-parametric bootstrap is performed optionally to assess the variability of replacement rate estimates. Maximum-likelihood trees, inferred using the estimated rate matrix, are also computed optionally for each input alignment. Finely tuned procedures and up-to-date ML software (PhyML 3.0, XRATE) are combined to perform all these heavy calculations on our clusters.",112,https://doi.org/10.1093/bioinformatics/btr435,1,
1958,A Fast and Efficient Method for Estimating Amino Acid Substitution Models.,"Amino acid substitution models (matrices) play important role for protein phylogenetics analysis and protein sequence alignment. Different approaches have been proposed to estimate amino acid substitution matrices since the time of Day Hoff in 1972. Currently, maximum likelihood approaches have been widely used to estimate popular matrices such as WAG, LG, FLU, etc. Although maximum likelihood approaches result in high quality matrices, they are slow and not applicable to very large datasets. The most time consuming step in estimating matrices is building phylogenetics trees from protein alignments. In this paper, we propose new methods to overcome the obstacle by splitting large alignments into small ones which still contain enough evolutionary information for estimating matrices. Experiments with both Pfam and FLU datasets showed that proposed methods were about three to nine times faster than the best current method while the quality of estimated matrices are nearly the same. Thus, our methods will enable researchers to estimate matrices from very large datasets.",112,https://doi.org/10.1109/KSE.2011.21,1,
1959,Protein Type Specific Amino Acid Substitution Models for Influenza Viruses.,"The amino acid substitution model (matrix) is a crucial part of protein sequence analysis systems. General amino acid substitution models have been estimated from large protein databases, however, they are not specific for influenza viruses. In previous study, we estimated the amino acid substitution model, FLU, for all influenza viruses. Experiments showed that FLU outperformed other models when analyzing influenza protein sequences. Influenza virus genomes consist of different protein types, which are different in both structures and evolutionary processes. Although FLU matrix is specific for influenza viruses, it is still not specific for influenza protein types. Since influenza viruses cause serious problems for both human health and social economics, it is worth to study them as specific as possible. In this paper, we used more than 27 million amino acids to estimate 11 protein type specific models for influenza viruses. Experiments showed that protein type specific models outperformed the FLU model, the best model for influenza viruses. These protein type specific models help researcher to conduct studies on influenza viruses more precisely.",112,https://doi.org/10.1109/KSE.2011.23,1,
1960,Improving Graph Convolutional Networks with Transformer Layer in social-based items recommendation.,"With the emergence of online social networks, social-based items recommendation has become a popular research direction. Recently, Graph Convolutional Networks have shown promising results by modeling the information diffusion process in graphs. It provides a unified framework for graph embedding that can leverage both the social graph structure and node features information. In this paper, we improve the embedding output of the graph-based convolution layer by adding a number of transformer layers. The transformer layers with attention architecture help discover frequent patterns in the embedding space which increase the predictive power of the model in the downstream tasks. Our approach is tested on two social-based items recommendation datasets, Ciao and Epinions and our model outperforms other graph-based recommendation baselines.",113,https://doi.org/10.1109/KSE53942.2021.9648823,1,
1961,Exploring Efficiency of GAN-based Generated URLs for Phishing URL Detection.,"The URL (Uniform Resource Locator) is used to refer to the resources on the Internet by giving hyperlinks to the websites. Different resources are referenced by different network addresses or different URLs. As a result, embedding malware on websites by using malicious URLs is one of the most dangerous types of cyberattacks today and poses a serious threats to the safety of systems. In order to detect the phishing URLs, the most commonly used approach recently is using deep learning networks with a large number of URL samples, including both malign and benign ones for training the deep networks. However, the available URL databases have a modest number of samples. In addition, the disadvantage of these databases is the imbalance distribution of malicious and non-malicious URL strings. In fact, it is difficult to collect or update malicious URLs because these URLs only exist for a short time, after being detected they are changed again and again. In order to solve this challenge, in this work, we propose to train a GAN network named WGAN-GP for generating malicious URLs from the available phishing URL data. We then integrate the generated phishing URL data into the existing URL database and perform two URL classifiers of LSTM and GRU to give the comparative results. The experiments on different quantities of URL samples show the improvement for URL classification by using WGAN-GP and LSTM classifier.",113,https://doi.org/10.1109/MAPR53640.2021.9585287,1,
1962,Reducing Blocking Artifacts in CNN-Based Image Steganography by Additional Loss Functions.,"Our work improves the encoded image quality from HiDDeN framework, an end-to-end image steganography based on deep convolution neural network. In the encoding phase of HiDDeN framework, to embed a message in a cover image, it is required to split the cover image into smaller image blocks and embed the message bits in each block in parallel. These embedded blocks are then combined to form an encoded image that has the same size as the cover image. This image reconstruction process causes artifacts that appear on the boundaries of the blocks. This can be explained by the fact that when message bits are embedded in the image blocks, the pixel-level information of each image block is unequally alternated. In order to reduce block artifacts, in this work we propose a blocking loss as an additional objective function in HiDDeN framework. This loss measures the difference between encoded images and modified versions of the cover images. The proposed method is evaluated on COCO 2014 and BOSS datasets and the experimental results show the effectiveness in reducing the block artifacts that appeared in the encoded images of HiDDeN framework. This has an important impact on increasing the invisibility or transparency of the steganography system. In addition, the experimental result on secrecy of the proposed method also indicates the same performance as the HiDDeN pipeline.",113,https://doi.org/10.1109/KSE50997.2020.9287408,1,
1963,Particulate Matter Concentration Estimation from Satellite Aerosol and Meteorological Parameters: Data-Driven Approaches.,"Estimation of Particulate Matter concentration (PM1, PM2.5 and PM10) from aerosol product derived from satellite images and meteorological parameters brings a great advantage in air pollution monitoring since observation range is no longer limited around ground stations and estimation accuracy will be increased significantly. In this article, we investigate the application of Multiple Linear Regression (MLR) and Support Vector Regression (SVR) to make empirical data models for PM1/2.5/10 estimation from satellite- and ground-based data. Experiments, which are carried out on data recorded in two year over Hanoi - Vietnam, not only indicate a case study of regional modeling but also present comparison of performance between a widely used technique (MLR) and an advanced method (SVR).",113,https://doi.org/10.1007/978-3-319-02741-8_30,1,
1964,Collaborative Smartphone-Based User Positioning in a Multiple-User Context Using Wireless Technologies.,"For the localization of multiple users, Bluetooth data from the smartphone is able to complement Wi-Fi-based methods with additional information, by providing an approximation of the relative distances between users. In practice, both positions provided by Wi-Fi data and relative distance provided by Bluetooth data are subject to a certain degree of noise due to the uncertainty of radio propagation in complex indoor environments. In this study, we propose and evaluate two approaches, namely Non-temporal and Temporal ones, of collaborative positioning to combine these two cohabiting technologies to improve the tracking performance. In the Non-temporal approach, our model establishes an error observation function in a specific interval of the Bluetooth and Wi-Fi output. It is then able to reduce the positioning error by looking for ways to minimize the error function. The Temporal approach employs an extended error model that takes into account the time component between users’ movements. For performance evaluation, several multi-user scenarios in an indoor environment are set up. Results show that for certain scenarios, the proposed approaches attain over 40% of improvement in terms of average accuracy",113,https://doi.org/10.3390/s20020405,1,
1965,Sharing Experience in Multitask Reinforcement Learning.,"In multitask reinforcement learning, tasks often have sub-tasks that share the same solution, even though the overall tasks are different. If the shared-portions could be effectively identified, then the learning process could be improved since all the samples between tasks in the shared space could be used. In this paper, we propose a Sharing Experience Framework (SEF) for simultaneously training of multiple tasks. In SEF, a confidence sharing agent uses task-specific rewards from the environment to identify similar parts that should be shared across tasks and defines those parts as shared-regions between tasks. The shared-regions are expected to guide task-policies sharing their experience during the learning process. The experiments highlight that our framework improves the performance and the stability of learning task-policies, and is possible to help task-policies avoid local optimums.",113,https://doi.org/10.24963/ijcai.2019/505,1,
1967,The Smartphone-Based Offline Indoor Location Competition at IPIN 2016: Analysis and Future Work.,"This paper presents the analysis and discussion of the off-site localization competition track, which took place during the Seventh International Conference on Indoor Positioning and Indoor Navigation (IPIN 2016). Five international teams proposed different strategies for smartphone-based indoor positioning using the same reference data. The competitors were provided with several smartphone-collected signal datasets, some of which were used for training (known trajectories), and others for evaluating (unknown trajectories). The competition permits a coherent evaluation method of the competitors’ estimations, where inside information to fine-tune their systems is not offered, and thus provides, in our opinion, a good starting point to introduce a fair comparison between the smartphone-based systems found in the literature. The methodology, experience, feedback from competitors and future working lines are described. ",113,https://doi.org/10.3390/s17030557,1,
1968,Smartphone-based user location tracking in indoor environment.,"This paper introduces our work in the framework of Track 3 of the IPIN 2016 Indoor Localization Competition, which addresses the smartphone-based tracking problem in an offline manner. Our approach splits the path-reconstruction into several smaller tasks, including building identification, floor identification, user direction and speed inference. For each task, a specific set of data from the provided log data is used. Evaluation is carried out using a cross validation scheme. To produce the robustness again noisy data, we combine several approaches into one on the basis of their testing results. By testing on the provided training data, we have a good accuracy on building and floor identification. For the task of tracking the user's position within the floor, the result is 10m at 3rd-quarter distance error after 3 minutes of walking.",113,https://doi.org/10.1109/IPIN.2016.7743704,1,
1969,The Grenoble System for the Social Touch Challenge at ICMI 2015.,"New technologies and especially robotics is going towards more natural user interfaces. Works have been done in different modality of interaction such as sight (visual computing), and audio (speech and audio recognition) but some other modalities are still less researched. The touch modality is one of the less studied in HRI but could be valuable for naturalistic interaction. However touch signals can vary in semantics. It is therefore necessary to be able to recognize touch gestures in order to make human-robot interaction even more natural. We propose a method to recognize touch gestures. This method was developed on the CoST corpus and then directly applied on the HAART dataset as a participation of the Social Touch Challenge at ICMI 2015. Our touch gesture recognition process is detailed in this article to make it reproducible by other research teams. Besides features set description, we manually filtered the training corpus to produce 2 datasets. For the challenge, we submitted 6 different systems. A Support Vector Machine and a Random Forest classifiers for the HAART dataset. For the CoST dataset, the same classifiers are tested in two conditions: using all or filtered training datasets. As reported by organizers, our systems have the best correct rate in this year's challenge (70.91% on HAART, 61.34% on CoST). Our performances are slightly better that other participants but stay under previous reported state-of-the-art results.",113,https://doi.org/10.1145/2818346.2830598,1,
1970,Compression Artifacts Image Patch database for Perceptual Quality Assessment.,"Ground truth is one of the most important component for training, testing, and benchmarking algorithms for objective quality assessment In this paper, we propose an image patch quality database with compression artifacts. We create a new database of image patches with High Efficiency Video Coding (HEVC) compression artifacts. Then, the subjective test is conducted in a controlled environment to obtain the ground truth of image patch quality, where we collect differential mean opinion scores (DMOS) from a larger amount of observers. Finally, the rank order correlation factors between DMOS and a set of popular image quality metrics are calculated and presented. The proposed database is expected for learning patch based IQA model for block size in video rate-distortion optimization.",114,https://doi.org/10.1109/KSE50997.2020.9287704,1,
1971,Classifying Non-elementary Movements in Vietnamese Mõ Dances.,"This paper proposes a method to classify non-elementary movements in Vietnamese dances. This classification method uses an OWL ontology called VDM (Vietnamese Dance Movements) recently developed by the authors. The VDM defines a taxonomy of dance movement classes and their relationships for the traditional Vietnamese dances taking into account the semantics of its art and its cultural anthropologists. The VDM terminology describes elementary movements (poses) as a dataset ontology importing the ontology VDM. These poses are results of dance sequences segmentation (using segmentation techniques). In this paper, we support the initial ontology VDM by complex classification rules written with SWRL (Semantic Web Rule Language, which is the OWL complementary language) to classify non-elementary movements. The objective is to entail classes of movement phrases, which are non-elementary basic movements with complete meaning and illustrated using Mõ dances. The classification result is the initial dataset VDM ontology augmented with class descriptions of non-elementary movements, which can be queried using the query language SQWRL (Semantic Query Web-enhanced Rule Language).",114,https://doi.org/10.1007/978-3-030-22216-1_10,1,
1972,An Ontology Web Application-based Annotation Tool for Intangible Culture Heritage Dance Videos.,"Collecting dance videos, preserving and promoting them after enriching the collected data has been significant actions in preserving Intangible culture heritage in South-East Asia. Whereas techniques for the conceptual modeling of the expressive semantics of dance videos are very complex, they are crucial to exploit effectively the video semantics. This paper proposes an ontology web-based dance video annotation system for representing the semantics of dance videos at different granularity levels. Especially, the system incorporates both syntactic and semantic features of pre-built dance ontology system in order to not only use the available semantic web system but also to create unity for users when annotating videos to minimize conflicts.",114,https://doi.org/10.1145/3347317.3357245,1,
1973,On the Detection of Video's Ethnic Vietnamese Thai Dance Movements.,"The problem addressed in this paper is the one of classifying Vietnamese dances' videos. In particular, we focus on an automatic detection of movements in the Ethnic Vietnamese Thai dances (ETVD). We first propose an ontology-based description of ETVD movements in terms of main movements' steps. We then associate with each movement step a profile containing typical features that characterize a movement step. The automatic detection of ETVD movements is based on a correlation method that matches movements' steps profiles with concepts present in frames of dances' videos. The last part of the paper contain experimental studies that show the good classification rate of our ETVD movement detection method.",114,https://doi.org/10.1109/SITIS.2019.00064,1,
1974,A Polynomial Algorithm for Merging Lightweight Ontologies in Possibility Theory Under Incommensurability Assumption.,"The context of this paper is the one of merging lightweight ontologies with prioritized or uncertain assertional bases issued from different sources. This is especially required when the assertions are provided by multiple and often conflicting sources having different reliability levels. We focus on the so-called egalitarian merging problem which aims to minimize the dissatisfaction degree of each individual source. The question addressed in this paper is how to merge prioritized assertional bases, in a possibility theory framework, when the uncertainty scales are not commensurable, namely when the sources do not share the same meaning of uncertainty scales. Using the notion of compatible scale, we provide a safe way to perform merging. The main result of the paper is that the egalitarian merging of prioritized assertional bases can be achieved in a polynomial time even if the uncertainty scales are not commensurable.",114,https://doi.org/10.5220/0006120804150422,1,
1975,Single view image based - 3D human pose reconstruction.,"In this paper, we propose an improved framework for estimating 3D human pose sequences from 2D videos. Prior methods were declared to solve this problem; however, they sometimes produced invalid poses because the constraint of result poses was not robust enough to satisfy anthropometric regular body pose. Our goal is to eliminate this drawback, provide more accurate estimated poses. The proposed framework consists of three stages. First, we start by generating a set of class-independent body part candidates for each video frame and constructing a fully connected graph from these candidates. Integer linear programming is used to label each candidate by a part class and assign it to individuals to form accurate and consistent pose. We then make the outputs from the first stage suitable for the next stage by inferring the 2D joint configuration. Second, from 2D poses resulted from the first stage, 3D poses are reconstructed by a matching pursuit algorithm operating on camera projection, all the output must adapt the anthropometric regular body pose constraint such as limb length and joint angle constraint. Third, we exploit the temporal and spatial information in videos to obtain more accurate 3D poses. Our experiments on CMU MOCAP dataset show that the proposed framework produces better results compared with prior works.",114,https://doi.org/10.1109/KSE.2017.8119445,1,
1976,Emotional Facial Expression Analysis in the Time Domain.,"Emotions have been studied for a long time and results show that they play an important role in human cognitive functions. In fact, emotions play an extremely important role during the communication between people. And the human face is the most communicative part of the body for expressing emotions; it is recognized that a link exists between facial activity and emotional states. In order to make computer applications more believable and friendly, giving them the ability to recognize and/or express emotions are research fields which have been much focused on. Being able to perform these tasks, firstly, we need to have knowledge about the relationship between emotion and facial activity. Up to now, there have been proposed researches on this relationship. However, almost all these researches focused on analyzing the relationship without taking into account time factors. They analyzed the relationship but did not examined it in the time domain. In this paper, we propose a work on analyzing the relationship between emotions and facial activity in the time domain. Our goal is finding the temporal patterns of facial activity of six basic emotions (happy, sad, angry, fear, surprise, disgust). To perform this task, we analyzed a spontaneous video database in order to consider how facial activities which are related to the six basic emotions happen temporally. From there, we bring out the general temporal patterns for facial expressions of each of the six emotions.",114,https://doi.org/10.1007/978-3-319-11680-8_39,1,
1977,Shift Error Analysis in Image Based 3D Skull Feature Reconstruction.,"3D skull is crucial in skull-based 3D facial reconstruction [1, 2, 3, 4, 5, 6, 7, 8]. In 3D reconstruction, especially in skull-based 3D facial reconstruction, features usually play an important role. Because, the accuracy in feature detection strongly affects the accuracy of the 3D final model. In this paper, we concentrate on accuracy of 3D reconstructed skull, one important part in skull-based 3D facial reconstruction. We discuss a cause of errors called shift errors when taking sequence of skull images. In addition, we analysis the effect of shift error in 3D reconstruction and propose solution to limit the effect.",114,https://doi.org/10.1109/KSE.2012.38,1,
1978,Facial Soft Tissue Thicknesses Prediction Using Anthropometric Distances.,"Predicting the face of an unidentified individual from its skeletal remains is a difficult matter. Obviously, if the soft tissue thicknesses at every location at the skull are known, we can easily rebuild the face from the skull model. Thus, the problem turns out to be predicting the soft tissue thicknesses for any given skull. With the rapid development of the computer, different techniques are being used in the community for prediction tasks and in recent years the concept of neural networks has emerged as one of them. The principal strength of the neural network is its ability to find patterns and irregularities as well as detecting multi-dimensional non-linear connections in data. In this paper, we propose a method of applying neural networks to predict the soft tissue thicknesses for facial reconstruction. We use the distances between anthropometric locations at the skull as input, and the soft tissue thicknesses as output, as this format is suitable for many machine learning mechanisms. These data is collected and measured from candidates using the Computed Tomography (CT) technique.",114,https://doi.org/10.1007/978-3-642-19953-0_12,1,
1979,3D Facial Reconstruction System from Skull for Vietnamese.,"Reconstructing the 3D facial model of an unidentified individual from his skull contributes considerable benefits to terms of archaeology, anthropology and forensic investigation but it is still significantly complicated matter. Computer aided system of 3D facial reconstruction based on skull has a great advantage of reduction in time consumption. Nevertheless, existing results either contain graphical artifacts or reflect incorrectly differences among reconstructed faces. In this paper, we propose a Vietnamese 3D facial reconstruction system from skull that can overcome existing problems. Machine learning is applied to CT image database of living Vietnamese to learn the relationship between skull measurements and soft tissue thicknesses. Then, the soft tissue thicknesses for each individual are calculated from his/her skull measurements. Moreover, instead of using a fixed number of dowels, extra dowels are added to deform a 3D template face, which ensures the smoothness of the 3D reconstructed face as well as represents best the characteristics of the face to be reconstructed.",114,https://doi.org/10.1109/KSE.2011.26,1,
1980,UFBoot2: improving the ultrafast bootstrap approximation,"The standard bootstrap (SBS), despite being computationally intensive, is widely used in maximum likelihood phylogenetic analyses. We recently proposed the ultrafast bootstrap approximation (UFBoot) to reduce computing time while achieving more unbiased branch supports than SBS under mild model violations. UFBoot has been steadily adopted as an efficient alternative to SBS and other bootstrap approaches. Here, we present UFBoot2, which substantially accelerates UFBoot and reduces the risk of overestimating branch supports due to polytomies or severe model violations. Additionally, UFBoot2 provides suitable bootstrap resampling strategies for phylogenomic data. UFBoot2 is 778 times (median) faster than SBS and 8.4 times (median) faster than RAxML rapid bootstrap on tested data sets. UFBoot2 is implemented in the IQ-TREE software package version 1.6 and freely available at http://www …",118,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dZyMRT0AAAAJ&citation_for_view=dZyMRT0AAAAJ:d1gkVwhDpl0C,,1
1981,MPBoot: fast phylogenetic maximum parsimony tree inference and bootstrap approximation,"The nonparametric bootstrap is widely used to measure the branch support of phylogenetic trees. However, bootstrapping is computationally expensive and remains a bottleneck in phylogenetic analyses. Recently, an ultrafast bootstrap approximation (UFBoot) approach was proposed for maximum likelihood analyses. However, such an approach is still missing for maximum parsimony. To close this gap we present MPBoot, an adaptation and extension of UFBoot to compute branch supports under the maximum parsimony principle. MPBoot works for both uniform and non-uniform cost matrices. Our analyses on biological DNA and protein showed that under uniform cost matrices, MPBoot runs on average 4.7 (DNA) to 7 times (protein data) (range: 1.2–20.7) faster than the standard parsimony bootstrap implemented in PAUP*; but 1.6 (DNA) to 4.1 times (protein data) slower than the standard bootstrap with a fast search routine in TNT (fast-TNT). However, for non-uniform cost matrices MPBoot is 5 (DNA) to 13 times (protein data) (range:0.3–63.9) faster than fast-TNT. We note that MPBoot achieves better scores more frequently than PAUP* and fast-TNT. However, this effect is less pronounced if an intensive but slower search in TNT is invoked. Moreover, experiments on large-scale simulated data show that while both PAUP* and TNT bootstrap estimates are too conservative, MPBoot bootstrap estimates appear more unbiased. MPBoot provides an efficient alternative to the standard maximum parsimony bootstrap procedure. It shows favorable performance in terms of run time, the capability of finding a maximum parsimony tree, and high bootstrap …",118,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dZyMRT0AAAAJ&citation_for_view=dZyMRT0AAAAJ:u5HHmVD_uO8C,,1
1982,Scholarometer: A social framework for analyzing impact across disciplines,"The use of quantitative metrics to gauge the impact of scholarly publications, authors, and disciplines is predicated on the availability of reliable usage and annotation data. Citation and download counts are widely available from digital libraries. However, current annotation systems rely on proprietary labels, refer to journals but not articles or authors, and are manually curated. To address these limitations, we propose a social framework based on crowdsourced annotations of scholars, designed to keep up with the rapidly evolving disciplinary and interdisciplinary landscape. We describe a system called Scholarometer, which provides a service to scholars by computing citation-based impact measures. This creates an incentive for users to provide disciplinary annotations of authors, which in turn can be used to compute disciplinary metrics. We first present the system architecture and several heuristics to deal with noisy bibliographic and annotation data. We report on data sharing and interactive visualization services enabled by Scholarometer. Usage statistics, illustrating the data collected and shared through the framework, suggest that the proposed crowdsourcing approach can be successful. Secondly, we illustrate how the disciplinary bibliometric indicators elicited by Scholarometer allow us to implement for the first time a universal impact measure proposed in the literature. Our evaluation suggests that this metric provides an effective means for comparing scholarly impact across disciplinary boundaries.",118,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dZyMRT0AAAAJ&citation_for_view=dZyMRT0AAAAJ:ufrVoPGSRksC,,1
1983,Crowdsourcing scholarly data,"One of the promises of Web Science is to leverage the wisdom of the crowds to give rise to emergent, bottom-up semantics, by making it easy for users to express relationships between arbitrary kinds of objects. Rather than starting with an ontology that determines the kinds of objects and relationships to be described and reasoned about, the idea is to give users the freedom to annotate arbitrary objects with arbitrary predicates, along with incentives for such annotations. Social tagging systems for images are one example, where the motivation can stem from the wish to organize and share one’s photos or from entertaining games to guess one another’s tags. Here we explore a similar approach in the domain of scholarly publications. We describe a system called Scholarometer, which provides a service to scholars by computing citation-based impact measures. This motivates users to provide disciplinary annotations for authors, which in turn can be used to compute for the first time measures that allow to compare authors’ impact across disciplinary boundaries. We show how this crowdsourcing approach can lead to emergent semantic networks to study interdisciplinary annotations and trends.",118,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dZyMRT0AAAAJ&citation_for_view=dZyMRT0AAAAJ:qjMakFHDy7sC,,1
1984,A new phylogenetic tree sampling method for maximum parsimony bootstrapping and proof-of-concept implementation,"Reconstructing maximum parsimony (MP) trees along with bootstrap for large multiple sequence alignments is a computationally challenging task. In this paper, we present MPBoot, a new method for approximating MP bootstrap based on the idea of a previously introduced method for maximum likelihood bootstrap. MPBoot economizes the computations by only sampling trees on the original alignment and simultaneously calculating exact parsimony scores for all bootstrap alignments. Preliminary experiments on simulated data showed that MPBoot is about 2-3.5 times faster and yields better parsimony scores than a fast bootstrap procedure implemented in TNT. MPBoot also produced less biased support value than TNT for small simulated data sets (<;= 200 sequences). However, for large number of sequences (>=500) MPBoot overestimates the probability that a clade is true, i.e., results in likely high false …",118,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dZyMRT0AAAAJ&citation_for_view=dZyMRT0AAAAJ:Tyk-4Ss8FVUC,,1
1985,Exploring the Kinh Vietnamese genomic database for the polymorphisms of the P450 genes toward precision public health,"Human cytochrome P450 (CYPs) genes are essential in metabolizing drugs. Due to their high polymorphism, population-specific studies are of great interest.",118,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dZyMRT0AAAAJ&citation_for_view=dZyMRT0AAAAJ:LkGwnXOMwfcC,,1
1986,A distributed algorithm for the parsimony bootstrap approximation,"Accelerating phylogenetic tree reconstruction and bootstrapping is critical, especially to support the study of the evolution of dangerous viruses. In this paper, we propose the MPBoot-MPI, a distributed algorithm efficiently implementing the idea of bootstrap approximation in MPBoot for a parallel computing environment of multiple computational nodes. MPBoot-MPI employs the master-worker paradigm and divides the work in MPBoot into three phases, each with a separate strategy to distribute computing among processes. Since the bootstrap trees are not calculated independently, processes must share results throughout task execution. We propose that when arriving at a checkpoint to report their results to the master process, worker processes apply a stochastic strategy to determine whether to perform the sending, thereby reducing the effect of latency caused by the large size of the message sent. Experiments …",118,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dZyMRT0AAAAJ&citation_for_view=dZyMRT0AAAAJ:W7OEmFMy1HYC,,1
1987,pQMaker: empirically estimating amino acid substitution models in a parallel environment,"Amino acid substitution matrices are central to the model-based methods for reconstructing evolutionary trees from amino acid sequences. QMaker is an efficient method for estimating general time-reversible amino acid substitution matrices from a large biological dataset containing thousands of protein alignments using maximum likelihood principle. It allows researchers to build an amino acid substitution model on their own to best fit their subsequent phylogenetic analyses. In this work, we propose an approach to parallelize computation in QMaker, named pQMaker. Moreover, we provide an open-source message passing interface implementation for pQMaker (https://github.com/canhnd58/IQ-TREE/tree/pqmaker) built upon the latest IQ-TREE package. Experiments on benchmark data sets show that our implementation has significant speed gains compared with the original QMaker.",118,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dZyMRT0AAAAJ&citation_for_view=dZyMRT0AAAAJ:UeHWp8X0CEIC,,1
1988,FULL MANUSCRIPT,"The standard bootstrap (SBS), despite being computationally intensive, is widely used in maximum likelihood phylogenetic analyses. We recently proposed the ultrafast bootstrap approximation (UFBoot) to reduce computing time while achieving more unbiased branch supports than SBS under mild model violations. UFBoot has been steadily adopted as an efficient alternative to SBS and other bootstrap approaches.",118,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dZyMRT0AAAAJ&citation_for_view=dZyMRT0AAAAJ:9yKSN-GCB0IC,,1
1993,""" Idea Maps"" of Research Fields","Increasing digital distribution of scientific literature has lead to more effective literature based discovery and deeper analysis of research fields as complex networks. One such network is the” idea map” of a research field, in which the concepts of the field are linked to one another on the basis of how frequently they are written about together. These idea maps can be explored up close, to see the relationships between ideas in the field and how they change over time. These networks can all be studied on a systems level, in which the gross organization of ideas can vary greatly from field to field.",118,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=dZyMRT0AAAAJ&citation_for_view=dZyMRT0AAAAJ:WF5omc3nYNoC,,1
1995,On the pheromone update rules of ant colony optimization approaches for the job shop scheduling problem,Ant Colony Optimization (ACO) system is an intelligent multi-agent system of the interacting artificial ants to solve the combinatorial optimization problems. Applying ACO approach in the typical NP-hard problem like job shop scheduling (JSS) problem is still an impressive and attractive challenge with the community. This paper proposes two improvements of ACO algorithm based on the convergence property of pheromone trails. Our improvements are better in both terms of accuracy and running time than the state-of-the-art Max-Min ant system by the simulation with the standard data sets.,120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:Tyk-4Ss8FVUC,,1
1996,AcoSeeD: An ant colony optimization for finding optimal spaced seeds in biological sequence search,"Similarity search in biological sequence database is one of the most popular and important bioinformatics tasks. Spaced seeds have been increasingly used to improve the quality and sensitivity of searching, for example, in seeded alignment methods. Finding optimal spaced seeds is a NP-hard problem. In this study we introduce an application of an Ant Colony Optimization (ACO) algorithm to address this problem in a metaheuristics framework. This method, called AcoSeeD, builds optimal spaced seeds in an elegant construction graph that uses the ACO standard framework with a modified pheromone update. Experimental results demonstrate that AcoSeeD brings a significant improvement of sensitivity while demanding the same computational time as other state-of-the-art methods. We also introduces an alternative way of using local search that exerts a fast approximation of the objective function in ACO.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:Se3iqnhoufwC,,1
1997,An extended TOPSIS method with unknown weight information in dynamic neutrosophic environment,"Decision-making activities are prevalent in human life. Many methods have been developed to address real-world decision problems. In some practical situations, decision-makers prefer to provide their evaluations over a set of criteria and weights. However, in many real-world situations, problems include a lack of weight information for the times, criteria, and decision-makers (DMs). To remedy such discrepancies, an optimization model has been proposed to determine the weights of attributes, times, and DMs. A new concept related to the correlation measure and some distance measures for the dynamic interval-valued neutrosophic set (DIVNS) are defined in this paper. An extend Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) method in the interval-valued neutrosophic set with unknown weight information in dynamic neutrosophic environments is developed. Finally, a practical example is discussed to illustrate the feasibility and effectiveness of the proposed method.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:W7OEmFMy1HYC,,1
1998,Ants can solve the parallel drone scheduling traveling salesman problem,"In this work, we are interested in studying the parallel drone scheduling traveling salesman problem (PDSTSP), where deliveries are split between a truck and a fleet of drones. The truck performs a common delivery tour, while the drones are forced to perform back and forth trips between customers and a depot. The objective is to minimize the completion time coming back to the depot of all the vehicles. We present a hybrid ant colony optimization (HACO) metaheuristic to solve the problem. Our algorithm is based on an idea from the literature that represents a PDSTSP solution as a permutation of all customers. And then a dynamic programming is used to decompose the customer sequence into a tour for the truck and trips for the drones. We propose a new dynamic programming combined with other problem-tailored components to efficiently solve the problem. When being tested on benchmark instances from the …",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:Zph67rFs4hoC,,1
1999,An efficient ant colony optimization algorithm for protein structure prediction,"Protein structure prediction is considered as one of the most long-standing and challenging problem in bioinformatics. In this paper, we present an efficient ant colony optimization algorithm to predict the protein structure on three-dimensional face-centered cubic lattice coordinates, using the hydrophobic-polar model and the Miyazawa-Jernigan model to calculate the free energy. The reinforcement learning information is expressed in the k-order Markov model, and the heuristic information is determined based on the increase of the total energy. On a set of benchmark proteins, the results show a remarkable efficiency of our algorithm in comparison with several state-of-the-art algorithms.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:_FxGoFyzp5QC,,1
2000,ACOHAP: an efficient ant colony optimization for the haplotype inference by pure parsimony problem,"Haplotype information plays an important role in many genetic analyses. However, the identification of haplotypes based on sequencing methods is both expensive and time consuming. Current sequencing methods are only efficient to determine conflated data of haplotypes, that is, genotypes. This raises the need to develop computational methods to infer haplotypes from genotypes.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:IjCSPb-OGe4C,,1
2002,MODELING MULTI-CRITERIA DECISION-MAKING IN DYNAMIC NEUTROSOPHIC ENVIRONMENTS BASED ON CHOQUET INTEGRAL,"Multi-attributes decision-making problem in dynamic neutrosophic environment is an open and highly-interesting research area with many potential applications in real life. The concept of the dynamic interval-valued neutrosophic set and its application for the dynamic decision-making are proposed recently, however the inter-dependence among criteria or preference is not dealt with in the proposed operations to well treat inter-dependence problems.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:d1gkVwhDpl0C,,1
2003,A novel and efficient ant colony optimization algorithm for protein 3D structure prediction,"Protein structure prediction (PSP) is considered as one of the most long-standing and challenging problem in bioinformatic. In this paper, we present an efficient ant colony optimization algorithm to predict the protein structure on three-dimensional face-centered cubic lattice coordinates, using hydrophobic-polar (HP) model and MiyazawaJernigan (MJ) model to calculate the free energy. The reinforcement learning information is expressed in the k-order Markov model, the heuristic information is determined based on the increase of the total energy. On a set of benchmark proteins, the results show a remarkable efficiency of our algorithm by comparing with the state-of-the-art algorithms.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:ufrVoPGSRksC,,1
2004,A novel ant based algorithm for multiple graph alignment,"Multiple graph alignment (MGA) is a new approach to analyze protein structure in order to exploring their functional similarity. In this article, we propose a two-stage memetic algorithm to solve the MGA problem, named ACO-MGA2, based on ant colony optimization metaheuristic. A local search procedure is applied only to the second stage of the algorithm to save runtime. Experimental results have shown that ACO-MGA2 outperforms state-of-the-art algorithms while producing alignments of better quality.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:8k81kl-MbHgC,,1
2005,Preliminary results on the whole genome analysis of a Vietnamese individual,"We present some preliminary results on the whole genome analysis of an anonymous Kinh Vietnamese (KHV) individual that was deeply sequenced to 34-fold using the Illumina sequencing technology. The sequenced genome covered 99.85% of the human reference genome (GRCh37). We discovered (1) 3408825 single polymorphism nucleotides (SNPs) of which 41396 (1.2%) were novel,(2) 654024 short indels of which 35263 (5.4%) were novel, ie not present in the dbSNP and 1000 genomes project databases. We also detected 10611 large SVs (the lengthÂ â‰¥ 100bp) ofÂ four types, ie large indels (90.6%), the inter-chromosomal translocations (3.1%), the inversions (3.4%) and the intra-chromosomal translocations (2.9%). This study is our initial step toward large-scale projects on Vietnamese population.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:zYLM7Y9cAGgC,,1
2006,GA_SVM: a genetic algorithm for improving gene regulatory activity prediction,"Gene regulatory activity prediction problem is one of the important steps to understand the significant factors for gene regulation in biology. The advents of recent sequencing technologies allow us to deal with this task efficiently. Amongst these, Support Vector Machine (SVM) has been applied successfully up to more than 80% accuracy in the case of predicting gene regulatory activity in Drosophila embryonic development. In this paper, we introduce a metaheuristic based on genetic algorithm (GA) to select the best parameters for regulatory prediction from transcriptional factor binding profiles. Our approach helps to improve more than 10% accuracy compared to the traditional grid search. The improvements are also significantly supported by biological experimental data. Thus, the proposed method helps boosting not only the prediction performance but also the potentially biological insights.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:9yKSN-GCB0IC,,1
2008,ACOGNA2: A novel algorithm for maximizing accuracy in global network alignment,"Proteins do not independently work, they interact with each other. Their interactions are modelled by protein-protein interaction (PPI) networks. PPI networks alignment problem has great significance in the biomedical field and has attracted the interest of many researchers. This article introduces a new algorithm called ACOGNA2 to global aligning the PPI networks. Experiments demonstrated that the proposed algorithm outperform the state-of-the-art algorithms.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:3fE2CSJIrl8C,,1
2009,An efficient two-phase ant colony optimization algorithm for the closest string problem,"Given a finite set S of strings of length m, the task of finding a string t that minimizes the Hamming distance from t to S, has wide applications. This paper presents a two-phase Ant Colony Optimization (ACO) algorithm for the problem. The first phase uses the Smooth Max-Min (SMMAS) rule to update pheromone trails. The second phase is a memetic algorithm that uses ACO method to generate a population of solutions in each iteration, and a local search technique on the two best solutions. The efficiency of our algorithm has been evaluated by comparing to the Ant-CSP algorithm.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:roLk4NBRz8UC,,1
2010,Meta-reg: A computational metaheuristic framework to improve SVM-based prediction of regulatory activity [J],"Gene regulatory activity prediction is an important step to understand which Transcription Factors (TFs) are important for regulation of gene expression in cells. The development of recent high throughput technologies and machine learning approaches allow us to archive this task more efficiently. Support Vector Machine (SVM) has been successfully applied for the case of predicting gene regulatory activity in Drosophila embryonic development. Here, we introduce meta-heuristic approaches to select the best parameters for regulatory prediction from transcription factor binding profiles. Experimental results show that our approach outperforms existing methods and the potentials for further analysis beyond the prediction.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:eQOLeE2rZwMC,,1
2011,A fast and efficient ant colony optimization for haplotype inference by pure parsimony,"Haplotype inference is a challenging computational problem in population genetics. We introduce an approach using Ant Colony Optimization (ACO) metaheuristic, named ACOHAP, to infer haplotypes from unphased Single Polymorphism Nucleotide (SNP) marker data. Our method employs an efficient method for constructing the ACO graph through which ants flexibly traverse to build haplotypes. ACOHAP also uses a well-performed pheromone trail update strategy and a local search to improve the performance. Experiments showed that ACOHAP outperformed the state-of-the-art methods for haplotype inference in both simulated and biological data.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:hqOjcs7Dif8C,,1
2012,Adaptive Large Neighborhood Search Enhances Global Protein-Protein Network Alignment,"Aligning protein-protein interaction networks from different species is a useful mechanism for figuring out orthologous proteins, predicting/verifying protein unknown functions or constructing evolutionary relationships. The network alignment problem is proved to be NP-hard, requiring exponential-time algorithms, which is not feasible for the fast growth of biological data. In this paper, we present a novel global protein-protein interaction network alignment algorithm, which is enhanced with an extended large neighborhood search heuristics. Evaluated on benchmark datasets of yeast, fly, human and worm, the proposed algorithm outperforms state-of-the-art algorithms. Furthermore, the complexity of ours is polynomial, thus being scalable to large biological networks in practice.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:2osOgNQ5qMEC,,1
2013,An Efﬁcient Ant Colony Optimization Algorithm for Protein Structure Prediction,"Protein structure prediction is considered as one of the most long-standing and challenging problem in bioinformatics. In this paper, we present an efﬁcient ant colony optimization algorithm to predict the protein structure on three-dimensional face-centered cubic lattice coordinates, using the hydrophobic–polar model and the Miyazawa-Jernigan model to calculate the free energy. The reinforcement learning information is expressed in the k-order Markov model, and the heuristic information is determined based on the increase of the total energy. On a set of benchmark proteins, the results show a remarkable efﬁciency of our algorithm in comparison with several state-of-the-art algorithms.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:KlAtU1dfN6UC,,1
2014,Ant colony optimization based founder sequence reconstruction,"Reconstruction of founder (ancestor) genes for a given population is an important problem in evolutionary biology. It involves finding a set of genes that can combine together to form genes of all individuals in that population. Such reconstruction can be modeled as a combinatorial optimization problem, in which we have to find a set of founder (gene) sequences so that the individuals in a given population can be generated by the smallest number of recombination on these founder sequences. In this paper we propose a novel ant colony optimization algorithm (ACO) based method, equipped with some important improvements, for the founder gene sequence reconstruction problem. The proposed method yields excellent performance when validating on 108 test sets from three benchmark datasets. Comparing with the best by far method for founder sequence reconstruction, our proposed method performs better in 45 test sets, equally well in 44 and worse only in 19 sets. These experimental results demonstrate the efficacy and perspective of our proposed method.",120,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=PLcDqLgAAAAJ&citation_for_view=PLcDqLgAAAAJ:MXK_kJrjxJIC,,1
2021,Transformation of UML models to CSP: A case study for graph transformation tools,"Graph transformation provides an intuitive mechanism for capturing model transformations. In the current paper, we investigate and compare various graph transformation tools using a compact practical model transformation case study carried out as part of the AGTIVE 2007 Tool Contest [22]. The aim of this case study is to generate formal CSP processes from high-level UML activity diagrams, which enables to carry out mathematical analysis of the system under design.",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:u5HHmVD_uO8C,,1
2022,Applications of graph transformations with industrial relevance,"This volume compiles all finally accepted papers presented at the 4th International Symposium on Applications of Graph Transformations with Industrial Relevance (AGTIVE 2011), which took place in October 2011 in Budapest, Hungary. The submissions underwent a thorough, two-round review process both before and after the symposium to enable the authors to carefully incorporate the suggestions of reviewers.",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:TFP_iSt0sucC,,1
2023,On integrating ocl and triple graph grammars,"Triple Graph Grammars (TGGs) tend to be a promising approach for explaining relationships between models in general, and model co-evolution and model consistency within model-driven development in particular. Declarative TGG descriptions can be translated into operational scenarios for model integration, model synchronization, and model transformation. To realize such scenarios, restrictions formulated with the Object Constraint Language (OCL) are an important factor. How to integrate TGGs and OCL is a topic of ongoing research activities. There are strong similarities between the result of such an integration and the Queries, Views and Transformations (QVT) standard of the Object Management Group (OMG). We propose a language for this integration: One part of this language has a one-one mapping to TGGs and the remaining part covers OCL concepts. The language is realized in our tool …",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:u-x6o8ySG0sC,,1
2024,On domain driven design using annotation-based domain specific language,"The aim of object-oriented domain-driven design (DDD) is to iteratively develop software around a realistic domain model. Recent work in DDD use an annotation-based extension of object-oriented programming language to build the domain model. This model becomes the basis for a ubiquitous language and is used as input to generate software. However, the annotation-based extensions of these work do not adequately address the primitive and essential structural and behavioural modelling requirements of practical software. Further, they do not precisely characterise the software that is generated from the domain model. In this paper, we propose a DSL-based DDD method to address these limitations. We make four contributions:  (1) an annotation-based domain-specific language (DSL) named DCSL, whose annotation extension expresses a set of essential structural constraints and the essential behaviour …",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:IWHjjKOFINEC,,1
2025,Precise model-driven transformations based on graphs and metamodels,"Presenting precisely models and supporting automatic manipulation of models are at the heart of model-centric software development. A formal foundation for these tasks is a necessity as well as a challenge. We present a model-driven approach based on the integration of two light-weight formal methods, the object constraint language (OCL) and triple graph grammars (TGGs). OCL together with metamodeling allows us to present precisely models. With TGGs we can carry out model manipulations, especially model transformations. We focus on explaining the tool which realizes our approach. This tool is developed as an extension of the UML-based specification environment (USE), which offers full OCL support. A case study showing the transformation between statecharts and extended hierarchical automata explains our approach.",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:d1gkVwhDpl0C,,1
2026,Domain-driven design using meta-attributes: a DSL-based approach,"Applying object-oriented domain-driven design in practice requires bridging the gaps that exist among the perceived domain class models of the key stakeholders involved. In this paper, we propose a domain-driven design method that uses meta-attributes with an aim to fill these gaps. Our method extends and generalises a previous work to use meta-attributes to build the domain class model. The meta-attributes are designed to not only make it easier for the designer and domain expert to collaboratively capture the domain-specific requirements in the model, but to ease the translation of the model to design specification. This specification is written in an object-oriented, internal DSL. To increase productivity, we define a generator function that realises the meta-mapping between the state and behaviour spaces of a domain class to automatically generate its behavioural specification. We demonstrate our method …",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:_kc_bZDykSQC,,1
2027,An OCL-based framework for model transformations,Model transformation is an important building block for model-driven approaches. It puts forward a necessity and a challenge to specify and realize model transformation as well as to ensure the correctness of transformations. This paper proposes an OCL-based framework for model transformations. The formal foundation of the framework is the integration of Triple Graph Grammars and the Object Constraint Language (OCL). The OCL-based transformation framework offers an on-the-fly verification of model transformations and means for transformation quality assurance.,124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:Se3iqnhoufwC,,1
2028,Checking the Conformance between Models Based on Scenario Synchronization.,"Narrowing the wide conceptual gap between problem and implementation domains is considered a significant factor within software engineering. Currently, such a relation is often obtained using mappings between metamodels for a structural semantics. This paper proposes an approach based on the integration of Triple Graph Grammars (TGGs) and the Object Constraint Language (OCL) in order to explain a behavioral relation between models at different levels of abstraction. Triple rules incorporating OCL allow us to synchronize execution scenarios of a system at two levels. In this way we obtain an integrated operational semantics of the models as well as the possibility for conformance verification between them. We illustrate our approach with a case study for the relation between use case and design models.",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:zYLM7Y9cAGgC,,1
2029,Domain-driven design patterns: A metadata-based approach,"Design pattern is the most common form of object oriented software reuse. In object oriented domain driven design, a number of high-level patterns have been identified and applied for over a decade. However, no concrete design patterns for domain modeling in this method have been published in the literature. A primary challenge in defining these design patterns is how to express their form in a way that eases their application to a specific domain that uses a specific object oriented programming platform. In this work, we propose a set of concrete design patterns, whose form is expressed in a domain class modeling language (DCML). DCML is based on UML and uses implementation-aware meta-attributes to define design metadata. We extend this language with new meta-attributes to support the proposed design patterns. Further, we discuss how domain-specific examples of these patterns are translated to …",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:aqlVkmm33-oC,,1
2030,A type system for counting logs of multi-threaded nested transactional programs,"We present a type system to estimate an upper bound for the resource consumption of nested and multi-threaded transactional programs. The resource is abstracted as transaction logs. In comparison to our previous work on type and effect systems for Transactional Featherweight Java, this work exploits the natural composition of thread creation to give types to sub-terms. As a result, our new type system is simpler and more effective than our previous one. More important, it is more precise than our previous type system. We also show a type inference algorithm that we have implemented in a prototype tool.",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:roLk4NBRz8UC,,1
2031,From graph transformation to ocl using use,"With the tool USE, UML class diagrams with additional OCL constraints can be validated and properties can be formally checked. Constraints may be class invariants and operation pre- and postconditions. USE builds system states with object diagrams and expresses system evolution with operations employing basic state manipulations by creating and destroying objects (nodes) and links (edges) and by modifying attributes.",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:9yKSN-GCB0IC,,1
2032,USLTG: Test case automatic generation by transforming use cases,"This paper proposes a transformation-based method to automatically generate functional test cases from use cases named USLTG (Use case Specification Language (USL)-based Test Generation). We first focus on developing a modeling language named Test Case Specification Language (TCSL) in order to express test cases. Test cases in TCSL can contain detailed information including test steps, test objects within steps, actions of test objects, and test data. Such information is often ignored in currently available test case specifications. We then aim to generate test cases in a TCSL model by a transformation from use cases that are represented by a USL. The USLTG transformation includes three main steps in generating (1) scenarios, (2) test data, and (3) a TCSL model. Within our transformation, the OCL solver is employed in order to build system snapshots as the part of test cases and to identify other test …",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:R3hNpaxXUhUC,,1
2033,A transformation-based method for test case automatic generation from use cases,"This paper proposes a method to automatically generate system test cases from use cases. Within our method, we take use cases as source model that are represented by the use case modeling language USL. The target model for test cases is precisely specified using a modeling language named Test Case Specification Language (TCSL). Comparing with current approaches, generated test cases within our approach contain more detailed information including test steps, test objects within steps, actions of test objects, and test data. We focus on defining a transformation between the USL source model and the TCSL target model. We realized our method with a support tool based on the OCL solver of USE and the model transformation framework in Eclipse.",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:-f6ydRqryjwC,,1
2034,USL: Towards Precise Specification of Use Cases for Model-Driven Development,"Use cases have been widely employed as an efficient means to capture and structure software requirements. A use case model is often represented by a loose combination between a UML use case diagram and a textual description in natural language. The use case model expressed in such a form often contains ambiguous and imprecise parts. This prevents integrating it into model-driven approaches, where use case models are often taken as the source of transformations. This paper introduces a domain specific language named the Use case Specification Language (USL) to precisely specify use cases with two main features:(1) The USL has a concrete syntax in graphical form that allows us to achieve the usability goal;(2) The precise semantics of USL that is defined by mapping the USL to a Labelled Transition System (LTS) opens a possibility for transformations from USL models to other artifacts such as …",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:dhFuZR0502QC,,1
2035,Generative software module development: A domain-driven design perspective,"Object-oriented domain-driven design (DDD) aims to develop software (iteratively) around a realistic model of the application domain, which both thoroughly captures the domain requirements and is technically feasible for implementation. The main focus of existing work in DDD has been on using an annotation-based extension of object-oriented programming language to build the domain model. However, these work do not consider software modules as first-class objects and thus lack a method for their development. In this paper, we propose a generative software module development method to bridge this gap. More specifically, we make the following contributions: (1) An annotation-based internal DSL named MCCL for configuring the modules. This language uses a reflective module configuration class design. (2) A module configuration class generator that generates a module configuration class from a …",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:QIV2ME_5wuYC,,1
2036,Calculating statically maximum log memory used by multi-threaded transactional programs,"During the execution of multi-threaded and transactional programs, when new threads are created or new transactions are started, memory areas called logs are implicitly allocated to store copies of shared variables so that the threads can independently manipulate these variables. It is not easy to manually calculate the peak of memory allocated for logs when programs have arbitrary mixes of nested transactions and new thread creations. We develop a static analysis to compute the amount of memory used by logs in the worst execution scenarios of the programs. We prove the soundness of our analysis and we show a prototype tool to infer the memory bound.",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:kNdYIx-mwKoC,,1
2037,Automating inference of ocl business rules from user scenarios,User Scenarios have been advocated as an effective means to capture requirements by describing the system-to-be at the instance or example level. This instance-level information is then used to infer a possible software specification consistent with the provided valid and invalid scenarios. So far existing approaches have often focused on the generation of static models but have omitted the inference of business rules that could complement the static models and improve the precision of the software specification. In this sense this paper provides a first set of invariant inference patterns that are applied on valid and invalid snapshots in order to generate OCL~(Object Constraint Language) integrity constraints that the system should always satisfy. We strengthen the confidence of inferred results based on the user's feedback of generated examples and counterexamples for the considered constraint. The approach is …,124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:YsMSGLbcyi4C,,1
2038,Triple Graph Grammars and OCL for Validating System Behavior,We propose an approach based on the integration of Triple Graph Grammars (TGGs) and the Object Constraint Language (OCL) for checking the conformance between use case and design models.,124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:qjMakFHDy7sC,,1
2039,On automating inference of OCL constraints from counterexamples and examples,"Within model-based approaches, defining domains and domain restrictions for conceptual models or metamodels is significant. Recently, a domain is often presented as a class diagram, and domain restrictions are expressed using the Object Constraint Language (OCL). An effective method to define a domain is based on a description of the domain at the instance and example level. So far such a method has often focused on the generation of structure aspects, but have omitted the inference of OCL restrictions that could complement the domain structure and improve the precision of the domain. This paper proposes an approach to automating the inference of OCL restrictions from a domain description in terms of counter- and examples. Candidates are generated by a problem solving, and irrelevant ones are eliminated using the user feedback on generated counter- and examples. Our approach is …",124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:_FxGoFyzp5QC,,1
2040,Validation of System Behavior Utilizing an Integrated Semantics of Use Case and Design Models,This paper summarizes an approach how to specify use cases and how to solve the problem of validating the conformance between the use case model and the design model. An integrated semantics of the two models is proposed. We employ UML-and OCL-based techniques as well as ideas from graph transformation. This research contributes to model transformation within the area of Model Driven Development (MDD).,124,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J3sBFOIAAAAJ&citation_for_view=J3sBFOIAAAAJ:2osOgNQ5qMEC,,1
2041,Axiomatic characterization of belief merging by negotiation,"Belief merging has been an active research field with many important applications. The major approaches for the belief merging problems, considered as arbitration processes, are based on the construction of the total pre-orders of alternatives using distance functions and aggregation functions. However, these approaches require that all belief bases are provided explicitly and the role of agents, who provide the belief bases, are not adequately considered. Therefore, the results are merely ideal and difficult to apply in the multi-agent systems. In this paper, we approach the merging problems from other point of view. Namely, we treat a belief merging problem as a game, in which rational agents participate in a negotiation process to find out a jointly consistent consensus trying to preserve as many important original beliefs as possible. To this end, a model of negotiation for belief merging is presented, a set …",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:MXK_kJrjxJIC,,1
2042,Inconsistency measures for probabilistic knowledge bases,"One of the major concerns in knowledge integration is inconsistencies in knowledge bases. An inconsistency measure is a tool that helps analyzing inconsistency knowledge bases and resolving inconsistencies. In recent years, a wide range of measures with desirable properties have been proposed, however, these measures often correspond to logical, or probabilistic-logical framework. In this paper, we investigate several inconsistency measures and their properties for the knowledge bases represented by probabilistic framework.",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:Wp0gIr-vW9MC,,1
2043,An axiomatic model for merging stratified belief bases by negotiation,"This paper presents an axiomatic model for merging stratified belief bases by negotiation. We introduce the concept of mapping solution, which maps the preferences of agents into layers, as a vehicle to represent the belief states of agents and their attitudes towards the negotiation situations. The belief merging process in our model is divided into two stages: in the first stage, the agents’ stratified belief bases are maped to their preferences, and in the second stage a negotiation between the agents is carried out based on these preferences. In this paper, a set of rational axioms for negotiation-based belief merging is proposed and a negotiation solution which satisfies the proposed axioms is introduced. Finally, the logical properties of a family of merging-by-negotiation operators are discussed.",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:KlAtU1dfN6UC,,1
2044,Merging belief bases by negotiation,"Belief merging has been an active research field with many important applications. Many approaches for belief merging have been proposed, but these approaches only take the belief bases as inputs without the adequate attention to the role of agents, who provide the belief bases, thus the results achieved are merely ideal and difficult to apply in the multi-agent systems. In this paper, we present a merging approach based on the negotiation techniques. A new model is proposed in which agents gradually build their common belief base from the beliefs that they provide in each round of negotiation. A set of postulates is also introduced to characterize the logical properties of the merging results.",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:3fE2CSJIrl8C,,1
2045,On the belief merging by negotiation,"Belief merging is an active research field with many important applications. Most existing work addresses the belief merging issue using a centralised approach. In this paper, we investigate a distributed approach to the problem of belief merging. The contribution of this paper is two-fold: (i) we develop a negotiation-based model for belief merging, and (ii) we investigate the computational complexity of the belief merging problem within the proposed framework. Through the proposed model of negotiation-based belief merging, we will present and discuss several significant logical properties and computational complexity results.",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:ULOm3_A8WrAC,,1
2046,Method for restoring consistency in probabilistic knowledge bases,"Ensuring consistency of knowledge systems is always one of the essential requirements because, without it, most of these systems become useless. Because of the importance, many studies have involved the restoration of consistency in knowledge systems. However, these approaches are only implemented on knowledge systems that are represented by logic or probabilistic logic, thus when we apply them to probabilistic knowledge systems, there are many inadequacies. To overcome these drawbacks, in this paper, we put forward a new model for restoring the consistency of a probabilistic knowledge base by focusing on changing the probabilities in this knowledge base via several inconsistency measures. To this end, a set of inconsistency measures is presented and a family of consistency restoring operators for probabilistic knowledge bases is introduced. Next, an axiomatic model consists of a set of axioms …",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:4DMP91E08xMC,,1
2047,Argumentation framework for merging stratified belief bases,"This paper introduces a new approach for belief merging by using argumentation technique. The key idea is to organize each belief merging process as a game in which participating agents use argumentation technique to debate on their own belief bases to achieve consensus i.e. a common belief base. To this end, we introduce a framework for merging belief by argumentation in which an argumentation-based belief merging protocol is proposed and a set of intuitive and rational postulates to characterize the merging results is introduced. Several logical properties of the family of argumentation-based belief merging operators are also pointed out and discussed.",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:M3ejUd6NZC8C,,1
2048,Solving inconsistencies in probabilistic knowledge bases via inconsistency measures,"In most knowledge-based systems, the guarantee of consistency is one of the essential tasks to ensure them to avoid the trivial cases. Because of this reason, a wide range of approaches has been proposed for restoring consistency. However, these approaches often correspond to logical, or probabilistic-logical framework. In this paper, we investigate a model for restoring the consistency of probabilistic knowledge bases by focusing on the method of changing the probabilities in such knowledge bases. To this aim, a process to restore the consistency based on inconsistency measures is introduced, a set of rational and intuitive axioms to characterize the restoring operators is proposed, and several logical properties are investigated and discussed.",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:9ZlFYXVOiuMC,,1
2049,An algorithm for agent knowledge integration using conjunctive and disjunctive structures,"In this paper an algorithm for integration of knowledge in multi-agent environments using logic disjunction and conjunction structures, is presented and analysed. The authors propose a distance function between the logic formulae and define a set of postulates (criteria) for integration. The worked out algorithm is analysed regarding the computation complexity and the proposed postulates.",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:Zph67rFs4hoC,,1
2050,Short time cardio-vascular pulses estimation for dengue fever screening via continuous-wave Doppler radar using empirical mode decomposition and continuous wavelet transform,"Contactless measurement of cardio-vascular pulse acts an essential role in clinical medical sectors. Estimating cardio-vascular pulses based on continuous-wave (CW) Doppler radar in limited time while maintaining the accuracy is a challenging task. In this paper, we propose a signal processing method that combines empirical mode decomposition (EMD) and continuous wavelet transform (CWT) for a short time estimation of heart rate (HR) and inter-beat-interval from radar signals. We evaluate performance of the proposed method using 85 patients with dengue fever and 40 healthy subjects. Subsequently, the estimated contactless HR is compared to that of a commercial contact-type medical device. The result shows that the HR can be estimated within a period of 5 s with an accuracy of 96.2 ± 2.5%. The patients with dengue fever show an elevated HR and a decreased standard deviation of heartbeat …",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:RHpTSmoSYBkC,,1
2051,Framework for merging probabilistic knowledge bases," Knowledge merging is of major concern in developing probabilistic expert systems. Each system provides a consistent probabilistic knowledge while the merged knowledge base is often inconsistent. Because of this reason, a wide range of approaches has been put forward to merge probabilistic knowledge bases. However, the input of the models is the set of possible probabilistic functions representing the original probabilistic knowledge bases. In this paper, we investigate a framework for merging probabilistic knowledge bases represented by the new form. To this aim, a process to merge probabilistic knowledge bases is introduced, several transformation methods for the representation of the original probabilistic knowledge base is presented, a set of merging operators is proposed, and several desirable logical properties are investigated and discussed.",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:dhFuZR0502QC,,1
2052,Merging possibilistic belief bases by argumentation,"Belief merging is one of active research fields with a large range of applications in Artificial Intelligence. Most of the work in this research field is in the centralized approach, however, it is difficult to apply to interactive systems such as multi-agent systems. In this paper, we introduce a new argumentation framework for belief merging. To this end, a constructive model to merge possiblistic belief bases built based on the famous general argumentation framework is proposed. An axiomatic model, including a set of rational and intuitive postulates to characterize the merging result is introduced and several logical properties are mentioned and discussed.",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:aqlVkmm33-oC,,1
2053,Security Policy Integration Method for Information Systems,"Policy-based security is an effective and convenient approach to manage information systems. By this approach, we can handle easily all behaviors of a system thought a set of rules. However, conflicts between rules are one of the most common problems we have to deal with in administrative process. In this paper, we propose a new axiomatic approach to solve this problem. Several postulates are presented and analyzed as well as some algorithms are proposed next. The algorithms also have been implemented and tested, and experimental results are presented.",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:kNdYIx-mwKoC,,1
2054,Conceptual and Rating Model of the V-SMARTH Smart University,"The conceptual model of smart university has been generalized as a digital transformation-oriented higher educational institution using digital infrastructure (digital legal, digital human resources, digital data, digital technologies and digital applications) to provide personalized learning services to learners of all generations in the country and around the world, meeting the lifelong learning requirements and sustainable development of individuals as well as nations. It is described through the V-SMARTH model, including 6 basic components of digital resources, open access learning materials, virtual learning environment, individualized education, interactive learning and digital platform. These elements come together in three pillars of digitization, digital learning model innovation and comprehensive digital transformation process. The study also approached the notion of the smartness and the readiness level of the smart university. In particular, the issue of performance metrics of smart universities has been developed and implemented with the UPM rating for the VNU University of Engineering and Technology.",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:iH-uZ7U-co4C,,1
2055,A distance-based approach for merging probabilistic knowledge bases,"In the stages of development of probabilistic expert systems, knowledge merging is a major concern. To deal with knowledge merging problems, several approaches have been put forward. However, in the proposed models, each original probabilistic knowledge base (PKB) is represented by a set of probabilistic functions fulfilling such knowledge base. The drawbacks of the solutions are that the output of model is also a set of probabilistic functions satisfying the resulting PKB and there is no algorithm for implementing the merging process of PKBs in which each of them consists of probabilistic constraints. In this paper, distance-based approach is utilized to propose a new method of merging PKBs to ensure that both the input and output of methods are represented by sets of probabilistic constraints. To this aim, the relationship between the probability rules and the probabilistic constraints, and the several …",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:qUcmZB5y_30C,,1
2056,Integration of knowledge in disjunctive structure on semantic level,Knowledge integration is a very important task in Knowledge Management field. The aim of this paper is to present an algorithm for knowledge integration based on disjunctive structures. A distance function between disjunctions is analyzed and some postulates for knowledge integration in this logic structure are introduced. Key properties of these postulates and aspects of algorithm are also investigated.,127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:4TOpqqG69KYC,,1
2057,A model for building probabilistic knowledge-based systems using divergence distances,"The knowledge-based systems (KBSs) in general and solving the knowledge merging problem in particular have seen a great surge of research activity in recent years. However, there still exist two main shortcomings that need to be addressed in the probabilistic framework. Firstly, the current methods only deal with the problems in which original probabilistic knowledge bases (PKBs) are required to be consistent and formed in the same structure. It is a very strong requirement and difficult to apply in practice. Secondly, only a few measures of distance between probability distributions have been studied to apply in existing models. To overcome these disadvantages, in this paper, we introduce a novel framework for merging PKBs. To this end, a theoretical model is introduced and several experiments are implemented. In theoretical model, some theorems are pointed out and proved to provide mathematical …",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:4JMBOYKVnBMC,,1
2059,Algorithms for Merging Probabilistic Knowledge Bases,"There is an increasing need to develop appropriate techniques for merging probabilistic knowledge bases (PKB) in knowledge-based systems. To deal with merging problems, several approaches have been put forward. However, in the proposed models, the representation of the merged probabilistic knowledge base is not similar to the representation of original knowledge bases. The drawback of the solutions is that probabilistic constraints on the set of input knowledge bases must have the same structure and there is no algorithm for implementing the merging process. In this paper, we proposed two algorithms for merging probabilistic knowledge bases represented by various structures. To this aim, the method of constraint deduction is investigated, a set of mean merging operators is proposed and several desirable logical properties are presented and discussed. These are the basis for building …",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:L8Ckcad2t8MC,,1
2060,A Learning Method based on Bisimulation in Inconsistent Knowledge Systems,"Inconsistencies may naturally occur in the considered application domains in Artificial Intelligence, for example as a result of data mining works in distributed sources. In order to solve inconsistent knowledge, several paraconsistent description logics have been proposed. In this paper, we face the problem of concept learning for an inconsistent knowledge base system based on bisimulation. This algorithm allows learning a concept from a training information system in a paraconsistent descriptive logic system with a set of positive items, negative items, and inconsistent items. Here, we present a system for learning concept in an inconsistent knowledge base and discuss preliminary experimental results obtained in the electronic application domain.",127,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GK26T04AAAAJ&citation_for_view=GK26T04AAAAJ:IWHjjKOFINEC,,1
2062,A technique for generating test data using genetic algorithm,"Automatic test data generation for path coverage is an undecidable problem and genetic algorithm (GA) has been used as one good solution. This paper presents a method for optimizing GA efficiency by identifying the most critical path clusters in a program under test. We do this by using the static program analysis to find all the paths having the path conditions with low probability in generating coverage data, then basing on these path conditions to adjust the procedure of generating new populations in GA. The proposed approach is also applied some program under tests. Experimental results show that improved GA which can generate suitable test data has higher path coverage than the traditional GA.",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:M3ejUd6NZC8C,,1
2063,Securing data in composite web services,"Service-oriented architecture (SOA) provides a solution for building information systems with reusability, flexibility, and extensibility. In SOA, new services can be developed by using existing services. BPEL is considered as the main means for composing services. In service composition, a composite service may be built from element services which belong to different administrative domains. Therefore, in order to exploit the benefits of service composition, we must carefully consider security issues. One of them is to protect users' information. In this paper, we describe an approach for securing BPEL composite Web services. We assume that some users' data received by the composite services needs securing. The proposed approach makes sure that when the data is sent out from the composite service appropriate security policies will be applied.",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:mVmsd5A6BfQC,,1
2064,Towards scalable agent-based web service systems: performance evaluation,"Web service technology is widely applied in building distributed information systems in order to provide flexibility and extensibility. As in the traditional web services, when the number of clients increases, the traffic problems of network may happen and the services may become overloaded. Distributed mobile agent platforms in Web service have become more mature in recent years, and may help reducing the load from Web servers as well as reduce the traffic for overall network. We have proposed an agent-based framework for Web services that is scalable and reliable. The framework uses mobile agents to solve the traffic problem on traditional Web service systems. In this paper, we study the performance of our proposed approach with the focus on response time. We simulate our framework by mathematical models on a network simulator tool. The simulation results showed that our proposed framework has …",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:L8Ckcad2t8MC,,1
2065,Improvements of Directed Automated Random Testing in Test Data Generation for C++ Projects,"This paper improves the breadth-first search strategy in directed automated random testing (DART) to generate a fewer number of test data while gaining higher branch coverage, namely Static DART or SDART for short. In addition, the paper extends the test data compilation mechanism in DART, which currently only supports the projects written in C, to generate test data for C++ projects. The main idea of SDART is when it is less likely to increase code coverage with the current path selection strategies, the static test data generation will be applied with the expectation that more branches are covered earlier. Furthermore, in order to extend the test data compilation of DART for C++ context, the paper suggests a general test driver technique for C++ which supports various types of parameters including basic types, arrays, pointers, and derived types. Currently, an experimental tool has been implemented based on the …",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:9ZlFYXVOiuMC,,1
2066,Generation of test data using genetic algorithm and constraint solver,"Search-based testing techniques using genetic algorithm (GA) can automatically generate test data that achieves high coverage on almost any given program under test. GA casts the path coverage test data generation as an optimization problem and applies efficient search-based algorithms to find suitable test cases. GA approaches scale well and can handle any source code and test criteria, but it still has some degrades when program under test has critical path clusters. This paper presents a method for improving GA efficiency by integrating a constraint solver to solve path conditions in which regular GA cannot generate test data for coverage. The proposed approach is also applied to some programs under test. Experimental results demonstrate that improved GA can generate suitable test data has higher path coverage than the regular one.",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:dhFuZR0502QC,,1
2067,Research productivity and quality of higher education institutions in Vietnam: an analysis based on the integrated database of web of science and scopus,"This study intergrates the WoS and Scopus articles by Vietnamese authors into the Vietnam Citation Gateway system of Vietnam National University, Hanoi (VNU), sets up a merging tool to filter the duplication for these two databases and establishes their bibliometric and citation index. Based on this integrated database, the reality of international publications of Vietnam and Vietnamese universities in the period of 2014-2018 was analyzed, benchmarked and discussed. Accordingly, in 2018, the annual productivity of international publications in Vietnam reached 10,000 articles, nearly 70% of those were contributed by higher education institutions. This shows a stong yearly increase in Vienam’s WoS & Scopus articles (of 34.7% for Vietnam in general and 41.6% for higher education institutions in particular). In terms of research quality, Vietnam has an average citation index of 9.2, which is comparable to that of Asian higher education institutions. Notably, the study clearly shows the correlation between the university autonomy and the improvement of research productivity and quality.",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:mB3voiENLucC,,1
2068,JCIA: A tool for change impact analysis of java EE applications,"This paper presents a novel tool for change impact analysis of Java EE applications named JCIA. Analyzing the source code of the Java EE applications is a big challenge because of the complexity and large scale of the applications. Moreover, components in Java EE applications are not only in Java language but also in different languages such as XHTML, XML, JSP. This tool analyzes source code of Java EE applications for building the dependency graphs (called JDG). The main idea for generating JDG is based on developing the source code analyzers for the typical technologies of Java EE such as JavaServer Faces, Context and Dependency Injection, Web services. Based on the obtained JDG and the given change sets, JCIA calculates the corresponding impact sets by applying the change impact analysis (CIA) based on change types and Wave-CIA method. The calculated impact sets help …",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:HDshCWvjkbEC,,1
2069,A correlation-aware negotiation approach for service composition,"Composing existing services to create new services has been considered as an important activity of developing service-oriented architecture systems. The increment of services that provide the same functionality but different qualities leads to the complexity in finding the best solution for a composite service. In a heterogeneous and dynamic environment, QoS negotiation provides a flexible means for choosing suitable atomic services for service compositions. However, most proposed negotiation approaches assume that services are independent of others in terms of quality. Consequently, these negotiation approaches are not able to handle the correlation factors among services. This paper presents a flexible correlation-aware negotiation approach for service compositions. In our approach, the service correlations are considered as factors affecting the choice of concrete services. The effectiveness of the …",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:4DMP91E08xMC,,1
2070,Variability fault localization: a benchmark,"Software fault localization is one of the most expensive, tedious, and time-consuming activities in program debugging. This activity becomes even much more challenging in Software Product Line (SPL) systems due to the variability of failures in SPL systems. These unexpected behaviors are caused by variability faults which can only be exposed under some combinations of system features. Although localizing bugs in non-configurable code has been investigated in-depth, variability fault localization in SPL systems still remains mostly unexplored. To approach this challenge, we propose a benchmark for variability fault localization with a large set of 1,570 buggy versions of six SPL systems and baseline variability fault localization performance results. Our hope is to engage the community to propose new and better approaches to the problem of variability fault localization in SPL systems.",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:4JMBOYKVnBMC,,1
2071,An approach to analyze software security requirements in ABAC model,"Security has been a crucial aspect of most applications especially, critical-safety softwares. In fact, losing or leaking of sensitive data can lead to huge losses for organizations so software developers must always find ways to ensure the security properties for their softwares. In practice, attribute-based access control (ABAC) has been an effective, flexible and popular method to mitigate the risks of unauthorized accesses to resources in large and complex systems. In this paper, we introduce an approach for checking ABAC rules from the source code of an application software against to its requirement specification. Our work includes of formal definitions about ABAC policy and access rules in softwares, a method to analyze access rules from the source code, and algorithms to analyze and validate access rules supporting for the verification process. The proposed approach can help programmers to detect the …",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:qxL8FJ1GzNcC,,1
2072,An Approach for Specifying Access Control Policy in J2EE Applications,"Most applications based on J2EE platform use role- based access control as an efficient mechanism to achieve security. The current approach for specifying access rule is based on methods of Enterprise JavaBeans (EJBs). In large-scale systems, where a large number of EJBs are used and the interactions between EJBs are complex, direct use of this method- based approach is error-prone and difficult to maintain. We propose an alternative approach for specifying access control policy based on the concept of business function.",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:QIV2ME_5wuYC,,1
2073,VSEC: Transformer-Based Model for Vietnamese Spelling Correction,"Spelling error correction is one of topics which have a long history in natural language processing. Although previous studies have achieved remarkable results, challenges still exist. In the Vietnamese language, a state-of-the-art method for the task infers a syllable’s context from its adjacent syllables. The method’s accuracy can be unsatisfactory, however, because the model may lose the context if two (or more) spelling mistakes stand near each other. In this paper, we propose a novel method to correct Vietnamese spelling errors. We tackle the problems of mistyped errors and misspelled errors by using a deep learning model. The embedding layer, in particular, is powered by the byte pair encoding technique. The sequence to sequence model based on the Transformer architecture makes our approach different from the previous works on the same problem. In the experiment, we train the model with a …",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:isC4tDSrTZIC,,1
2074,Ranking Warnings of Static Analysis Tools Using Representation Learning,"Static analysis tools are frequently used to detect potential vulnerabilities in software systems. However, an inevitable problem of these tools is their large number of warnings with a high false positive rate, which consumes time and effort for investigating. In this paper, we present DeFP, a novel method for ranking static analysis warnings. Based on the intuition that warnings which have similar contexts tend to have similar labels (true positive or false positive), DeFP is built with two BiLSTM models to capture the patterns associated with the contexts of labeled warnings. After that, for a set of new warnings, DeFP can calculate and rank them according to their likelihoods to be true positives (i.e., actual vulnerabilities). Our experimental results on a dataset of 10 real-world projects show that using DeFP, by investigating only 60% of the warnings, developers can find +90% of actual vulnerabilities. Moreover, DeFP improves the state-of-the-art approach 30% in both Precision and Recall.",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:bEWYMUwI8FkC,,1
2075,A Variability Fault Localization Approach for Software Product Lines,"Software fault localization is one of the most expensive, tedious, and time-consuming activities in program debugging. This activity becomes even much more challenging in Software Product Line (SPL) systems due to variability of failures. These unexpected behaviors are induced by variability faults which can only be exposed under some combinations of system features. The interaction among these features causes the failures of the system. Although localizing bugs in single-system engineering has been studied in-depth, variability fault localization in SPL systems still remains mostly unexplored. In this article, we present VarCop, a novel and effective variability fault localization approach. For an SPL system failed by variability bugs, VarCop isolates suspicious code statements by analyzing the overall test results of the sampled products and their source code. The isolated suspicious statements are the …",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:iH-uZ7U-co4C,,1
2076,API parameter recommendation based on language model and program analysis,"APIs are extensively and frequently used in source code to leverage existing libraries and improve programming productivity. However, correctly and effectively using APIs, especially from unfamiliar libraries, is a non-trivial task. Although various approaches have been proposed for recommending API method calls in code completion, suggesting actual parameters for such APIs still needs further investigating. In this paper, we introduce FLUTE, an efficient and novel approach combining program analysis and language models for recommending API parameters. With FLUTE, the source code of programs is first analyzed to generate syntactically legal and type-valid candidates. Then, these candidates are ranked using language models. Our empirical results on two large real-world projects Netbeans and Eclipse indicate that FLUTE achieves 80% and +90% in Top-1 and Top-5 Precision, which means the tool …",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:k_IJM867U9cC,,1
2077,Practical approach to access the impact of global variables on program parallelism,"Global variables may have a significant impact on preventing programs from automatic parallelism. This paper introduces a practical approach to measure the effect of global variables on program parallelism. First, we conduct static data dependence analysis among program variables and represent such dependencies by a Variable Dependence Graph. Then, we analyze this graph for measuring and identifying which global variables have a significant impact on program parallelism. To evaluate this approach, we conduct experiments on 20 benchmark programs and an industrial application. The experimental results show that half of the studied programs contain large impact variables which may be the cause of preventing programs from parallel execution.",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:qUcmZB5y_30C,,1
2078,Quy trình kiểm tra trùng lặp trong nhóm văn bản,"Dinh Hieu Vo, Ngọc Sơn Nguyễn, Minh Tuấn Trần, Văn Sơn Nguyễn",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:R3hNpaxXUhUC,,1
2079,On Rectifying the Mapping between Articles and Institutions in Bibliometric Databases,"Today, bibliometric databases are indispensable sources for researchers and research institutions. The main role of these databases is to find research articles and estimate the performance of researchers and institutions. Regarding the evaluation of the research performance of an organization, the accuracy in determining institutions of authors of articles is decisive. However, current popular bibliometric databases such as Scopus and Web of Science have not addressed this point efficiently. To this end, we propose an approach to revise the authors’ affiliation information of articles in bibliometric databases. We build a model to classify articles to institutions with high accuracy by assembling the bag of words and n-grams techniques for extracting features of affiliation strings. After that, these features are weighted to determine their importance to each institution. Affiliation strings of articles are transformed into the new feature space by integrating weights of features and local characteristics of words and phrases contributing to the sequences. Finally, on the feature space, the support vector classifier method is applied to learn a predictive model. Our experimental result shows that the proposed model’s accuracy is about 99.1%.",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:hFOr9nPyWt4C,,1
2080,Relation Extraction in Vietnamese Text via Piecewise Convolution Neural Network with Word-Level Attention,"With the explosion of information technology, the Internet now contains enormous amounts of data, so the role of information extraction systems becomes very important. Relation Extraction is a sub-task of Information Extraction, which focuses on classifying the relationship between the entity pairs mentioned in the text. In recent years, despite the many new methods have been introduced, Relation Extraction still receives attention from researchers for languages in general and Vietnamese in particular.Relation Extraction can be addressed in a variety of ways, including supervised learning methods, unsupervised and semi-supervised methods. Recent studies in the English language have shown that Relation Extraction using deep learning method in the supervised or semi-supervised domains is achieving optimal and superior results over traditional non-deep learning methods. However, researches in Vietnamese …",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:-f6ydRqryjwC,,1
2081,On the Compliance of Access Control Policies in Web Applications,"Model-View-Controller (MVC) architecture has commonly used in the implementation of web applications. These systems often incorporate security policies to ensure their reliability. Role-based access control (RBAC) is one of the effective solutions for reducing resources access violations of a system. This paper introduces an approach to check the compliance of a web application under MVC architecture with its RBAC specification. By investigating the system architecture and source code analysis, our approach conducts with extracting a list of resources access permissions, constructing a resources exploitation graph and organizing an access control matrix according to roles of a web application. The approach aims at checking two violation cases of web applications: (i) the presence of unspecified access rules and (ii) the absence of specified access rules. We illustrate the proposed approach by a case …",128,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=0FqthhoAAAAJ&citation_for_view=0FqthhoAAAAJ:ZeXyd9-uunAC,,1
2082,SMT for polynomial constraints on real numbers,"This paper preliminarily reports an SMT for solving polynomial inequalities over real numbers. Our approach is a combination of interval arithmetic (over-approximation, aiming to decide unsatisfiability) and testing (under-approximation, aiming to decide satisfiability) to sandwich precise results. In addition to existing interval arithmetic, such as classical intervals and affine intervals, we newly design Chebyshev Approximation Intervals, focusing on multiplications of the same variables, like Taylor expansions. When testing cannot find a satisfiable instance, this framework is designed to start a refinement loop by splitting input ranges into smaller ones (although this refinement loop implementation is left to future work). Preliminary experiments on small benchmarks from SMT-LIB are also shown.",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:u5HHmVD_uO8C,,1
2083,raSAT: An SMT solver for polynomial constraints,"This paper presents the raSAT SMT solver for polynomial constraints, which aims to handle them over both reals and integers with simple unified methodologies: (1) raSAT loop for inequalities, which extends the interval constraint propagation with testing to accelerate SAT detection, and (2) a non-constructive reasoning for equations over reals, based on the generalized intermediate value theorem.",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:08ZZubdj9fEC,,1
2085,raSAT: an SMT solver for polynomial constraints,"This paper presents raSAT SMT solver, which is aimed to handle polynomial constraints over both reals and integers with simple unified methodologies. Its three main features are (1) a raSAT loop for inequalities, which adds testing to interval constraint propagation to accelerate SAT detection, (2) a non-constructive reasoning for equations over reals based on the generalized intermediate value theorem, and (3) soundness of floating-point arithmetic that is guaranteed by (a) rounding up/down over-approximations of intervals, and (b) confirmation of a satisfying instance detected by testing using the iRRAM package, which guarantees error bounds.",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:u9iWguZQMMsC,,1
2086,Wrapping Computer Algebra is Surprisingly Successful for Non-Linear SMT,"We report on a prototypical tool for Satisfiability Modulo Theory solving for quantifier-free formulas in Non-linear Real Arithmetic or, more precisely, real closed fields, which uses a computer algebra system as the main component. This is complemented with two heuristic techniques, also stemming from computer algebra, viz. interval constraint propagation and subtropical satisfiability. Our key idea is to make optimal use of existing knowledge and work in the symbolic computation community, reusing available methods and implementations to the most possible extent. Experimental results show that our approach is surprisingly efficient in practice.",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:P5F9QuxV20EC,,1
2087,Formal Analysis of Database Trigger Systems Using Event-B,"Most modern relational database systems use triggers to implement automatic tasks in response to specific events happening inside or outside a system. A database trigger is a human readable block code without any formal semantics. Frequently, people can check if a trigger is designed correctly after it is executed or by manual checking. In this article, the authors introduce a new method to model and verify database trigger systems using Event-B formal method at design phase. First, the authors make use of similar mechanism between triggers and Event-B events to propose a set of rules translating a database trigger system into Event-B constructs. Then, the authors show how to verify data constraint preservation properties and detect infinite loops of trigger execution with RODIN/Event-B. The authors also illustrate the proposed method with a case study. Finally, a tool named Trigger2B which partly supports the …",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:WbkHhVStYXYC,,1
2088,raSAT: SMT for Polynomial Inequality,"This paper presents an iterative approximation refinement, called raSAT loop, which solves polynomial inequality constraints on real numbers. The approximation scheme consists of interval arithmetic (over-approximation, aiming to decide unsatisfiability) and testing (underapproximation, aiming to decide satisfiability). If both of them fail to decide, input intervals are refined by decomposition. raSAT loop is implemented as an SMT raSAT with miniSAT 2.2 as a backend SAT solver. Experiments including simple benchmarks for estimating effects of input measures (ie, degrees, number of variables, and number of constraints) and QF NRA benchmarks from SMT-LIB show that raSAT is comparable to Z3 4.3, and sometimes outperforms, especially with high degree of polynomials.",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:D03iK_w7-QYC,,1
2089,Checking the consistency between ucm and psm using a graph-based method,"Checking the consistency in component models at design phase is essential in component-based software engineering (CBSE). In our previous work, we proposed an approach for verifying automatically the matching between protocol state machines (PSMs) and the use case map (UCM), using the B method. Due to the expressive power of B notations, however, we cannot describe the parallel processing in the implementation machine, particularly we are not able to express all features (such as AND-forks/joins, OR-forks/joins) of UCMs in a B implementation machine. In this work, we propose an approach to solve the expression problem of UCM features using a graph-based algorithm. The UCM path which describes the interaction between components is extracted and then decomposed into sequential events' paths if it has AND-forks/joins and/or OR-forks/joins. Each of sequential eventspsila paths will be …",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:UeHWp8X0CEIC,,1
2090,Checking compliance of program with secureUML model,"Access control is one of the most efficient ways to restrict resource access violations. However, in software development process, programs may not be fully complied with access policies represented in specifications. In this paper, we present an approach to verify the access policies compliance between a SecureUML model and its software program. We extract access control rules specified in SecureUML model, analyze source code of its program and propose an algorithm to check the compliance between two paradigms. Our approach can help programmers to detect some resource access violations and to improve the quality of software systems.",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:pyW8ca7W8N0C,,1
2091,Sat and SMT: their algorithm designs and applications,"SAT and SMT (SAT modulo theory) solvers aim to find a satisfiable instance of given constraints. SAT solver accepts a boolean constraints in a conjunctive normal form (CNF), such as, and if a CNF is satisfiable, returns an instance, such as. SMT solver accepts constraints described in background theory, such as arithmetic. Part 1 is devoted to explain the textbook concepts and usage of SAT and SMT solvers. For their usage, we investigate how to encode problems into CNF. Examples are taken from puzzles. Although puzzles are problems on bounded domains, there is certain hierarchy of difficulties, corresponding to the logical hierarchy of problems. Our examples are SUDOKU [2], Logic pictures [3], and Slither link [4] 1, 2, which correspond to descriptions in CNF, general propositional logic, and higher order logic, respectively. As conversion techniques to efficient CNFs, a popular Tseitin conversion and two …",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:d1gkVwhDpl0C,,1
2092,On the Compliance of Access Control Policies in Web Applications,"Model-View-Controller (MVC) architecture has commonly used in the implementation of web applications. These systems often incorporate security policies to ensure their reliability. Role-based access control (RBAC) is one of the effective solutions for reducing resources access violations of a system. This paper introduces an approach to check the compliance of a web application under MVC architecture with its RBAC specification. By investigating the system architecture and source code analysis, our approach conducts with extracting a list of resources access permissions, constructing a resources exploitation graph and organizing an access control matrix according to roles of a web application. The approach aims at checking two violation cases of web applications: (i) the presence of unspecified access rules and (ii) the absence of specified access rules. We illustrate the proposed approach by a case …",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:tS2w5q8j5-wC,,1
2094,raSAT 0.3 and raSAT 0.4 (exp) for SMT-COMP 2016,"From the experiments, we observe that raSAT 0.2 performs better than raSAT 0.3 in determining satisfiability of polynomial constraints. This is because “heavy” intervals decomposition in raSAT 0.2 results in speedy appearance of small variables’ intervals which consequently make the result of Interval Arithmetic more precise. In order to cooperate the ability of detecting unsatisfiability of raSAT 0.3 and the ability of detecting satisifiability of raSAT 0.2, raSAT 0.4 is a parallel combination of those two versions. The combination is done via a Python script. In addition, raSAT 0.2 in this combination has been enhanced with the revision of the application of the IVT.",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:9yKSN-GCB0IC,,1
2095,Integrating and checking access permissions in object oriented models,"Encapsulation in object oriented models refers to the bundling of data with the methods that operate on that data, it is used to hide the values or state of a structured data objects and preventing unauthorized parties' direct access to them. Access control mechanism in information systems is the process of mediating every request to resources and determining whether the request should be granted or denied. This paper proposes an approach allowing us to integrate access control mechanisms in object oriented paradigm. More detail, we define the access permissions between methods of classes in a UML model and providing a prototype to verify if the program implementation satisfied their access permission constraints.",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:zYLM7Y9cAGgC,,1
2096,Phương pháp sinh các ca kiểm thử tự động từ các mô hình thiết kế UML và ngôn ngữ ràng buộc đối tượng OCL,"In software development, testing is the crucial and integral process to produce a reliable and high quality system. Automated test case generationplays a significant role in practice and a lot of researches on it has been investigated in recent years. The paper proposes an automated test case generation approach based on UML sequence diagrams, class diagrams and Object Constraint Language(OCL). Comparing with other test case generation, the approaches[7, 8, 9] have achieved message paths coverage, pre- and postcondition coverage while ourapproach also gain extra boundary coverage and association-end multiplicity coverage. In addition, the UML sequence diagrams can contain other nested sequence diagrams and apply to interactive operators such as alternative, option, break, sequence, negative, strict, and ignore. Our approach is aimed at high coverage of testing and reducing the number of testcases for generation",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:Tyk-4Ss8FVUC,,1
2097,SMT for Polynomial Constraints and Its Applications,"Solving polynomial constraints plays an important role in program verification, eg, checking roundoff/overflow error with fixed point or floating point arithmetic, measures for proving termination, and loop invariant generation. Tarski proved that polynomial constraints over real numbers (algebraic numbers) are decidable, and later Collins proposed Quantifier Elimination by Cylindrical Algebraic Decomposition, which is nowadays implemented in Mathematica, Maple/SynRac, Reduce/Redlog, and QEPCAD. However, it is DEXPTIME with regard to the number of variables, and works fine in practice up to 5 variables and lower degrees. For instance, 8 variables with degree 10 already require 20–30 hours by supercomputer.",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:Tiz5es2fbqcC,,1
2100,Dynamically reconfiguration architecture for embedded systems using Kaffe,"In designing embedded systems, the exploration and synthesis of diíĩerent design aỉtematives and co-verification of a specific implementation are the most demanding tasks. Kaffe, an open-source technology, proviđes a platform for building a runtime environment and integrating diíĩerent design methodologies. Integrating Kaffe into embedded systems is the cornerstone of java-like technologies, allowing possibilities such as the development of portable programs on mobile devices. In this paper, vve introduce a co-design environment based on Kaffe vvhich supports the speciíication and prototyping of dynamicalỉy reconíigurable embedded syslems.",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:bFI3QPDXJZMC,,1
2101,Tutorial: SAT and SMT their algorithm designs and applications,"SAT and SMT (SAT modulo theory) aim to find a satisfiable instance of given constraints. SAT computes a boolean instance of constraints in a conjunctive normal form (CNF) of propositional logic, and SMT accepts constraints described in background theory, such as arithmetic. This tutorial consists of Part 1: SAT and Part 2: SMT. Part 1 focuses on SAT solver, and we will overview defacto-standard algorithm designs, such as non-chronological back tracking with implication graphs, conflict driven learning, and two watched literals [1]. Then, we investigate how to encode problems into CNF. Examples are taken from puzzles. Although puzzles are problems on bounded domains, there is certain hierarchy of difficulties, corresponding to the logical hierarchy of problems. Our examples are SUDOKU [2], Logic pictures [3], and Slitherlink [4] 12, which correspond to descriptions in CNF, general propositional logic, and higher order logic, respectively. As conversion techniques to efficient CNFs, a popular Tseitin conversion and two special techniques (for the latter two, respectively) are introduced.",130,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=kq2ht6wAAAAJ&citation_for_view=kq2ht6wAAAAJ:B3FOqHPlNUQC,,1
2103,The efficiency of indicator-based local search for multi-objective combinatorial optimisation problems,"In the last few years, a significant number of multi-objective metaheuristics have been proposed in the literature in order to address real-world problems. Local search methods play a major role in many of these metaheuristic procedures. In this paper, we adapt a recent and popular indicator-based selection method proposed by Zitzler and Künzli in 2004, in order to define a population-based multi-objective local search. The proposed algorithm is designed in order to be easily adaptable, parameter independent and to have a high convergence rate. In order to evaluate the capacity of our algorithm to reach these goals, a large part of the paper is dedicated to experiments. Three combinatorial optimisation problems are tested: a flow shop problem, a ring star problem and a nurse scheduling problem. The experiments show that our algorithm can be applied with success to different types of multi-objective …",132,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GaqY-X4AAAAJ&citation_for_view=GaqY-X4AAAAJ:9yKSN-GCB0IC,,1
2104,Single view image based - 3D human pose reconstruction.,"In this paper, we propose an improved framework for estimating 3D human pose sequences from 2D videos. Prior methods were declared to solve this problem; however, they sometimes produced invalid poses because the constraint of result poses was not robust enough to satisfy anthropometric regular body pose. Our goal is to eliminate this drawback, provide more accurate estimated poses. The proposed framework consists of three stages. First, we start by generating a set of class-independent body part candidates for each video frame and constructing a fully connected graph from these candidates. Integer linear programming is used to label each candidate by a part class and assign it to individuals to form accurate and consistent pose. We then make the outputs from the first stage suitable for the next stage by inferring the 2D joint configuration. Second, from 2D poses resulted from the first stage, 3D poses are reconstructed by a matching pursuit algorithm operating on camera projection, all the output must adapt the anthropometric regular body pose constraint such as limb length and joint angle constraint. Third, we exploit the temporal and spatial information in videos to obtain more accurate 3D poses. Our experiments on CMU MOCAP dataset show that the proposed framework produces better results compared with prior works.",117,https://doi.org/10.1109/KSE.2017.8119445,1,
2105,Obtaining better non-dominated sets using volume dominance,"An important goal in multiobjective optimisation is to find a good set of non-dominated solutions that is both well-distributed and well-converged. Most multiobjective optimisation algorithms use the conventional Pareto dominance relationship. Over recent years, new approaches for the dominance relationship such as relaxed Pareto dominance, have been proposed. The interest in relaxed forms of Pareto dominance has been increasing due to their capability to find extreme values in the objective space. This paper conducts a short review on relaxed forms of Pareto dominance in the literature. A new form of relaxed Pareto dominance, called volume dominance, is presented afterward. The results based on comparison between volume dominance and Pareto dominance using SEAMO2, SPEA2 and NSGA2 show that the proposed volume dominance is capable of obtaining a better and smoother trade-off front, and it …",132,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GaqY-X4AAAAJ&citation_for_view=GaqY-X4AAAAJ:u5HHmVD_uO8C,,1
2106,A simple evolutionary algorithm with self-adaptation for multi-objective nurse scheduling,"We present a multi-objective approach to tackle a real-world nurse scheduling problem using an evolutionary algorithm. The aim is to generate a few good quality non-dominated schedules so that the decision-maker can select the most appropriate one. Our approach is designed around the premise of ‘satisfying individual nurse preferences’ which is of practical significance in our problem. We use four objectives to measure the quality of schedules in a way that is meaningful to the decision-maker. One objective represents staff satisfaction and is set as a target. The other three objectives, which are subject to optimisation, represent work regulations and workforce demand. Our algorithm incorporates a self-adaptive decoder to handle hard constraints and a re-generation strategy to encourage production of new genetic material. Our results show that our multi-objective approach produces good quality …",132,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GaqY-X4AAAAJ&citation_for_view=GaqY-X4AAAAJ:IjCSPb-OGe4C,,1
2107,Heuristic approach for automated shelf space allocation,"Shelf space allocation is the problem of efficiently arranging retail products on shelves in order to maximise profit, improve stock control, improve customer satisfaction, etc. Most work reported in the literature on this problem has focused on the case of large retailers such as big supermarkets. The interest here is to tackle this problem in the context of small retail shops where different issues arise when compared to large retailers. This paper proposes a heuristic approach to automate shelf space allocation in small retail shops. Several initialisation heuristics and local search moves are incorporated into the proposed method which generates high quality practical arrangements represented graphically as simple planograms.",132,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GaqY-X4AAAAJ&citation_for_view=GaqY-X4AAAAJ:u-x6o8ySG0sC,,1
2108,Building Population-Specific Reference Genomes: A Case Study of Vietnamese Reference Genome.,"The human reference genome is an essential tool for studying human genomes. The standard reference genome is constructed from genomes of a few donors. The 1000 genomes project has revealed a huge amount of genetic differences between diverse populations. It is therefore naturally questioned whether the standard reference genome can work well for all human genome studies or population-specific reference genomes are needed accordingly. In this paper, we present a pipeline for constructing and evaluating a population-specific reference genome. The pipeline was examined on building the Vietnamese reference genome from 100 Kinh Vietnamese genomes obtained from the 1000 genomes project. Experiments showed that the resulting Vietnamese reference genome was better than the standard reference genome at analyzing Vietnamese genomic data. It helped improve the quality of short reads mapping and genotype calling for Vietnamese genomes. The pipeline is applicable for building and evaluating other population-specific reference genomes. For the first time the Vietnamese reference genome, which is now available for further Vietnamese genome studies, was successfully built.",121,https://doi.org/10.1109/KSE.2015.49,1,
2109,A pageranking based method for identifying characteristic genes of a disease.,"Ranking is an importance task in the research field of complex biological networks, including gene networks. In this article, we propose a method for ranking the genes causing a specific type of disease, special characteristics of samples under a particular condition. Genes with the highest scores may be considered as the key factors leading to the development of disease type in the studied samples. The experience results on the gene expression data of two subtypes of Leukemia cancer disease, i.e., ALB and ALT for B-cells and T-cells are respectively shown.",121,https://doi.org/10.1109/ICNSC.2008.4525457,1,
2110,D3NER: biomedical named entity recognition using CRF-biLSTM improved with fine-tuned embeddings of various linguistic information.,"Recognition of biomedical named entities in the textual literature is a highly challenging research topic with great interest, playing as the prerequisite for extracting huge amount of high-valued biomedical knowledge deposited in unstructured text and transforming them into well-structured formats. Long Short-Term Memory (LSTM) networks have recently been employed in various biomedical named entity recognition (NER) models with great success. They, however, often did not take advantages of all useful linguistic information and still have many aspects to be further improved for better performance.

Results",121,https://doi.org/10.1093/bioinformatics/bty356,1,
2111,Large-scale Exploration of Neural Relation Classification Architectures.,"Experimental performance on the task of relation classification has generally improved using deep neural network architectures. One major drawback of reported studies is that individual models have been evaluated on a very narrow range of datasets, raising questions about the adaptability of the architectures, while making comparisons between approaches difficult. In this work, we present a systematic large-scale analysis of neural relation classification architectures on six benchmark datasets with widely varying characteristics. We propose a novel multi-channel LSTM model combined with a CNN that takes advantage of all currently popular linguistic and architectural features. Our ‘Man for All Seasons’ approach achieves state-of-the-art performance on two datasets. More importantly, in our view, the model allowed us to obtain direct insights into the continued challenges faced by neural language models on this task.",121,https://doi.org/10.18653/v1/d18-1250,1,
2112,Improving chemical-induced disease relation extraction with learned features based on convolutional neural network.,"There have been an increasing number of various machine learning-based models successfully proposed and applied for automatic chemical-induced disease (CID) relation extraction. They, however, usually require carefully handcrafted rich feature sets, which rely on expert knowledge, thus require expensive human labor but normally still cannot generalize data well enough. In this paper, we propose a CID relation extraction model that learns features automatically through a Convolutional Neural Network (CNN) instead of traditional handcrafted features. We exploit the shortest dependency path between a disease and a chemical for identifying their CID relation. Dependency relations, with and without their direction information, are further investigated. Experimental results on benchmark datasets (namely the BioCreative V dataset) are very potential, demonstrating the effectiveness of our proposed model for CID relation extraction.",121,https://doi.org/10.1109/KSE.2017.8119474,1,
2113,Sieve-based coreference resolution enhances semi-supervised learning model for chemical-induced disease relation extraction.,"The BioCreative V chemical-disease relation (CDR) track was proposed to accelerate the progress of text mining in facilitating integrative understanding of chemicals, diseases and their relations. In this article, we describe an extension of our system (namely UET-CAM) that participated in the BioCreative V CDR. The original UET-CAM system’s performance was ranked fourth among 18 participating systems by the BioCreative CDR track committee. In the Disease Named Entity Recognition and Normalization (DNER) phase, our system employed joint inference (decoding) with a perceptron-based named entity recognizer (NER) and a back-off model with Semantic Supervised Indexing and Skip-gram for named entity normalization. In the chemical-induced disease (CID) relation extraction phase, we proposed a pipeline that includes a coreference resolution module and a Support Vector Machine relation extraction model. The former module utilized a multi-pass sieve to extend entity recall. In this article, the UET-CAM system was improved by adding a ‘silver’ CID corpus to train the prediction model. This silver standard corpus of more than 50 thousand sentences was automatically built based on the Comparative Toxicogenomics Database (CTD) database. We evaluated our method on the CDR test set. Results showed that our system could reach the state of the art performance with F1 of 82.44 for the DNER task and 58.90 for the CID task. Analysis demonstrated substantial benefits of both the multi-pass sieve coreference resolution method (F1 + 4.13%) and the silver CID corpus (F1 +7.3%).",121,https://doi.org/10.1093/database/baw102,1,
2115,AcoSeeD: An Ant Colony Optimization for Finding Optimal Spaced Seeds in Biological Sequence Search.,"Similarity search in biological sequence database is one of the most popular and important bioinformatics tasks. Spaced seeds have been increasingly used to improve the quality and sensitivity of searching, for example, in seeded alignment methods. Finding optimal spaced seeds is a NP-hard problem. In this study we introduce an application of an Ant Colony Optimization (ACO) algorithm to address this problem in a metaheuristics framework. This method, called AcoSeeD, builds optimal spaced seeds in an elegant construction graph that uses the ACO standard framework with a modified pheromone update. Experimental results demonstrate that AcoSeeD brings a significant improvement of sensitivity while demanding the same computational time as other state-of-the-art methods. We also introduces an alternative way of using local search that exerts a fast approximation of the objective function in ACO.",121,https://doi.org/10.1007/978-3-642-32650-9_19,1,
2116,Use of structural DNA properties for the prediction of transcription-factor binding sites in Escherichia coli.,"Recognition of genomic binding sites by transcription factors can occur through base-specific recognition, or by recognition of variations within the structure of the DNA macromolecule. In this article, we investigate what information can be retrieved from local DNA structural properties that is relevant to transcription factor binding and that cannot be captured by the nucleotide sequence alone. More specifically, we explore the benefit of employing the structural characteristics of DNA to create binding-site models that encompass indirect recognition for the Escherichia coli model organism. We developed a novel methodology [Conditional Random fields of Smoothed Structural Data (CRoSSeD)], based on structural scales and conditional random fields to model and predict regulator binding sites. The value of relying on local structural-DNA properties is demonstrated by improved classifier performance on a large number of biological datasets, and by the detection of novel binding sites which could be validated by independent data sources, and which could not be identified using sequence data alone. We further show that the CRoSSeD-binding-site models can be related to the actual molecular mechanisms of the transcription factor DNA binding, and thus cannot only be used for prediction of novel sites, but might also give valuable insights into unknown binding mechanisms of transcription factors.

",121,https://doi.org/10.1093/nar/gkq1071,1,
2117,Flexible network reconstruction from relational databases with Cytoscape and CytoSQL.,"Molecular interaction networks can be efficiently studied using network visualization software such as Cytoscape. The relevant nodes, edges and their attributes can be imported in Cytoscape in various file formats, or directly from external databases through specialized third party plugins. However, molecular data are often stored in relational databases with their own specific structure, for which dedicated plugins do not exist. Therefore, a more generic solution is presented.",121,https://doi.org/10.1186/1471-2105-11-360,1,
2118,Prediction of kinase-specific phosphorylation sites using conditional random fields.,"Phosphorylation is a crucial post-translational protein modification mechanism with important regulatory functions in biological systems. It is catalyzed by a group of enzymes called kinases, each of which recognizes certain target sites in its substrate proteins. Several authors have built computational models trained from sets of experimentally validated phosphorylation sites to predict these target sites for each given kinase. All of these models suffer from certain limitations, such as the fact that they do not take into account the dependencies between amino acid motifs within protein sequences in a global fashion.

Results: We propose a novel approach to predict phosphorylation sites from the protein sequence. The method uses a positive dataset to train a conditional random field (CRF) model. The negative training dataset is used to specify the decision threshold corresponding to a desired false positive rate. Application of the method on experimentally verified benchmark phosphorylation data (Phospho.ELM) shows that it performs well compared to existing methods for most kinases. This is to our knowledge that the first report of the use of CRFs to predict post-translational modification sites in protein sequences.",121,https://doi.org/10.1093/bioinformatics/btn546,1,
2119,An improved version of volume dominance for multi-objective optimisation,"This paper proposes an improved version of volume dominance to assign fitness to solutions in Pareto-based multi-objective optimisation. The impact of this revised volume dominance on the performance of multi-objective evolutionary algorithms is investigated by incorporating it into three approaches, namely SEAMO2, SPEA2 and NSGA2 to solve instances of the 2-, 3- and 4- objective knapsack problem. The improved volume dominance is compared to its previous version and also to the conventional Pareto dominance. It is shown that the proposed improved volume dominance helps the three algorithms to obtain better non-dominated fronts than those obtained when the two other forms of dominance are used.",132,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GaqY-X4AAAAJ&citation_for_view=GaqY-X4AAAAJ:qjMakFHDy7sC,,1
2120,Anatomical sketch understanding: Recognizing explicit and implicit structure.,Sketching is ubiquitous in medicine. Physicians commonly use sketches as part of their note taking in patient records and to help convey diagnoses and treatments to patients. Medical students frequently use sketches to help them think through clinical problems in individual and group problem solving. Applications ranging from automated patient records to medical education software could benefit greatly from the richer and more natural interfaces that would be enabled by the ability to understand sketches. In this paper we take the first steps toward developing a system that can understand anatomical sketches.,122,https://doi.org/10.1016/j.artmed.2006.07.010,1,
2121,Adaptive and assortative mating scheme for evolutionary multi-objective algorithms,"We are interested in the role of restricted mating schemes in the context of evolutionary multi-objective algorithms. In this paper, we propose an adaptive assortative mating scheme that uses similarity in the decision space (genotypic assortative mating) and adapts the mating pressure as the search progresses. We show that this mechanism improves the performance of the simple evolutionary algorithm for multi-objective optimisation (SEAMO2) on the multiple knapsack problem.",132,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GaqY-X4AAAAJ&citation_for_view=GaqY-X4AAAAJ:UeHWp8X0CEIC,,1
2122,Speeding up and enhancing a large-scale fingerprint identification system on GPU.,"Fingerprint identification is one of the most common biometric feature problems which is used in many applications. Although state-of-the-art algorithms are very accurate, the need for fast processing a big database of millions of fingerprints is highly demanding. In this paper, we propose to adapt the fingerprint matching algorithm based on Minutia Cylinder-Code on Graphics Processing Units to speed up the matching. Another contribution of this paper is to add a consolidation stage after matching to enhance the precision. The experimental results on a GTX-680 and K40 tesla devices with standard data-sets prove that the proposed algorithm can be comparable with the state-of-the-art approach, and is suitable for a real-time identification application.",122,https://doi.org/10.1080/24751839.2017.1404712,1,
2123,Automatic Detection of Singular Points in Fingerprint Images Using Convolution Neural Networks.,"Minutiae based matching, the most popular approach used in fingerprint matching algorithms, is to calculate the similarity by finding the maximum number of matched minutiae pairs in two given fingerprints. With no prior knowledge about anchor/clue to match, this becomes a combinatorial problem. Global features of the fingerprints (e.g., singular core and delta points) can be used as the anchor to speed up the matching process. Most approaches use the conventional Poincare Index method with additional techniques to improve the detection of the core and delta points. Our approach uses Convolution Neural Networks which gained state-of-the-art results in many computer vision tasks to automatically detect those points. With the experimental results on FVC2002 database, we achieved the accuracy and false alarm of (96%, 7.5%) and (90%, 6%) for detecting core, and delta points, correspondingly. These results are comparative to those of the detection algorithms with human knowledge.",122,https://doi.org/10.1007/978-3-319-54430-4_20,1,
2124,Exploiting GPU for Large Scale Fingerprint Identification.,"Fingerprints are the most used biometrics features for identification. Although state-of-the-art algorithms are very accurate, but the need for fast processing speed for databases containing millions fingerprints is highly demanding. GPU devices are widely used in parallel computing tasks for its efficiency and low-cost. In this paper, we propose to adapt minutia cylinder-code (MCC) matching algorithm, an efficient algorithm in term of accuracy to GPU. The proposed method fits well with the architecture of the GPU that makes it easy to implement. The results of our experiments with a GTX- 680 device show that the proposed algorithm can perform 8.5 millions matches in a second that is suitable for real time identification systems having databases containing millions of fingerprints.",122,https://doi.org/10.1007/978-3-662-49381-6_66,1,
2125,Toward a Deep Learning Approach for Detecting PHP Webshell.,"The most efficient way of securing Web applications is searching and eliminating threats therein (from both malwares and vulnerabilities). In case of having Web application source codes, Web security can be improved by performing the task to detecting malicious codes, such as Web shells. In this paper, we proposed a model using a deep learning approach to detect and identify the malicious codes inside PHP source files. Our method relies on (i) pattern matching techniques by applying Yara rules to build a malicious and benign datasets, (ii) converting the PHP source codes to a numerical sequence of PHP opcodes and (iii) applying the Convolutional Neural Network model to predict a PHP file whether embedding a malicious code such as a webshell. Thus, we validate our approach with different webshell collections from reliable source published in Github. The experiment results show that the proposed method achieved the accuracy of 99.02% with 0.85% false positive rate.",123,https://doi.org/10.1145/3368926.3369733,1,
2126,An Efficient Parallel Method for Optimizing Concurrent Operations on Social Networks.,"This paper presents our approach to optimize the performance of both reading and writing concurrent operations on large-scale social networks. Here, we focus on the directed and unweighted relationships among members in a social network. It can then be illustrated as a directed, unweighted graph. Moreover, determining the relationship between any two members is similar to finding the shortest path between two vertices. With such a large-scale dynamic social network, we face the problem of having concurrent operations from adding or removing edges dynamically while one may ask to determine the relationship between two members. To solve this issue, we propose an efficient parallel method based on (i) utilizing an appropriate data structure, (ii) parallelizing the updating actions and (iii) improving the performance of query processing by both reducing the searching space and computing in multi-threaded parallel. Our method was validated by the datasets from SigMod Contest 2016 and SNAP DataSet Collections with fine positive experimental results compared to other solutions.",123,https://doi.org/10.1007/978-3-319-90287-6_10,1,
2127,An Efficient Parallel Algorithm for Computing the Closeness Centrality in Social Networks.,"Closeness centrality is an substantial metric used in large-scale network analysis, in particular social networks. Determining closeness centrality from a vertex to all other vertices in the graph is a high complexity problem. Prior work has a strong focuses on the algorithmic aspect of the problem, and little attention has been paid to the definition of the data structure supporting the implementation of the algorithm. Thus, we present in this paper an efficient algorithm to compute the closeness centrality of all nodes in a social network. Our algorithm is based on (i) an appropriate data structure for increasing the cache hit rate, and then reducing amount of time accessing the main memory for the graph data, and (ii) an efficient and parallel complete BFS search to reduce the execution time. We tested performance of our algorithm, namely BigGraph, with five different real-world social networks and compare the performance to that of current approaches including TeexGraph and NetworKit. Experiment results show that BigGraph is faster than TeexGraph and NetworKit 1.27-2.12 and 14.78-68.21 times, respectively.",123,https://doi.org/10.1145/3287921.3287981,1,
2128,Optimizing the shortest path query on large-scale dynamic directed graph.,"This paper presents our approach in order to optimize the shortest path query on a large-scale directed, dynamic graph such as a social network. For this kind of graph, edges can be dynamically added or removed while a user asks to determine the shortest path between two vertices. To solve this problem, we propose a strategic solution based on (i) an appropriate data structure, (ii) the optimized update actions (insertions and deletions) and (iii) by improving the performance of query processing by both reducing the searching space and computing in multithreaded parallel. Thus, graph is globally organized by the adjacent lists in order to improve the cache hit ratio and the update action performance. The reduction of searching space is based on the way of calculating the potential enqueued vertices. Cilkplus is chosen to parallelize the consecutive queries. Our strategy was validated by the datasets from SigMod Contest 2016 and SNAP DataSet Collection with the good experimental results.",123,https://doi.org/10.1145/3006299.3006321,1,
2129,A study of evolutionary multiobjective algorithms and their application to knapsack and nurse scheduling problems,"Evolutionary algorithms (EAs) based on the concept of Pareto dominance seem the most suitable technique for multiobjective optimisation. In multiobjective optimisation, several criteria (usually conflicting) need to be taken into consideration simultaneously to assess a quality of a solution. Instead of finding a single solution, a set of trade-off or compromise solutions that represents a good approximation to the Pareto optimal set is often required. This thesis presents an investigation on evolutionary algorithms within the framework of multiobjective optimisation. This addresses a number of key issues in evolutionary multiobjective optimisation. Also, a new evolutionary multiobjective (EMO) algorithm is proposed. Firstly, this new EMO algorithm is applied to solve the multiple 0/1 knapsack problem (a wellknown benchmark multiobjective combinatorial optimisation problem) producing competitive results when compared to other state-of-the-art MOEAs. Secondly, this thesis also investigates the application of general EMO algorithms to solve real-world nurse scheduling problems. One of the challenges in solving real-world nurse scheduling problems is that these problems are highly constrained and specific-problem heuristics are normally required to handle these constraints. These heuristics have considerable influence on the search which could override the effect that general EMO algorithms could have in the solution process when applied to this type of problems. This thesis outlines a proposal for a general approach to model the nurse scheduling problems without the requirement of problem-specific heuristics so that general EMO algorithms …",132,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GaqY-X4AAAAJ&citation_for_view=GaqY-X4AAAAJ:d1gkVwhDpl0C,,1
2130,A Method to Ensure Compliance with Attribute and Role Based Access Control Policy for Executing BPMN Models.,"The stringent control of access rights during business processes execution is an important technique to ensure systems security. Business processes are often designed and operated based on models represented by domain-specific languages, such as BPMN. Moreover, access control policies are often studied and specified based on access control models, such as Role-based Access Control (RBAC) and Attribute-based Access Control (ABAC). These security techniques have several challenges that need to be addressed, such as (1) ensuring consistency of RBAC/ABAC policy specifications and (2) ensuring compliance with RBAC/ABAC policies when executing a business process. In this paper, we propose using a metamodeling technique to take advantage of UML and OCL’s expressive power in order to facilitate validation and verification of RBAC/ABAC policies. Within our approach, the RBAC metamodel is extended so that ABAC constraints for complex business rules could be captured and checked. We build a support tool by incorporating Activiti (the support tool for specifying and implementing BPMN models) with USE (UML-based Specification Environment). The proposed method is experimented and evaluated for the process of liquidating the individual teaching contracts of a training management system.",124,https://doi.org/10.1109/ICSSE52999.2021.9538430,1,
2131,Automatic Extraction of Analysis Class Diagrams from Use Cases.,"At the early phase of software development, functional requirements of the software often need to be represented in the developer's language, resulting in a so-called analysis model. Current works in literature aim to increase automation in software development by either generating automatically the analysis model from a use case specification or transforming the analysis model to a design model. However, up to now, to precisely specify use cases is still a challenge, preventing us from realizing this aim. This paper proposes a method to extract analysis classes from a use case specification. Within our method, use cases are represented using our domain-specific modeling language named USL. We then define algorithms with transformation rules as a representation of analysis patterns in order to extract analysis classes from the USL use case model. We develop a support tool for our method in which transformation rules are realized using the ATL model-to-model transformation technique.",124,https://doi.org/10.1109/KSE50997.2020.9287702,1,
2132,On Analyzing Rule-Dependencies to Generate Test Cases for Model Transformations.,"Quality model transformations play a key role in the successful realization of Model Driven Engineering in practice. In the relational model transformations, rule dependency relations directly impact quality properties such as correctness, completeness, and information preservation. The analysis of rule dependencies from the declarative specification is expected to bring advantages for testing transformation properties.In this paper, we proposed a black-box approach for testing relational model transformations based on the analysis of the declarative specification using Triple Graph Grammar (TGG) rules. We exploit declarative TGG rules to capture the rule dependencies. Then, rule dependencies are combined together using the t-way testing technique to create test case descriptions. We transform patterns representing the input test condition and the oracle function of a test case description into OCL (Object Constraint Language) constraints to facilitate automatically generating input test models by solving constraints and querying interesting properties on the output models.",124,https://doi.org/10.1109/KSE.2019.8919486,1,
2133,jDomainApp: A Module-Based Domain-Driven Software Framework.,"Object-oriented domain-driven design (DDD) has been advocated to be the most common form of DDD, thanks to the popularity of object-oriented development methodologies and languages. Although the DDD method prescribes a set of design patterns for the domain model, it provides no languages or tools that realise these patterns. There have been several software frameworks developed to address this gap. However, these frameworks have not tackled two important software construction issues: generative, module-based software construction and development environment integration. In this paper, we propose a framework, named jDomainApp, and an Eclipse IDE plugin to address these issues. In particular, we extend our recent works on DDD to propose a software configuration language that expresses the software configuration, needed to automatically generate software from a set of modules. The modules are automatically generated using a module configuration language that we defined in a previous work. We demonstrate the framework and plug-in using a real-world software example. Further, we evaluate the performance of software construction to show that it is scalable to handle large software.",124,https://doi.org/10.1145/3368926.3369657,1,
2134,USL: A Domain-Specific Language for Precise Specification of Use Cases and Its Transformations.,"A use case model is often represented by a UML use case diagram and loosely structured textual descriptions. The use case model expressed in such a form contains ambiguous and imprecise parts. This prevents integrating it into model-driven approaches, where use case models are often taken as the source of transformations. In this paper, we introduce a domain-specific language named the Use case Specification Language (USL) to precisely specify use cases. We define the abstract syntax of USL using a metamodel together with OCL wellformedness rules and then provide a graphical concrete syntax for
the usability goal. We also define a precise semantics for USL by mapping USL models to Labelled Transition Systems (LTSs). It opens a possibility to transform USL models to software artifacts such as test cases and design models. We focus on a transformation from a USL model to a template-based use case description in order to illustrate our method. A language evaluation of USL is also performed in this paper.",124,https://doi.org/10.31449/inf.v42i3.2253,1,
2135,An approach for testing model transformations.,"Model transformations play a critical role in Model Driven Engineering. Such an importance has created a demand for transformation testing strategies and tools. This paper introduces an approach for testing model transformations using classifying terms. Classifying terms give a fine-grained control over the selection of input data and test oracles. By integrating this technique into graph-based transformation frameworks like RTL, we provide a way to validate model transformations using a graphical interface, in which the generation of input models and validation of output models are handled automatically.",124,https://doi.org/10.1109/KSE.2018.8573367,1,
2136,On Model Finding with Constraint Patterns.,"Class models are often employed to represent domains. In order for class models to conform to their intended domain semantics, we need to ensure their precision and consistency. Precision can be achieved by augmenting the models with constraints, and consistency can be achieved by avoiding contradictory constraints. Model finding is a technique to maintain this kind of consistency. It automatically checks the satisfiability of models by using example instances. However, this technique is often inefficient when applied to large models due to the huge search space. In this paper, we propose a technique to narrow the search space by using patterns to represent constraints. Constraint patterns allow us to put further restrictions on the search space. Our case study illustrates that using constraint patterns in this way can improve the verification process.",124,https://doi.org/10.3233/978-1-61499-674-3-279,1,
2137,On Scenario Synchronization.,"In software development a system is often viewed by various models at different levels of abstraction. It is very difficult to maintain the consistency between them for both structural and behavioral semantics. This paper focuses on a formal foundation for presenting scenarios and reasoning the synchronization between them. We represent such a synchronization using a transition system, where a state is viewed as a triple graph presenting the connection of current scenarios, and a transition is defined as a triple graph transformation rule. As a result, the conformance property can be represented as a Computational Tree Logic (CTL) formula and checked by model checkers. We define the transition system using our extension of UML activity diagrams together with Triple Graph Grammars (TGGs) incorporating Object Constraint Language (OCL). We illustrate the approach with a case study of the relation between a use case model and a design model. The work is realized using the USE tool.",124,https://doi.org/10.1007/978-3-642-15643-4_9,1,
2138,Implementing Advanced RBAC Administration Functionality with USE.,"Role-based access control (RBAC) is a powerful means for laying out and developing higher-level organizational policies such as separation of duty, and for simplifying the security management process. One of the important aspects of RBAC is authorization constraints that express such organizational policies. While RBAC has generated a great interest in the security community, organizations still seek a flexible and effective approach to impose role-based authorization constraints in their security-critical applications. In particular, today often only basic RBAC concepts have found their way into commercial RBAC products; specifically, authorization constraints are not widely supported. In this paper, we present an RBAC administration tool that can enforce certain kinds of role-based authorization constraints such as separation of duty constraints. The authorization constraint functionality is based upon the OCL validation tool USE. We also describe our practical experience that we gained on integrating OCL functionality into a prototype of an RBAC administration tool that shall be extended to a product in the future.",124,https://doi.org/10.14279/tuj.eceasst.15.177,1,
2139,Circall: fast and accurate methodology for discovery of circular RNAs from paired-end RNA-sequencing data.,"Circular RNA (circRNA) is an emerging class of RNA molecules attracting researchers due to its potential for serving as markers for diagnosis, prognosis, or therapeutic targets of cancer, cardiovascular, and autoimmune diseases. Current methods for detection of circRNA from RNA sequencing (RNA-seq) focus mostly on improving mapping quality of reads supporting the back-splicing junction (BSJ) of a circRNA to eliminate false positives (FPs). We show that mapping information alone often cannot predict if a BSJ-supporting read is derived from a true circRNA or not, thus increasing the rate of FP circRNAs.",125,https://doi.org/10.1186/s12859-021-04418-8,1,
2140,Vehicle Mode and Driving Activity Detection Based on Analyzing Sensor Data of Smartphones.,"In this paper, we present a flexible combined system, namely the Vehicle mode-driving Activity Detection System (VADS), that is capable of detecting either the current vehicle mode or the current driving activity of travelers. Our proposed system is designed to be lightweight in computation and very fast in response to the changes of travelers’ vehicle modes or driving events. The vehicle mode detection module is responsible for recognizing both motorized vehicles, such as cars, buses, and motorbikes, and non-motorized ones, for instance, walking, and bikes. It relies only on accelerometer data in order to minimize the energy consumption of smartphones. By contrast, the driving activity detection module uses the data collected from the accelerometer, gyroscope, and magnetometer of a smartphone to detect various driving activities, i.e., stopping, going straight, turning left, and turning right. Furthermore, we propose a method to compute the optimized data window size and the optimized overlapping ratio for each vehicle mode and each driving event from the training datasets. The experimental results show that this strategy significantly increases the overall prediction accuracy. Additionally, numerous experiments are carried out to compare the impact of different feature sets (time domain features, frequency domain features, Hjorth features) as well as the impact of various classification algorithms (Random Forest, Naïve Bayes, Decision tree J48, K Nearest Neighbor, Support Vector Machine) contributing to the prediction accuracy. Our system achieves an average accuracy of 98.33% in detecting the vehicle modes and an average accuracy of 98.95% in recognizing the driving events of motorcyclists when using the Random Forest classifier and a feature set containing time domain features, frequency domain features, and Hjorth features. Moreover, on a public dataset of HTC company in New Taipei, Taiwan, our framework obtains the overall accuracy of 97.33% that is considerably higher than that of the state-of the art",125,https://doi.org/10.3390/s18041036,1,
2141,Abnormal Behavior Detection Based on Smartphone Sensors.,"There are a lot of applications were developed to take advance of smartphone sensors for utilizing the personal services such as health-care, walk-counting, routing etc. Users behavior analysis is attracted a lot of researches interested with various approaches. We proposed a novel framework to detect the abnormal driving behavior using smartphone sensors. It named Abnormal Behavior Detection System (ABDS). The system keep track the driver activities during he’s trip based on smartphone sensors. The Practice Swarm Optimization (PSO) algorithm is used to automatically select suitable features extracted from sensors data. The oriented accelerometer is used to detect activity. The abnormal behavior is collected and labeled then detection by Artificial Neural Network (ANN). The implementation shown the promising results in case of seven activities (stop, moving, acceleration, deceleration, turn left, turn right and U-turn) with 86.71% accuracy.",125,https://doi.org/10.1007/978-3-319-77818-1_19,1,
2142,A Novel Mobile Online Vehicle Status Awareness Method Using Smartphone Sensors.,"In this paper, we proposed an efficient method with flexible framework for vehicle status awareness using smartphone sensors, so called Mobile Online Vehicle Status Awareness System (MOVSAS). The system deployed while users to put their smartphones in any position and at any direction. In our proposed framework, principal component analysis (PCA) algorithm is used to selected suitable features from set of combining features on time-base, power-based and frequency-based domain, which extracted from accelerometer sensor data. The classification model using Random Forest (RF), Naïve Bayes (NB), K-Nearest Neighbor (KNN), and Support Vector Machine (SVM) algorithms to deploy for awareness issues of vehicle status. The refining model is proposed using Artificial Neural Network (ANN) algorithm aim to improved accuracy prediction vehicle status results before. Training data sets, which are collected and the dynamic feedback also helping improved accuracy of system. A number of experiments are shown that the high accuracy of MOVSAS with vehicle kinds as bicycle, motorbike and car.",125,https://doi.org/10.1007/978-981-10-4154-9_4,1,
2143,A novel traffic routing method using hybrid Ant Colony System based on genetic algorithm.,"The Ant Colony System (ACS) is a variant of Ant colony optimization algorithm. It is very famous with Traveling Salesman Problem. The parameters of ACS in tour construction, global and local updating pheromone direct effort to get the best result of this but they are often manually setting up. Beside, building the heuristic function to decide a choice next node on tour is potential research approach. Therefore, this paper present a foundation framework that propose a novel method to solve traffic routing problem by hybrid ant colony system (ACS) based on genetic algorithm, visualization online changing traffic light, it is called (GACSS). In the GACSS, We use genetic algorithm (GA) optimize ACS parameters aim to attain shortest trips and time with new functions helping them to update global and local pheromone. Our experiment with GACSS deploys in VANETsim with real map from open street map project, any vehicle kind and updating traffic light real-time able. Our framework gained higher results than A-Star and classical ACS algorithm. It is not only shorter length but also smaller time for trip.",125,https://doi.org/10.1109/ICOIN.2017.7899563,1,
2144,Hyper-volume evolutionary algorithm,"We propose a multi-objective evolutionary algorithm (MOEA), named the Hyper-volume Evolutionary Algorithm (HVEA). The algorithm is characterised by three components. First, individual fitness evaluation depends on the current Pareto front, specifically on the ratio of its dominated hyper-volume to the current Pareto front hyper-volume, hence giving an indication of how close the individual is to the current Pareto front. Second, a ranking strategy classifies individuals based on their fitness instead of Pareto dominance, individuals within the same rank are non-guaranteed to be mutually non-dominated. Third, a crowding assignment mechanism that adapts according to the individual’s neighbouring area, controlled by the neighbouring area radius parameter, and the archive of non-dominated solutions. We perform extensive experiments on the multiple 0/1 knapsack problem using different greedy repair methods to …",132,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GaqY-X4AAAAJ&citation_for_view=GaqY-X4AAAAJ:zYLM7Y9cAGgC,,1
2145,Radical Scavenging Activity of Natural-Based Cassaine Diterpenoid Amides and Amines.,"The radical scavenging capacities of four new cassaine diterpenoid amides including 3β-hydroxydinorerythrosuamide (1), 3β-acetoxydinorerythrosuamide (2), 3β-tigloyloxydinorerythrosuamide (3), and 6α-hydroxydinorcassamide (4) present in leaf extract and four new cassaine diterpenoid amines namely erythroformine A (5), erythroformine B (6), 6α-hydroxy-nor-cassamine (7), and nor-erythrosuamine (8) recently identified in the extract of the bark of Erythrophleum fordii were elucidated using density functional theory (DFT) method. Different thermochemical properties characterizing antioxidant potential including bond dissociation enthalpy (BDE), proton affinity (PA), and adiabatic ionization potential (IP) were calculated at the B3LYP/6-311G(d,p) level of theory. Scavenging reaction mechanisms of cassaine diterpenes toward HOO• radical including formal hydrogen transfer (FHT; either hydrogen atom transfer (HAT) or proton coupled electron transfer (PCET)), radical adduct formation (RAF), single electron transfer (SET), and proton transfer (PT) were studied in the gas phase, water, and benzene. The potential energy profiles and kinetic calculations for the FHT and RAF reactions were calculated at 298.15 K. The results showed that all the studied compounds present strong antioxidant activity via HAT mechanism with BDEs varying from 69.4 to 77.9 kcal/mol. While solvents have only a slight effect on HAT and RAF mechanisms, SET and PT reactions are likely to occur in polar media. Among the studied compounds, 3 is the most reactive one both for HAT (H-abstraction at C7, ΔH −11.3 kcal/mol) and for RAF (radical addition at C13 = C15 bond, ΔH −5.2 kcal/mol) reactions. The rate constants of these reactions are also comparable with several referenced antioxidants such as Trolox and ascorbic acid.",125,https://doi.org/10.1021/acs.jcim.8b00847,1,
2146,Reconciliation and local gene tree rearrangement can be of mutual profit.,"Reconciliation methods compare gene trees and species trees to recover evolutionary events such as duplications, transfers and losses explaining the history and composition of genomes. It is well-known that gene trees inferred from molecular sequences can be partly erroneous due to incorrect sequence alignments as well as phylogenetic reconstruction artifacts such as long branch attraction. In practice, this leads reconciliation methods to overestimate the number of evolutionary events. Several methods have been proposed to circumvent this problem, by collapsing the unsupported edges and then resolving the obtained multifurcating nodes, or by directly rearranging the binary gene trees. Yet these methods have been defined for models of evolution accounting only for duplications and losses, i.e. can not be applied to handle prokaryotic gene families.",125,https://doi.org/10.1186/1748-7188-8-12,1,
2147,Accounting for Gene Tree Uncertainties Improves Gene Trees and Reconciliation Inference.,"We propose a reconciliation heuristic accounting for gene duplications, losses and horizontal transfers that specifically takes into account the uncertainties in the gene tree. Rearrangements are tried for gene tree edges that are weakly supported, and are accepted whenever they improve the reconciliation cost. We prove useful properties on the dynamic programming matrix used to compute reconciliations, which allows to speed-up the tree space exploration when rearrangements are generated by Nearest Neighbor Interchanges (NNI) edit operations. Experimental results on simulated and real data confirm that running times are greatly reduced when considering the above-mentioned optimization in comparison to the naïve rearrangement procedure. Results also show that gene trees modified by such NNI rearrangements are closer to the correct (simulated) trees and lead to more correct event predictions on average. ",125,https://doi.org/10.1007/978-3-642-33122-0_10,1,
2148,An evolutionary algorithm for multi-objective nurse scheduling,"The problem that is introduced in this chapter is the nurse scheduling problem in the Ophthalmological ward at the Queens Medical Centre University Hospital NHS Trust (QMC) in Nottingham, UK proposed by Beddoe [8]. This problem benchmark will be used in the rest of the thesis to investigate the performance of SEAMO-R to nurse scheduling.",132,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GaqY-X4AAAAJ&citation_for_view=GaqY-X4AAAAJ:2osOgNQ5qMEC,,1
2149,Resolving Inconsistencies in Probabilistic Knowledge Bases by Quantitative Modification.,"Resolving the inconsistency that appears in knowledge bases is an extremely important stage in the process of merging knowledge bases. With probabilistic context, many studies have solved the inconsistency of a probabilistic knowledge base (PKB) with different approaches. However, these previous focus on addressing the problem of restoring consistency in a PKB by employing the principles of maximum entropy or building the consistency restorer with complex creeping functions. We discover that it is really difficult to find the functions that are utilized in the creeping consistency restorers. Therefore, the process of finding these functions may lead to a poor performance. Based on that, we proposed the inconsistency solvers for a PKB, where we consider revising the functions involved in handling the inconsistency in a simpler direction. Two the inconsistency solvers are the equitable deformation inconsistency solver and the amerced deformation inconsistency solver. Corresponding to each solver, an algorithm to solve inconsistencies in a PKB is proposed. Moreover, the cost of each algorithm also are considered and proved.",127,https://doi.org/10.1109/KSE53942.2021.9648673,1,
2150,The Front-End Readout as an Encoder IC for Magneto-Resistive Linear Scale Sensors.,"This study proposes a front-end readout circuit as an encoder chip for magneto-resistance (MR) linear scales. A typical MR sensor consists of two major parts: one is its base structure, also called the magnetic scale, which is embedded with multiple grid MR electrodes, while another is an “MR reader” stage with magnets inside and moving on the rails of the base. As the stage is in motion, the magnetic interaction between the moving stage and the base causes the variation of the magneto-resistances of the grid electrodes. In this study, a front-end readout IC chip is successfully designed and realized to acquire temporally-varying resistances in electrical signals as the stage is in motions. The acquired signals are in fact sinusoids and co-sinusoids, which are further deciphered by the front-end readout circuit via newly-designed programmable gain amplifiers (PGAs) and analog-to-digital converters (ADCs). The PGA is particularly designed to amplify the signals up to full dynamic ranges and up to 1 MHz. A 12-bit successive approximation register (SAR) ADC for analog-to-digital conversion is designed with linearity performance of ±1 in the least significant bit (LSB) over the input range of 0.5–2.5 V from peak to peak. The chip was fabricated by the Taiwan Semiconductor Manufacturing Company (TSMC) 0.35-micron complementary metal oxide semiconductor (CMOS) technology for verification with a chip size of 6.61 mm2, while the power consumption is 56 mW from a 5-V power supply. The measured integral non-linearity (INL) is −0.79–0.95 LSB while the differential non-linearity (DNL) is −0.68–0.72 LSB. The effective number of bits (ENOB) of the designed ADC is validated as 10.86 for converting the input analog signal to digital counterparts. Experimental validation was conducted. A digital decoder is orchestrated to decipher the harmonic outputs from the ADC via interpolation to the position of the moving stage. It was found that the displacement measurement error is within ±15 µm for a measuring range of 10 mm.",127,https://doi.org/10.3390/s16091416,1,
2151,On the Use of Surplus Division to Facilitate Efficient Negotiation in the Presence of Incomplete Information.,"Classic results in game theory state that private information is a cause for a negotiation to end with suboptimal outcome. Subse- quently, private information is a compelling explanation for the frequent occurrence of negotiation breakdowns or costly delays. In this paper, we propose a mechanism for improving efficiency of negotiation outcome for multilateral negotiations with incomplete information (i.e., negotiators holding private information). This objective is achieved by introducing biased distribution of the resulting surplus created by the negotiators’ joint offers to prevent negotiators from misrepresenting their valuations of the negotiation outcomes. Our mechanism is based on rewarding concession-making agents with larger shares of the obtainable surplus.We show that the probabilities that the agents with private information make concession are accordingly increased. This allows for better efficiency to be achieved.",127,https://doi.org/10.1016/j.procs.2016.08.142,1,
2152,A Consensus-Based Integration Method for Security Rules.,"Policy-based security is an effective approach to manage knowledge systems by handling all behaviors of a system thought a set of rules. This approach has such advantages as capacity to define general high-level targets, ease for configuration, and flexibility in development and maintenance. However, the resolution of conflicts is unavoidable requirement because of many elements of subjectivity as well as objectivity in administrative processes. To this end, several works have been done, and they gave concrete results. In this paper, we will propose a new approach to solve conflicts and to integrate rules in a policy. A new representation of rules is given, the distances between rules are defined as well as postulates are presented and analyzed. Algorithms for integrating policy also have been proposed and examined.",127,https://doi.org/10.1007/978-3-642-04595-0_7,1,
2153,An automated test data generation method for void pointers and function pointers in C/C plus plus libraries and embedded projects,"Automated test data generation for unit testing C/C++ functions using concolic methods is well-known for improving software quality while reducing human testing effort. However, there have been only a few researches related to generating test data for void pointers and function pointers which are commonly used in C/C++ libraries and embedded projects. This paper proposes a concolic-based method named VFP (Void and Function Pointers test data generation) to generate test data for void pointers and function pointers. The key idea of VFP method is to preprocess the source code of the project under test to find all possible types of void pointers and references of function pointers. These types and references are used in the initial test data generating phase of the concolic testing method. VFP method is implemented in VFP verification tool to test on various C/C++ libraries and embedded projects. The …",132,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GaqY-X4AAAAJ&citation_for_view=GaqY-X4AAAAJ:W7OEmFMy1HYC,,1
2154,An automated test data generation method for void pointers and function pointers in C/C++ libraries and embedded projects,"Automated test data generation for unit testing C/C++ functions using concolic methods is well-known for improving software quality while reducing human testing effort. However, there have been only a few researches related to generating test data for void pointers and function pointers which are commonly used in C/C++ libraries and embedded projects. This paper proposes a concolic-based method named VFP (Void and Function Pointers test data generation) to generate test data for void pointers and function pointers. The key idea of VFP method is to preprocess the source code of the project under test to find all possible types of void pointers and references of function pointers. These types and references are used in the initial test data generating phase of the concolic testing method. VFP method is implemented in VFP verification tool to test on various C/C++ libraries and embedded projects. The …",132,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GaqY-X4AAAAJ&citation_for_view=GaqY-X4AAAAJ:Y0pCki6q_DkC,,1
2155,Improving Diversity and Quality of Adversarial Examples in Adversarial Transformation Network,"This paper proposes a method to mitigate two major issues of Adversarial Transformation Networks (ATN) including the low diversity and the low quality of adversarial examples. In order to deal with the first issue, this research proposes a stacked convolutional autoencoder based on pattern to generalize ATN. This proposed autoencoder could support different patterns such as all-feature pattern, border feature pattern, and class model map pattern. In order to deal with the second issue, this paper presents an algorithm to improve the quality of adversarial examples in terms of L0-norm and L2-norm. This algorithm employs an adversarial feature ranking heuristics such as JSMA and COI to prioritize adversarial features. To demonstrate the advantages of the proposed method, comprehensive experiments have been conducted on the MNIST dataset and the CIFAR-10 dataset. For the first issue, the proposed autoencoder can generate diverse adversarial examples with the average success rate above 99%. For the second issue, the proposed algorithm could not only improve the quality of adversarial examples significantly but also maintain the average success rate. In terms of L0-norm, the proposed algorithm could decrease from hundreds of adversarial features to one adversarial feature. In terms of L2-norm, the proposed algorithm could reduce the average distance considerably. These results show that the proposed method is capable of generating high-quality and diverse adversarial examples in practice.",132,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=GaqY-X4AAAAJ&citation_for_view=GaqY-X4AAAAJ:Tyk-4Ss8FVUC,,1
2156,Stochastic alternating direction method of multipliers,"The Alternating Direction Method of Multipliers (ADMM) has received lots of attention recently due to the tremendous demand from large-scale and data-distributed machine learning applications. In this paper, we present a stochastic setting for optimization problems with non-smooth composite objective functions. To solve this problem, we propose a stochastic ADMM algorithm. Our algorithm applies to a more general class of convex and nonsmooth objective functions, beyond the smooth and separable least squares loss used in lasso. We also demonstrate the rates of convergence for our algorithm under various structural assumptions of the stochastic function: O (1/\sqrtt) for convex functions and O (\log t/t) for strongly convex functions. Compared to previous literature, we establish the convergence rate of ADMM for convex problems in terms of both the objective value and the feasibility violation. A novel application named Graph-Guided SVM is proposed to demonstrate the usefulness of our algorithm.",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:MXK_kJrjxJIC,,1
2157,Benchmarking a reduced multivariate polynomial pattern classifier,"A novel method using a reduced multivariate polynomial model has been developed for biometric decision fusion where simplicity and ease of use could be a concern. However, much to our surprise, the reduced model was found to have good classification accuracy for several commonly used data sets from the Web. In this paper, we extend the single output model to a multiple outputs model to handle multiple class problems. The method is particularly suitable for problems with small number of features and large number of examples. The basic component of this polynomial model boils down to construction of new pattern features which are sums of the original features and combination of these new and original features using power and product terms. A linear regularized least-squares predictor is then built using these constructed features. The number of constructed feature terms varies linearly with the order of …",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:5nxA0vEk-isC,,1
2158,Feasibility of detecting prostate cancer by ultraperformance liquid chromatography–mass spectrometry serum metabolomics,"Prostate cancer (PCa) is the second leading cause of cancer-related mortality in men. The prevalent diagnosis method is based on the serum prostate-specific antigen (PSA) screening test, which suffers from low specificity, overdiagnosis, and overtreatment. In this work, untargeted metabolomic profiling of age-matched serum samples from prostate cancer patients and healthy individuals was performed using ultraperformance liquid chromatography coupled to high-resolution tandem mass spectrometry (UPLC-MS/MS) and machine learning methods. A metabolite-based in vitro diagnostic multivariate index assay (IVDMIA) was developed to predict the presence of PCa in serum samples with high classification sensitivity, specificity, and accuracy. A panel of 40 metabolic spectral features was found to be differential with 92.1% sensitivity, 94.3% specificity, and 93.0% accuracy. The performance of the IVDMIA was …",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:kNdYIx-mwKoC,,1
2159,Highly-accurate metabolomic detection of early-stage ovarian cancer,High performance mass spectrometry was employed to interrogate the serum metabolome of early-stage ovarian cancer (OC) patients and age-matched control women. The resulting spectral features were used to establish a linear support vector machine (SVM) model of sixteen diagnostic metabolites that are able to identify early-stage OC with 100% accuracy in our patient cohort. The results provide evidence for the importance of lipid and fatty acid metabolism in OC and serve as the foundation of a clinically significant diagnostic test.,133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:KlAtU1dfN6UC,,1
2160,Netcodec: Community detection from individual activities,The real social network and associated communities are often hidden under the declared friend or group lists in social networks. We usually observe the manifestation of these hidden networks and communities in the form of recurrent and time-stamped individuals' activities in the social network. Inferring the underlying network and finding coherent communities are therefore two key challenges in social networks analysis.,133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:3fE2CSJIrl8C,,1
2161,An empirical comparison of nine pattern classifiers,"There are many learning algorithms available in the field of pattern classification and people are still discovering new algorithms that they hope will work better. Any new learning algorithm, beside its theoretical foundation, needs to be justified in many aspects including accuracy and efficiency when applied to real life problems. In this paper, we report the empirical comparison of a recent algorithm RM, its new extensions and three classical classifiers in different aspects including classification accuracy, computational time and storage requirement. The comparison is performed in a standardized way and we believe that this would give a good insight into the algorithm RM and its extension. The experiments also show that nominal attributes do have an impact on the performance of those compared learning algorithms.",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:8k81kl-MbHgC,,1
2162,Deep learning for epileptic spike detection,"In the clinical diagnosis of epilepsy using electroencephalogram (EEG) data, an accurate automatic epileptic spikes  detection system is highly useful and  meaningful in that the conventional manual process is not only very tedious and time-consuming, but also subjective since it depends on the knowledge and experience of the doctors. In this paper, motivated by significant advantages and lots of achieved successes of deep learning in  data mining, we apply Deep Belief Network (DBN), which is one of the breakthrough models laid the foundation for deep learning, to detect epileptic spikes in EEG data. It is really useful in practice because the promising quality evaluation of the spike detection system is higher than \%.  In particular, to construct  accurate detection model for non-spikes and spikes, a new set of detailed features of epileptic spikes is proposed. These features were then fed to the DBN which is modified from a generative model into a discriminative model to aim at classification accuracy. The experiment results indicate that it is possible to use deep learning models for epileptic spike detection with very high performance in item of sensitivity, selectivity, specificity and accuracy  92.82%,  97.83% , 96.41%, and 96.87%, respectively.",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:mVmsd5A6BfQC,,1
2163,Sharing experience in multitask reinforcement learning,"In multitask reinforcement learning, tasks often have sub-tasks that share the same solution, even though the overall tasks are different. If the shared-portions could be effectively identified, then the learning process could be improved since all the samples between tasks in the shared space could be used. In this paper, we propose a Sharing Experience Framework (SEF) for simultaneously training of multiple tasks. In SEF, a confidence sharing agent uses task-specific rewards from the environment to identify similar parts that should be shared across tasks and defines those parts as shared-regions between tasks. The shared-regions are expected to guide task-policies sharing their experience during the learning process. The experiments highlight that our framework improves the performance and the stability of learning task-policies, and is possible to help task-policies avoid local optimums.",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:HDshCWvjkbEC,,1
2164,Adaptation to changes in multimodal biometric authentication,"Multimodal biometric authentication is gaining more and more attention recently. In (Toh, 2003), a reduced multivariate polynomial model has been used to combine the biometric classifiers with good performance. Although the model parameter can be solved very quickly, it may not be suitable in case when new registration is a frequent task. Moreover, due to wear and tear of sensors, the distribution of match scores can change with time and hence the performance of a fixed classifier may be affected. In this paper, we introduce an adaptive algorithm to track these changes. The new algorithm is demonstrated to address both the registration and match scores distribution changing problems using two biometrics",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:UebtZRa9Y70C,,1
2165,A continuous-time mutually-exciting point process framework for prioritizing events in social media,"The overwhelming amount and rate of information update in online social media is making it increasingly difficult for users to allocate their attention to their topics of interest, thus there is a strong need for prioritizing news feeds. The attractiveness of a post to a user depends on many complex contextual and temporal features of the post. For instance, the contents of the post, the responsiveness of a third user, and the age of the post may all have impact. So far, these static and dynamic features has not been incorporated in a unified framework to tackle the post prioritization problem. In this paper, we propose a novel approach for prioritizing posts based on a feature modulated multi-dimensional point process. Our model is able to simultaneously capture textual and sentiment features, and temporal features such as self-excitation, mutual-excitation and bursty nature of social interaction. As an evaluation, we also curated a real-world conversational benchmark dataset crawled from Facebook. In our experiments, we demonstrate that our algorithm is able to achieve the-state-of-the-art performance in terms of analyzing, predicting, and prioritizing events. In terms of interpretability of our method, we observe that features indicating individual user profile and linguistic characteristics of the events work best for prediction and prioritization of new events.",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:ULOm3_A8WrAC,,1
2166,Polyp segmentation in colonoscopy images using ensembles of U-Nets with EfficientNet and asymmetric similarity loss function,"Automatic polyp detection and segmentation are highly desirable for colon screening due to polyp miss rate by physicians during colonoscopy, which is about 25%. Diagnosis of polyps in colonoscopy videos is a challenging task due to variations in the size and shape of polyps. In this paper, we adapt U-net and evaluate its performance with different modern convolutional neural networks as its encoder for polyp segmentation. One of the major challenges in training networks for polyp segmentation raises when the data are unbalanced, polyp pixels are often much lower in numbers than non-polyp pixels. A trained network with unbalanced data may make predictions with high precision and low recall, being severely biased toward the non-polyp class which is particularly undesired because false negatives are more important than false positives. We propose an asymmetric similarity loss function to address this …",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:e5wmG9Sq2KIC,,1
2167,Accurate prediction of optimal cancer drug therapies from molecular profiles by a machine-learning algorithm.,": A major goal of cancer genomic medicine is the accurate prediction of optimal personalized therapies from the molecular profiles of individual patient tumors. With notable exceptions, this goal has yet to be realized across a broad spectrum of cancers due to the complexity of the disease and our limited understanding of the causal mechanisms involved. Methods: We have developed a robust machine learning method (SVM) to accurately link individual genomic profiles to drug responsiveness. Results: The algorithm predicts the responsiveness of a diversity of human cancer cell lines to individual and combination treatments of eight cancer drugs (carboplatin, cisplatin, docetaxel, paclitaxel, doxorubicin, erlotinib, gemcitabine and topotecan) with 80-100% accuracy. Gene expression data were found to be generally more informative than DNA sequence data in the generation of optimally …",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:4DMP91E08xMC,,1
2168,Some issues pertaining to adaptive multimodal biometric authentication,"In this paper, we address some issues pertaining to adaptive multimodal biometric authentication. These issues include new user registration, sensor decay and small sample data size. A recursive formulation is introduced to track changes due to new user registration and possible sensor decay. The small sample size problem is handled using a feature scaling-space learning technique. Empirical experiments are conducted to observe the effects.",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:0EnyYjriUFMC,,1
2169,Aerial image semantic segmentation using neural search network architecture,"In remote sensing data analysis and computer vision, aerial image segmentation is a crucial research topic, which has many applications in environmental and urban planning. Recently, deep learning is using to tackle many computer vision problem, including aerial image segmentation. Results have shown that deep learning gains much higher accuracy than other methods on many benchmark data sets. In this work, we propose a neural network called NASNet-FCN, which based on Fully Convolutional Network - a frame work for solving semantic segmentation problem and image feature extractor derived from state-of-the-art object recognition network called Neural Search Network Architecture. Our networks are trained and judged by using benchmark dataset from ISPRS Vaihingen challenge. Results show that our methods achieved state-of-the-art accuracy with potential improvements.",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:9ZlFYXVOiuMC,,1
2170,Real-time Image Semantic Segmentation Networks with Residual Depth-wise Separable Blocks,"Semantic image segmentation plays a key role in obtaining pixel-level understanding of images. In recent years, researchers have tackled this problem by using deep learning methods instead of traditional computer vision methods. Because of the development of technologies like autonomous vehicles and indoor robots, segmentation techniques, that have not only high accuracy but also the capability of running in real-time on embedded platform and mobile devices, are in high demand. In this work, we have proposed a new convolutional module, named Residual depth-wise separable, and a fast and efficient convolutional neural network for segmentation. The proposed method is compared against other state of the art real-time models. The experiment results illustrate that our method is efficient in computation while achieves state of the art performance in term of accuracy.",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:qUcmZB5y_30C,,1
2171,ViWOZ: A Multi-Domain Task-Oriented Dialogue Systems Dataset For Low-resource Language,"Most of the current task-oriented dialogue systems (ToD), despite having interesting results, are designed for a handful of languages like Chinese and English. Therefore, their performance in low-resource languages is still a significant problem due to the absence of a standard dataset and evaluation policy. To address this problem, we proposed ViWOZ, a fully-annotated Vietnamese task-oriented dialogue dataset. ViWOZ is the first multi-turn, multi-domain tasked oriented dataset in Vietnamese, a low-resource language. The dataset consists of a total of 5,000 dialogues, including 60,946 fully annotated utterances. Furthermore, we provide a comprehensive benchmark of both modular and end-to-end models in low-resource language scenarios. With those characteristics, the ViWOZ dataset enables future studies on creating a multilingual task-oriented dialogue system.",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:JV2RwH3_ST0C,,1
2172,Robust loss functions: Defense mechanisms for deep architectures,"Current deep learning methods and technologies have reached the level of deployment in softwares and hardwares for real-life applications. However, recent studies have shown deep learning architectures are highly vulnerable to attacks and exploitation via input perturbations. In this works, we investigate the effects of these attacks on the outputs of each layer of deep architectures and on theirs performance in term of classification accuracy measures. The results show that without defense mechanism, even simple attacks devastated deep architectures' outputs in every layer and theirs classification performance. We then propose multiple defense mechanisms in order to protect deep architectures and to make them more robust to input perturbation attacks.",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:dhFuZR0502QC,,1
2173,Hyperbolic function networks for pattern classification,"Pattern classification is an important field of research since it encompasses a wide range of information processing problems of great application significance. The multi-layer Feedforward Neural Network provides a natural choice for pattern classification because it has been shown to be a universal approximator. However, the training process remains much to be a trial-and-error effort since no learning algorithm can guarantee convergence to global optimal solution within finite iterations. In this work, we introduce a network model which is linear in parameters to generate different combinations of hyperbolic basis functions to achieve some approximation and classification properties. This is to circumvent the iterative training problem as seen in layered neural network learning. With appropriate formulation and extension of a decision fusion model to cater for pattern classification, extensive experiments have been …",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:hqOjcs7Dif8C,,1
2174,Automatic scan range for dose-reduced multiphase CT imaging of the liver utilizing CNNs and Gaussian models,"Multiphase CT scanning of the liver is performed for several clinical applications; however, radiation exposure from CT scanning poses a nontrivial cancer risk to the patients. The radiation dose may be reduced by determining the scan range of the subsequent scans by the location of the target of interest in the first scan phase. The purpose of this study is to present and assess an automatic method for determining the scan range for multiphase CT scans. Our strategy is to first apply a CNN-based method for detecting the liver in 2D slices, and to use a liver range search algorithm for detecting the liver range in the scout volume. The target liver scan range for subsequent scans can be obtained by adding safety margins achieved from Gaussian liver motion models to the scan range determined from the scout. Experiments were performed on 657 multiphase CT volumes obtained from multiple hospitals. The …",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:blknAaTinKkC,,1
2175,Deep Learning-Based Water Crystal Classification,"Much of the earth’s surface is covered by water. As was pointed out in the 2020 edition of the World Water Development Report, climate change challenges the sustainability of global water resources, so it is important to monitor the quality of water to preserve sustainable water resources. Quality of water can be related to the structure of water crystal, the solid-state of water, so methods to understand water crystals can help to improve water quality. As a first step, a water crystal exploratory analysis has been initiated with the cooperation with the Emoto Peace Project (EPP). The 5K EPP dataset has been created as the first world-wide small dataset of water crystals. Our research focused on reducing the inherent limitations when fitting machine learning models to the 5K EPP dataset. One major result is the classification of water crystals and how to split our small dataset into several related groups. Using the 5K EPP dataset of human observations and past research on snow crystal classification, we created a simple set of visual labels to identify water crystal shapes, in 13 categories. A deep learning-based method has been used to automatically do the classification task with a subset of the label dataset. The classification achieved high accuracy when using a fine-tuning technique.",133,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xnnOvh4AAAAJ&citation_for_view=xnnOvh4AAAAJ:maZDTaKrznsC,,1
2176,An efficient scheme for reducing overhead in data-centric storage sensor networks,"In Data-Centric Storage sensor networks where numerous sensors collaborate to monitor and report occurrences of environmental events, GPSR routing algorithm can be used to route event and query packets efficiently. However, the routing algorithm usually routes packets completely around perimeters enclosing destination locations. In this paper, we show that such perimeter walks cause significant overhead in many practical settings of sensor networks. We then propose an efficient pruning scheme to reduce overhead of perimeter walks. Our algorithm incurs no additional communications overhead; only requires the knowledge of node transmission range; but can significantly reduce overhead of insertions and queries.",137,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=piaD2uUAAAAJ&citation_for_view=piaD2uUAAAAJ:roLk4NBRz8UC,,1
2177,Group-aware service discovery using effect ontology for conflict resolution in ubiquitous environment,"In this paper, we propose a group-aware service discovery architecture resolving conflict problem in ubiquitous computing. In the past, researches on resolving conflict have used service's QoS, user's preference, and user's intention. However, previous researches have the problem that it applies resolution scheme to non-conflicting situation because it does not consider space concept. Therefore, we propose a group-aware service discovery architecture resolving conflict problem considering interaction space that is affecting scope of task to increase user's satisfaction. In this architecture, service ontology model including interaction space as well as QoS, preference, and intention expresses service information and task information. With service ontology model, the architecture detects and resolves conflict situation. And discovery scheme that is proposed resolution method finds non-conflict service with interaction …",137,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=piaD2uUAAAAJ&citation_for_view=piaD2uUAAAAJ:Tyk-4Ss8FVUC,,1
2178,Efficient route discovery for reactive routing protocols with lazy topology exchange and condition bearing route discovery,"We propose LTE and CBRD to reduce route discovery overhead of reactive routing protocols for Mobile Ad hoc Networks (MANETs). LTE proactively distributes topology information of the network by using a lazy update policy. That topology information is used by CBRD to optimize route discoveries. CBRD is a reactive route discovery mechanism which employs topology information provided by LTE to restrict route discovery floods to limited regions containing desired destinations. Our simulation results have shown that LTE and CBRD efficiently reduce route discovery overhead as well as route discovery delay. Also, they improve routing performance of flooding dependent reactive routing protocols like AODV in low- and moderate-traffic networks.",137,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=piaD2uUAAAAJ&citation_for_view=piaD2uUAAAAJ:YsMSGLbcyi4C,,1
2179,An Elasticity Framework for Distributed Message Queuing Telemetry Transport Brokers,"Internet of Things (IoT) applications are increasingly making impact in all areas of humanlife. Day by day, its chatty embedded devices have been generating tons of data requiring effectivenetwork infrastructure. To deliver millions of IoT messages back and fort with as few faults aspossible, participation of communication protocols like MQTT is a must. Lightweight blueprintand friendly battery are just two of many advantages of this protocol making it become a dominantin IoT world. In real application scenarios, distributed MQTT solutions are usually required sincecentralized MQTT approach is incapable of dealing with huge amount of data. Although distributedMQTT solutions are scalable, they do not adapt to fluctuations of traffic workload. This might costIoT service provider because of redundant computation resources. This leads to the need of a novelapproach that can adapt its size changes in workload. This article proposes such an elastic solutionby proposing a flexible MQTT framework. Our MQTT framework uses off-the-shelf componentsto obtain server’s elasticity while keeping IoT applications intact. Experiments are conducted tovalidate elasticity function provided by an implementation of our framework.",137,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=piaD2uUAAAAJ&citation_for_view=piaD2uUAAAAJ:KlAtU1dfN6UC,,1
2180,An enhanced semantic-based cache replacement algorithm for web systems,"As Web traffics is increasing on the Internet, caching solutions for Web systems are becoming more important since they can greatly expand system scalability. An important part of a caching solution is cache replacement policy, which is responsible for selecting victim items that should be removed in order to make space for new objects. Typical replacement policies used in practice only take advantage of temporal reference locality by removing the least recently/frequently requested items from the cache. Although those policies work well in memory or filesystem cache, they are inefficient for Web systems since they do not exploit semantic relationship between Web items. This paper presents a semantic-aware caching policy that can be used in Web systems to enhance scalability. The proposed caching mechanism defines semantic distance from a web page to a set of pivot pages and use the semantic distances …",137,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=piaD2uUAAAAJ&citation_for_view=piaD2uUAAAAJ:qjMakFHDy7sC,,1
2182,Dual parent multicast graph for failure resilient peer-to-peer multimedia streaming,"Failure resiliency is one of major concerns in design-ing peer-to-peer systems for continuous multimedia streaming since peers can leave or fail without notices. Building resilient multicast networks based on parent multiplicity is an approach to combat network failures and improve stream continuity. In this paper, we model the problem of generating resilient multicast graphs with parent multiplicity, called Dual Parent Multicast Graph (DPMG), as two sub-problems: constructing a regular graph on a set of labeled vertices; and labeling edges of that graph. Our proposed algorithms to solve the two sub-problems produce a DPMG with good resiliency property especially for locally and burstly occurred failures in peer-to-peer networks.",137,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=piaD2uUAAAAJ&citation_for_view=piaD2uUAAAAJ:zYLM7Y9cAGgC,,1
2184,An Implementation of Firewall as a Service for OpenStack Virtualization Systems,"In this paper, we propose and implement a firewalling service for cloud system using OpenStack. The service, called FWaaS - Firewall as a Service, is offloaded from and loosely coupled with Openstack cloud system. It can be utilised to provision firewall functions and it supports a rich set of packet filtering capabilities, from link layer up to application layer. The service is lightweight but shows that it could prevent efficiently threats from outside of the networks with low level of resource consumption.",137,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=piaD2uUAAAAJ&citation_for_view=piaD2uUAAAAJ:YOwf2qJgpHMC,,1
2185,Correlation-Sensitive Next-Basket Recommendation,"Items adopted by a user over time are indicative of the underlying preferences. We are concerned with learning such preferences from observed sequences of adoptions for recommendation. As multiple items are commonly adopted concurrently, e.g., a basket of grocery items or a sitting of media consumption, we deal with a sequence of baskets as input, and seek to recommend the next basket. Intuitively, a basket tends to contain groups of related items that support particular needs. Instead of recommending items independently for the next basket, we hypothesize that incorporating information on pairwise correlations among items would help to arrive at more coherent basket recommendations. Towards this objective, we develop a hierarchical network architecture codenamed Beacon to model basket sequences. Each basket is encoded taking into account the relative importance of items and correlations among item pairs. This encoding is utilized to infer sequential associations along the basket sequence. Extensive experiments on three public real-life datasets showcase the effectiveness of our approach for the next-basket recommendation problem.",140,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=irb6x1cAAAAJ&citation_for_view=irb6x1cAAAAJ:eQOLeE2rZwMC,,1
2186,Basket-Sensitive Personalized Item Recommendation,"Personalized item recommendation is useful in narrowing down the list of options provided to a user. In this paper, we address the problem scenario where the user is currently holding a basket of items, and the task is to recommend an item to be added to the basket. Here, we assume that items currently in a basket share some association based on an underlying latent need, eg, ingredients to prepare some dish, spare parts of some device. Thus, it is important that a recommended item is relevant not only to the user, but also to the existing items in the basket. Towards this goal, we propose two approaches. First, we explore a factorization-based model called BFM that incorporates various types of associations involving the user, the target item to be recommended, and the items currently in the basket. Second, based on our observation that various recommendations towards constructing the same basket should have similar likelihoods, we propose another model called CBFM that further incorporates basket-level constraints. Experiments on three real-life datasets from different domains empirically validate these models against baselines based on matrix factorization and association rules.",140,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=irb6x1cAAAAJ&citation_for_view=irb6x1cAAAAJ:2osOgNQ5qMEC,,1
2187,Modeling Sequential Preferences with Dynamic User and Context Factors,"Users express their preferences for items in diverse forms, through their liking for items, as well as through the sequence in which they consume items. The latter, referred to as “sequential preference”, manifests itself in scenarios such as song or video playlists, topics one reads or writes about in social media, etc. The current approach to modeling sequential preferences relies primarily on the sequence information, i.e., which item follows another item. However, there are other important factors, due to either the user or the context, which may dynamically affect the way a sequence unfolds. In this work, we develop generative modeling of sequences, incorporating dynamic user-biased emission and context-biased transition for sequential preference. Experiments on publicly-available real-life datasets as well as synthetic data show significant improvements in accuracy at predicting the next item in a sequence.",140,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=irb6x1cAAAAJ&citation_for_view=irb6x1cAAAAJ:d1gkVwhDpl0C,,1
2188,Modeling Contemporaneous Basket Sequences with Twin Networks for Next-Item Recommendation,"Our interactions with an application frequently leave a heterogeneous and contemporaneous trail of actions and adoptions (eg, clicks, bookmarks, purchases). Given a sequence of a particular type (eg, purchases)--referred to as the target sequence, we seek to predict the next item expected to appear beyond this sequence. This task is known as next-item recommendation. We hypothesize two means for improvement. First, within each time step, a user may interact with multiple items (a basket), with potential latent associations among them. Second, predicting the next item in the target sequence may be helped by also learning from another supporting sequence (eg, clicks). We develop three twin network structures modeling the generation of both target and support basket sequences. One based on"" Siamese networks"" facilitates full sharing of parameters between the two sequence types. The other two based on"" fraternal networks"" facilitate partial sharing of parameters. Experiments on real-world datasets show significant improvements upon baselines relying on one sequence type.",140,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=irb6x1cAAAAJ&citation_for_view=irb6x1cAAAAJ:zYLM7Y9cAGgC,,1
2189,Modular graph transformer networks for multi-label image classification,"With the recent advances in graph neural networks, there is a rising number of studies on graph-based multi-label classification with the consideration of object dependencies within visual data. Nevertheless, graph representations can become indistinguishable due to the complex nature of label relationships. We propose a multi-label image classification framework based on graph transformer networks to fully exploit inter-label interactions. The paper presents a modular learning scheme to enhance the classification performance by segregating the computational graph into multiple sub-graphs based on the modularity. The proposed approach, named as Modular Graph Transformer Networks (MGTN), is capable of employing multiple backbones for better information propagation over different sub-graphs guided by graph transformers and convolutions. We validate our framework on MSCOCO and Fashion550K datasets to demonstrate massive improvements for multi-label image classification.",140,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=irb6x1cAAAAJ&citation_for_view=irb6x1cAAAAJ:5nxA0vEk-isC,,1
2190,A model of vietnamese person named entity question answering system,"In this paper, we proposed a Vietnamese named entity question answering (QA) model. This model applies an analytical question method using CRF machine learning algorithm combined with two automatic answering strategies: indexed sentences database-based and Google search engine-based. We gathered a Vietnamese question dataset containing about 2000 popular “Who, Whom, Whose” questions to evaluate our question chunking method and QA model. According to experiments, question chunking phase acquired the average F1 score of 92.99%. Equally significant, in our QA evaluation, experimental results illustrated that our approaches were completely reasonable and realistic with 74.63% precision and 87.9% ability to give the answers.",140,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=irb6x1cAAAAJ&citation_for_view=irb6x1cAAAAJ:u-x6o8ySG0sC,,1
2191,Privacy-preserving visual content tagging using graph transformer networks,"With the rapid growth of Internet media, content tagging has become an important topic with many multimedia understanding applications, including efficient organisation and search. Nevertheless, existing visual tagging approaches are susceptible to inherent privacy risks in which private information may be exposed unintentionally. The use of anonymisation and privacy-protection methods is desirable, but with the expense of task performance. Therefore, this paper proposes an end-to-end framework (SGTN) using Graph Transformer and Convolutional Networks to significantly improve classification and privacy preservation of visual data. Especially, we employ several mechanisms such as differential privacy based graph construction and noise-induced graph transformation to protect the privacy of knowledge graphs. Our approach unveils new state-of-the-art on MS-COCO dataset in various semi-supervised …",140,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=irb6x1cAAAAJ&citation_for_view=irb6x1cAAAAJ:UebtZRa9Y70C,,1
2192,Reintel: A multimodal data challenge for responsible information identification on social network sites,"This paper reports on the ReINTEL Shared Task for Responsible Information Identification on social network sites, which is hosted at the seventh annual workshop on Vietnamese Language and Speech Processing (VLSP 2020). Given a piece of news with respective textual, visual content and metadata, participants are required to classify whether the news is `reliable' or `unreliable'. In order to generate a fair benchmark, we introduce a novel human-annotated dataset of over 10,000 news collected from a social network in Vietnam. All models will be evaluated in terms of AUC-ROC score, a typical evaluation metric for classification. The competition was run on the Codalab platform. Within two months, the challenge has attracted over 60 participants and recorded nearly 1,000 submission entries.",140,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=irb6x1cAAAAJ&citation_for_view=irb6x1cAAAAJ:0EnyYjriUFMC,,1
2193,Reinforced data sampling for model diversification,"With the rising number of machine learning competitions, the world has witnessed an exciting race for the best algorithms. However, the involved data selection process may fundamentally suffer from evidence ambiguity and concept drift issues, thereby possibly leading to deleterious effects on the performance of various models. This paper proposes a new Reinforced Data Sampling (RDS) method to learn how to sample data adequately on the search for useful models and insights. We formulate the optimisation problem of model diversification  in data sampling to maximise learning potentials and optimum allocation by injecting model diversity. This work advocates the employment of diverse base learners as value functions such as neural networks, decision trees, or logistic regressions to reinforce the selection process of data subsets with multi-modal belief. We introduce different ensemble reward mechanisms, including soft voting and stochastic choice to approximate optimal sampling policy. The evaluation conducted on four datasets evidently highlights the benefits of using RDS method over traditional sampling approaches. Our experimental results suggest that the trainable sampling for model diversification is useful for competition organisers, researchers, or even starters to pursue full potentials of various machine learning tasks such as classification and regression. The source code is available at https://github.com/probeu/RDS.",140,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=irb6x1cAAAAJ&citation_for_view=irb6x1cAAAAJ:Se3iqnhoufwC,,1
2194,Co-reference Resolution in Vietnamese Documents Based on Support Vector Machines,"Co-reference resolution task still poses many challenges due to the complexity of the Vietnamese language, and the lack of standard Vietnamese linguistic resources. Based on the mention-pair model of Rahman and Ng. (2009) and the characteristics of Vietnamese, this paper proposes a model using support vector machines (SVM) to solve the co-reference in Vietnamese documents. The corpus used in experiments to evaluate the proposed model was constructed from 200 articles in cultural and social categories from vnexpress.net newspaper website. The results of the initial experiments of the proposed model achieved 76.51% accuracy in comparison with that of the baseline model of 73.79% with similar features.",140,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=irb6x1cAAAAJ&citation_for_view=irb6x1cAAAAJ:u5HHmVD_uO8C,,1
2195,Multimodal Review Generation with Privacy and Fairness Awareness,"Users express their opinions towards entities (eg, restaurants) via online reviews which can be in diverse forms such as text, ratings, and images. Modeling reviews are advantageous for user behavior understanding which, in turn, supports various user-oriented tasks such as recommendation, sentiment analysis, and review generation. In this paper, we propose MG-PriFair, a multimodal neural-based framework, which generates personalized reviews with privacy and fairness awareness. Motivated by the fact that reviews might contain personal information and sentiment bias, we propose a novel differentially private (dp)-embedding model for training privacy guaranteed embeddings and an evaluation approach for sentiment fairness in the food-review domain. Experiments on our novel review dataset show that MG-PriFair is capable of generating plausibly long reviews while controlling the amount of exploited user data and using the least sentiment biased word embeddings. To the best of our knowledge, we are the first to bring user privacy and sentiment fairness into the review generation task. The dataset and source codes are available at https://github. com/ReML-AI/MG-PriFair.",140,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=irb6x1cAAAAJ&citation_for_view=irb6x1cAAAAJ:hqOjcs7Dif8C,,1
2196,Modeling Multi-Intent Basket Sequences for Next-Basket Recommendation,"Recommendation systems have a preponderance in assisting customers to save time by suggesting relevant options. With this convenience, a customer may purchase multiple items in a browsing session, referred to as an item basket. The notion of basket manifests his underlying preference of multiple implicit intentions, which becomes more sophisticated once considering the basket sequence of his chronological intersession list. With the objective of modeling basket sequences, most of previous methods hypothesize a homogeneous intention in each basket. The exploitation on multi-intent basket sequences for the recommendation task becomes an emerging demand. In this work, we present a novel framework named MIBS to model multi-intent basket sequences to recommend next basket of relevant items. Given a user's basket sequence, each basket is encoded via aggregating the item-item correlation matrix …",140,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=irb6x1cAAAAJ&citation_for_view=irb6x1cAAAAJ:8k81kl-MbHgC,,1
2198,VLSP shared task: sentiment analysis,"Sentiment analysis is a Natural Language Processing (NLP) task of identifying or extracting the sentiment content of a text unit. This task has become an active research topic since the early 2000s. During the two last editions of the VLSP workshop series, the shared task on Sentiment Analysis (SA) for Vietnamese has been organized in order to provide an objective evaluation measurement about the performance (quality) of sentiment analysis tools, and encourage the development of Vietnamese sentiment analysis systems, as well as to provide benchmark datasets for this task. The first campaign in 2016 only focused on the sentiment polarity classification, with a dataset containing reviews of electronic products. The second campaign in 2018 addressed the problem of Aspect Based Sentiment Analysis (ABSA) for Vietnamese, by providing two datasets containing reviews in restaurant and hotel domains. These data are accessible for research purpose via the VLSP website vlsp. org. vn/resources. This paper describes the built datasets as well as the evaluation results of the systems participating to these campaigns.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:b1wdh0AR-JQC,,1
2199,HSD shared task in VLSP campaign 2019: Hate speech detection for social good,"The paper describes the organisation of the ""HateSpeech Detection"" (HSD) task at the VLSP workshop 2019 on detecting the fine-grained presence of hate speech in Vietnamese textual items (i.e., messages) extracted from Facebook, which is the most popular social network site (SNS) in Vietnam. The task is organised as a multi-class classification task and based on a large-scale dataset containing 25,431 Vietnamese textual items from Facebook. The task participants were challenged to build a classification model that is capable of classifying an item to one of 3 classes, i.e., ""HATE"", ""OFFENSIVE"" and ""CLEAN"". HSD attracted a large number of participants and was a popular task at VLSP 2019. In particular, there were 71 teams signed up for the task, 14 of them submitted results with 380 valid submissions from 20th September 2019 to 4th October 2019.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:k8Z6L05lTy4C,,1
2200,Learning to recognize phenotype candidates in the auto-immune literature using SVM re-ranking,"The identification of phenotype descriptions in the scientific literature, case reports and patient records is a rewarding task for bio-medical text mining. Any progress will support knowledge discovery and linkage to other resources. However because of their wide variation a number of challenges still remain in terms of their identification and semantic normalisation before they can be fully exploited for research purposes. This paper presents novel techniques for identifying potential complex phenotype mentions by exploiting a hybrid model based on machine learning, rules and dictionary matching. A systematic study is made of how to combine sequence labels from these modules as well as the merits of various ontological resources. We evaluated our approach on a subset of Medline abstracts cited by the Online Mendelian Inheritance of Man database related to auto-immune diseases. Using partial matching the best micro-averaged F-score for phenotypes and five other entity classes was 79.9%. A best performance of 75.3% was achieved for phenotype candidates using all semantics resources. We observed the advantage of using SVM-based learn-to-rank for sequence label combination over maximum entropy and a priority list approach. The results indicate that the identification of simple entity types such as chemicals and genes are robustly supported by single semantic resources, whereas phenotypes require combinations. Altogether we conclude that our approach coped well with the compositional structure of phenotypes in the auto-immune domain.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:hqOjcs7Dif8C,,1
2201,Sentiment analysis and user similarity for social recommender system: An experimental study,"Social recommender system has become an emerging research topic due to the prevalence of online social networking services during the past few years. A social recommender model can be considered the combination of a recommender model and a social information model. Many approaches have been proposed to exploit the social interaction or connections among users to overcome the defect of traditional recommender systems assuming that all the users are independent and identically distributed. In this paper, we propose a social recommender system using memory based collaborative filtering models with user-oriented methods as basic models, in which we conduct an analysis on the correlations between social relations and user interest similarities. We also combine techniques of sentiment analysis to get dataset of users with their favorite products; this dataset is the input for the social …",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:9ZlFYXVOiuMC,,1
2202,The UET-CAM System in the BioCreAtIvE V CDR Task,"In this paper, we describe a system developed for the BioCreative V chemical-disease relation (CDR) task. The Disease Named Entity Recognition and Normalization (DNER) model employs joint learning using a perceptronbased named entity recognizer (NER) and a back-off model for named entity normalization (NEN). In order to maximize both precision and recall, our NEN adopts a sequential back-off ensemble approach based on Semantic Supervised Indexing (SSI)-a supervised Word Embedding (WE) method-giving results by inference from training data, and Skip-gram-an unsupervised WE methodtaking advantage of large unlabeled data. In the Chemical-induced diseases relation extraction (CID) model, we firstly resolve co-references by using a multipass sieve to identify cross-sentence references for entities, thus enabling intrasentence relations to be discovered more easily. Following this we extract CID relations using a support vector machine model trained on supervised sentence data from the CDR training and development dataset. We evaluated our method on both the DNER test set and the CID test set. Results show an F1 of 76.44 for the DNER task, and a best performance of 51.6 on the CID task using the multipass sieve.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:mVmsd5A6BfQC,,1
2203,A hybrid approach to ﬁnding phenotype candidates in genetic texts,"Named entity recognition (NER) has been extensively studied for the names of genes and gene products but there are few proposed solutions for phenotypes. Phenotype terms are expected to play a key role in inferring gene function in complex heritable diseases but are intrinsically difficult to analyse due to their complex semantics and scale. In contrast to previous approaches we evaluate state-of-the-art techniques involving the fusion of machine learning on a rich feature set with evidence from extant domain knowledge-sources. The techniques are validated on two gold standard collections including a novel annotated collection of 112 abstracts derived from a systematic search of the Online Mendelian Inheritance of Man database for auto-immune diseases. Encouragingly the hybrid model outperforms a HMM, a CRF and a pure knowledge-based method to achieve an F1 of 77.07. Disagreement analysis points to further improvements on this emerging NE task. The annotated corpus and guidelines are available on request.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:0EnyYjriUFMC,,1
2204,Vlsp shared task: Named entity recognition,"Named Entities (NE) are phrases that contain the names of persons, organizations, locations, times and quantities, monetary values, percentages, etc. Named Entity Recognition (NER) is the task of recognizing named entities in documents. NER is an important subtask of Information Extraction, which has attracted researchers all over the world since 1990s. For Vietnamese language, although there exist some research projects and publications on NER task before 2016, no systematic comparison of the performance of NER systems has been done. In 2016, the organizing committee of the VLSP workshop decided to launch the first NER shared task, in order to get an objective evaluation of Vietnamese NER systems and to promote the development of high quality systems. As a result, the first dataset with morpho-syntactic and NE annotations has been released for benchmarking NER systems. At VLSP 2018, the NER shared task has been organized for the second time, providing a bigger dataset containing texts from various domains, but without morpho-syntactic annotation. These resources are available for research purpose via the VLSP website vlsp. org. vn/resources. In this paper, we describe the datasets as well as the evaluation results obtained from these two campaigns.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:gsN89kCJA0AC,,1
2205,Sieve-based coreference resolution enhances semi-supervised learning model for chemical-induced disease relation extraction,"The BioCreative V chemical-disease relation (CDR) track was proposed to accelerate the progress of text mining in facilitating integrative understanding of chemicals, diseases and their relations. In this article, we describe an extension of our system (namely UET-CAM) that participated in the BioCreative V CDR. The original UET-CAM system’s performance was ranked fourth among 18 participating systems by the BioCreative CDR track committee. In the Disease Named Entity Recognition and Normalization (DNER) phase, our system employed joint inference (decoding) with a perceptron-based named entity recognizer (NER) and a back-off model with Semantic Supervised Indexing and Skip-gram for named entity normalization. In the chemical-induced disease (CID) relation extraction phase, we proposed a pipeline that includes a coreference resolution module and a Support Vector Machine relation …",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:vbGhcppDl1QC,,1
2206,A model of vietnamese person named entity question answering system,"In this paper, we proposed a Vietnamese named entity question answering (QA) model. This model applies an analytical question method using CRF machine learning algorithm combined with two automatic answering strategies: indexed sentences database-based and Google search engine-based. We gathered a Vietnamese question dataset containing about 2000 popular “Who, Whom, Whose” questions to evaluate our question chunking method and QA model. According to experiments, question chunking phase acquired the average F1 score of 92.99%. Equally significant, in our QA evaluation, experimental results illustrated that our approaches were completely reasonable and realistic with 74.63% precision and 87.9% ability to give the answers.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:Y0pCki6q_DkC,,1
2207,User interest analysis with hidden topic in news recommendation system,"To take advantage of the Internet - vast but complicated information resources, Recommendation systems help users find out information they need by providing them personalized suggestions. This research area is receiving more and more attention from researchers and used in some famous websites like EBay, Amazon, etc. In this paper, we proposed a Recommendation System for Vietnamese electronic newspaper which uses content-based filtering techniques associating with the attention of users shown in user's profile. These users' attentions are determined by inferring a set of common Hidden Topics from the documents which users preferred. Experimental results showed that approach is feasible with positive results and its capabilities for reality development.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:zYLM7Y9cAGgC,,1
2208,Automatic named entity set expansion using semantic rules and wrappers for unary relations,"Named Entity Set Expansion is the task from a set given a small number of named entities in same type finds a more complete Named Entity set. Two famous Named Entity expansion systems using the web are Google Sets and Boo!Wa!. In this paper, we proposed an approach using Vietnamese semantic rule combined with wrappers for unary relations to expand a given set of Vietnamese Named Entities. Our model using the results which are provided by Google search engine in Vietnam, consists of six main phases. We did the experiments for many times to choose the rank method that gave the best results is Page Rank, and mixing coefficient is 0.6. Experimental results showed that the feasible approach with average precision is 96.4% with 5 first entities and 92.6% with 10 first entities.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:IjCSPb-OGe4C,,1
2209,Improving chemical-induced disease relation extraction with learned features based on convolutional neural network,"There have been an increasing number of various machine learning-based models successfully proposed and applied for automatic chemical-induced disease (CID) relation extraction. They, however, usually require carefully handcrafted rich feature sets, which rely on expert knowledge, thus require expensive human labor but normally still cannot generalize data well enough. In this paper, we propose a CID relation extraction model that learns features automatically through a Convolutional Neural Network (CNN) instead of traditional handcrafted features. We exploit the shortest dependency path between a disease and a chemical for identifying their CID relation. Dependency relations, with and without their direction information, are further investigated. Experimental results on benchmark datasets (namely the BioCreative V dataset) are very potential, demonstrating the effectiveness of our proposed model for …",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:ILKRHgRFtOwC,,1
2210,The impact of near domain transfer on biomedical named entity recognition,"Current research in fully supervised biomedical named entity recognition (bioNER) is often conducted in a setting of low sample sizes. Whilst experimental results show strong performance in-domain it has been recognised that quality suffers when models are applied to heterogeneous text collections. However the causal factors have until now been uncertain. In this paper we describe a controlled experiment into near domain bias for two Medline corpora on hereditary diseases. Five strategies are employed for mitigating the impact of near domain transference including simple transference, pooling, stacking, class re-labeling and feature augmentation. We measure their effect on f-score performance against an in domain baseline. Stacking and feature augmentation mitigate f-score loss but do not necessarily result in superior performance except for selected classes. Simple pooling of data across domains failed to exploit size effects for most classes. We conclude that we can expect lower performance and higher annotation costs if we do not adequately compensate for the distributional dissimilarities of domains during learning.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:aqlVkmm33-oC,,1
2211,VnLoc: A Real--Time News Event Extraction Framework for Vietnamese,"Event Extraction is a complex and interesting topic in Information Extraction that includes event extraction methods from free text or web data. The result of event extraction systems can be used in several fields such as risk analysis systems, online monitoring systems or decide support tools. In this paper, we introduce a method that combines lexico -- semantic and machine learning to extract event from Vietnamese news. Furthermore, we concentrate to describe event online monitoring system named VnLoc based on the method that was proposed above to extract event in Vietnamese language. Besides, in experiment phase, we have evaluated this method based on precision, recall and F1 measure. At this time of experiment, we on investigated on three types of event: FIRE, CRIME and TRANSPORT ACCIDENT.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:UeHWp8X0CEIC,,1
2212,An integrated approach using conditional random fields for named entity recognition and person property extraction in Vietnamese text,"Personal names are among one of the most frequently searched items in web search engines and a person entity is always associated with numerous properties. In this paper, we propose an integrated model to recognize person entity and extract relevant values of a pre-defined set of properties related to this person simultaneously for Vietnamese. We also design a rich feature set by using various kind of knowledge resources and a apply famous machine learning method CRFs to improve the results. The obtained results show that our method is suitable for Vietnamese with the average result is 84 % of precision, 82.56% of recall and 83.39 % of F-measure. Moreover, performance time is pretty good, and the results also show the effectiveness of our feature set.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:W7OEmFMy1HYC,,1
2213,Social-spam profile detection based on content classification and user behavior,"Web-based social system enables new community-based opportunities for participants to engage, share and interact. The rapid growth of Facebook has triggered a dramatic increase in spam volume and sophistication. Spammers post their status or comment in Page to send spam content to their friends or other users in the network. In this paper, we consider the problem of detecting spam accounts on Facebook based on comment content and user social behavior. We will propose a hybrid approach using Maximum Entropy (Maxent) model for classifying user comments as either spam or non-spam. We carefully conducted an empirical evaluation for our model on a large collection of comments in Vietnamese Facebook Pages and achieved promising results with an average accuracy of more than 90%.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:EYYDruWGBe4C,,1
2214,Improving named entity recognition in vietnamese texts by a character-level deep lifelong learning model,"Named entity recognition (NER) is a fundamental task which affects the performance of its dependent task, e.g. machine translation. Lifelong machine learning (LML) is a continuous learning process, in which the knowledge base accumulated from previous tasks will be used to improve future learning tasks having few samples. Since there are a few studies on LML based on deep neural networks for NER, especially in Vietnamese, we propose a lifelong learning model based on deep learning with a CRFs layer, named DeepLML–NER, for NER in Vietnamese texts. DeepLML–NER includes an algorithm to extract the knowledge of “prefix-features” of named entities in previous domains. Then the model uses the knowledge in the knowledge base to solve the current NER task. Preprocessing and model parameter tuning are also investigated to improve the performance. The effect of the model was demonstrated by …",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:lmc2jWPfTJgC,,1
2216,A solution for grouping Vietnamese synonym feature words in product reviews,"Feature-based opinion mining is an interesting opinion mining issue. For this problem, feature words/phrases are discovered at sentence level. However, customers usually use different words/phrases referring to the same feature in reviews. To produce a meaningful summary, synonym feature words/phrases in domain, need to be grouped under the same feature. This paper proposes a solution for grouping synonym features in Vietnamese customer reviews based on semi-supervised SVM-kNN classification and HAC clustering. Experimental results on reviews in mobile phone domain demonstrate that the proposed method is promising for the task. The Purity, Accuracy measures are 0.68 and 0.65 respectively.",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:u-x6o8ySG0sC,,1
2217,Uetrice at mediqa 2021: A prosper-thy-neighbour extractive multi-document summarization model,"This paper describes a system developed to summarize multiple answers challenge in the MEDIQA 2021 shared task collocated with the BioNLP 2021 Workshop. We propose an extractive summarization architecture based on several scores and state-of-the-art techniques. We also present our novel prosper-thy-neighbour strategies to improve performance. Our model has been proven to be effective with the best ROUGE-1/ROUGE-L scores, being the shared task runner up by ROUGE-2 F1 score (over 13 participated teams).",142,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hkchOf4AAAAJ&citation_for_view=hkchOf4AAAAJ:i2xiXl-TujoC,,1
2218,Observation of  Resonances Consistent with Pentaquark States in  Decays,"Observations of exotic structures in the J/ψ p channel, which we refer to as charmonium-pentaquark states, in Λ b 0→ J/ψ K− p decays are presented. The data sample corresponds to an integrated luminosity of 3 fb− 1 acquired with the LHCb detector from 7 and 8 TeV p p collisions. An amplitude analysis of the three-body final state reproduces the two-body mass and angular distributions. To obtain a satisfactory fit of the structures seen in the J/ψ p mass spectrum, it is necessary to include two Breit-Wigner amplitudes that each describe a resonant state. The significance of each of these resonances is more than 9 standard deviations. One has a mass of 4380±8±29 MeV and a width of 205±18±86 MeV, while the second is narrower, with a mass of 4449.8±1.7±2.5 MeV and a width of 39±5±19 MeV. The preferred J P assignments are of opposite parity, with one state having spin 3/2 and the other 5/2.",143,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5r28iccAAAAJ&citation_for_view=5r28iccAAAAJ:u5HHmVD_uO8C,,1
2219,Test of Lepton Universality Using  Decays,"A measurement of the ratio of the branching fractions of the B+→ K+ μ+ μ− and B+→ K+ e+ e− decays is presented using proton-proton collision data, corresponding to an integrated luminosity of 3.0 fb− 1, recorded with the LHCb experiment at center-of-mass energies of 7 and 8 TeV. The value of the ratio of branching fractions for the dilepton invariant mass squared range 1< q 2< 6 GeV 2/c 4 is measured to be 0.74 5− 0.074+ 0.090 (stat)±0.036 (syst). This value is the most precise measurement of the ratio of branching fractions to date and is compatible with the standard model prediction within 2.6 standard deviations.",143,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5r28iccAAAAJ&citation_for_view=5r28iccAAAAJ:2osOgNQ5qMEC,,1
2220,Evidence for  Violation in Time-Integrated  Decay Rates,"A search for time-integrated C P violation in D 0→ h− h+(h= K, π) decays is presented using 0.62 fb− 1 of data collected by LHCb in 2011. The flavor of the charm meson is determined by the charge of the slow pion in the D*+→ D 0 π+ and D*−→ D 0 π− decay chains. The difference in C P asymmetry between D 0→ K− K+ and D 0→ π− π+, Δ A C P≡ A C P (K− K+)− A C P (π− π+), is measured to be [− 0.82±0.21 (stat)±0.11 (syst)]%. This differs from the hypothesis of C P conservation by 3.5 standard deviations.",143,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5r28iccAAAAJ&citation_for_view=5r28iccAAAAJ:Tyk-4Ss8FVUC,,1
2221,Measurement of Form-Factor-Independent Observables in the Decay ,"We present a measurement of form-factor-independent angular observables in the decay B 0→ K*(892) 0 μ+ μ−. The analysis is based on a data sample corresponding to an integrated luminosity of 1.0 fb− 1, collected by the LHCb experiment in p p collisions at a center-of-mass energy of 7 TeV. Four observables are measured in six bins of the dimuon invariant mass squared q 2 in the range 0.1< q 2< 19.0 GeV 2/c 4. Agreement with recent theoretical predictions of the standard model is found for 23 of the 24 measurements. A local discrepancy, corresponding to 3.7 Gaussian standard deviations is observed in one q 2 bin for one of the observables. Considering the 24 measurements as independent, the probability to observe such a discrepancy, or larger, in one is 0.5%.",143,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5r28iccAAAAJ&citation_for_view=5r28iccAAAAJ:IjCSPb-OGe4C,,1
2222,Prompt charm production in pp collisions at s= 7TeV,"Charm production at the LHC in pp collisions at s= 7 TeV is studied with the LHCb detector. The decays D 0→ K− π+, D+→ K− π+ π+, D⁎+→ D 0 (K− π+) π+, D s+→ ϕ (K− K+) π+, Λ c+→ p K− π+, and their charge conjugates are analysed in a data set corresponding to an integrated luminosity of 15 nb− 1. Differential cross-sections d σ/d p T are measured for prompt production of the five charmed hadron species in bins of transverse momentum and rapidity in the region 0< p T< 8 GeV/c and 2.0< y< 4.5. Theoretical predictions are compared to the measured differential cross-sections. The integrated cross-sections of the charm hadrons are computed in the above p T-y range, and their ratios are reported. A combination of the five integrated cross-section measurements gives σ (c c¯) p T< 8 GeV/c, 2.0< y< 4.5= 1419±12 (stat)±116 (syst)±65 (frag) μb, where the uncertainties are statistical, systematic, and due to the …",143,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5r28iccAAAAJ&citation_for_view=5r28iccAAAAJ:zYLM7Y9cAGgC,,1
2223,First Evidence for the Decay ,"A search for the rare decays B s 0→ μ+ μ− and B 0→ μ+ μ− is performed with data collected in 2011 and 2012 with the LHCb experiment at the Large Hadron Collider. The data samples comprise 1.1 fb− 1 of proton-proton collisions at s= 8 TeV and 1.0 fb− 1 at s= 7 TeV. We observe an excess of B s 0→ μ+ μ− candidates with respect to the background expectation. The probability that the background could produce such an excess or larger is 5.3× 10− 4 corresponding to a signal significance of 3.5 standard deviations. A maximum-likelihood fit gives a branching fraction of B (B s 0→ μ+ μ−)=(3.2− 1.2+ 1.5)× 10− 9, where the statistical uncertainty is 95% of the total uncertainty. This result is in agreement with the standard model expectation. The observed number of B 0→ μ+ μ− candidates is consistent with the background expectation, giving an upper limit of B (B 0→ μ+ μ−)< 9.4× 10− 10 at 95% confidence level.",143,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5r28iccAAAAJ&citation_for_view=5r28iccAAAAJ:u-x6o8ySG0sC,,1
2224,Evidence for  Violation in Time-Integrated  Decay Rates,"A search for time-integrated C P violation in D 0→ h− h+(h= K, π) decays is presented using 0.62 fb− 1 of data collected by LHCb in 2011. The flavor of the charm meson is determined by the charge of the slow pion in the D*+→ D 0 π+ and D*−→ D 0 π− decay chains. The difference in C P asymmetry between D 0→ K− K+ and D 0→ π− π+, Δ A C P≡ A C P (K− K+)− A C P (π− π+), is measured to be [− 0.82±0.21 (stat)±0.11 (syst)]%. This differs from the hypothesis of C P conservation by 3.5 standard deviations.",143,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5r28iccAAAAJ&citation_for_view=5r28iccAAAAJ:qjMakFHDy7sC,,1
2225,Measurement of σ (pp→ bb¯ X) at s= 7 TeV in the forward region,Decays of b hadrons into final states containing a D 0 meson and a muon are used to measure the b b¯ production cross-section in proton–proton collisions at a centre-of-mass energy of 7 TeV at the LHC. In the pseudorapidity interval 2< η< 6 and integrated over all transverse momenta we find that the average cross-section to produce b-flavoured or b¯-flavoured hadrons is (75.3±5.4±13.0) μb.,143,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5r28iccAAAAJ&citation_for_view=5r28iccAAAAJ:9yKSN-GCB0IC,,1
2226,Differential Branching Fraction and Angular Analysis of the Decay ,"The angular distributions and the partial branching fraction of the decay B 0→ K* 0 μ+ μ− are studied by using an integrated luminosity of 0.37 fb− 1 of data collected with the LHCb detector. The forward-backward asymmetry of the muons, A FB, the fraction of longitudinal polarization, F L, and the partial branching fraction d B/d q 2 are determined as a function of the dimuon invariant mass. The measurements are in good agreement with the standard model predictions and are the most precise to date. In the dimuon invariant mass squared range 1.00–6.00 GeV 2/c 4, the results are A FB=− 0.06− 0.14+ 0.13±0.04, F L= 0.55±0.10±0.03, and d B/d q 2=(0.42±0.06±0.03)× 10− 7 c 4/GeV 2. In each case, the first error is statistical and the second systematic.",143,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5r28iccAAAAJ&citation_for_view=5r28iccAAAAJ:UeHWp8X0CEIC,,1
2227,Measurement of prompt hadron production ratios in pp collisions at ,"The charged-particle production ratios , K −/K +, π −/π +, , (K ++K −)/(π ++π −) and  are measured with the LHCb detector using 0.3 nb−1 of pp collisions delivered by the LHC at  and 1.8 nb−1 at . The measurements are performed as a function of transverse momentum p T and pseudorapidity η. The production ratios are compared to the predictions of several Monte Carlo generator settings, none of which are able to describe adequately all observables. The ratio  is also considered as a function of rapidity loss, Δy≡y beam−y, and is used to constrain models of baryon transport.",143,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5r28iccAAAAJ&citation_for_view=5r28iccAAAAJ:d1gkVwhDpl0C,,1
2228,Large-scale exploration of neural relation classification architectures,"Experimental performance on the task of relation classification has generally improved using deep neural network architectures. One major drawback of reported studies is that individual models have been evaluated on a very narrow range of datasets, raising questions about the adaptability of the architectures, while making comparisons between approaches difficult. In this work, we present a systematic large-scale analysis of neural relation classification architectures on six benchmark datasets with widely varying characteristics. We propose a novel multi-channel LSTM model combined with a CNN that takes advantage of all currently popular linguistic and architectural features. Our ‘Man for All Seasons’ approach achieves state-of-the-art performance on two datasets. More importantly, in our view, the model allowed us to obtain direct insights into the continued challenges faced by neural language models on this task. Example data and source code are available at: https://github. com/aidantee/MASS.",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:u-x6o8ySG0sC,,1
2229,A richer-but-smarter shortest dependency path with attentive augmentation for relation extraction,"To extract the relationship between two entities in a sentence, two common approaches are (1) using their shortest dependency path (SDP) and (2) using an attention model to capture a context-based representation of the sentence. Each approach suffers from its own disadvantage of either missing or redundant information. In this work, we propose a novel model that combines the advantages of these two approaches. This is based on the basic information in the SDP enhanced with information selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but- Smarter SDP). To exploit the representation behind the RbSP structure effectively, we develop a combined deep neural model with a LSTM network on word sequences and a CNN on RbSP. Experimental results on the SemEval-2010 dataset demonstrate improved performance over competitive baselines. The data and source code are available at https: //github.com/catcd/RbSP.",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:zYLM7Y9cAGgC,,1
2230,Improving chemical-induced disease relation extraction with learned features based on convolutional neural network,"There have been an increasing number of various machine learning-based models successfully proposed and applied for automatic chemical-induced disease (CID) relation extraction. They, however, usually require carefully handcrafted rich feature sets, which rely on expert knowledge, thus require expensive human labor but normally still cannot generalize data well enough. In this paper, we propose a CID relation extraction model that learns features automatically through a Convolutional Neural Network (CNN) instead of traditional handcrafted features. We exploit the shortest dependency path between a disease and a chemical for identifying their CID relation. Dependency relations, with and without their direction information, are further investigated. Experimental results on benchmark datasets (namely the BioCreative V dataset) are very potential, demonstrating the effectiveness of our proposed model for …",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:u5HHmVD_uO8C,,1
2231,Detection of distributed denial of service attacks using automatic feature selection with enhancement for imbalance dataset,"With the development of technology, the highly accessible internet service is the biggest demand for most people. Online network, however, has been suffering from malicious attempts to disrupt essential web technologies, resulting in service failures. In this work, we introduced a model to detect and classify Distributed Denial of Service attacks based on neural networks that take advantage of a proposed automatic feature selection component. The experimental results on CIC-DDoS 2019 dataset have demonstrated that our proposed model outperformed other machine learning-based model by large margin. We also investigated the effectiveness of weighted loss and hinge loss on handling the class imbalance problem.",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:Tyk-4Ss8FVUC,,1
2232,A hybrid deep learning architecture for sentence unit detection,"Automatic speech recognition systems currently deliver an unpunctuated sequence of words which is hard to peruse for human and degrades the performance of the downstream natural language processing tasks. In this paper, we propose a hybrid approach for Sentence Unit Detection, in which the focus is on adding the full stop [.]to the unstructured text. Our model profits from the advantage of two dominant deep learning architectures: (i)the ability to learn the long dependencies in both directions of a bidirectional Long Short-Term Memory; (ii)the ability to capture the local context with Convolutional Neural Networks. We also empirically study the training objective of our networks using extra-loss and further investigate the impacts of each model component on the overall result. Experiments conducted on two large-scale datasets demonstrated that the proposed architecture outperforms previous separated …",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:9yKSN-GCB0IC,,1
2233,Uetrice at mediqa 2021: A prosper-thy-neighbour extractive multi-document summarization model,"This paper describes a system developed to summarize multiple answers challenge in the MEDIQA 2021 shared task collocated with the BioNLP 2021 Workshop. We propose an extractive summarization architecture based on several scores and state-of-the-art techniques. We also present our novel prosper-thy-neighbour strategies to improve performance. Our model has been proven to be effective with the best ROUGE-1/ROUGE-L scores, being the shared task runner up by ROUGE-2 F1 score (over 13 participated teams).",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:YsMSGLbcyi4C,,1
2234,An investigation of word embeddings with deep bidirectional lstm for sentence unit detection in automatic speech transcription,"This work investigates the effectiveness of using the word based and sub-word based embedding representations as input for a deep bidirectional Long Short-Term Memory Network for Sentence Unit Detection in Automatic Speech Recognition transcription. Our experimental results show that using sub-word based embedding can significantly improve the SUD performance when a limited text is used to train both the word embedding and the SUD model. The SUD model using the sub-word based embedding gains up to 2.07% absolute improvement in F1-score as compared to the best model trained with the word-based embedding. When tested on a domain-mismatch condition, the SUD model with sub-word based embedding trained from the in-domain data gives an approximate 2 % and 1 % improvement over the best model using out-of-domain embedding with reference and ASR transcription with 29.5 …",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:2osOgNQ5qMEC,,1
2235,Overview of VLSP RelEx shared task: A Data Challenge for Semantic Relation Extraction from Vietnamese News,"This paper reports the overview of RelEx shared task for semantic relation extraction from Vietnamese News, which is hosted at the seventh annual workshop on Vietnamese Language and Speech Processing (VLSP 2020). This task focuses on classifying entity pairs in Vietnamese News text into four different, non-overlapping categories of semantic relations defined in advance. In order to generate a fair benchmark, we build a humanannotated dataset of 1,056 documents and 5,900 instances of semantic relations, collected from Vietnamese News in several domains. All models will be evaluated in terms of macro- and micro-averaged F1 scores, two typical evaluation metrics for semantic relation extraction problem.",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:0EnyYjriUFMC,,1
2236,Improving semantic relation extraction system with compositional dependency unit on enriched shortest dependency path,"Experimental performance on the task of relation extraction/classification has generally improved using deep neural network architectures. In which, data representation has been proven to be one of the most influential factors to the model’s performance but still has many limitations. In this work, we take advantage of compressed information in the shortest dependency path (SDP) between two corresponding entities to classify the relation between them. We propose (i) a compositional embedding that combines several dominant linguistic as well as architectural features and (ii) dependency tree normalization techniques for generating rich representations for both words and dependency relations in the SDP. We also present a Convolutional Neural Network (CNN) model to process the proposed SDP enriched representation. Experimental results for both general and biomedical data demonstrate the …",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:UeHWp8X0CEIC,,1
2237,THANOS: The Aspect Classification Model for Imbalanced Vietnamese E-commerce Review Data,"This article introduces several methods for aspect classification using machine learning models for data collected from customers' reviews on two e-commerce sites and mainly focuses on handling data imbalance to improve classifier performance. To this end, we describe the problem as a binary classifier at the sentence level. Sentences will be expressed as feature vectors using One-hot combined with Chi-square statistics, and we will use basic machine learning models such as Naive Bayes, SVM, Random Forest, Linear Regression for training before classifying the aspects included in. We tested the strategy on a dataset generated in Vietnamese based on the comments from users who purchased products on e-commerce sites. The experimental results suggest that the method produces better outcomes than before the imbalance handling.",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:MXK_kJrjxJIC,,1
2238,Attention-Based Deep Learning Model for Aspect Classification on Vietnamese E-commerce Data,"This article introduces methods for applying Deep Learning in identifying aspects from written commentaries on Shopee e-commerce sites. The used datasets are two sets of Vietnamese consumers' comments about purchased products in two domains. Words and sentences will be performed as vectors, or characteristic matrices through language models such as one-hot, fastText, PhoBERT. We then used Convolutional Neural Network (CNN) and the Fully Connected Neural Network (Multilayer perceptron - MLP) to learn the aspects which are mentioned in the comments. Experimental results showed that our team's methods achieved much better results than traditional learning algorithm using other word-level vectors such as SVM, Naïve Bayes, etc.",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:8k81kl-MbHgC,,1
2239,A Hybrid Multi-answer Summarization Model for the Biomedical Question-Answering System,"In natural language processing problems, text summarization is a difficult problem and always attracts attention from the research community, especially working on biomedical text data which lacks supporting tools and techniques. In this scientific research report, we propose a multi-document summarization model for the responses in the biomedical question and answer system. Our model includes components which is a combination of many advanced techniques as well as some improved methods proposed by authors. We present research methods applied to two main approaches: an extractive summarization architecture based on multi scores and state-of-the-art techniques, presenting our novel prosper-thy-neighbor strategies to improve performance; EAHS model (Extractive-Abstractive hybrid model) based on a denoising auto-encoder for pre-training sequence-to-sequence models (BART). In which we …",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:5nxA0vEk-isC,,1
2240,Models Distillation with Lifelong Deep Learning for Vietnamese Biomedical Named Entity Recognition,"In realistic data, named entities may appear in a variety of rich contexts with unique characteristics and the performance of the named entity recognition (NER) task directly affects other NLP problems. Although both lifelong learning and deep learning have proven effective in many problems, including NER, the suitable combination of these two research directions is still limited. This paper describes a lifelong deep learning model for Vietnamese Biomedical NER based on model distillation mechanism. Our approach achieves potential results, helps to boost 2.16% compared to original deep learning model.",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:hqOjcs7Dif8C,,1
2241,Aspect-Based Sentiment Analysis Using Mini-Window Locating Attention for Vietnamese E-commerce Reviews,"This article illustrates a system developed to tackle Aspect-based sentiment classification for Vietnamese E-commerce reviews. We employ supervised learning models based on Deep Learning application and multiple classic classifiers such as Random Forest, Decision Tree, Support Vector Machine, etc. to sort out the model performs best with our dataset. Our method obtained the maximum Micro-Average and Macro-Average Performance of 95%. Furthermore, we present how our Vietnamese manually-annotated multi-aspect dataset in two domains: Technology and Mother & Baby was prepared.",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:UebtZRa9Y70C,,1
2242,Exploiting document graphs for inter-sentence relation extraction,"Most previous relation extraction (RE) studies have focused on intra-sentence relations and haveignored relations that span sentences, i.e. inter-sentence relations. Such relations connect entities at thedocument level rather than as relational facts in a single sentence. Extracting facts that are expressed acrosssentences leads to some challenges and requires different approaches than those usually applied in recentintra-sentence relation extraction. Despite recent results, there are still limitations to be overcome.",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:Y0pCki6q_DkC,,1
2243,QASA: Advanced Document Retriever for Open-Domain Question Answering by Learning to Rank Question-Aware Self-Attentive Document Representations,"For information consumers, being able to obtain a short and accurate answer for a query is one of the most desirable features. This motivation, along with the rise of deep learning, has led to a boom in open-domain Question Answering (QA) research. While the problem of machine comprehension has received multiple success with the help of large training corpora and the emergence of attention mechanism, the development of document retrieval in open-domain QA is lagged behind. In this work, we propose a novel encoding method for learning question-aware self-attentive document representations. By applying pair-wise ranking approach to these encodings, we build a Document Retriever, called QASA, which is then integrated with a machine reader to form a complete open-domain QA system. Our system is thoroughly evaluated using QUASAR-T dataset and shows surpassing results compared to other state …",145,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=3mZclRIAAAAJ&citation_for_view=3mZclRIAAAAJ:IjCSPb-OGe4C,,1
2244,D3NER: biomedical named entity recognition using CRF-biLSTM improved with fine-tuned embeddings of various linguistic information,"Recognition of biomedical named entities in the textual literature is a highly challenging research topic with great interest, playing as the prerequisite for extracting huge amount of high-valued biomedical knowledge deposited in unstructured text and transforming them into well-structured formats. Long Short-Term Memory (LSTM) networks have recently been employed in various biomedical named entity recognition (NER) models with great success. They, however, often did not take advantages of all useful linguistic information and still have many aspects to be further improved for better performance.",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:9yKSN-GCB0IC,,1
2245,Learning to recognize phenotype candidates in the auto-immune literature using SVM re-ranking,"The identification of phenotype descriptions in the scientific literature, case reports and patient records is a rewarding task for bio-medical text mining. Any progress will support knowledge discovery and linkage to other resources. However because of their wide variation a number of challenges still remain in terms of their identification and semantic normalisation before they can be fully exploited for research purposes. This paper presents novel techniques for identifying potential complex phenotype mentions by exploiting a hybrid model based on machine learning, rules and dictionary matching. A systematic study is made of how to combine sequence labels from these modules as well as the merits of various ontological resources. We evaluated our approach on a subset of Medline abstracts cited by the Online Mendelian Inheritance of Man database related to auto-immune diseases. Using partial matching the best micro-averaged F-score for phenotypes and five other entity classes was 79.9%. A best performance of 75.3% was achieved for phenotype candidates using all semantics resources. We observed the advantage of using SVM-based learn-to-rank for sequence label combination over maximum entropy and a priority list approach. The results indicate that the identification of simple entity types such as chemicals and genes are robustly supported by single semantic resources, whereas phenotypes require combinations. Altogether we conclude that our approach coped well with the compositional structure of phenotypes in the auto-immune domain.",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:qjMakFHDy7sC,,1
2246,Large-scale exploration of neural relation classification architectures,"Experimental performance on the task of relation classification has generally improved using deep neural network architectures. One major drawback of reported studies is that individual models have been evaluated on a very narrow range of datasets, raising questions about the adaptability of the architectures, while making comparisons between approaches difficult. In this work, we present a systematic large-scale analysis of neural relation classification architectures on six benchmark datasets with widely varying characteristics. We propose a novel multi-channel LSTM model combined with a CNN that takes advantage of all currently popular linguistic and architectural features. Our ‘Man for All Seasons’ approach achieves state-of-the-art performance on two datasets. More importantly, in our view, the model allowed us to obtain direct insights into the continued challenges faced by neural language models on this task. Example data and source code are available at: https://github. com/aidantee/MASS.",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:W7OEmFMy1HYC,,1
2247,A richer-but-smarter shortest dependency path with attentive augmentation for relation extraction,"To extract the relationship between two entities in a sentence, two common approaches are (1) using their shortest dependency path (SDP) and (2) using an attention model to capture a context-based representation of the sentence. Each approach suffers from its own disadvantage of either missing or redundant information. In this work, we propose a novel model that combines the advantages of these two approaches. This is based on the basic information in the SDP enhanced with information selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but- Smarter SDP). To exploit the representation behind the RbSP structure effectively, we develop a combined deep neural model with a LSTM network on word sequences and a CNN on RbSP. Experimental results on the SemEval-2010 dataset demonstrate improved performance over competitive baselines. The data and source code are available at https: //github.com/catcd/RbSP.",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:roLk4NBRz8UC,,1
2248,The UET-CAM system in the BioCreAtIvE V CDR task,"In this paper, we describe a system developed for the BioCreative V chemical-disease relation (CDR) task. The Disease Named Entity Recognition and Normalization (DNER) model employs joint learning using a perceptronbased named entity recognizer (NER) and a back-off model for named entity normalization (NEN). In order to maximize both precision and recall, our NEN adopts a sequential back-off ensemble approach based on Semantic Supervised Indexing (SSI)-a supervised Word Embedding (WE) method-giving results by inference from training data, and Skip-gram-an unsupervised WE methodtaking advantage of large unlabeled data. In the Chemical-induced diseases relation extraction (CID) model, we firstly resolve co-references by using a multipass sieve to identify cross-sentence references for entities, thus enabling intrasentence relations to be discovered more easily. Following this we extract CID relations using a support vector machine model trained on supervised sentence data from the CDR training and development dataset. We evaluated our method on both the DNER test set and the CID test set. Results show an F1 of 76.44 for the DNER task, and a best performance of 51.6 on the CID task using the multipass sieve.",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:IjCSPb-OGe4C,,1
2249,A hybrid approach to finding phenotype candidates in genetic texts,"Named entity recognition (NER) has been extensively studied for the names of genes and gene products but there are few proposed solutions for phenotypes. Phenotype terms are expected to play a key role in inferring gene function in complex heritable diseases but are intrinsically difficult to analyse due to their complex semantics and scale. In contrast to previous approaches we evaluate state-of-the-art techniques involving the fusion of machine learning on a rich feature set with evidence from extant domain knowledge-sources. The techniques are validated on two gold standard collections including a novel annotated collection of 112 abstracts derived from a systematic search of the Online Mendelian Inheritance of Man database for auto-immune diseases. Encouragingly the hybrid model outperforms a HMM, a CRF and a pure knowledge-based method to achieve an F1 of 77.07. Disagreement analysis points to further improvements on this emerging NE task. The annotated corpus and guidelines are available on request.",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:Tyk-4Ss8FVUC,,1
2250,Sieve-based coreference resolution enhances semi-supervised learning model for chemical-induced disease relation extraction,"The BioCreative V chemical-disease relation (CDR) track was proposed to accelerate the progress of text mining in facilitating integrative understanding of chemicals, diseases and their relations. In this article, we describe an extension of our system (namely UET-CAM) that participated in the BioCreative V CDR. The original UET-CAM system’s performance was ranked fourth among 18 participating systems by the BioCreative CDR track committee. In the Disease Named Entity Recognition and Normalization (DNER) phase, our system employed joint inference (decoding) with a perceptron-based named entity recognizer (NER) and a back-off model with Semantic Supervised Indexing and Skip-gram for named entity normalization. In the chemical-induced disease (CID) relation extraction phase, we proposed a pipeline that includes a coreference resolution module and a Support Vector Machine relation …",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:eQOLeE2rZwMC,,1
2251,Automatic named entity set expansion using semantic rules and wrappers for unary relations,"Named Entity Set Expansion is the task from a set given a small number of named entities in same type finds a more complete Named Entity set. Two famous Named Entity expansion systems using the web are Google Sets and Boo!Wa!. In this paper, we proposed an approach using Vietnamese semantic rule combined with wrappers for unary relations to expand a given set of Vietnamese Named Entities. Our model using the results which are provided by Google search engine in Vietnam, consists of six main phases. We did the experiments for many times to choose the rank method that gave the best results is Page Rank, and mixing coefficient is 0.6. Experimental results showed that the feasible approach with average precision is 96.4% with 5 first entities and 92.6% with 10 first entities.",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:zYLM7Y9cAGgC,,1
2252,Improving chemical-induced disease relation extraction with learned features based on convolutional neural network,"There have been an increasing number of various machine learning-based models successfully proposed and applied for automatic chemical-induced disease (CID) relation extraction. They, however, usually require carefully handcrafted rich feature sets, which rely on expert knowledge, thus require expensive human labor but normally still cannot generalize data well enough. In this paper, we propose a CID relation extraction model that learns features automatically through a Convolutional Neural Network (CNN) instead of traditional handcrafted features. We exploit the shortest dependency path between a disease and a chemical for identifying their CID relation. Dependency relations, with and without their direction information, are further investigated. Experimental results on benchmark datasets (namely the BioCreative V dataset) are very potential, demonstrating the effectiveness of our proposed model for …",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:UeHWp8X0CEIC,,1
2253,An integrated approach using conditional random fields for named entity recognition and person property extraction in Vietnamese text,"Personal names are among one of the most frequently searched items in web search engines and a person entity is always associated with numerous properties. In this paper, we propose an integrated model to recognize person entity and extract relevant values of a pre-defined set of properties related to this person simultaneously for Vietnamese. We also design a rich feature set by using various kind of knowledge resources and a apply famous machine learning method CRFs to improve the results. The obtained results show that our method is suitable for Vietnamese with the average result is 84 % of precision, 82.56% of recall and 83.39 % of F-measure. Moreover, performance time is pretty good, and the results also show the effectiveness of our feature set.",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:Se3iqnhoufwC,,1
2254,Detection of distributed denial of service attacks using automatic feature selection with enhancement for imbalance dataset,"With the development of technology, the highly accessible internet service is the biggest demand for most people. Online network, however, has been suffering from malicious attempts to disrupt essential web technologies, resulting in service failures. In this work, we introduced a model to detect and classify Distributed Denial of Service attacks based on neural networks that take advantage of a proposed automatic feature selection component. The experimental results on CIC-DDoS 2019 dataset have demonstrated that our proposed model outperformed other machine learning-based model by large margin. We also investigated the effectiveness of weighted loss and hinge loss on handling the class imbalance problem.",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:UebtZRa9Y70C,,1
2255,A Method for Building a Labeled Named Entity Recognition Corpus Using Ontologies,"Building a labeled corpus which contains sufficient data and good coverage along with solving the problems of cost, effort and time is a popular research topic in natural language processing. The problem of constructing automatic or semi-automatic training data has become a matter of the research community. For this reason, we consider the problem of building a corpus in phenotype entity recognition problem, class-specific feature detectors from unlabeled data based on over 10260 unique terms (more than 15000 synonyms) describing human phenotypic features in the Human Phenotype Ontology (HPO) and about 9000 unique terms (about 24000 synonyms) of mouse abnormal phenotype descriptions in the Mammalian Phenotype Ontology. This corpus evaluated on three corpora: Khordad corpus, Phenominer 2012 and Phenominer 2013 corpora with Maximum Entropy and Beam Search method …",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:WF5omc3nYNoC,,1
2256,Exploring a Probabilistic Earley Parser for Event Composition in Biomedical Texts,"We describe a high precision system for extracting events of biomedical significance that was developed during the BioNLP shared task 2013 and tested on the Cancer Genetics data set. The system achieved an F-score on the development data of 73.67 but was ranked 5th out of six with an F-score of 29.94 on the test data. However, precision was the second highest ranked on the task at 62.73. Analysis suggests the need to continue to improve our system for complex events particularly taking into account cross-domain differences in argument distributions.",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:u-x6o8ySG0sC,,1
2257,Uetfishes at mediqa 2021: Standing-on-the-shoulders-of-giants model for abstractive multi-answer summarization,"This paper describes a system developed to summarize multiple answers challenge in the MEDIQA 2021 shared task collocated with the BioNLP 2021 Workshop. We present an abstractive summarization model based on BART, a denoising auto-encoder for pre training sequence-to-sequence models. As focusing on the summarization of answers to consumer health questions, we propose a query-driven filtering phase to choose useful information from the input document automat ically. Our approach achieves potential results, rank no.2 (evaluated on extractive references) and no.3 (evaluated on abstractive references) in the final evaluation.",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:0EnyYjriUFMC,,1
2258,A Targeted Topic Model based Multi-Label Deep Learning Classification Framework for Aspect-based Opinion Mining,"Recently, deep Convolutional Neural Network (CNN) model has achieved remarkable results in Natural Language Processing (NLP) tasks, such as information retrieval, relation classification, semantic parsing, sentence modeling and other traditional NLP tasks, etc. On the other hand, topic modeling method has been proved to be effective by exploiting hidden knowledge in a corpus of documents. Motivated from these successes, we propose a framework that takes the advantages of closure domain measure to get enriched knowledge from close domains to the training dataset to improve the CNN model, and apply a Targeted Topic Model to take more detailed exploration on each labeled aspect of an opinion. Experimental results on different scenarios show the effectiveness of the proposed framework for multi-label classification task in comparison to other related models on the same Hotel review dataset.",151,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=LBYOZ00AAAAJ&citation_for_view=LBYOZ00AAAAJ:Y0pCki6q_DkC,,1
2259,Reducing false positives of static analysis for sei cert c coding standard,"Static analysis tools have important roles in detecting violations in source code which is not compliant with coding standards. However, for industrious size systems, these tools report an overwhelming number of violations, which contain many false positives. This research considers the SEI CERT C Coding Standard and proposes a method for automatically reducing false positives of static analysis tools by combining static analysis and deductive verification. Firstly, static analysis tools are used to detect positions in the source code which may not conform to the SEI CERT C. Secondly, behavioral properties of the program at the detected positions are described by ANSI/ISO C Specification Language regarding the SEI CERT C rule or recommendation they may violate. Deductive verification is then used to prove whether these properties satisfy conventions of the SEI CERT C. Our experiment with source code for …",157,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=r72XGlsAAAAJ&citation_for_view=r72XGlsAAAAJ:IjCSPb-OGe4C,,1
2260,Multiple Program Analysis Techniques Enable Precise Check for SEI CERT C Coding Standard,"Static analysis tools have demonstrated their ability to find non-compliant code of coding standards. However, for industrial-sized systems, static analysis tools frequently report a large number of warnings, which contain both true positives and false positives. In this research, to enable precise check for SEI CERT C Coding Standard, we combine static analysis with three different techniques. Firstly, a static analysis tool is used to detect non-compliant code, which are positions that may violate a SEI CERT C rule or recommendation. Each detected position is called a warning. Secondly, deductive verification, model checking, and pattern matching are used to verify whether each warning is a true positive or a false positive. Our experiments with two automotive applications show that this approach can help to improve the accuracy to check for SEI CERT C Coding Standard. We verify nearly 60% warnings of …",157,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=r72XGlsAAAAJ&citation_for_view=r72XGlsAAAAJ:UeHWp8X0CEIC,,1
2261,Variability fault localization: a benchmark,"Software fault localization is one of the most expensive, tedious, and time-consuming activities in program debugging. This activity becomes even much more challenging in Software Product Line (SPL) systems due to the variability of failures in SPL systems. These unexpected behaviors are caused by variability faults which can only be exposed under some combinations of system features. Although localizing bugs in non-configurable code has been investigated in-depth, variability fault localization in SPL systems still remains mostly unexplored. To approach this challenge, we propose a benchmark for variability fault localization with a large set of 1,570 buggy versions of six SPL systems and baseline variability fault localization performance results. Our hope is to engage the community to propose new and better approaches to the problem of variability fault localization in SPL systems.",157,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=r72XGlsAAAAJ&citation_for_view=r72XGlsAAAAJ:zYLM7Y9cAGgC,,1
2262,API parameter recommendation based on language model and program analysis,"APIs are extensively and frequently used in source code to leverage existing libraries and improve programming productivity. However, correctly and effectively using APIs, especially from unfamiliar libraries, is a non-trivial task. Although various approaches have been proposed for recommending API method calls in code completion, suggesting actual parameters for such APIs still needs further investigating. In this paper, we introduce FLUTE, an efficient and novel approach combining program analysis and language models for recommending API parameters. With FLUTE, the source code of programs is first analyzed to generate syntactically legal and type-valid candidates. Then, these candidates are ranked using language models. Our empirical results on two large real-world projects Netbeans and Eclipse indicate that FLUTE achieves 80% and +90% in Top-1 and Top-5 Precision, which means the tool …",157,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=r72XGlsAAAAJ&citation_for_view=r72XGlsAAAAJ:YsMSGLbcyi4C,,1
2263,Ranking Warnings of Static Analysis Tools Using Representation Learning,"Static analysis tools are frequently used to detect potential vulnerabilities in software systems. However, an inevitable problem of these tools is their large number of warnings with a high false positive rate, which consumes time and effort for investigating. In this paper, we present DeFP, a novel method for ranking static analysis warnings. Based on the intuition that warnings which have similar contexts tend to have similar labels (true positive or false positive), DeFP is built with two BiLSTM models to capture the patterns associated with the contexts of labeled warnings. After that, for a set of new warnings, DeFP can calculate and rank them according to their likelihoods to be true positives (i.e., actual vulnerabilities). Our experimental results on a dataset of 10 real-world projects show that using DeFP, by investigating only 60% of the warnings, developers can find +90% of actual vulnerabilities. Moreover, DeFP improves the state-of-the-art approach 30% in both Precision and Recall.",157,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=r72XGlsAAAAJ&citation_for_view=r72XGlsAAAAJ:W7OEmFMy1HYC,,1
2264,A Variability Fault Localization Approach for Software Product Lines,"Software fault localization is one of the most expensive, tedious, and time-consuming activities in program debugging. This activity becomes even much more challenging in Software Product Line (SPL) systems due to variability of failures. These unexpected behaviors are induced by variability faults which can only be exposed under some combinations of system features. The interaction among these features causes the failures of the system. Although localizing bugs in single-system engineering has been studied in-depth, variability fault localization in SPL systems still remains mostly unexplored. In this article, we present VarCop, a novel and effective variability fault localization approach. For an SPL system failed by variability bugs, VarCop isolates suspicious code statements by analyzing the overall test results of the sampled products and their source code. The isolated suspicious statements are the …",157,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=r72XGlsAAAAJ&citation_for_view=r72XGlsAAAAJ:Y0pCki6q_DkC,,1
2265,Practical approach to access the impact of global variables on program parallelism,"Global variables may have a significant impact on preventing programs from automatic parallelism. This paper introduces a practical approach to measure the effect of global variables on program parallelism. First, we conduct static data dependence analysis among program variables and represent such dependencies by a Variable Dependence Graph. Then, we analyze this graph for measuring and identifying which global variables have a significant impact on program parallelism. To evaluate this approach, we conduct experiments on 20 benchmark programs and an industrial application. The experimental results show that half of the studied programs contain large impact variables which may be the cause of preventing programs from parallel execution.",157,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=r72XGlsAAAAJ&citation_for_view=r72XGlsAAAAJ:Tyk-4Ss8FVUC,,1
2267,Social-spam profile detection based on content classification and user behavior,"Web-based social system enables new community-based opportunities for participants to engage, share and interact. The rapid growth of Facebook has triggered a dramatic increase in spam volume and sophistication. Spammers post their status or comment in Page to send spam content to their friends or other users in the network. In this paper, we consider the problem of detecting spam accounts on Facebook based on comment content and user social behavior. We will propose a hybrid approach using Maximum Entropy (Maxent) model for classifying user comments as either spam or non-spam. We carefully conducted an empirical evaluation for our model on a large collection of comments in Vietnamese Facebook Pages and achieved promising results with an average accuracy of more than 90%.",159,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=4KiBC1YAAAAJ&citation_for_view=4KiBC1YAAAAJ:u5HHmVD_uO8C,,1
2268,Improving named entity recognition in vietnamese texts by a character-level deep lifelong learning model,"Named entity recognition (NER) is a fundamental task which affects the performance of its dependent task, e.g. machine translation. Lifelong machine learning (LML) is a continuous learning process, in which the knowledge base accumulated from previous tasks will be used to improve future learning tasks having few samples. Since there are a few studies on LML based on deep neural networks for NER, especially in Vietnamese, we propose a lifelong learning model based on deep learning with a CRFs layer, named DeepLML–NER, for NER in Vietnamese texts. DeepLML–NER includes an algorithm to extract the knowledge of “prefix-features” of named entities in previous domains. Then the model uses the knowledge in the knowledge base to solve the current NER task. Preprocessing and model parameter tuning are also investigated to improve the performance. The effect of the model was demonstrated by …",159,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=4KiBC1YAAAAJ&citation_for_view=4KiBC1YAAAAJ:qjMakFHDy7sC,,1
2269,N-tier machine learning-based architecture for DDoS attack detection,"Distributed Denial of Service (DDoS) attack is a menace to network security that aims at exhausting the target networks with malicious traffic. With simple but powerful attack mechanisms, it introduces an immense threat to the current Internet community. In this paper, we propose a novel multi-tier architecture intrusion detection model based on a machine learning method that possibly detects DDoS attacks. We evaluate our model using the newly released dataset CICDDoS2019, which contains a comprehensive variety of DDoS attacks and address the gaps of the existing current datasets. Experimental results indicated that the proposed method is more efficient than other existing ones. The experiments demonstrated that the proposed model accurately recognize DDoS attacks outperforming the state-of-the-art by F1-score.",159,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=4KiBC1YAAAAJ&citation_for_view=4KiBC1YAAAAJ:YsMSGLbcyi4C,,1
2270,A Character-Level Deep Lifelong Learning Model for Named Entity Recognition in Vietnamese Text,"Lifelong Machine Learning (LML) is a continuous learning process, in which the knowledge learned from previous tasks is accumulated in the knowledge base, then the knowledge will be used to support future learning tasks, for which it may be only a few of samples exists. However, there is a little of studies on LML based on deep neural networks for Named Entity Recognition (NER), especial in Vietnamese. We propose DeepLML-NER model, a lifelong learning model based on using deep learning methods with a CRFs layer, for NER in Vietnamese text. DeepLML-NER includes an algorithm to extract the knowledge of “prefix-features” of Named Entities in previous domains. Then the model uses the knowledge stored in the knowledge base to solve a new NER task. The effect of the model was demonstrated by in-domain and cross-domain experiments, achieving promising results.",159,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=4KiBC1YAAAAJ&citation_for_view=4KiBC1YAAAAJ:UeHWp8X0CEIC,,1
2271,A Definition of Covering Based Decision Table and Its Sample Applications,"Covering based rough set, an extension of the traditional rough set theory, which uses the cover set of the universe set instead of the partition of the universe, has proven to be both theoretical and attractive in terms of applications. Corresponding to the decision table in traditional rough set theory, the concept of covering decision system has been defined. In this paper, we propose a decision table type based on covers, including the condition lattice of covers, and the decision lattice of covers. Two tasks on covering based decision table are also introduced. We also demonstrate the applications of the covering based decision table in collaborative filtering that corresponds to the classification in the traditional decision table, and in constraint based association rule mining to indicate this covering decision table concept has a potential application.",159,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=4KiBC1YAAAAJ&citation_for_view=4KiBC1YAAAAJ:eQOLeE2rZwMC,,1
2272,A Large Scale Multi-label Text Classification Methodusing Z-Label LDA,"Multi-label Learning (MLL) is a supervised learning model that hasattracted much attention of the research community in recent yearsbecause of its wide variety applicability. In this paper, we built amulti-label classification model using Latent Dirichlet Allocationwith Topic-in-set Knowlegde (z-Label LDA) on the Vietnamesedata domain. z-Label LDA is a variant of LDA which is intended toprovide additional supervised information as a hidden topic into theLDA called ""z-label"". We also have experimented on the dataset inthe field of Education collected from Vietnamese online newspapers.Parallel, we applied the hidden topic model LDA to generate aprior-knowledge dataset comprising topics and typical keywordsrepresenting each topic. The supervised information also makesthe topic assignment more consistent. With this approach, theeffectiveness of the model has been demonstrated experimentally,this paper has obtained initial positive results.",159,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=4KiBC1YAAAAJ&citation_for_view=4KiBC1YAAAAJ:W7OEmFMy1HYC,,1
2274,Design criteria for optimally tuned nonlinear energy sinks—part 1: transient regime,The context of present work is related to the study of strongly nonlinear absorbers (NESs) aimed to attenuate vibrations induced in a single degree-of-freedom oscillator and working under the principle of targeted energy transfer (TET). The purpose motivated by practical considerations is here to establish a design criterion permitting to first ensure whether NES absorber is active or not and second to provide a nonlinear stiffness lower bound for optimal energy absorption during pumping phases. An asymptotic expansion of dynamic equations of motion under transient regime enables to emphasize a new definition of activation energy and to investigate the influence of damping upon the efficiency of one-way channeled energy transfer. Methodology is straightforwardly extended to the case of multiple NES attached in parallel to the primary oscillator. Numerical benchmark simulations corroborate the …,161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:qxL8FJ1GzNcC,,1
2276,Muscle hernias of the leg: a case report and comprehensive review of the literature,"A case involving a retired, elderly male war veteran with a symptomatic peroneus brevis muscle hernia causing superficial peroneal nerve compression with chosen surgical management is presented. Symptomatic muscle hernias of the extremities occur most commonly in the leg and are a rare cause of chronic leg pain. Historically, treating military surgeons pioneered the early documentation of leg hernias observed in active military recruits. A focal fascial defect can cause a muscle to herniate, forming a variable palpable subcutaneous mass, and causing pain and potentially neuropathic symptoms with nerve involvement. While the true incidence is not known, the etiology has been classified as secondary to a congenital (or constitutional) fascial weakness, or acquired fascial defect, usually secondary to direct or indirect trauma. The highest occurrence is believed to be in young, physically active males …",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:Zph67rFs4hoC,,1
2277,The texture of livelihoods: Migration and making a living in Hanoi,"Through the experience of 30 rural migrants to Hanoi, this paper reconstructs their livelihood histories, linking the experience of our migrant subjects with wider transformations in Vietnam’s economy and society. We argue for an ‘everyday’ political economy that recognises the fluid connections between the state, society and economy, and the individual. While highlighting the indeterminacy and contingency of life, we recognise the wider context that contributes to the production of the livelihood textures we discern. The characteristic livelihood pathways of the migrant households are shown to echo research undertaken in rural, often less economically vibrant, contexts.",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:u5HHmVD_uO8C,,1
2278,"Overview of digital breast tomosynthesis: Clinical cases, benefits and disadvantages","In France, the national breast cancer-screening program is based on mammography combined with clinical breast examination, and sometimes breast ultrasound for patients with high breast density. Digital breast tomosynthesis is a currently assessed 3D imaging technique in which angular projections of the stationary compressed breast are acquired automatically. When combined with mammography, clinicians can review both conventional (2D) as well as three-dimensional (3D) data. The purpose of this article is to review recent reports on this new breast imaging technique and complements this information with our personal experience. The main advantages of tomosynthesis are that it facilitates the detection and characterization of breast lesions, as well as the diagnosis of occult lesions in dense breasts. However, to do this, patients are exposed to higher levels of radiation than with 2D mammography. In …",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:HoB7MX3m0LUC,,1
2280,Single treatment of PPP road project options,"Recent publications have shown the role that options play in addressing the uncertainty, risk, and fairness in public–private partnership (PPP or P3) agreements in toll road projects. Typically, the literature uses financial market options techniques and applies these by analogy. Each option is presented and analyzed in standalone papers. In contrast, this paper presents an original single unifying approach for analyzing all PPP toll road options, an approach based on discounted probabilistic cash flows. The approach is a straightforward extension of conventional engineering viability analysis of projects, and does not rely on the financial market options literature. It offers a ready way to evaluate multiple options, requires minimal financial and mathematical knowledge, and hence can be readily implemented by practitioners. The paper also serves as a state-of-the-art literature summary as related to options in PPP toll …",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:eQOLeE2rZwMC,,1
2281,Parallelized trinomial option pricing model on GPU with CUDA,"The need for speed has always been an endless quest in the field of derivatives pricing. Real-time risk analysis of a book of derivatives is extremely demanding in terms of computing power. In the past, one of the trick-of-the trade was to pre-compute vectors of greeks for each derivatives product and use interpolations in real-time in order to compute the generated risk. This is now a thing of the past: we are now able to compute prices for derivatives with an increase of speed of up to two orders of magnitude, allowing real-time risk analyses. This article describes the trinomial option pricing model (or Cox-Ross-Rubinstein model) parallelized on GPU (Graphical Processing Unit) which is a simplified version of the model used by Arbitragis Trading to price options and to compute real-time scenario risk analyses. The method described in this article is an extension of the parallel pattern of the binomial method …",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:2P1L_qKh6hAC,,1
2283,First field evaluation of the optimized CE marked Abbott protocol for HIV RNA testing on dried blood spot in a routine clinical setting in Vietnam,"Background Viral load (VL) monitoring of HIV-infected patients in decentralized areas is limited due to logistic constraints. Dried Blood Spots (DBS) offer the opportunity to collect samples in remote area which can be easily transferred and tested at a central laboratory. The MOVIDA (Monitoring Of Viral load In Decentralized Area) project evaluated the performance of VL measurements on DBS using the new CE marked optimized Abbott protocol.   Methods HIV-1 infected adults from three outpatient clinics in Hanoi (Vietnam) were enrolled into the study between 1 March and 13 April 2017. VL was measured on DBS using the optimized protocol provided by the manufacturer and compared to plasma VL as reference method on the Abbott m2000rt RealTime HIV-1 platform. Sensitivity was defined as the ability for DBS samples to correctly identify VL failure at the threshold of 1000 copies/mL of plasma, while specificity represented the ability to identify patients with a plasma HIV-RNA VL of <1000 copies/mL.   Results A total of 203 patients were enrolled in the study, of which 152 (75%) were male. Median age was 38 [inter quartile range: 34–43] years. Of these patients, 37 were untreated, 38 on ART for <6 months and 117 were on ART for ≥6 months. A strong correlation between VL results in plasma and from DBS was observed (ρ = 0.95; p<0.001). Plasma VL was ≥1000 copies/mL in 71 patients. The sensitivity of DBS was 90.1% (95% confidence interval [CI]: 80.7–95.9) and the specificity was 96.2% (95% CI: 91.4–98.8).   Conclusions The new optimized Abbott DBS protocol performed well in this study, meeting the WHO performance criteria for …",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:UebtZRa9Y70C,,1
2285,Yet another method for heterogeneous data fusion and preprocessing in proactive decision support systems: distributed architecture approach,"In the multi-sensors environment, the crucial issue is collecting data from different sources. The paper considers the problem of data gathering from different data sources in the framework of proactive intelligent decision support systems design. The aim is to provide the invariate access to heterogeneous data stored in a data warehouse for further processing. There are two types of data sources are considered in the paper: machine or sensor data and video streams. We propose an ‘on-fly’ method of heterogeneous data fusion and preprocessing toward to minimisation of execution time of queries. A proposed method is implemented in the five-layer distributed architecture of the system based on Apache Kafka and Spark Streaming technology. The main conclusion is that in case of heterogeneous data (like video and loged data) and functional requirements for query execution over these data, the distributed …",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:J_g5lzvAfSwC,,1
2286,Magnetoresistive performances in exchange-biased spin valves and their roles in low-field magnetic sensing applications,"The magnetoresistive properties of pinned spin valves (SV) and their roles in low-field sensing applications were characterized. The magnetoresistive parameters were extracted, including the exchange bias (Heb) field as a function of the iron content in the CoFe layer and the antiferromagnetic (AFM) thickness, the magnetoresistance (MR) ratio versus the spacer thickness, the coercivity (Hc) as a function of the seed layer, and the composite layer [NiFe/Co] used. These parameters are crucial in determining the features of the magnetic sensors. Eventually, the selected SV film structure of (Si/SiO2)/Ta(50 Å)/[NiFe(30 Å)/Co(15 Å)]/Cu(24 Å)/Co80Fe20(25 Å)/IrMn(100 Å)/Ta(50 Å) was found significant, and the SV elements were patterned using the lithographic lift-off method with the active cell dimensions of 2 μm × 150 μm. To define a pinning axis, a cool-field anneal was applied at 250 °C for 30 min in a magnetic field …",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:TQgYirikUcIC,,1
2287,Biosynthesis of metallic nanoparticles from waste Passiflora edulis peels for their antibacterial effect and catalytic activity,"Recently, the waste agricultural materials have been widely considerable for green synthesis of noble metallic nanoparticles (MNPs) due to cost efficiency and environmental protection. This study has presented a simple method for the preparation of silver nanoparticles (AgNPs) and gold nanoparticles (AuNPs) utilizing aqueous extract of waste Passiflora edulis peel (PEP) as reducing and stabilizing agents. The formation of MNPs was optimized reaction conditions to obtain the best colloidal solutions. The characterizations of the biosynthesized MNPs were performed by analysis techniques such as Fourier transmission infrared spectroscopy (FTIR), X-ray diffraction (XRD), high-resolution transmission electron microscopy (HRTEM), selected area electron diffraction (SAED). The TEM data confirmed PEP-AgNPs and PEP-AuNPs in the spherical shape with mean size of 25 nm and 7 nm, respectively. The XRD and …",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:ZeXyd9-uunAC,,1
2289,Detect malware in android firmware based on distributed network environment,"Android operating system always occupies the highest market share in mobile operating systems. Security analysis on Android operating systems often focuses on analyzing applications (APK files) when installed on the phone. There are few studies analyzing Android firmware, especially customized Android firmware. In this study, we propose a model to analyze Android firmware using a distribution model. The proposed system can be deployed on a distributed network system, which allows for multiple Android firmware analysis requests at the same time. Experimental results show that the performance of the proposed system is better than the single node model. Besides, we also discovered many malware in preinstalled applications in custom Android firmware in the wild.",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:k_IJM867U9cC,,1
2290,Tracking events in mobile device management system,"In this paper, we use the Message Queue Telemetry Transport (MQTT) protocol to track events of user mobile device and record database for analyzing later with special criteria (key performance indicators). It also shows how to establish a publish/subscribe connection between mobile device (MQTT client) and a MQTT broker for publish events. MQTT brokers can listen in incoming events through the input adaptors, store those events and then, visual dashboard show all events.",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:YFjsv_pBGBYC,,1
2291,Optimization of an auto-thermal ammonia synthesis reactor using cyclic coordinate method,"The ammonia synthesis system is an important chemical process used in the manufacture of fertilizers, chemicals, explosives, fibers, plastics, refrigeration. In the literature, many works approaching the modeling, simulation and optimization of an auto-thermal ammonia synthesis reactor can be found. However, they just focus on the optimization of the reactor length while keeping the others parameters constant. In this study, the other parameters are also considered in the optimization problem such as the temperature of feed gas enters the catalyst zone, the initial nitrogen proportion. The optimal problem requires the maximization of an objective function which is multivariable function and subject to a number of equality constraints involving the solution of coupled differential equations and also inequality constraint. The cyclic coordinate search was applied to solve the multivariable-optimization problem. In each …",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:hC7cP41nSMkC,,1
2293,Applying matrix theory to classify real solvable Lie algebras having 2-dimensional derived ideals,"We present a new approach to the problem of classifying real solvable Lie algebras having 2-dimensional derived ideals. Partial results on this problem were obtained by Schöbel in 1993 and by Janisse in 2010. In association with the results of Eberlein in 2003, we achieve a full list of real solvable Lie algebras having 2-dimensional derived ideals.",161,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Q4Q6ZMgAAAAJ&citation_for_view=Q4Q6ZMgAAAAJ:7PzlFSSx8tAC,,1
2298,Some procedures for extending random operators,"Let X and Y be separable Banach spaces and E be a subset of X. By a random operator from X into Y we mean a rule Φ that assigns to each element x ∈ X a unique Y - valued random variable Φx. Taking into account many circumstances in which the inputs are also subject to the influence of a random environment there arises the need to define the action of Φ on some random inputs. In this paper, some procedures for extending random mappings will be proposed. Some conditions under which a random mapping can be extended to apply to all X-valued random variables will be presented.",164,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=fG5GW7cAAAAJ&citation_for_view=fG5GW7cAAAAJ:2osOgNQ5qMEC,,1
2299,Weak laws of large numbers for weighted coordinatewise pairwise NQD random vectors in Hilbert spaces,"In this paper, we investigate weak laws of large numbers for weighted coordinatewise pairwise negative quadrant dependence random vectors in Hilbert spaces in the case that the decay order of tail probability is r for some 0< r< 2. Moreover, we extend results concerning Pareto-Zipf distributions and St. Petersburg game.",164,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=fG5GW7cAAAAJ&citation_for_view=fG5GW7cAAAAJ:9yKSN-GCB0IC,,1
2300,On the almost sure convergence for sums of negatively superadditive dependent random vectors in Hilbert spaces and its application,"This paper develops almost sure convergence for sums of negatively superadditive dependent random vectors in Hilbert spaces, we obtain Chung type SLLN and the Jaite type SLLN for sequences of negatively superadditive dependent random vectors in Hilbert spaces. Rate of convergence is studied through considering almost sure convergence to 0 of tail series. As an application, the almost sure convergence of degenerate von Mises-statistics is investigated.",164,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=fG5GW7cAAAAJ&citation_for_view=fG5GW7cAAAAJ:u-x6o8ySG0sC,,1
2301,Bayesian adaptive lasso with variational Bayes for variable selection in high-dimensional generalized linear mixed models,"This article describes a full Bayesian treatment for simultaneous fixed-effect selection and parameter estimation in high-dimensional generalized linear mixed models. The approach consists of using a Bayesian adaptive Lasso penalty for signal-level adaptive shrinkage and a fast Variational Bayes scheme for estimating the posterior mode of the coefficients. The proposed approach offers several advantages over the existing methods, for example, the adaptive shrinkage parameters are automatically incorporated, no Laplace approximation step is required to integrate out the random effects. The performance of our approach is illustrated on several simulated and real data examples. The algorithm is implemented in the R package glmmvb and is made available online.",164,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=fG5GW7cAAAAJ&citation_for_view=fG5GW7cAAAAJ:IjCSPb-OGe4C,,1
2302,Constructing response surface designs with orthogonal quadratic effects using cyclic Generators,"The central composite designs (CCDs [1]) and small composite designs (SCDs [2,3]) are designs for sequential experimentation for response surface optimization. The CCDs for fitting the second-order response surface require a 2-level factorial or a resolution V fraction at the first stage (screening stage). The SCDs developed for fitting the same model require many fewer runs at the first stage as they only require a resolution III* fraction. This paper introduces an algorithm which can augment a 2-level first-order design with additional 3-level runs to form a second-order design. This algorithm does not require the 2-level first-order design in stage I to be a resolution V or resolution III* fraction. These augmented runs are made up of circulant matrices. Since CCDs and SCDs are special cases of the designs constructed this way, we call the new designs generalized composite designs or GCDs. Like CCDs and SCDs …",164,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=fG5GW7cAAAAJ&citation_for_view=fG5GW7cAAAAJ:qjMakFHDy7sC,,1
2303,"Simulation of tropical cyclone tracks in the offshore of Haiphong, Vietnam","In this paper, we simulate a large number of synthetic cyclone tracks for the offshore of Haiphong, Vietnam based on the 114 historical storms observed from 1951 to 2007 in this area. With these synthetic tracks, th6 assessment of damage risks can be improved'",164,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=fG5GW7cAAAAJ&citation_for_view=fG5GW7cAAAAJ:roLk4NBRz8UC,,1
2304,Density Estimates for Solutions of Stochastic Functional Differential Equations,"In this article, we investigate the density of the solution to a class of stochastic functional differential equations by means of Malliavin calculus. Our aim is to provide upper and lower Gaussian estimates for the density.",164,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=fG5GW7cAAAAJ&citation_for_view=fG5GW7cAAAAJ:UeHWp8X0CEIC,,1
2305,Sharing experience in multitask reinforcement learning,"In multitask reinforcement learning, tasks often have sub-tasks that share the same solution, even though the overall tasks are different. If the shared-portions could be effectively identified, then the learning process could be improved since all the samples between tasks in the shared space could be used. In this paper, we propose a Sharing Experience Framework (SEF) for simultaneously training of multiple tasks. In SEF, a confidence sharing agent uses task-specific rewards from the environment to identify similar parts that should be shared across tasks and defines those parts as shared-regions between tasks. The shared-regions are expected to guide task-policies sharing their experience during the learning process. The experiments highlight that our framework improves the performance and the stability of learning task-policies, and is possible to help task-policies avoid local optimums.",166,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_25LJb4AAAAJ&citation_for_view=_25LJb4AAAAJ:u-x6o8ySG0sC,,1
2307,PCA-Based Robust Motion Data Recovery,"Human motion tracking is a prevalent technique in many fields. A common difficulty encountered in motion tracking is the corrupted data is caused by detachment of markers in 3D motion data or occlusion in 2D tracking data. Most methods for missing markers problem may quickly become ineffective when gaps exist in the trajectories of multiple markers for an extended duration. In this paper, we propose the principal component eigenspace based gap filling methods that leverage a training sample set for estimation. The proposed method is especially beneficial in the scenario of motion data with less predictable or repeated movement patterns, and that of even missing entire frames within an interval of a sequence. To highlight algorithm robustness, we perform algorithms on twenty test samples for comparison. The experimental results show that our methods are numerical stable and fast to work.",166,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_25LJb4AAAAJ&citation_for_view=_25LJb4AAAAJ:d1gkVwhDpl0C,,1
2308,Sentiment analysis and user similarity for social recommender system: An experimental study,"Social recommender system has become an emerging research topic due to the prevalence of online social networking services during the past few years. A social recommender model can be considered the combination of a recommender model and a social information model. Many approaches have been proposed to exploit the social interaction or connections among users to overcome the defect of traditional recommender systems assuming that all the users are independent and identically distributed. In this paper, we propose a social recommender system using memory based collaborative filtering models with user-oriented methods as basic models, in which we conduct an analysis on the correlations between social relations and user interest similarities. We also combine techniques of sentiment analysis to get dataset of users with their favorite products; this dataset is the input for the social …",170,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IyJkK8QAAAAJ&citation_for_view=IyJkK8QAAAAJ:u-x6o8ySG0sC,,1
2309,Social-spam profile detection based on content classification and user behavior,"Web-based social system enables new community-based opportunities for participants to engage, share and interact. The rapid growth of Facebook has triggered a dramatic increase in spam volume and sophistication. Spammers post their status or comment in Page to send spam content to their friends or other users in the network. In this paper, we consider the problem of detecting spam accounts on Facebook based on comment content and user social behavior. We will propose a hybrid approach using Maximum Entropy (Maxent) model for classifying user comments as either spam or non-spam. We carefully conducted an empirical evaluation for our model on a large collection of comments in Vietnamese Facebook Pages and achieved promising results with an average accuracy of more than 90%.",170,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IyJkK8QAAAAJ&citation_for_view=IyJkK8QAAAAJ:u5HHmVD_uO8C,,1
2310,A new lifelong topic modeling method and its application to vietnamese text multi-label classification,"Lifelong machine learning is emerging in recent years thanks to its ability to use past knowledge for current problem. Lifelong topic modeling algorithms, such as LTM and AMC, are proposed and they are very useful. However, these algorithms focus on learning bias on the topic level not the domain level. This paper proposes a lifelong topic modeling method, which focuses on learning bias on the domain level based on a proposed domain closeness measure, and an application framework for multi-label classification on Vietnamese texts. Experimental results on three previously solved Vietnamese texts, and five different current Vietnamese text datasets in combination with different topic set sizes showed that our proposed method is better than AMC method for all cases.",170,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IyJkK8QAAAAJ&citation_for_view=IyJkK8QAAAAJ:qjMakFHDy7sC,,1
2311,N-tier machine learning-based architecture for DDoS attack detection,"Distributed Denial of Service (DDoS) attack is a menace to network security that aims at exhausting the target networks with malicious traffic. With simple but powerful attack mechanisms, it introduces an immense threat to the current Internet community. In this paper, we propose a novel multi-tier architecture intrusion detection model based on a machine learning method that possibly detects DDoS attacks. We evaluate our model using the newly released dataset CICDDoS2019, which contains a comprehensive variety of DDoS attacks and address the gaps of the existing current datasets. Experimental results indicated that the proposed method is more efficient than other existing ones. The experiments demonstrated that the proposed model accurately recognize DDoS attacks outperforming the state-of-the-art by F1-score.",170,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IyJkK8QAAAAJ&citation_for_view=IyJkK8QAAAAJ:9yKSN-GCB0IC,,1
2312,Vietnamese Fake News Detection Based on Hybrid Transfer Learning Model and TF-IDF,"There are a lot of studies about fake news detection on English social networks. However, Vietnamese fake news detection on social networks still limit. In this paper, we propose a new approach for Vietnamese Fake News Detection on Social Network Sites using a pre-train language model PhoBERT combine with Term Frequency - Inverse Document Frequency (TF-IDF) for word embedding and Convolutional Neural Network (CNN) for features extracting. Our proposed model is trained and evaluated on the dataset of Reliable Intelligence Identification on Vietnamese SNSs (ReINTEL) shared task. We process text data into two scenarios: raw data and processed data to elucidate the hypothesis of pre-processing data on social networks. In addition, we use the different extra features to improve the efficiency of model. We compare our proposed model with the baseline methods. The proposed model achieved …",170,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IyJkK8QAAAAJ&citation_for_view=IyJkK8QAAAAJ:UeHWp8X0CEIC,,1
2314,A Definition of Covering Based Decision Table and Its Sample Applications,"Covering based rough set, an extension of the traditional rough set theory, which uses the cover set of the universe set instead of the partition of the universe, has proven to be both theoretical and attractive in terms of applications. Corresponding to the decision table in traditional rough set theory, the concept of covering decision system has been defined. In this paper, we propose a decision table type based on covers, including the condition lattice of covers, and the decision lattice of covers. Two tasks on covering based decision table are also introduced. We also demonstrate the applications of the covering based decision table in collaborative filtering that corresponds to the classification in the traditional decision table, and in constraint based association rule mining to indicate this covering decision table concept has a potential application.",170,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=IyJkK8QAAAAJ&citation_for_view=IyJkK8QAAAAJ:d1gkVwhDpl0C,,1
2324,Domain-independent Intent Extraction from Online Texts.,"Identifying user’s intents from texts on online channels has a wide range of applications from entrepreneurship, banking to e-commerce. However,
intent identification is not a simple task due to the intent and its attributes are various and strongly depend on the domain of data. If the number of intent domains increases, the number of intent’s attributes will get bigger. As a result, the complexity of intent extraction task grows up significantly. Additionally, when a new domain comes, it involves considerable physical efforts to define specific labels for intent and attributes for that domain. Hence, it would be much better to come up with a new method for extracting user’s intents which is not
dependent on a specific domain. In our research, we study the problem of domain-independent intent identification from posts and comments crawled from social networks and discussion forums. We present ten general labels, i.e. labels do not depend on a specific domain, and utilize them when extracting intent and its related information. We also propose a map between general labels and domain-specific labels. We extensively conduct experiments to explore the efficiency of using general labels compared to specific labels in extracting user’s intents when the number of intent domains increases. Our study is conducted on a medium-sized dataset from three selected domains: Tourism, Real Estate and Transportation. In term of accuracy, when the number
of domains grows, our proposal achieves significantly better results than domain-specific method in identifying user’s intent.

",133,https://www.cys.cic.ipn.mx/ojs/index.php/CyS/article/view/3158,1,
2325,Integrating Word Embeddings into IBM Word Alignment Models.,"Word alignment models are used to generate word-aligned parallel text which is used in statistical machine translation systems. Currently, the most popular word alignment models are IBM models which have been widely applied in a large number of translation systems. The parameters of IBM models are estimated by using Maximum Likelihood principle, i.e. by counting the co-occurrence of words in the parallel text. This way of parameter estimation leads to the “ambiguity” problem when some words stand together in many sentence pairs but each of them is not translation of any other. Additionally, this method requires large amount of training data to achieve good results. However, parallel text which is used to train the IBM models is usually limited for low-resource languages. In this work, we try to solve these two problems by adding semantic information to the models. Our semantic information is derived from word embeddings which only need monolingual data to train. We deploy evaluation on a language pair that has great differences in grammar structure, English-Vietnamese. Even with this challenged task, our proposed models gain significant improvements in word alignment result and help increasing translation quality.",133,https://doi.org/10.1109/KSE.2018.8573418,1,
2326,A technique for extracting behavioral sequence patterns from GPS recorded data.,"The mobile wireless market has been attracting many customers. Technically, the paradigm of anytime-anywhere connectivity raises previously unthinkable challenges, including the management of million of mobile customers, their profiles, the profiles-based selective information dissemination, and server-side computing infrastructure design issues to support such a large pool of users automatically and intelligently. In this paper, we propose a data mining technique for discovering frequent behavioral patterns from a collection of trajectories gathered by Global Positioning System. Although the search space for spatiotemporal knowledge is extremely challenging, imposing spatial and temporal constraints on spatiotemporal sequences makes the computation feasible. Specifically, the mined patterns are incorporated with synthetic constraints, namely spatiotemporal sequence length restriction, minimum and maximum timing gap between events, time window of occurrence of the whole pattern, inclusion or exclusion event constraints, and frequent movement patterns predictive of one ore more classes. The algorithm for mining all frequent constrained patterns is named cAllMOP. Moreover, to control the density of pattern regions a clustering algorithm is exploited. The proposed method is efficient and scalable. Its efficiency is better than that of the previous algorithms AllMOP and GSP with respect to the compactness of discovered knowledge, execution time, and memory requirement.",136,https://doi.org/10.1007/s00607-013-0333-1,1,
2327,Forecasting moving object position based on temporal patterns.,"Knowing the future position of a mobile user bring huge benefit to applications in the field of location-based services (LBSs). One of the major advantages is that helps LBSs provide targeted content or disseminate advertisement to mobile users at the right time right place. This research introduces a new technique for forecasting future position. It is based on temporal patterns discovered from the historical user mobility. The algorithm is validated in consideration of precision and recall and spatial granularity. The experimental results show that the proposed technique outperforms the existing one. With this research achievement, we give an efficient support to the LBSs provider in monitoring user intelligently and sending information to user in a push-driven fashion. Apart from the support of timely and desired services and enhanced automation, the technique helps overcome some existing issues such as network flooding due to the massive tracking of users, the latencies of the positioning systems in providing and information delivery. Accordingly, the positioning is more reliable, which enables the service provider to effectively and efficiently offer location-based services with high frequency.",136,v,1,
2328,Improving Simulation of Continuous Emotional Facial Expressions by Analyzing Videos of Human Facial Activities.,"Conversational agents are receiving significant attention from multi-agent and human computer interaction research societies. In order to make conversational agents more believable and friendly, giving them the ability to express emotions is one of research fields which have drawn a lot of attention lately. In this paper, we propose a work on analysis of how emotional facial activities happen temporally. Our goal is to find the temporal patterns of facial activity of six basic emotions in order to improve the simulation of continuous emotional facial expressions on a 3D face of an embodied agent. Using facial expression recognition techniques, we first analyze a spontaneous video database in order to consider how facial activities are related to six basic emotions temporally. From there, we bring out the general temporal patterns for facial expressions of the six basic emotions. Then, based on the temporal patterns, we propose a scheme for displaying continuous emotional states of a conversational agent on a 3D face.",136,https://doi.org/10.1007/978-3-319-13191-7_18,1,
2329,A novel clustering method for animal trajectory analysis using Wireless Sensor Network.,"Animal plays an important role in our Earth, researching the movements of animals is very helpful for us to conserve rare and precious species as well as food exploration. In this paper, we employ Wireless Sensor Networks (WSNs) with the potential for highly increased spatial and temporal resolution of measurement data. Hence WSNs promise enhanced tracking of animals without human intervention. To help experts making a better species and habitat assessment as well as conversation strategies, we propose an Extended Hierarchical Path clustering eHPCl method for analyzing the mobility of wild animals. A predictive mobility algorithm is also presented, which help experts solve the problems in data allocation and management. A system that simulates the mobility of animals is implemented. Performance of the proposed method is finally evaluated in terms of running time and estimation accuracy.",136,https://doi.org/10.1109/ICAwST.2013.6765442,1,
2330,An Environmental Monitoring System for Managing Spatiotemporal Sensor Data over Sensor Networks.,"In a wireless sensor network, sensors collect data about natural phenomena and transmit them to a server in real-time. Many studies have been conducted focusing on the processing of continuous queries in an approximate form. However, this approach is difficult to apply to environmental applications which require the correct data to be stored. In this paper, we propose a weather monitoring system for handling and storing the sensor data stream in real-time in order to support continuous spatial and/or temporal queries. In our system, we exploit two time-based insertion methods to store the sensor data stream and reduce the number of managed tuples, without losing any of the raw data which are useful for queries, by using the sensors’ temporal attributes. In addition, we offer a method for reducing the cost of the join operations used in processing spatiotemporal queries by filtering out a list of irrelevant sensors from query range before making a join operation. In the results of the performance evaluation, the number of tuples obtained from the data stream is reduced by about 30% in comparison to a naïve approach, thereby decreasing the query execution time.",136,https://doi.org/10.3390/s120403997,1,
2331,Online discovery of Heart Rate Variability patterns in mobile healthcare services.,"Recent years, advances in day-to-day wearable sensors have led to the development of low powered physiological sensor platforms, which can be integrated in body area networks, a new enabling technology for real-time health monitoring. The bottleneck in health state awareness is the algorithm that has to interpret the sensor data. Nowadays Coronary Heart Disease (CHD) is still the leading cause of death. Many classification techniques such as decision tree and neural networks proposed for an early detection of individual at risk for CHD are not able to continuously detect heart state based on sensor data stream. In this study, we propose an online three-layer neural network to recognize Heart Rate Variability (HRV) patterns related to CHD risk in consideration of daily activities. ECG sensor data is preprocessed using Poincaré plot encoding. Incremental learning is utilized to train the network with new data without forgetting the previously learned patterns. The algorithm is named Poincaré-based HRV patterns discovering Incremental Artificial neural Network (PHIAN). When a sample is presented, the nodes in the hidden layer of PHIAN compete for determining the node with the highest similarity to the input. Error variables associated with the neuron units are used as criteria for new node insertion in hopes of allowing the network to learn new patterns and reducing classification error. However, the node insertion has to be stopped in the overlapping decision areas. We suppose that the overlaps between classes have lower probability than the centric part of the classes. Therefore, after a period of learning we remove the nodes with no neighbor. Plus, the error probability density is taken into account instead of input probability density. Finally, the predictive capability of PHIAN is compared with three previous classification models, namely Self-Organizing Map (SOM), Growing Neural Gas (GNG), and Multilayer Perceptron (MLP) in terms of classification error and network structure. The results show that PHIAN outperforms the existing techniques. Our proposed model can be efficiently applied to early detection of abnormal conditions and prevent the abnormal becoming serious",136,https://doi.org/10.1016/j.jss.2010.05.074,1,
2332,A method for predicting future location of mobile user for location-based services system.,"Convergence of location-aware devices, wireless communications, and geographic information system (GIS) functionalities has been enabling the deployment of a new generation of selective information disseminating services and location-based services (LBSs). Current LBSs use information about current locations of users to provide services, such as nearest features of interest, they request. Although the common computing strategy in LBSs benefits the users, there are additional benefits when future locations are predicted. One major advantage of location prediction is that it provides LBSs with extended resources, mainly time, to improve system reliability which in turn increases the users’ confidence and the demand for LBSs. In this study, we propose a movement Rule-based Location Prediction method (RLP), to guess the user’s future location for LBSs. Its performance is assessed with respect to precision and recall. In comparison with the previous technique, the prediction accuracy of ours is higher. With the proposed approach, we give an efficient support to the LBSs provider in monitoring user intelligently and sending information to user in a push-driven fashion. Apart from the support of timely and desired services and enhanced automation, the technique helps overcome some existing issues such as network flooding due to the massive tracking of users, the latencies of the positioning systems in providing and information delivery. Accordingly, the positioning is more reliable, which enables the service provider to effectively and efficiently offer location-based services with high frequency.",136,https://doi.org/10.1016/j.cie.2008.07.009,1,
2333,Discovery of Spatiotemporal Patterns in Mobile Environment.,"The converge of location-aware devices, GIS functionalities and the increasing accuracy and availability of positioning technologies pave the way to a range of new types of location-based services. The field of spatiotemporal data mining where relationships are defined by spatial and temporal aspect of data is encountering big challenges since the increased search space of knowledge. In this study, we aim to propose algorithms for mining spatiotemporal patterns in mobile environment. Moving patterns are generated utilizing two algorithms called All_MOP and Max_MOP. The first one mines all frequent patterns and the other discovers only maximal frequent patterns. Our approach is applicable to location-based services such as tourist service, traffic service, and so on.",136,https://doi.org/10.1007/11610113_100,1,
2334,Representation and Manipulation of Geospatial Objects with Indeterminate Extents.,"A spatiotemporal database system manages data of geospatial objects whose geometry change over time and vagueness is their inherent nature. In this paper, we concentrate on the applications dealing with objects whose extents are indeterminate and change constantly. To support geographic information systems (GIS) in representing and handing such objects efficiently, we propose a fuzzy spatiotemporal data model called FSTDM based on fuzzy set theory. Also, we introduce algorithms for proceeding queries depending on whether the predicate is fuzzy spatial, temporal, or fuzzy spatiotemporal (FST). This work is applicable to many applications handling time-varying geospatial data, including global change (as in climate or land cover change), social (demographic, health, etc.) applications.",136,https://doi.org/10.1007/978-3-540-31849-1_48,1,
2335,Experimental Validations on Low-Error Estimation Models in Determining the Maximum Specific Absorption Rate of Multi-Antenna Mobile Handsets.,"In advanced wireless communication systems, multiantenna mobile handsets are developed for high data rate transmissions. In compliance with international standards and guidelines for human exposure to radio-frequency (RF) fields from these devices, it is necessary to evaluate the maximum specific absorption rate (SAR) of them. In this paper, we present low-error estimation models, which can be useful for fast and accurate determinations of the maximum SAR of a multi-antenna mobile handsets. Fundamentals of the estimation models, built from physical and mathematical perspectives, are first presented and analyzed. For validating the effectiveness of the models, we conduct several SAR measurements for a simple two-antenna mobile handset prototype. By using the estimation models, we perform SAR estimations and compare the estimated SAR with measured data. It is confirmed that the proposed low-error estimation models can provide more accurate estimated SARs, compared with the estimation techniques in previous studies. For example, the maximum estimation error caused by a conventional technique of about 3% can be decreased to 1% by the proposed estimation technique with an estimation order of three.",138,https://doi.org/10.1109/VTS-APWCS.2019.8851656,1,
2336,A New Measurement Technique to Determine the Maximum SAR of Multiple-Antenna Transmitters Using $K$-Order Models and Scalar E-Field Probes.,"In this paper, we develop an K-order model expressing the specific absorption rate (SAR) at an observation point when evaluating the electromagnetic exposure to multiple-antenna transmitters. By using the proposed model, we will be able to precisely estimate the SARs for different combinations of the relative phases of the sources, and thus to identify the maximum SAR. Detailed discussions on the K-order model and the estimation technique will be presented. We also conduct several measurements to verify the performance of the proposed estimation technique, and compare the effectiveness the proposed technique with conventional techniques. As a result, it is confirmed that the proposed technique with K-order estimations can considerably reduce the estimation errors, and thus giving highly precise estimated SAR values. In particular, when evaluating of the SAR of a three-antenna configuration, the maximum estimation error caused by conventional estimations is about 6%, which can be reduced to about 3% or 2% by the 1- and 2-order estimations, respectively.",138,https://doi.org/10.1109/ICT.2019.8798802,1,
2337,On the usage of character distribution for the detection of web attacks.,"Character distribution has been extensively used in literature to build models for the detection of web attacks. This paper explores that character distribution models should be built at attribute level in order to achieve a reasonable accuracy. However, attaching detection models to every single attribute leads to high memory and time complexities, which make attribute-specific models less practical. To remove these barriers, a simple yet effective solution has been proposed. In more details, by exploiting the language function of characters, character distribution can be reduced in size and rearranged in an intentional manner so that both time and memory complexities are reduced significantly. Detection models that use minimized and rearranged character distribution are, therefore, highly efficient and practical, especially suitable to large, high-traffic web applications.",138,https://doi.org/10.1109/KSE.2017.8119435,1,
2338,A New Scheme to Enhance Bandwidth of Printed Dipole for Wideband Applications.,"This paper presents a new technique to enhance the bandwidth of a printed dipole antenna for ultra-wideband applications. The basic idea is to exploit mutual coupling between the feeding line, which is designed closed and paralleled to dipole arms, the dipole arms and other elements of the antenna. Dipole arms, feeding lines as well as other parts are investigated in order to expand antenna bandwidth while still retaining antenna compactness. Based on the proposed technique, we develop two sample printed dipole antennas for advanced wireless communications. One is an ultra-wideband antenna which is suitable for multi-band-mode ultra-wideband applications or being a sensing antenna in cognitive radio. The other is a reconfigurable antenna which would be applicable for wideband cognitive radios. Antenna characteristics such as radiation patterns, current distributions, and gains at different frequencies are also investigated for both sample antennas.",138,https://doi.org/10.1587/transcom.E97.B.773,1,
2339,Efficient Core Selection for Multicast Routing in Mobile Ad Hoc Networks.,"PUMA (Protocol for Unified Multicasting through Announcements) is a mesh-based multicast routing protocol for mobile ad hoc networks distinguishing from others in its class by the use of only multicast control packets for mesh establishment and maintenance, allowing it to achieve impressive performances in terms of packet delivery ratios and control overhead. However, one of the main drawbacks of the PUMA protocol is that the core of the mesh remains fixed during the whole execution process. In this paper, we present an improvement of PUMA by introducing to it an adaptive core selection mechanism. The improved protocol produces higher packet delivery ratios and lower delivery time while incurring only little additional control overhead.",138,https://doi.org/10.1007/978-3-319-06569-4_31,1,
2340,A Simple Broadband Antenna for MIMO Applications in Cognitive Radio.,"In this paper, a simple type of printed dipole is proposed for Multi-Input Multi-Output (MIMO) applications in cognitive radio. The antenna is composed of a transmission line and a dipole. Some examinations of key factors and optimized parameters of the antenna are presented. The measured results illustrate that the proposed antenna offers a bandwidth of over 50% for Voltage Standing Wave Ratio (VSWR) less than 2, extending from 2.4GHz to 4.0GHz. The antenna peak gain in E-plane and radiation patterns at different frequencies are also explored. In addition, based on the proposed antenna, we introduce two simple broadband arrays for MIMO applications in cognitive radio. One has two ports and the other has four ports. Measurement results indicate that the arrays also work in a broad bandwidth. Mutual couplings between ports in each array are kept under -10dB at the low frequencies and under -20dB at the high frequencies of bandwidth of the arrays. Furthermore, we utilized the antenna arrays for some MIMO experiments to estimate the channel capacity in a wide frequency range.",138,https://doi.org/10.1587/transcom.E95.B.18,1,
2341,Wideband MIMO Compact Antennas with Tri-Polarizations.,"Two designs of wideband compact MIMO antenna using printed dipoles are proposed in this paper. One is a three-port orthogonal polarization antenna and the other is a cube-six-port antenna. Measured results for the antennas show that they resonate at 2.6GHz and support a bandwidth of over 400MHz. The worst mutual coupling for the three-port orthogonal polarization antenna is kept under -20dB whereas that level of the cube-six-port antenna is -18dB. A number of experiments are conducted on MIMO systems with these compact antennas and linear antenna arrays. Measured data are analyzed to examine channel characteristics, such as cumulative distribution functions (CDFs) of eigenvalues. Furthermore, the effect of different antenna configurations on channel capacity is highlighted and discussed. A high data rate capacity can be achieved with the compact antennas, particularly from the cube-six-port variant. These antennas might be applied in actual MIMO systems in wireless communications.",138,https://doi.org/10.1587/transcom.E94.B.1982,1,
2342,Hybrid contention-based geographie routing in wireless sensor networks.,"Beaconless and contention-based geographic routing is an attractive approach to resource-constrained wireless sensor networks. Aggressive contention is the most cost-efficient form of contention-based geographic routing since it uses no control packet. Nevertheless, to avoid duplicated data packets, aggressive contention must set restriction on the contention area. Consequently, its packet delivery rate is limited. On the other hand, non-aggressive contention maximizes packet delivery rate by making use of full contention area. As the compensation, control packets have to be used and additional delay is introduced.

In this paper, we propose Hybrid Contention-Based Geographic Routing (HCGR) - a protocol that takes full aggressive contention and uses non-aggressive contention for recovering aggressive contention from failure. If aggressive contention succeeds, nonaggressive contention is suppressed. In cases where aggressive contention fails, non-aggressive contention is taken place to deliver data packets. Thus, HCGR can maximize packet delivery rate while keeping its overheads reasonably low. We implement HCGR and single-form protocols in the network simulator ns-2, conduct extensive simulations and present simulation results.",138,https://doi.org/10.1145/2069216.2069236,1,
2343,Greedy Geographic Routing with Path Optimization in Wireless Sensor Networks.,"In this paper, we propose Greedy with Path Optimization Routing (GPOR), a novel geographic routing protocol for wireless sensor networks. GPOR finds initial routing paths by following a greedy with recovery strategy, then uses a follow-up technique to optimize the paths. An attempt is also made to create routing entries applicable to destination areas rather than individual nodes. Main advantages of GPOR are path optimization and void avoidance capacities. We implement GPOR in ns-2 and present simulation results.",138,https://doi.org/10.1109/RIVF.2010.5633232,1,
2344,Topological Boundary Detection in Wireless Sensor Networks.,"The awareness of boundaries in wireless sensor networks has many benefits. The identification of boundaries is especially challenging since typical wireless sensor networks consist of low-capability nodes that are unaware of their geographic location. In this paper, we propose a simple, efficient algorithm to detect nodes that are near the boundary of the sensor field as well as near the boundaries of holes. Our algorithm relies purely on the connectivity information of the underlying communication graph and does not require any information on the location of nodes. We introduce the 2-neighbor graph concept, and then make use of it to identify nodes near boundaries. The results of our experiment show that our algorithm carries out the task of topological boundary detection correctly and efficiently.",138,https://doi.org/10.3745/JIPS.2009.5.3.145,1,
2345,An Efficient Algorithm to Extract Control Flow-Based Features for IoT Malware Detection.,"Control flow-based feature extraction method has the ability to detect malicious code with higher accuracy than traditional text-based methods. Unfortunately, this method has been encountered with the NP-hard problem, which is infeasible for the large-sized and high-complexity programs. To tackle this, we propose a control flow-based feature extraction dynamic programming algorithm for fast extraction of control flow-based features with polynomial time O(⁠N2⁠), where N is the number of basic blocks in decompiled executable codes. From the experimental results, it is demonstrated that the proposed algorithm is more efficient and effective in detecting malware than the existing ones. Applying our algorithm to an Internet of Things dataset gives better results on three measures: Accuracy = 99.05%, False Positive Rate = 1.31% and False Negative Rate = 0.66%.",139,https://doi.org/10.1093/comjnl/bxaa087,1,
2346,A Novel Framework to Classify Malware in MIPS Architecture-Based IoT Devices.,"Malware on devices connected to the Internet via the Internet of Things (IoT) is evolving and is a core component of the fourth industrial revolution. IoT devices use the MIPS architecture with a large proportion running on embedded Linux operating systems, but the automatic analysis of IoT malware has not been resolved. We proposed a framework to classify malware in IoT devices by using MIPS-based system behavior (system call—syscall) obtained from our F-Sandbox passive process and machine learning techniques. The F-Sandbox is a new type for IoT sandbox, automatically created from the real firmware of the specialized IoT devices, inheriting the specialized environment in the real firmware, therefore creating a diverse environment for sandboxing as an important characteristic of IoT sandbox. This framework classifies five families of IoT malware with F1-Weight = 97.44%.",139,https://doi.org/10.1155/2019/4073940,1,
2347,CFDVex: A Novel Feature Extraction Method for Detecting Cross-Architecture IoT Malware.,"The widespread adoption of Internet of Things (IoT) devices built on different architectures gave rise to the creation and development of multi-architecture malware for mass compromise. Cross-architecture malware detection plays an important role in detecting malware early on devices using new or strange architectures. Prior knowledge of malware detection on traditional architectures can be inherited for the same task on new and uncommon ones. Basing on CFD and Vex intermediate representation, we propose a feature selection method to detect cross-architecture malware, called CFDVex. Experimental evaluation of the proposed approach on our large IoT dataset achieved good results for cross-architecture malware detection. We only trained a SVM model by Intel 80386 architecture samples, our method could detect the IoT malware for the MIPS architecture samples with 95.72% of accuracy and 2.81% false positive rate.",139,https://doi.org/10.1145/3368926.3369702,1,
2348,Power Save Protocol Using Chain Based Routing.,"Sensor networks are deployed in numerous military and civil applications, such as remote target detection, weather monitoring, weather forecast, natural resource exploration and disaster management. Despite having many potential applications, wireless sensor networks still face a number of challenges due to their particular characteristics that other wireless networks, like cellular networks or mobile ad hoc networks do not have. The most difficult challenge of the design of wireless sensor networks is the limited energy resource of the battery of the sensors. This limited resource restricts the operational time that wireless sensor networks can function in their applications. Routing protocols play a major part in the energy efficiency of wireless sensor networks because data communication dissipates most of the energy resource of the networks. In many situations, a base station only needs a summary of the gathered information. For example, the base station might only require the maximum temperature of all sub-regions, each covered by a sensor or the average temperature of all sensors in the network. For similar types of application, data aggregation can be applied at all sensor nodes before the data is forwarded to the base station. The above discussions imply a new family of protocols called chain-based protocols. In the protocols, all sensor nodes sense and gather data in an energy efficient manner by cooperating with their closest neighbors. The gathering process can be done until an elected node calculates the final data and sends the data to the base station.",139,https://doi.org/10.1007/978-3-642-36642-0_19,1,
2349,A Hypercuboid-Based Machine Learning Algorithm for Malware Classification.,"Malware attacks have been among the most serious threats to cyber security in the last decade. Antimalware software can help safeguard information systems and minimize their exposure to the malware. Most of anti-malware programs detect malware instances based on signature or pattern matching. Data mining and machine learning techniques can be used to automatically detect models and patterns behind different types of malware variants. However, traditional machine-based learning techniques such as SVM, decision trees and naive Bayes seem to be only suitable for detecting malicious code, not effective enough for complex problems such as classification. In this article, we propose a new prototype extraction method for non-traditional prototype-based machine learning classification. The prototypes are extracted using hypercuboids. Each hypercuboid covers all training data points of a malware family. Then we choose the data points nearest to the hyperplanes as the prototypes. Malware samples will be classified based on the distances to the prototypes. Experiments results show that our proposition leads to F1 score of 96.5% for classification of known malware and 97.7% for classification of unknown malware, both better than the original prototype-based classification method.",139,https://doi.org/10.1109/RIVF51545.2021.9642093,1,
2350,IoT Malware Classification Based on System Calls.,"IoT devices play an important role in the industrial revolution 4.0. However, this type of device may exhibit specific security vulnerabilities that can be easily exploited to cause botnet attacks and other malicious activities. In this paper, we introduce a new method for classification and clustering of IoT malware behaviors through system call monitoring. Our method is constructed from multiple one-class SVM classifiers and has the ability to classify known malware with F1-Score over 98% and probability to detect unknown malware up to 97%. Unknown malware instances with similar behaviors can also be grouped together so new classes of malware will be discovered.",139,https://doi.org/10.1109/RIVF48685.2020.9140763,1,
2351,A Way to Estimate TCP Throughput under Low-Rate DDoS Attacks: One TCP Flow.,"TCP-targeted low-rate distributed denial-of-service (LDDoS) attacks were first introduced by A. Kuzmanovic and E. Knightly in 2003. The authors also proposed a simple model to quantify TCP throughput under LDDoS attacks. Since then, there have been many researchers attemping to estimate the throughput, such as Luo et al. We agree with them upon the sketch of TCP congestion window under a successful LDDoS attack but we find out that there are more cases than what has been specified. Moreover, the relative error of Luo's estimation method is still high. Our goal in this paper is to propose a simple but more accurate method to estimate TCP throughput of a single TCP flow under such DDoS attacks. Our estimation values in various scenarios are compared with the results of simulations performed with NS-2 simulator, so that the effectiveness of our method is illustrated.",139,https://doi.org/10.1109/RIVF48685.2020.9140777,1,
2352,Techniques for Improving Performance of the CPR-Based Approach.,"TCP-targeted low-rate distributed denial-of-service (LDDoS) attacks have created an opportunity for attackers to reduce their total attaking rate (and hence, the detection probability of the attacks) while inflicting the same damage to TCP flows as traditional flooding-based DDoS attacks. CPR-based approach has been proposed by Zhang et al. to detect and filter this kind of DDoS attacks, but its performance in terms of TCP throughput under attack is shown to be limited by the way it calculates CPR for each flow. In this paper, we will propose some modifications to the CPR-based approach in order to increase its performance. Simulation results show that the modifications can increase performance significantly.",139,https://doi.org/10.1145/3287921.3287940,1,
2353,Using CPR Metric to Detect and Filter Low-Rate DDoS Flows.,"TCP-targeted low-rate distributed denial-of-service (LDDoS) attacks pose a serious challenge to the reliability and security of the Internet. Among various proposed solutions, we are particularly interested in the Congestion Participation Rate (CPR) metric and the CPR-based approach. Through a simulation study, we show that the existing algorithm cannot simultaneously achieve high TCP throughput while under attack and good fairness performance for new legitimate TCP flows in normal times. We then propose a new version of the CPR-based approach to overcome the tradeoff. Simulation results show that it preserves TCP throughput while under attack fairly well, yet maintains fairness for new TCP flows in normal times.",139,https://doi.org/10.1145/3155133.3155161,1,
2354,Efficient Core Selection for Multicast Routing in Mobile Ad Hoc Networks.,"PUMA (Protocol for Unified Multicasting through Announcements) is a mesh-based multicast routing protocol for mobile ad hoc networks distinguishing from others in its class by the use of only multicast control packets for mesh establishment and maintenance, allowing it to achieve impressive performances in terms of packet delivery ratios and control overhead. However, one of the main drawbacks of the PUMA protocol is that the core of the mesh remains fixed during the whole execution process. In this paper, we present an improvement of PUMA by introducing to it an adaptive core selection mechanism. The improved protocol produces higher packet delivery ratios and lower delivery time while incurring only little additional control overhead.",139,https://doi.org/10.1007/978-3-319-06569-4_31,1,
2355,Hybrid contention-based geographie routing in wireless sensor networks.,"Beaconless and contention-based geographic routing is an attractive approach to resource-constrained wireless sensor networks. Aggressive contention is the most cost-efficient form of contention-based geographic routing since it uses no control packet. Nevertheless, to avoid duplicated data packets, aggressive contention must set restriction on the contention area. Consequently, its packet delivery rate is limited. On the other hand, non-aggressive contention maximizes packet delivery rate by making use of full contention area. As the compensation, control packets have to be used and additional delay is introduced.

In this paper, we propose Hybrid Contention-Based Geographic Routing (HCGR) - a protocol that takes full aggressive contention and uses non-aggressive contention for recovering aggressive contention from failure. If aggressive contention succeeds, nonaggressive contention is suppressed. In cases where aggressive contention fails, non-aggressive contention is taken place to deliver data packets. Thus, HCGR can maximize packet delivery rate while keeping its overheads reasonably low. We implement HCGR and single-form protocols in the network simulator ns-2, conduct extensive simulations and present simulation results.",139,https://doi.org/10.1145/2069216.2069236,1,
2356,Greedy Geographic Routing with Path Optimization in Wireless Sensor Networks.,"In this paper, we propose Greedy with Path Optimization Routing (GPOR), a novel geographic routing protocol for wireless sensor networks. GPOR finds initial routing paths by following a greedy with recovery strategy, then uses a follow-up technique to optimize the paths. An attempt is also made to create routing entries applicable to destination areas rather than individual nodes. Main advantages of GPOR are path optimization and void avoidance capacities. We implement GPOR in ns-2 and present simulation results.",139,https://doi.org/10.1109/RIVF.2010.5633232,1,
2357,Improving Freenet's Performance by Adaptive Clustering Cache Replacement.,This paper proposes an adaptive clustering cache replacement scheme to improve the performance of Freenet peer-to-peer networks. Efficient data retrieval in peer-to-peer networks in general and in Freenet in particular is a challenging problem. We follow the approach of using the small-world behavior to model the data cache replacement in Freenet and propose an extension to it: an adaptive clustering cache replacement scheme instead of LRU and enhanced-clustering cache replacement. The chosen seed key is not fixed but may vary depending on the request change in order to increase the adaptability of Freenet. Simulation results show that our proposed scheme improves the performance of Freenet better in terms of higher request hit ratio and lower average hops per request in comparison to the two mentioned schemes at heavy workload.,139,https://doi.org/10.1109/RIVF.2009.5174645,1,
2358,Low power coordination in wireless ad-hoc networks.,"Distributed wireless ad-hoc networks (DWANs) pose numerous technical challenges. Among them, two are widely considered as crucial: autonomous localized operation and minimization of energy consumption. We address the fundamental problem of how to maximize life-time of the network by using only local information while preserving network connectivity. We start by introducing the Care-Free Sleep (CS) Theorem that provides provably optimal necessary and sufficient conditions for a node to turn off its radio while ensuring that global connectivity is not affected.The CS theorem is the basis for an efficient localized algorithm that decides which node will turn its radio off, and for how long. The effectiveness of the approach is demonstrated using numerous simulations of the performance of the algorithm over a wide range of network parameters.",139,https://doi.org/10.1145/871506.871623,1,
2359,Randomized Mobile Agent Based Routing in Wireless Networks.,"We propose a novel approach for shortest path routing in wireless mobile networks. The approach makes use of n mobile agents initially launched from n mobile nodes forming the network. The agents move randomly from node to node and update routing information as they go. The approach is presented in this paper with two protocols. Both of them exhibit good performance in terms of the network and computing resource consumptions. The first protocol relies on independent mobile agents and imposes a minimum bandwidth requirement on individual mobile agents. Each agent carries the link state of its creator and this information remains unchanged except when the mobile agent returns to the home node. The second protocol is a refinement of the first protocol, with some form of interaction between the mobile agents. Each agent maintains the routing table of its creator instead of link state. The randomly walking agents spread the update information and compute the shortest paths via exchanging network state information between the routing tables they carry and the routing tables at the nodes they traverse. The correctness of the protocols is proven. Our analysis shows that the agent cooperation improves the system performance when dealing with topology and link cost changes.",139,https://doi.org/10.1142/S0129054101000539,1,
2360,Randomized Adaptive Routing Based on Mobile Agents.,"The mobile agent has been shown to be a convenient, efficient, and robust approach to designing distributed protocols. This paper introduces an adaptive, mobile agent-based routing algorithm. While performing a random walk through the network, mobile agents sense changes in the network state, and trigger the computation of updated values for the routing tables. The randomized moving strategy exerts a negligible and balanced impact on the network resources, both in terms of bandwidth and computation. The memory requirement is very moderate. The correctness of the protocol is proven and its complexity is analyzed.",139,https://doi.org/10.1109/ISPAN.1999.778968,1,
2361,Introducing a New Dataset for Event Detection in Cybersecurity Texts.,"Detecting cybersecurity events is necessary to keep us informed about the fast growing number of such events reported in text. In this work, we focus on the task of event detection (ED) to identify event trigger words for the cybersecurity domain. In particular, to facilitate the future research, we introduce a new dataset for this problem, characterizing the manual annotation for 30 important cybersecurity event types and a large dataset size to develop deep learning models. Comparing to the prior datasets for this task, our dataset involves more event types and supports the modeling of document-level information to improve the performance. We perform extensive evaluation with the current state-of-the-art methods for ED on the proposed dataset. Our experiments reveal the challenges of cybersecurity ED and present many research opportunities in this area for the future work.",140,https://doi.org/10.18653/v1/2020.emnlp-main.433,1,
2362,SWIPT-Based Nonorthogonal Multiple Access under Arbitrary Nakagami-m Fading with Direct Links.,"This paper studies the joint impact of simultaneous wireless information and power transfer (SWIPT) and nonorthogonal multiple access (NOMA) to the cooperative relay (CoR) network where direct links exist. Over Nakagami-m fading environments, the near users employ decode-and-forward (DF) and energy harvesting (EH) to assist the transmission from the source to the far users. Exploiting the time-switching protocol (TSP) and power-splitting protocol (PSP) to the CoR-based NOMA system, analytical results for the outage probability are derived, and the corresponding throughput is obtained. Comparative results show that the PSP outperforms the TSP at low transmit power, while at high-transmit-power regime, the TSP provides similar performance as the PSP.",141,https://doi.org/10.1155/2021/4124230,1,
2363,ViNMT: Neural Machine Translation Tookit.,"We present an open-source toolkit for neural machine translation (NMT). The new toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along with many other improvements detailed below, in order to create a self-contained, simple to use, consistent and comprehensive framework for Machine Translation tasks of various domains. It is tooled to support both bilingual and multilingual translation tasks, starting from building the model from respective corpora, to inferring new predictions or packaging the model to serving-capable JIT format.",141,https://arxiv.org/abs/2112.15272,1,
2364,Object Detection and Text Recognition in Large-scale Technical Drawings.," In this digital transformation era, the demand for automatic pattern extraction from printed materials has never been higher, making it one of the most eminent problems nowadays. In this paper, we propose a new method for pattern recognition in highly complex technical drawings. Our method is a pipeline system that includes two phases: (1) detecting the objects that contain the patterns of interest with improvements to processing large-scale image, and (2) performing character recognition on the objects if they are text patterns with improvements to post-processing task. Our experiments on nearly five thousand real technical drawings show promising results and the capability to reduce manual labeling effort to a great extent.",141,https://doi.org/10.5220/0010314406120619,1,
2365,A Classifier-Based Preordering Approach for English-Vietnamese Statistical Machine Translation.,"Reordering is of essential importance problem for phrase based statistical machine translation (SMT). In this paper, we propose an approach to automatically learn reordering rules as preprocessing step based on a dependency parser in phrase-based statistical machine translation for English to Vietnamese. We used dependency parsing and rules extracting from training the features-rich discriminative classifiers for reordering source-side sentences. We evaluated our approach on English-Vietnamese machine translation tasks, and showed that it outperform the baseline phrase-based SMT system.",141,https://doi.org/10.1007/978-3-319-75487-1_7,1,
2366,A reordering model for Vietnamese-English statistical machine translation using dependency information.,"Reordering is a major challenge in machine translation (MT) between two languages with significant differences in word order. In this paper, we present an approach to learn reordering rules as pre-processing step based on a dependency parser in phrase-based statistical machine translation (SMT) from Vietnamese to English. Dependency parser and transformation rules are used to reorder the source sentence and applied for systems translating Vietnamese to English. We evaluated our approach on Vietnamese-English machine translation tasks, and showed that it outperform the baseline phrase-based SMT system.",141,https://doi.org/10.1109/RIVF.2016.7800281,1,
2367,The English-Vietnamese machine translation system for IWSLT 2015.,"In this paper we have described our system for IWSLT2015
machine translation. Focusing primarily on the EnglishVietnamese and Vietnamese-English translation direction.
Our additions for Moses phrase-based SMT and Phrasal
SMT include two language model with monolingual training
set for English and Vietnamese.
We submitted two systems to IWSLT 2015 evaluations for English to Vietnamese Machine Translation
and Vietnamese to English Machine Translation. Our
systems is including sub-systems: 6 based on Phrasal
toolkit [Green et al.2014] and 6 others base Moses toolkit
[Koehn et al.2007b]. The systems conducted with IWSLT
2015 data using with extension language model using monolingual training data.",141,https://aclanthology.org/2015.iwslt-evaluation.12,1,
2368,Parameter Learning for Statistical Machine Translation Using CMA-ES.,"Minimum error rate training (MERT) is probably still the most widely used parameter learning algorithm in statistical machine translation [1] (SMT). However, it does not support the use of large number of learning features (e.g. 30 features or more). Moreover, acting on parameter space, MERT is only a local optimization algorithm. In this paper, we investigate for the first time the use of metaheuristics and global optimization techniques for the problem of learning parameters in SMT. In particular, We replace MERT with the well-known meta-heuristics for global optimization called CovarianceMatrixAdaptation Evolution Strategy (CMAES) [2]. We test the effectiveness of CMA-ES by conducting SMT experiments on an English-Vietnamese corpus. The results show that the improved SMT system using CMA-ES achieved superior BLEU scores compared to the baseline SMT system using MERT both on the dev and test data sets.",141,https://doi.org/10.1007/978-3-319-11680-8_34,1,
2369,Vietnamese Text Accent Restoration with Statistical Machine Translation.,"Vietnamese accentless texts exist on parallel with official vietnamese documents and
play an important role in instant message,
mobile SMS and online searching. Understanding correctly these texts is not simple because of the lexical ambiguity caused
by the diversity in adding diacritics to a
given accentless sequence. There have been
some methods for solving the vietnamese
accentless texts problem known as accent
prediction and they have obtained promising results. Those methods are usually based
on distance matching, n-gram, dictionary of
words and phrases and heuristic techniques.
In this paper, we propose a new method
solving the accent prediction. Our method
combine the strength of previous methods
(combining n-gram method and phrase dictionary in general). This method considers
the accent predicting as statistical machine
translation (SMT) problem with source language as accentless texts and target language
as accent texts, respectively. We also improve quality of accent predicting by applying some techniques such as adding dictionary, changing order of language model and
tuning. The achieved result and the ability
to enhance proposed system are obviously
promising",141,https://aclanthology.org/Y13-1044/,1,
2370,Improving Statistical Machine Translation with Processing Shallow Parsing.,"Reordering is of essential importance for
phrase based statistical machine translation
(SMT). In this paper, we would like to
present a new method of reordering in phrase
based SMT. We inspired from (Xia and McCord, 2004) using preprocessing reordering approaches. We used shallow parsing and transformation rules to reorder the
source sentence. The experiment results
from English-Vietnamese pair showed that our
approach achieves significant improvements
over MOSES which is the state-of-the art
phrase based system.",141,https://aclanthology.org/Y12-1043/,1,
2371,Shallow Syntactic Preprocessing for Statistical Machine Translation.,"Reordering is of essential importance for phrase based statistical machine translation. In this paper, we would like to present a new method of reordering in phrase based statistical machine translation. We inspired from [1] using preprocessing reordering approaches. We used shallow parsing and transformation rules to reorder the source sentence. The experiment results from English-Vietnamese pair showed that our approach achieves significant improvements over MOSES which is the state-of-the art phrase based system.",141,https://doi.org/10.1007/978-3-642-33983-7_11,1,
2372,Improving a Lexicalized Hierarchical Reordering Model Using Maximum Entropy.,"In this paper, we present a reordering model
based on Maximum Entropy. This model
is extended from a hierarchical reordering
model with PBSMT (Galley and Manning,
2008), which integrates syntactic information
directly in decoder as features of MaxEnt
model. The advantages of this model are (1)
maintaining the strength of phrase based approach with a hierarchical reordering model,
(2) many kinds of linguistic information integrated in PBSMT as arbitrary features of MaxEntropy model. The experiment results with
English-Vietnamese pair showed that our approach achieves improvements over the system which use a lexical hierarchical reordering model (Galley and Manning, 2008).",141,https://aclanthology.org/2009.mtsummit-papers.10,1,
2373,A Tree-to-String Phrase-based Model for Statistical Machine Translation.,"Though phrase-based SMT has achieved high
translation quality, it still lacks of generalization ability to capture word order differences
between languages. In this paper we describe
a general method for tree-to-string phrasebased SMT. We study how syntactic transformation is incorporated into phrase-based
SMT and its effectiveness. We design syntactic transformation models using unlexicalized
form of synchronous context-free grammars.
These models can be learned from sourceparsed bitext. Our system can naturally make
use of both constituent and non-constituent
phrasal translations in the decoding phase. We
considered various levels of syntactic analysis ranging from chunking to full parsing.
Our experimental results of English-Japanese
and English-Vietnamese translation showed
a significant improvement over two baseline
phrase-based SMT systems.",141,https://aclanthology.org/W08-2119/,1,
2374,Reordering pPhrase-based machine translation over chunks.,"The paper presents a new method for reordering in phrase based statistical machine translation (PBMT). Our method is based on previous chunk-level reordering methods for PBMT. First, we parse the source language sentence to a chunk tree, according to the method developed by [16]. Second, we apply a series of transformation rules which are learnt automatically from the parallel corpus to the chunk tree over chunk level. Finally, we integrate a global reordering model directly in a decoder as a graph of phrases, and solve the overlapping phrase and chunk problem. The experimental results with English-Vietnamese pairs show that our method outperforms the baseline PBMT in both accuracy and speed.",141,https://doi.org/10.1109/RIVF.2008.4586342,1,
2375,A Syntactic Transformation Model for Statistical Machine Translation.,"We describe a syntactic transformation model based on the probabilistic context-free grammar. This model is trained by using bilingual corpus and a broad coverage parser of the source language. Then we present two methods to solve the word-order problem using the transformational model. The first method deals with this problem in the preprocessing phase. There is no reordering in the decoding phase. The second method employs the syntactic transformation model in the decoding phase for phrase reordering within chunks. Speed is an advantage of this method. We considered translation from English to Vietnamese and from English to French. Our experiments showed significant BLEU-score improvements in comparison with Pharaoh, a state-of-the-art phrase-based SMT system.",141,https://doi.org/10.1142/S0219427907001603,1,
2376,THANOS: The Aspect Classification Model for Imbalanced Vietnamese E-commerce Review Data.,"This article introduces several methods for aspect classification using machine learning models for data collected from customers' reviews on two e-commerce sites and mainly focuses on handling data imbalance to improve classifier performance. To this end, we describe the problem as a binary classifier at the sentence level. Sentences will be expressed as feature vectors using One-hot combined with Chi-square statistics, and we will use basic machine learning models such as Naive Bayes, SVM, Random Forest, Linear Regression for training before classifying the aspects included in. We tested the strategy on a dataset generated in Vietnamese based on the comments from users who purchased products on e-commerce sites. The experimental results suggest that the method produces better outcomes than before the imbalance handling.",142,https://doi.org/10.1109/IALP54817.2021.9675163,1,
2377,Aspect-Based Sentiment Analysis Using Mini-Window Locating Attention for Vietnamese E-commerce Reviews.,"This article illustrates a system developed to tackle Aspect-based sentiment classification for Vietnamese E-commerce reviews. We employ supervised learning models based on Deep Learning application and multiple classic classifiers such as Random Forest, Decision Tree, Support Vector Machine, etc. to sort out the model performs best with our dataset. Our method obtained the maximum Micro-Average and Macro-Average Performance of 95%. Furthermore, we present how our Vietnamese manually-annotated multi-aspect dataset in two domains: Technology and Mother & Baby was prepared.",142,https://doi.org/10.1109/KSE53942.2021.9648637,1,
2378,Identifying Micro-influencers on Social Media using User Graph Construction Approach.,"Nowadays Influencers are responsible for setting trends and influencing public opinion on social media. As a result, identifying and uncovering influencers can assist organizations in having a good impact on their users. We investigated influencer recognition techniques utilizing a graph method in this research, with the goal of categorizing influencers into different sectors. The purpose of this paper is to concentrate the report on creating a graph in the social networking platform Facebook in order to locate influencers. A collection of edges and vertices represent this graph. To discover actual influencers, we create graphs and compare the application of two graph ranking algorithms, PageRank and HITS.",142,https://doi.org/10.1109/KSE53942.2021.9648780,1,
2379,A Hybrid Multi-answer Summarization Model for the Biomedical Question-Answering System.,"In natural language processing problems, text summarization is a difficult problem and always attracts attention from the research community, especially working on biomedical text data which lacks supporting tools and techniques. In this scientific research report, we propose a multi-document summarization model for the responses in the biomedical question and answer system. Our model includes components which is a combination of many advanced techniques as well as some improved methods proposed by authors. We present research methods applied to two main approaches: an extractive summarization architecture based on multi scores and state-of-the-art techniques, presenting our novel prosper-thy-neighbor strategies to improve performance; EAHS model (Extractive-Abstractive hybrid model) based on a denoising auto-encoder for pre-training sequence-to-sequence models (BART). In which we propose a question-driven filtering phase to optimize the selection of the most useful information. Our propose model has achieved positive results with the best ROUGE-1/ROUGE-L scores being the runner-up by ROUGE-2 F1 score by extractive summarization results (over 24 participated teams in MEDIQA2021).",142,https://doi.org/10.1109/KSE53942.2021.9648640,1,
2380,Attention-Based Deep Learning Model for Aspect Classification on Vietnamese E-commerce Data.,"This article introduces methods for applying Deep Learning in identifying aspects from written commentaries on Shopee e-commerce sites. The used datasets are two sets of Vietnamese consumers' comments about purchased products in two domains. Words and sentences will be performed as vectors, or characteristic matrices through language models such as one-hot, fastText, PhoBERT. We then used Convolutional Neural Network (CNN) and the Fully Connected Neural Network (Multilayer perceptron - MLP) to learn the aspects which are mentioned in the comments. Experimental results showed that our team's methods achieved much better results than traditional learning algorithm using other word-level vectors such as SVM, Naïve Bayes, etc.",142,https://doi.org/10.1109/KSE53942.2021.9648690,1,
2381,A Method for Building a Labeled Named Entity Recognition Corpus Using Ontologies.,"Building a labeled corpus which contains sufficient data and good coverage along with solving the problems of cost, effort and time is a popular research topic in natural language processing. The problem of constructing automatic or semi-automatic training data has become a matter of the research community. For this reason, we consider the problem of building a corpus in phenotype entity recognition problem, class-specific feature detectors from unlabeled data based on over 10260 unique terms (more than 15000 synonyms) describing human phenotypic features in the Human Phenotype Ontology (HPO) and about 9000 unique terms (about 24000 synonyms) of mouse abnormal phenotype descriptions in the Mammalian Phenotype Ontology. This corpus evaluated on three corpora: Khordad corpus, Phenominer 2012 and Phenominer 2013 corpora with Maximum Entropy and Beam Search method. The performance is good for three corpora, with F-scores of 31.71% and 35.77% for Phenominer 2012 corpus and Phenominer 2013 corpus; 78.36% for Khordad corpus.",142,https://doi.org/10.1007/978-3-319-17996-4_13,1,
2382,Exploring a Probabilistic Earley Parser for Event Composition in Biomedical Texts.,"We describe a high precision system for extracting events of biomedical significance that
was developed during the BioNLP shared task
2013 and tested on the Cancer Genetics data
set. The system achieved an F-score on the development data of 73.67 but was ranked 5th out
of six with an F-score of 29.94 on the test data.
However, precision was the second highest
ranked on the task at 62.73. Analysis suggests
the need to continue to improve our system for
complex events particularly taking into account cross-domain differences in argument
distributions. ",142,https://aclanthology.org/W13-2019/,1,
2383,Co-reference Resolution in Vietnamese Documents Based on Support Vector Machines.,"Co-reference resolution task still poses many challenges due to the complexity of the Vietnamese language, and the lack of standard Vietnamese linguistic resources. Based on the mention-pair model of Rahman and Ng. (2009) and the characteristics of Vietnamese, this paper proposes a model using support vector machines (SVM) to solve the co-reference in Vietnamese documents. The corpus used in experiments to evaluate the proposed model was constructed from 200 articles in cultural and social categories from vnexpress.net newspaper website. The results of the initial experiments of the proposed model achieved 76.51% accuracy in comparison with that of the baseline model of 73.79% with similar features.",142,https://doi.org/10.1109/IALP.2011.63,1,
2384,Method for Improving Quality of Adversarial Examples.,"To evaluate the robustness of DNNs, most of the adversarial methods such as FGSM, box-constrained L-BFGS, and ATN generate adversarial examples with small Lp-norm. However, these adversarial examples might contain many redundant perturbations. Removing these perturbations increases the quality of adversarial examples. Therefore, this paper proposes a method to improve the quality of adversarial examples by recognizing and then removing such perturbations. The proposed method includes two phases namely the autoencoder training phase and the improvement phase. In the autoencoder training phase, the proposed method trains an autoencoder that learns how to recognize redundant perturbations. In the second phase, the proposed method uses the trained autoencoder in combination with the greedy improvement step to produce more high-quality adversarial examples. The experiments on MNIST and CIFAR-10 have shown that the proposed method could improve the quality of adversarial examples significa",143,https://doi.org/10.5220/0010814400003116,1,
2385,"A survey on adverse drug reaction studies: data, tasks and machine learning methods.","Adverse drug reaction (ADR) or drug side effect studies play a crucial role in drug discovery. Recently, with the rapid increase of both clinical and non-clinical data, machine learning methods have emerged as prominent tools to support analyzing and predicting ADRs. Nonetheless, there are still remaining challenges in ADR studies.",143,https://doi.org/10.1093/bib/bbz140,1,
2386,Boosting prior knowledge in streaming variational Bayes.,"Exploiting prior/human knowledge is an effective way to enhance Bayesian models, especially in cases of sparse or noisy data, for which building an entirely new model is not always possible. There is a lack of studies on the effect of external prior knowledge in streaming environments, where the data come sequentially and infinitely. In this work, we show the problem of vanishing prior knowledge in streaming variational Bayes. This is a serious drawback in various applications. We then develop a simple framework to boost the external prior when learning a Bayesian model from data streams. By boosting, the prior knowledge can be maintained and efficiently exploited through each minibatch of streaming data. We evaluate the performance of our framework in four scenarios: streaming in synthetic data, streaming sentiment analysis, streaming learning for latent Dirichlet allocation, and streaming text classification, in comparison with the methods that do not keep priors. From extensive experiments, we find that when provided good external knowledge, our framework can improve the performance of a Bayesian model, often by a significant margin for noisy and short text streams.",143,https://doi.org/10.1016/j.neucom.2020.10.026,1,
2387,Improved Automated CASH Optimization with Tree Parzen Estimators for Class Imbalance Problems.,"The imbalanced classification problem is very relevant in both academic and industrial applications. The task of finding the best machine learning model to use for a specific imbalanced dataset is complicated due to a large number of existing algorithms, each with its own hyperparameters. The Combined Algorithm Selection and Hyperparameter optimization (CASH) has been introduced to tackle both aspects at the same time. However, CASH has not been studied in detail in the class imbalance domain, where the best combination of resampling technique and classification algorithm is searched for, together with their optimized hyperparameters. Thus, we target the CASH problem for imbalanced classification. We experiment with a search space of 5 classification algorithms, 21 resampling approaches and 64 relevant hyperparameters in total. Moreover, we investigate performance of 2 well-known optimization approaches: Random search and Tree Parzen Estimators approach which is a kind of Bayesian optimization. For comparison, we also perform grid search on all combinations of resampling techniques and classification algorithms with their default hyperparameters. Our experimental results show that a Bayesian optimization approach outperforms the other approaches for CASH in this application domain.",143,https://doi.org/10.1109/DSAA53316.2021.9564147,1,
2388,An Efficient Approach for SIMO Systems using Adaptive Fuzzy Hierarchical Sliding Mode Control.,"The paper addresses the problem of efficiently controlling a class of single input multiple output (SIMO) under-actuated robotic systems such as a two dimensional inverted pendulum cart or a two dimensional overhead crane. It is first proposed to employ the hierarchical sliding mode control approach to design a control law, which guarantees stability and anti-swing of the vehicle when it is driven on a predefined trajectory. More importantly, the unknown and uncertain parameters of the system caused by its actuator nonlinearity and external disturbances are adaptively estimated and inferred by the proposed fuzzy logic mechanism, which results in the efficient operation of the SIMO under-actuated system in real time. The proposed algorithm was then implemented in the synthetic environment, where the obtained results demonstrate its effectiveness.",143,https://doi.org/10.1109/ICARSC52212.2021.9429793,1,
2389,Adaptive Dynamic Programming based Control Scheme for Uncertain Two-Wheel Robots.,"The paper addresses the problem of effectively controlling a two-wheel robot given its inherent non-linearity and parameter uncertainties. In order to deal with the unknown and uncertain dynamics of the robot, it is proposed to employ the adaptive dynamic programming, a reinforcement learning based technique, to develop an optimal control law. It is interesting that the proposed algorithm does not require kinematic parameters while finding the optimal state controller is guaranteed. Moreover, convergence of the optimal control scheme is theoretically proved. The proposed approach was implemented in a synthetic two-wheel robot where the obtained results demonstrate its effectiveness.",143,https://doi.org/10.1109/ICARSC52212.2021.9429805,1,
2390,Adaptive Fuzzy Backstepping Control for Ballbot Segway.,"This paper proposes an adaptive fuzzy backstepping control (AFBSC) for a Segway ball system. AFBSC is developed through a combination of two primary backstepping controllers (BSC) to enhance the control performance of the system, in which a Sugeno fuzzy model adjusts the parameters in the control design. The simulation results verify that the better performance of the AFBSC in comparison with the conventional BSC technique.",143,https://doi.org/10.1109/ICSSE52999.2021.9538496,1,
2391,Automated Test Data Generation for Typescript Web Applications.,"This research proposes a Pattern-based Unit Testing method, namely PUT, to generate test data automatically for Typescript web applications. The main idea is to analyze the internal structure of source code to detect patterns of attribute usages. These patterns are then used to generate test data directly. An implemented tool and experimental results are presented to demonstrate the effectiveness of PUT in comparison with the automated random method. It shows that PUT could increase function coverage by 3.57 %-23.81 %, statement coverage by 0.5%-27.41 %, and branch coverage by 3.3%-39.24%. Therefore, PUT could provide potential usefulness for automated unit testing of Typescript web applications in practice.",143,https://doi.org/10.1109/KSE53942.2021.9648782,1,
2392,An Autoencoder-based Method for Targeted Attack on Deep Neural Network Models.,"This paper presents an autoencoder-based method for a targeted attack on deep neural network models, named AE4DNN. The proposed method aims to improve the existing targeted attacks in terms of their generalization, transferability, and the trade-off between the quality of adversarial examples and the computational cost. The idea of AE4DNN is that an autoencoder model is trained from a balanced subset of the training set. The trained autoencoder model is then used to generate adversarial examples from the remaining subset of the training set, produce adversarial examples from new samples, and attack other DNN models. To demonstrate the effectiveness of AE4DNN, the compared methods are box-constrained L-BFGS, Carlini-Wagner ‖L‖ 2 attack, and AAE. The comprehensive experiment on MNIST has shown that AE4DNN can gain a better transferability, improve generalization, and generate high quality of adversarial examples while requiring a low cost of computation. This initial result demonstrates the potential ability of AE4DNN in practice, which would help to reduce the effort of testing deep neural network models.",143,https://doi.org/10.1109/RIVF51545.2021.9642102,1,
2393,Efficient AutoML via Combinational Sampling.,"Automated machine learning (AutoML) aims to automatically produce the best machine learning pipeline, i.e., a sequence of operators and their optimized hyperparameter settings, to maximize the performance of an arbitrary machine learning problem. Typically, AutoML based Bayesian optimization (BO) approaches convert the AutoML optimization problem into a Hyperparameter Optimization (HPO) problem, where the choice of algorithms is modeled as an additional categorical hyperparameter. In this way, algorithms and their local hyperparameters are referred to as the same level. Consequently, this approach makes the resulting initial sampling less robust. In this study, we describe a first attempt to formulate the AutoML optimization problem as its nature instead of transfer it into a HPO problem. To take advantage of this paradigm, we propose a novel initial sampling approach to maximize the coverage of the AutoML search space to help BO construct a robust surrogate model. We experiment with 2 independent scenarios of AutoML with 2 operators and 6 operators over 117 benchmark datasets. Results of our experiments demonstrate that the performance of BO significantly improved by using our sampling approach.",143,https://doi.org/10.1109/SSCI50451.2021.9660073,1,
2394,Cartoon Explanations of Image Classifiers.,"We present {CartoonX} (Cartoon Explanation), a novel model-agnostic explanation method tailored towards image classifiers and based on the rate-distortion explanation (RDE) framework. Natural images are roughly piece-wise smooth signals -- also called cartoon-like images -- and tend to be sparse in the wavelet domain. CartoonX is the first explanation method to exploit this by requiring its explanations to be sparse in the wavelet domain, thus extracting the {relevant piece-wise smooth} part of an image instead of relevant pixel-sparse regions. We demonstrate that CartoonX can reveal novel valuable explanatory information, particularly for misclassifications. Moreover, we show that CartoonX achieves a lower distortion with fewer coefficients than other state-of-the-art methods",143,https://arxiv.org/abs/2110.03485,1,
2395,A Rate-Distortion Framework for Explaining Black-box Model Decisions.,"We present the Rate-Distortion Explanation (RDE) framework, a mathematically well-founded method for explaining black-box model decisions. The framework is based on perturbations of the target input signal and applies to any differentiable pre-trained model such as neural networks. Our experiments demonstrate the framework's adaptability to diverse data modalities, particularly images, audio, and physical simulations of urban environments.",143,https://arxiv.org/abs/2110.08252,1,
2396,CentSmoothie: Central-Smoothing Hypergraph Neural Networks for Predicting Drug-Drug Interactions.,"Predicting drug-drug interactions (DDI) is the problem of predicting side effects (unwanted outcomes) of a pair of drugs using drug information and known side effects of many pairs. This problem can be formulated as predicting labels (i.e. side effects) for each pair of nodes in a DDI graph, of which nodes are drugs and edges are interacting drugs with known labels. State-of-the-art methods for this problem are graph neural networks (GNNs), which leverage neighborhood information in the graph to learn node representations. For DDI, however, there are many labels with complicated relationships due to the nature of side effects. Usual GNNs often fix labels as one-hot vectors that do not reflect label relationships and potentially do not obtain the highest performance in the difficult cases of infrequent labels. In this paper, we formulate DDI as a hypergraph where each hyperedge is a triple: two nodes for drugs and one node for a label. We then present CentSmoothie, a hypergraph neural network that learns representations of nodes and labels altogether with a novel central-smoothing formulation. We empirically demonstrate the performance advantages of CentSmoothie in simulations as well as real datasets.",143,https://arxiv.org/abs/2112.07837,1,
2397,Neural Poisson Factorization.,"Predicting drug-drug interactions (DDI) is the problem of predicting side effects (unwanted outcomes) of a pair of drugs using drug information and known side effects of many pairs. This problem can be formulated as predicting labels (i.e. side effects) for each pair of nodes in a DDI graph, of which nodes are drugs and edges are interacting drugs with known labels. State-of-the-art methods for this problem are graph neural networks (GNNs), which leverage neighborhood information in the graph to learn node representations. For DDI, however, there are many labels with complicated relationships due to the nature of side effects. Usual GNNs often fix labels as one-hot vectors that do not reflect label relationships and potentially do not obtain the highest performance in the difficult cases of infrequent labels. In this paper, we formulate DDI as a hypergraph where each hyperedge is a triple: two nodes for drugs and one node for a label. We then present CentSmoothie, a hypergraph neural network that learns representations of nodes and labels altogether with a novel central-smoothing formulation. We empirically demonstrate the performance advantages of CentSmoothie in simulations as well as real datasets.",143,https://doi.org/10.1109/ACCESS.2020.2994239,1,
2398,Coastal Wetland Classification with Deep U-Net Convolutional Networks and Sentinel-2 Imagery: A Case Study at the Tien Yen Estuary of Vietnam.,"The natural wetland areas in Vietnam, which are transition areas from inland and ocean, play a crucial role in minimizing coastal hazards; however, during the last two decades, about 64% of these areas have been converted from the natural wetland to the human-made wetland. It is anticipated that the conversion rate continues to increase due to economic development and urbanization. Therefore, monitoring and assessment of the wetland are essential for the coastal vulnerability assessment and geo-ecosystem management. The aim of this study is to propose and verify a new deep learning approach to interpret 9 of 19 coastal wetland types classified in the RAMSAR and MONRE systems for the Tien Yen estuary of Vietnam. Herein, a Resnet framework was integrated into the U-Net to optimize the performance of the proposed deep learning model. The Sentinel-2, ALOS-DEM, and NOAA-DEM satellite images were used as the input data, whereas the output is the predefined nine wetland types. As a result, two ResU-Net models using Adam and RMSprop optimizer functions show the accuracy higher than 85%, especially in forested intertidal wetlands, aquaculture ponds, and farm ponds. The better performance of these models was proved, compared to Random Forest and Support Vector Machine methods. After optimizing the ResU-Net models, they were also used to map the coastal wetland areas correctly in the northeastern part of Vietnam. The final model can potentially update new wetland types in the southern parts and islands in Vietnam towards wetland change monitoring in real time. ",143,https://doi.org/10.3390/rs12193270,1,
2399,Exploring Dimensionality Reduction Techniques for Efficient Surrogate-Assisted optimization.,"Constructing surrogate models of high dimensional optimization problems is challenging due to the computational complexity involved. This paper empirically investigates the practicality of major dimensionality reduction techniques for encapsulating the high dimensional design space into compact representations. Such low dimensional representations of the design space can be utilized for constructing the surrogate models efficiently. Based on historical mainstays and recent developments in deep learning, we study four dimensionality reduction techniques in this paper, namely Principal Component Analysis, Kernel Principal Component Analysis, Autoencoders and Variational Autoencoders. We evaluate and compare these techniques based on quality assessments of the corresponding low dimensional surrogate models on a diverse range of test cases. These test cases are defined on combinations of three dimensionsalities, ten well-known benchmark problems from the continuous optimization domain and two surrogate modeling techniques, namely Kriging and Polynomials. Our results clearly demonstrate the superiority of Autoencoders and Principal Component Analysis on the criteria of modeling accuracy and global optimality respectively.",143,https://doi.org/10.1109/SSCI47803.2020.9308465,1,
2400,Improvements of Directed Automated Random Testing in Test Data Generation for C++ Projects.,"This paper improves the breadth-first search strategy in directed automated random testing (DART) to generate a fewer number of test data while gaining higher branch coverage, namely Static DART or SDART for short. In addition, the paper extends the test data compilation mechanism in DART, which currently only supports the projects written in C, to generate test data for C++ projects. The main idea of SDART is when it is less likely to increase code coverage with the current path selection strategies, the static test data generation will be applied with the expectation that more branches are covered earlier. Furthermore, in order to extend the test data compilation of DART for C++ context, the paper suggests a general test driver technique for C++ which supports various types of parameters including basic types, arrays, pointers, and derived types. Currently, an experimental tool has been implemented based on the proposal in order to demonstrate its efficacy in practice. The results have shown that SDART achieves higher branch coverage with a fewer number of test data in comparison with that of DART in practice.",143,https://doi.org/10.1142/S0218194019500402,1,
2401,Measuring Ground Subsidence in Ha Noi Through the Radar Interferometry Technique Using TerraSAR-X and Cosmos SkyMed Data.,"Multitemporal synthetic aperture radar (SAR) interferometry (InSAR) is a widely used technique to measure the ground subsidence and has already shown its ability to map such phenomena on a large spatial scale with millimetric accuracy from space. In Vietnam, to have independent SAR data for surface risk applications, a new X-band SAR mission (JV-LOTUSat) has been scheduled for launch for the 2019-2020 timeframe. However, Vietnam is located in tropical regions where their conditions are impacted by strong atmosphere. The aim of this article is to provide a better understanding of the capabilities of the X-band for estimating the ground subsidence under tropical atmospheric conditions. Analysis is carried out on two stacks, TerraSAR-X and Cosmos SkyMed X-band, from 2011 to 2014 in Ha Noi. We show that the results on the ground subsidence from InSAR processing can describe consistently the subsidence area based on ground measurements. This article demonstrates that the InSAR technique can be effective at detecting and estimating the subsidence phenomena even with the X-band and under conditions typical of tropical regions. The displacement results from TerraSAR-X and Cosmos SkyMed datasets are consistent, with a correlation coefficient (R 2 ) of 0.91 for the period during which their coverage overlaps. Groundwater overexploitation is one of the main causes of the ground subsidence in Ha Noi. This study provides strong support for the scientific potential of the X-band SAR space-borne mission in Vietnam and other tropical countries because it demonstrates the feasibility of the ground subsidence estimates by the X-band SAR, even in conditions impacted by strong atmosphere.",143,https://doi.org/10.1109/JSTARS.2019.2937398,1,
2402,Hyperparameter Optimisation for Improving Classification under Class Imbalance.,"Although the class-imbalance classification problem has caught a huge amount of attention, hyperparameter optimisation has not been studied in detail in this field. Both classification algorithms and resampling techniques involve some hyperparameters that can be tuned. This paper sets up several experiments and draws the conclusion that, compared to using default hyperparameters, applying hyperparameter optimisation for both classification algorithms and resampling approaches can produce the best results for classifying the imbalanced datasets. Moreover, this paper shows that data complexity, especially the overlap between classes, has a big impact on the potential improvement that can be achieved through hyperparameter optimisation. Results of our experiments also indicate that using resampling techniques cannot improve the performance for some complex datasets, which further emphasizes the importance of analyzing data complexity before dealing with imbalanced datasets.",143,https://doi.org/10.1109/SSCI44817.2019.9002679,1,
2403,Highly Accurate Step Counting at Various Walking States Using Low-Cost Inertial Measurement Unit Support Indoor Positioning System.,"Accurate step counting is essential for indoor positioning, health monitoring systems, and other indoor positioning services. There are several publications and commercial applications in step counting. Nevertheless, over-counting, under-counting, and false walking problems are still encountered in these methods. In this paper, we propose to develop a highly accurate step counting method to solve these limitations by proposing four features: Minimal peak distance, minimal peak prominence, dynamic thresholding, and vibration elimination, and these features are adaptive with the user’s states. Our proposed features are combined with periodicity and similarity features to solve false walking problem. The proposed method shows a significant improvement of 99.42% and 96.47% of the average of accuracy in free walking and false walking problems, respectively, on our datasets. Furthermore, our proposed method also achieves the average accuracy of 97.04% on public datasets and better accuracy in comparison with three commercial step counting applications: Pedometer and Weight Loss Coach installed on Lenovo P780, Health apps in iPhone 5s (iOS 10.3.3), and S-health in Samsung Galaxy S5 (Android 6.01). ",143,https://doi.org/10.3390/s18103186,1,
2404,Behavior-based video recommendation using adaptive neuro-fuzzy system on social TV.,"User-based collaborative filtering often considers a set of users who rated on a target item and computes similarities between other users and the target user to select his/her neighbors, then extrapolates the target user’s rating from the neighbors’ ratings. This traditional approach uses only the neighbors’ ratings for recommendation measurement. However, according to our study, dissimilar users whose ratings still significantly influence to the target user’s rating prediction. In addition, to choose a video to watch, a user often takes in to consideration multi criteria. We analyze users’ behavior to choose a video. They often explore genres or tags, then read abstraction before choosing a video to watch. Therefore, their ratings and the information of a video have a strong correlation. Therefore, based on the fuzzy neural network, a new collaborative filtering method for video recommendation is proposed. Here, the fuzzy neural network is used to learn users’ ratings with respect to their behaviors. The proposal here is to adjust a model of the neural network with input is users’ behavior and output is their ratings for each target video. Concretely, the behavior of a user (or user profile) is learned by the users’ ratings and the information of the corresponding videos. In addition, for each target video, all users’ profile who made ratings on it will be collected. Then each profile is treated as an input of the fuzzy neural network and the corresponding rating value is treated as output of the fuzzy neural network. The rating of a user on the target video will be predicted based on the trained neural network. The experiments with netflix dataset reveals that the proposed method is a significantly effective approach.",143,https://doi.org/10.3233/JIFS-169155,1,
2405,Keeping Priors in Streaming Bayesian Learning.,"Exploiting prior knowledge in the Bayesian learning process is one way to improve the quality of Bayesian model. To the best of our knowledge, however, there is no formal research about the influence of prior in streaming environment. In this paper, we address the problem of using prior knowledge in streaming Bayesian learning, and develop a framework for keeping priors in streaming learning (KPS) that maintains knowledge from the prior through each minibatch of streaming data. We demonstrate the performance of our framework in two scenarios: streaming learning for latent Dirichlet allocation and streaming text classification in comparison with methods that do not keep prior.",143,https://doi.org/10.1007/978-3-319-57529-2_20,1,
2406,A Test Data Generation Method for C/C++ Projects.,"This research proposes an automated test data generation method for C/C++ projects to generate the lower number of test data while gaining higher code coverage in comparison with KLEE, CAUT, PathCrawler, and CREST. In order to do that, the proposed method contributes an algorithm named loop depth first search by combining both static testing and concolic testing together. Besides, the paper also provides an improvement symbolic execution for avoiding the initial test data problem in the concolic testing. Currently, a tool supporting the proposed method has been developed and applied to test on different C/C++ projects in several software companies. The experimental results show the higher coverage with the lower number of test data compared with the existing methods. The experimental results display the effectiveness and practical usefulness of the proposed method for automated test data generation in practice.",143,https://doi.org/10.1145/3155133.3155144,1,
2407,Dividing Agents on the Grid for Large Scale Simulation.,"Multi-agent based simulation is an important methodology that uses models incorporating agents to evaluate research conclusions. When the simulation involves a large number of agent, however, it requires extensively high computational power. In that case, all agents in the simulation model should be distributed in a way so that agents can be run in parallel on multiple computational nodes to gain the required performance speed up. In this paper, we present a framework for large scale multi-agent based simulation on grid. We have modified the desktop grid platform BOINC for multi-agent based simulation. Assuming that the agents interact locally with the environment, we proposed an approach to divide the agents for grid nodes so that we can keep load balancing for the distributed simulation while optimizing the communication between grid nodes and the grid server. We have implemented the food foraging simulation to evaluate the feasibility of the framework.",150,https://doi.org/10.1007/978-3-540-89674-6_25,1,
2408,Solving the N-Body Problem with the ALiCE Grid System.,"The grid enables large-scale aggregation and sharing of computational resources. In this paper, we introduce a method for solving the N-body problem on a cluster-grid using our grid system, ALiCE (Adaptive and scaLable internet-based Computing Engine). The modified Barnes-Hut algorithm allows the N-body problem to be solved adaptively using compute resources on-demand. The N-body program is written using ALiCE object programming template. Our experiments varying the number of bodies per task and the number of computation nodes demonstrate the feasibility of exploiting parallelism on a grid system.",150,https://doi.org/10.1007/3-540-36184-7_10,1,
2409,Enhancing Performance of Lexical Entailment Recognition for Vietnamese based on Exploiting Lexical Structure Features.,"The lexical entailment recognition problem aims to identify the is-a relation between words. The problem has recently been receiving research attention in the natural language processing field. In this study, we propose a novel method (VLER) for this problem on Vietnamese. For this purpose, we first exploit such lexical structure information of words as a feature, then combine this feature with vectors representation of words such as a unique feature for recognizing the relation. Moreover, we applied a number of methods based on word embedding and supervised learning, experimental results showed that our method achieves the best performance in the hypernymy detection task than other methods in terms of accuracy.",154,https://doi.org/10.1109/KSE.2018.8573391,1,
2410,Iterative Multilingual Neural Machine Translation for Less-Common and Zero-Resource Language Pairs.,"Research on providing machine translation
systems for unseen language pairs is gaining
increasing attention in recent years. However,
the quality of their systems is poor for most
language pairs, especially for less-common
pairs such as Khmer-Vietnamese. In this
paper, we show a simple iterative traininggenerating-filtering-training process that utilizes all available pivot parallel data to generate synthetic data for unseen directions.
In addition, we propose a filtering method
based on word alignments and the longest
parallel phrase to filter out noise sentence
pairs in the synthetic data. Experiment results on zero-shot Khmer→Vietnamese and
Indonesian→Vietnamese directions show that
our proposed model outperforms some strong
baselines and achieves a promising result
under the zero-resource condition on ALT
benchmarks. Besides, the results also indicate
that our model can easily improve their quality
with a small amount of real parallel data.",154,https://aclanthology.org/2020.paclic-1.24/,1,
2411,Enhancing the Quality of Phrase-Table in Statistical Machine Translation for Less-Common and Low-Resource Languages.,"The phrase-table plays an important role in traditional phrase-based statistical machine translation (SMT) system. During translation, a phrase-based SMT system relies heavily on phrase-table to generate outputs. In this paper, we propose two methods for enhancing the quality of phrase-table. The first method is to recompute phrase-table weights by using vector representations similarity. The remaining method is to enrich the phrase-table by integrating new phrase-pairs from an extended dictionary and projections of word vector presentations on the target-language space. Our methods produce an attainment of up to 0.21 and 0.44 BLEU scores on in-domain and cross-domain (Asian Language Treebank - ALT) English - Vietnamese datasets respectively.",154,https://doi.org/10.1109/IALP.2018.8629188,1,
2412,Verifying Java Object Invariants at Runtime.,"An object invariant consisting of a set of properties that must hold for all instances of a class at any time is usually used in object-oriented design. However, verifying object invariants at runtime is always a challenging task in software verification. This paper proposes a method for verifying invariants of Java objects at runtime using AOP. Suppose that a software application is designed using UML models and its constraints are specified in OCL expressions, the software is then implemented, by default, using the UML design. They propose to construct verifiable aspects which are automatically generated from OCL constraints. These aspects can be woven into Java code to check whether object invariants are violated at runtime. Benefiting from AOP in separation of crosscutting concerns and weaving mechanisms, generated aspects can do the verification task whenever values of objects' attributes are changed. A Verification Aspect Generator (VAG) tool has been developed allowing the automatic generation of verifying aspects from the UML/OCL constraints.",157,https://doi.org/10.1142/S0218194011005281,1,
2413,Web Search Clustering and Labeling with Hidden Topics.,"Web search clustering is a solution to reorganize search results (also called “snippets”) in a more convenient way for browsing. There are three key requirements for such post-retrieval clustering systems: (1) the clustering algorithm should group similar documents together; (2) clusters should be labeled with descriptive phrases; and (3) the clustering system should provide high-quality clustering without downloading the whole Web page.

This article introduces a novel framework for clustering Web search results in Vietnamese which targets the three above issues. The main motivation is that by enriching short snippets with hidden topics from huge resources of documents on the Internet, it is able to cluster and label such snippets effectively in a topic-oriented manner without concerning whole Web pages. Our approach is based on recent successful topic analysis models, such as Probabilistic-Latent Semantic Analysis, or Latent Dirichlet Allocation. The underlying idea of the framework is that we collect a very large external data collection called “universal dataset,” and then build a clustering system on both the original snippets and a rich set of hidden topics discovered from the universal data collection. This can be seen as a richer representation of snippets to be clustered. We carry out careful evaluation of our method and show that our method can yield impressive clustering quality.",157,https://doi.org/10.1145/1568292.1568295,1,
2414,Transformer-Based Approaches for Legal Text Processing.,"In this paper, we introduce our approaches using Transformer-based models for different problems of the COLIEE 2021 automatic legal text processing competition. Automated processing of legal documents is a challenging task because of the characteristics of legal documents as well as the limitation of the amount of data. With our detailed experiments, we found that Transformer-based pretrained language models can perform well with automated legal text-processing problems with appropriate approaches. We describe in detail the processing steps for each task such as problem formulation, data processing and augmentation, pretraining, finetuning. In addition, we introduce to the community two pretrained models that take advantage of parallel translations in legal domain, NFSP and NMSP. In which, NFSP achieves the state-of-the-art result in Task 5 of the competition. Although the paper focuses on technical reporting, the novelty of its approaches can also be an useful reference in automated legal document processing using Transformer-based models.",160,https://doi.org/10.1007/s12626-022-00102-2,1,
2415,Vietnamese Legal Question Answering with combined features and deep learning.,"Legal Question Answering is an arduous problem that is divided into certain phases, each with its own set of challenges. In this work, we have accomplished three tasks given by the ALQAC 2021 competition, which are aimed at addressing the aforementioned problem, by proposing the combined features (cosine similarity of TF-IDF, an average of word embedding; and Jaccard distance) accompanied by a classification model for task 1; ensemble learning multiple deep learning models for task 2. Finally, we employed a specifically modified mechanism for long documents to undertake task 3. All three methods perform satisfactory results and have profuse potential improvements.",160,https://doi.org/10.1109/KSE53942.2021.9648797,1,
2416,ParaLaw Nets - Cross-lingual Sentence-level Pretraining for Legal Text Processing.,"Ambiguity is a characteristic of natural language, which makes expression ideas flexible. However, in a domain that requires accurate statements, it becomes a barrier. Specifically, a single word can have many meanings and multiple words can have the same meaning. When translating a text into a foreign language, the translator needs to determine the exact meaning of each element in the original sentence to produce the correct translation sentence. From that observation, in this paper, we propose ParaLaw Nets, a pretrained model family using sentence-level cross-lingual information to reduce ambiguity and increase the performance in legal text processing. This approach achieved the best result in the Question Answering task of COLIEE-2021.",160,https://arxiv.org/abs/2106.13403,1,
2417,JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE 2021.,"COLIEE is an annual competition in automatic computerized legal text processing. Automatic legal document processing is an ambitious goal, and the structure and semantics of the law are often far more complex than everyday language. In this article, we survey and report our methods and experimental results in using deep learning in legal document processing. The results show the difficulties as well as potentials in this family of approaches.",160,https://arxiv.org/abs/2106.13405,1,
2418,Keyphrase generation for Vietnamese administrative documents: a collaborative approach.,"Keyphrases of a given document can be considered as its condensed summary. Unsupervised models focus on extracting keyphrases based only on the information contained in that document without interacting with other documents. While a good performance supervised learning model for keyphrase generation requires a massive effort to build training data, which can not generalize to new domains. Moreover, according to human perception, a user would comprehend the topic expressed in a document better if that user has already read other documents that express the same topic. Based on the above idea, we proposed a collaborative keyphrase generation system (CollabKG): a novel semi-supervised method by leveraging limited labeled data. The amount of labeled data will be enriched over time by the user. In our work, we conduct research on a large scale dataset consisting of 500,000 Vietnamese administrative documents. In CollabKG, each document is represented as a feature vector, and a cluster pruning algorithm is employed to accelerate finding the most similar documents. The generated keyphrases were manually evaluated for relevance and accuracy. In the final, the result we achieved shows high ratification. Therefore, we can conclude that CollabKG has good performance and fits a real-time system.",160,https://doi.org/10.1109/KSE50997.2020.9287477,1,
2419,Learning to Transform Vietnamese Natural Language Queries into SQL Commands.,"In the field of data management, users traditionally manipulates their data using structured query language (SQL). However, this method requires an understanding of relational database, data schema, and SQL syntax as well as the way it works. Database manipulation using natural language, therefore, is much more convenient since any normal user can interact with their data without a background of database and SQL. This is, however, really tough because transforming natural language commands into SQL queries is a challenging task in natural language processing and understanding. In this paper, we propose a novel two-phase approach to automatically analyzing and converting natural language queries into the corresponding SQL forms. In our approach, the first phase is component segmentation which identifies primary clauses in SQL such as SELECT, FROM, WHERE, ORDER BY, etc. The second phase is slot- filling that helps extract sub-components for each primary clause such as SELECT column(s), SELECT aggregation operation, etc. We carefully conducted an empirical evaluation for our method using conditional random fields (CRFs) on a medium-sized corpus of natural language queries in Vietnamese, and have achieved promising results with an average accuracy of more than 90%.",160,https://doi.org/10.1109/KSE.2019.8919393,1,
2420,Identifying User Intents in Vietnamese Spoken Language Commands and Its Application in Smart Mobile Voice Interaction.,"This paper presents a lightweight machine learning model and a fast conjunction matching method to the problem of identifying user intents behind their spoken text commands. These model and method were integrated into a mobile virtual assistant for Vietnamese (VAV) to understand what mobile users mean to carry out on their smartphones via their commands. User intent, in the scope of our work, is an action associated with a particular mobile application. Given an input spoken command, its application will be identified by an accurate classifier while the action will be determined by a flexible conjunction matching algorithm. Our classifier and conjunction matcher are very compact in order that we can store and execute them right on mobile devices. To evaluate the classifier and the matcher, we annotated a medium-sized data set, conducting various experiments with different settings, and achieving impressive accuracy for both the application and action identification.",160,https://doi.org/10.1007/978-3-662-49381-6_19,1,
2421,Optimizing GLCNMO version 2 method to detect Vietnam's urban expansion.,"No global scale land cover classification method performs with high accuracy at local scale. This study tries to develop a classification algorithm for urban area in Vietnam. This is the first assessment of the Global Land Cover by National Mapping Organizations (GLCNMO) version 2 method for producing global urban map in 2008. An improved and optimized algorithm is then developed based on the GLCNMO method for Vietnam taking into account of local natural and social conditions. Improving method is then applied to produce urban maps of Vietnam for the years of 2008 and 2015. Accuracy assessment showed that the improved method can achieve up to 13% higher precision and 10% higher F1 measure as compared to the global GNCNMO method. Also, an increasing trend was observed in population density in urban area in the period from 2008 to 2015 in Vietnam which may correspond to fast urbanization process in the country. The cities also tend to become less green in 2015 than 2008 as indicated by comparing the Normalized Difference Vegetation Index (NDVI) between the two years",165,https://doi.org/10.1109/KSE.2016.7758072,1,
2422,Improving Graph Convolutional Networks with Transformer Layer in social-based items recommendation.,"With the emergence of online social networks, social-based items recommendation has become a popular research direction. Recently, Graph Convolutional Networks have shown promising results by modeling the information diffusion process in graphs. It provides a unified framework for graph embedding that can leverage both the social graph structure and node features information. In this paper, we improve the embedding output of the graph-based convolution layer by adding a number of transformer layers. The transformer layers with attention architecture help discover frequent patterns in the embedding space which increase the predictive power of the model in the downstream tasks. Our approach is tested on two social-based items recommendation datasets, Ciao and Epinions and our model outperforms other graph-based recommendation baselines.",165,https://doi.org/10.1109/KSE53942.2021.9648823,1,
2423,Exploring Efficiency of GAN-based Generated URLs for Phishing URL Detection.,"The URL (Uniform Resource Locator) is used to refer to the resources on the Internet by giving hyperlinks to the websites. Different resources are referenced by different network addresses or different URLs. As a result, embedding malware on websites by using malicious URLs is one of the most dangerous types of cyberattacks today and poses a serious threats to the safety of systems. In order to detect the phishing URLs, the most commonly used approach recently is using deep learning networks with a large number of URL samples, including both malign and benign ones for training the deep networks. However, the available URL databases have a modest number of samples. In addition, the disadvantage of these databases is the imbalance distribution of malicious and non-malicious URL strings. In fact, it is difficult to collect or update malicious URLs because these URLs only exist for a short time, after being detected they are changed again and again. In order to solve this challenge, in this work, we propose to train a GAN network named WGAN-GP for generating malicious URLs from the available phishing URL data. We then integrate the generated phishing URL data into the existing URL database and perform two URL classifiers of LSTM and GRU to give the comparative results. The experiments on different quantities of URL samples show the improvement for URL classification by using WGAN-GP and LSTM classifier.",165,https://doi.org/10.1109/MAPR53640.2021.9585287,1,
2424,Reducing Blocking Artifacts in CNN-Based Image Steganography by Additional Loss Functions.,"Our work improves the encoded image quality from HiDDeN framework, an end-to-end image steganography based on deep convolution neural network. In the encoding phase of HiDDeN framework, to embed a message in a cover image, it is required to split the cover image into smaller image blocks and embed the message bits in each block in parallel. These embedded blocks are then combined to form an encoded image that has the same size as the cover image. This image reconstruction process causes artifacts that appear on the boundaries of the blocks. This can be explained by the fact that when message bits are embedded in the image blocks, the pixel-level information of each image block is unequally alternated. In order to reduce block artifacts, in this work we propose a blocking loss as an additional objective function in HiDDeN framework. This loss measures the difference between encoded images and modified versions of the cover images. The proposed method is evaluated on COCO 2014 and BOSS datasets and the experimental results show the effectiveness in reducing the block artifacts that appeared in the encoded images of HiDDeN framework. This has an important impact on increasing the invisibility or transparency of the steganography system. In addition, the experimental result on secrecy of the proposed method also indicates the same performance as the HiDDeN pipeline.",165,https://doi.org/10.1109/KSE50997.2020.9287408,1,
2425,Vision Memory for Target Object Navigation Using Deep Reinforcement Learning: An Empirical Study.,"Recently, a number of methods have been conducted by combining deep neural network and reinforcement learning to solve problems. Neural networks have strong ability to deal with high dimensional data and a good mean to learn features while reinforcement learning allows a system to learn by experience optimal action control and adjust its behavior to new environments. It thus may be used in mobile robot navigation tasks with perceptron as deep neural network and learning to control by reinforcement learning. We first investigate some issues in agents learning to navigate in indoor environments such as how robot memorize vision information and how it discovers the environments. We show that agents can rely on some position with vision information in navigation task such as reduce training time and locating object efficiently. Some approaches based on deep reinforcement learning are step by step discussed and proposed to deal with each problem in target object navigation task. The key ideas include adding checking point as vision memory and using auxiliary learning task to enhance agents to discover the environment.",166,https://doi.org/10.1109/SMC.2018.00553,1,
2426,ECHONET Lite-based IoT Platform for Smart Homes.,"In this paper, we introduce an ECHONET Lite based IoT platform which allows households to set up smart home environment with ease as well as allows service providers to deploy their IoT applications easily and securely. Our proposed platform is fully implemented with three main features: a simple process for integrating new IoT devices into smart home environment, a seamless data transfer mechanism between IoT devices and service providers, and a RESTful API for verifying and supporting service providers to deploy their IoT applications into our platform. The experimental results verified the practicability of our proposed IoT platform.",169,https://doi.org/10.1109/RIVF48685.2020.9140736,1,
2427,Adaptive Smart Lighting Control based on Genetic Algorithm.,"Smart lighting system plays an important role in smart homes because of its convenience comparing with boring and tedious on/off switches. With conventional smart lighting systems, users can easily change the lighting level of a room with a smart phone or voice control device. However, design an automated lighting system with energy saving is still a challenge. In this paper, we propose an adaptive smart lighting system using a control algorithm based on Genetic Algorithm (GA). Our system turns on and off lights that match the user's desire brightness and utilizes nature illuminance efficiently to save as much electric energy as possible. The control algorithm based on GA provides the optimized light turning pattern to the system and can adapt with the change of lighting environment and user requirement. Our evaluation results showed that the execution time of our algorithm is short enough to be used in real environment.",169,https://doi.org/10.1109/APCC47188.2019.9026473,1,
2430,Generate Test Data from C/C++ Source Code using Weighted CFG and Boundary Values,"This paper presents two test data automatic generation methods which are based on weighted control flow graph (named WCFT) and boundary values of input parameters (named BVTG). Firstly, WCFT method generates a CFG from a given unit function, updates weight for it, then generates test data from the greatest weight test paths. In the meantime, WCFT can And dead code that can be used for automatic source code errors fix. Secondly, BVTG method generates test data from boundary values of input parameters of the given unit function. The combination of the two generated test data sets from these two methods will improve the error detection ability while maintaining a high code coverage. An implemented tool (named WCFT4Cpp) and experimental results are also presented to show the effectiveness of the two proposed methods in both time required to generate test data and error detection ability.",173,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=S7lKdWYAAAAJ&citation_for_view=S7lKdWYAAAAJ:2osOgNQ5qMEC,,1
2431,Improving Diversity and Quality of Adversarial Examples in Adversarial Transformation Network,"This paper proposes a method to mitigate two major issues of Adversarial Transformation Networks (ATN) including the low diversity and the low quality of adversarial examples. In order to deal with the first issue, this research proposes a stacked convolutional autoencoder based on pattern to generalize ATN. This proposed autoencoder could support different patterns such as all-feature pattern, border feature pattern, and class model map pattern. In order to deal with the second issue, this paper presents an algorithm to improve the quality of adversarial examples in terms of L0-norm and L2-norm. This algorithm employs an adversarial feature ranking heuristics such as JSMA and COI to prioritize adversarial features. To demonstrate the advantages of the proposed method, comprehensive experiments have been conducted on the MNIST dataset and the CIFAR-10 dataset. For the first issue, the proposed autoencoder can generate diverse adversarial examples with the average success rate above 99%. For the second issue, the proposed algorithm could not only improve the quality of adversarial examples significantly but also maintain the average success rate. In terms of L0-norm, the proposed algorithm could decrease from hundreds of adversarial features to one adversarial feature. In terms of L2-norm, the proposed algorithm could reduce the average distance considerably. These results show that the proposed method is capable of generating high-quality and diverse adversarial examples in practice.",173,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=S7lKdWYAAAAJ&citation_for_view=S7lKdWYAAAAJ:IjCSPb-OGe4C,,1
2432,Method for Improving Quality of Adversarial Examples,"To evaluate the robustness of DNNs, most of the adversarial methods such as FGSM, box-constrained LBFGS, and ATN generate adversarial examples with small Lp-norm. However, these adversarial examples might contain many redundant perturbations. Removing these perturbations increases the quality of adversarial examples. Therefore, this paper proposes a method to improve the quality of adversarial examples by recognizing and then removing such perturbations. The proposed method includes two phases namely the autoencoder training phase and the improvement phase. In the autoencoder training phase, the proposed method trains an autoencoder that learns how to recognize redundant perturbations. In the second phase, the proposed method uses the trained autoencoder in combination with the greedy improvement step to produce more high-quality adversarial examples. The experiments on MNIST and CIFAR-10 have shown that the proposed method could improve the quality of adversarial examples significantly. In terms of L0-norm, the distance decreases by about 82%-95%. In terms of L2-norm, the distance drops by around 56%-81%. Additionally, the proposed method has a low computational cost. This shows the potential ability of the proposed method in practice.",173,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=S7lKdWYAAAAJ&citation_for_view=S7lKdWYAAAAJ:zYLM7Y9cAGgC,,1
2433,An Autoencoder-based Method for Targeted Attack on Deep Neural Network Models,"This paper presents an autoencoder-based method for a targeted attack on deep neural network models, named AE4DNN. The proposed method aims to improve the existing targeted attacks in terms of their generalization, transferability, and the trade-off between the quality of adversarial examples and the computational cost. The idea of AE4DNN is that an autoencoder model is trained from a balanced subset of the training set. The trained autoencoder model is then used to generate adversarial examples from the remaining subset of the training set, produce adversarial examples from new samples, and attack other DNN models. To demonstrate the effectiveness of AE4DNN, the compared methods are box-constrained L-BFGS, Carlini-Wagner ‖L‖ 2  attack, and AAE. The comprehensive experiment on MNIST has shown that AE4DNN can gain a better transferability, improve generalization, and generate high …",173,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=S7lKdWYAAAAJ&citation_for_view=S7lKdWYAAAAJ:UeHWp8X0CEIC,,1
2434,A novel framework to classify malware in mips architecture-based iot devices,"Malware on devices connected to the Internet via the Internet of Things (IoT) is evolving and is a core component of the fourth industrial revolution. IoT devices use the MIPS architecture with a large proportion running on embedded Linux operating systems, but the automatic analysis of IoT malware has not been resolved. We proposed a framework to classify malware in IoT devices by using MIPS-based system behavior (system call—syscall) obtained from our F-Sandbox passive process and machine learning techniques. The F-Sandbox is a new type for IoT sandbox, automatically created from the real firmware of the specialized IoT devices, inheriting the specialized environment in the real firmware, therefore creating a diverse environment for sandboxing as an important characteristic of IoT sandbox. This framework classifies five families of IoT malware with F1-Weight = 97.44%.",174,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HtL1XDkAAAAJ&citation_for_view=HtL1XDkAAAAJ:d1gkVwhDpl0C,,1
2435,IoT Malware Classification Based on System Calls,"IoT devices play an important role in the industrial revolution 4.0. However, this type of device may exhibit specific security vulnerabilities that can be easily exploited to cause botnet attacks and other malicious activities. In this paper, we introduce a new method for classification and clustering of IoT malware behaviors through system call monitoring. Our method is constructed from multiple one-class SVM classifiers and has the ability to classify known malware with F1-Score over 98% and probability to detect unknown malware up to 97%. Unknown malware instances with similar behaviors can also be grouped together so new classes of malware will be discovered.",174,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HtL1XDkAAAAJ&citation_for_view=HtL1XDkAAAAJ:2osOgNQ5qMEC,,1
2436,Efficient Incremental Instance-based Learning Algorithms for Open World Malware Classification,"Malware is growing rapidly in number and become more and more sophisticated. To prevent them we need to collect samples continuously and update them to the classifier. In this paper, we will propose a method to update new labeled samples of malware to the classifier easily without re-training everything. The classifier can be updated by both labeled malware samples of an existing class or a new class. Our method also has the ability to detect samples of unknown families. Experiments are performed over the traditional computer malware dataset and the IoT malware dataset. The results have shown that our method can reach the macro F1-score almost the same re-train everything but take significantly less time.",174,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=HtL1XDkAAAAJ&citation_for_view=HtL1XDkAAAAJ:u-x6o8ySG0sC,,1
2439,"A new genus of the family Portunidae (Crustacea: Decapoda: Brachyura) and the identity of Portunus (Cycloachelous) yoronensis Sakai, 1974","A new genus is established for Neptunus (Achelous) dubia Laurie, 1906, a species that ranges from the eastern Indian Ocean to the Philippines and French Polynesia. This species is probably closest to Cycloachelous Ward, 1942, with regard to the carapace shape and form of the chelipeds, but has a very unique adult male sternal medial groove with two prominent deep depressions on sternite 8 in the sterno-abdominal cavity to accommodate distinctive male first gonopods, that differs from all known portunid genera. Portunus (Cycloachelous) yoronensis Sakai, 1974, described from Japan, is confirmed to be a junior synonym of N.(A.) dubia Laurie, 1906.",182,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=V7GTOVUAAAAJ&citation_for_view=V7GTOVUAAAAJ:u5HHmVD_uO8C,,1
2440,"Liocarcinus corrugatus (Pennant, 1777)(Crustacea: Brachyura: Portunidae): a cosmopolitan brachyuran species?","The swimming crab genus Liocarcinus is restricted to European marine ecosystems, except L. corrugatus (Pennant, 1777), which also occurs in the Indo-West Pacific. To ascertain if the two populations represent a cosmopolitan species or a species complex, we used a series of morphological, morphometric and molecular phylogenetic analyses on European and Asian material of L. corrugatus. We further investigated the phylogenetic context of the genus Liocarcinus with related portunid taxa to establish its monophyly. Liocarcinus is paraphyletic with respect to Polybius henslowii. We propose that P. henslowii is a valid species, most likely the result of a very recent speciation within L. holsatus. The first gonopods of Asian and European L. corrugatus are similar in shape, but the carapaces of the Asian specimens are significantly longer. While L. corrugatus is monophyletic, the European and Asian populations separate into two distinct clades with high support. Based on the consistent morphological difference and the deep genetic differentiation (p-distance of 4.5% for COX1), the European and Asian populations of L. corrugatus should be treated as two distinct species. An old name is available and the Asian population should now be recognised as Liocarcinus strigilis Stimpson, 1858.",182,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=V7GTOVUAAAAJ&citation_for_view=V7GTOVUAAAAJ:2osOgNQ5qMEC,,1
2442,"Monomia calla, a new species of swimming crab (Decapoda, Portunidae) from Madagascar and the Philippines.","A new species of portunid crab (Brachyura: Portunidae) from southern Madagascar and central Philippines is described. Monomia calla sp. nov. is most similar in morphology to M. lecromi (Moosa, 1996) from the Chesterfield Islands in the southwestern Pacific Ocean, and M. rubromarginata (Lanchester, 1900) from the South China Sea, Singapore, Malay Archipelago and the northern half of Australia. The new species can be easily distinguished from all congeners by the distinctive morphology of the male first gonopod. The number of species of the Indo-West Pacific genus Monomia now stands at 12.",182,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=V7GTOVUAAAAJ&citation_for_view=V7GTOVUAAAAJ:u-x6o8ySG0sC,,1
2443,"Species Composition and Distributional Characteristics of Freshwater Decapod Crustaceans (Crustacea: Decapoda) and Molluscs (Mollusca) in Xuan Son National Park, Phu Tho Province","The present study has recorded 32 species, 22 genera and 14 families of freshwater decapod crustaceans and molluscs in Xuan Son National Park. Eight species were new additions to the list of species known from this National Park, one of them was recorded from Vietnam for the first time, Bithynia funiculata Walker, 1927. Among 14 families, four were decapod crustaceans, and 10 were molluscs. Decapoda was found with seven species (accounted for 21.9% of the total species number), and four genera (accounted for 18% of the total genus number found in the area), while Mollusca was found with 25 species (78.1%) and 18 genera (82%). There was no significant difference in the number of species among different habitats.",182,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=V7GTOVUAAAAJ&citation_for_view=V7GTOVUAAAAJ:d1gkVwhDpl0C,,1
2444,"The land snail genus Rhiostoma Benson, 1860 from Vietnam (Gastropoda: Caenogastropoda: Cyclophoridae), with description of a new species","A report is given on the genus Rhiostoma (Cyclophoridae) collected in Vietnam during recent fieldwork. Locality data, Vietnam distribution and general distribution are given for each species, including references to previously published records. Five named species are listed, of which a new species is described, Rhiostoma ningbien sp. nov., from Dien Bien, North-western Vietnam.",182,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=V7GTOVUAAAAJ&citation_for_view=V7GTOVUAAAAJ:qjMakFHDy7sC,,1
2445,Variability Fault Localization: A Benchmark,"Software fault localization is one of the most expensive, tedious, and time-consuming activities in program debugging. This activity becomes even much more challenging in Software Product Line (SPL) systems due to the variability of failures in SPL systems. These unexpected behaviors are caused by variability faults which can only be exposed under some combinations of system features. Although localizing bugs in non-configurable code has been investigated in-depth, variability fault localization in SPL systems still remains mostly unexplored. To approach this challenge, we propose a benchmark for variability fault localization with a large set of 1,570 buggy versions of six SPL systems and baseline variability fault localization performance results. Our hope is to engage the community to propose new and better approaches to the problem of variability fault localization in SPL systems.",185,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bfekvKMAAAAJ&citation_for_view=bfekvKMAAAAJ:2osOgNQ5qMEC,,1
2446,Ranking Warnings of Static Analysis Tools Using Representation Learning,"Static analysis tools are frequently used to detect potential vulnerabilities in software systems. However, an inevitable problem of these tools is their large number of warnings with a high false positive rate, which consumes time and effort for investigating. In this paper, we present DeFP, a novel method for ranking static analysis warnings. Based on the intuition that warnings which have similar contexts tend to have similar labels (true positive or false positive), DeFP is built with two BiLSTM models to capture the patterns associated with the contexts of labeled warnings. After that, for a set of new warnings, DeFP can calculate and rank them according to their likelihoods to be true positives (i.e., actual vulnerabilities). Our experimental results on a dataset of 10 real-world projects show that using DeFP, by investigating only 60% of the warnings, developers can find +90% of actual vulnerabilities. Moreover, DeFP improves the state-of-the-art approach 30% in both Precision and Recall.",185,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bfekvKMAAAAJ&citation_for_view=bfekvKMAAAAJ:UeHWp8X0CEIC,,1
2447,A Variability Fault Localization Approach for Software Product Lines,"Software fault localization is one of the most expensive, tedious, and time-consuming activities in program debugging. This activity becomes even much more challenging in Software Product Line (SPL) systems due to variability of failures. These unexpected behaviors are induced by variability faults which can only be exposed under some combinations of system features. The interaction among these features causes the failures of the system. Although localizing bugs in single-system engineering has been studied in-depth, variability fault localization in SPL systems still remains mostly unexplored. In this article, we present VarCop, a novel and effective variability fault localization approach. For an SPL system failed by variability bugs, VarCop isolates suspicious code statements by analyzing the overall test results of the sampled products and their source code. The isolated suspicious statements are the …",185,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bfekvKMAAAAJ&citation_for_view=bfekvKMAAAAJ:qjMakFHDy7sC,,1
2448,On Rectifying the Mapping between Articles and Institutions in Bibliometric Databases,"Today, bibliometric databases are indispensable sources for researchers and research institutions. The main role of these databases is to find research articles and estimate the performance of researchers and institutions. Regarding the evaluation of the research performance of an organization, the accuracy in determining institutions of authors of articles is decisive. However, current popular bibliometric databases such as Scopus and Web of Science have not addressed this point efficiently. To this end, we propose an approach to revise the authors’ affiliation information of articles in bibliometric databases. We build a model to classify articles to institutions with high accuracy by assembling the bag of words and n-grams techniques for extracting features of affiliation strings. After that, these features are weighted to determine their importance to each institution. Affiliation strings of articles are transformed into the new feature space by integrating weights of features and local characteristics of words and phrases contributing to the sequences. Finally, on the feature space, the support vector classifier method is applied to learn a predictive model. Our experimental result shows that the proposed model’s accuracy is about 99.1%.",185,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bfekvKMAAAAJ&citation_for_view=bfekvKMAAAAJ:u5HHmVD_uO8C,,1
2449,AES Datapath Optimization Strategies for Low-Power Low-Energy Multisecurity-Level Internet-of-Things Applications,"Connected devices are getting attention because of the lack of security mechanisms in current Internet-of-Thing (IoT) products. The security can be enhanced by using standardized and proven-secure block ciphers as advanced encryption standard (AES) for data encryption and authentication. However, these security functions take a large amount of processing power and power/energy consumption. In this paper, we present our hardware optimization strategies for AES for high-speed ultralow-power ultralow-energy IoT applications with multiple levels of security. Our design supports multiple security levels through different key sizes, power and energy optimization for both datapath and key expansion. The estimated power results show that our implementation may achieve an energy per bit comparable with the lightweight standardized algorithm PRESENT of less than 1 pJ/b at 10 MHz at 0.6 V with throughput of …",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:zYLM7Y9cAGgC,,1
2450,Ultra low-power and low-energy 32-bit datapath AES architecture for IoT applications,"In this paper, we propose a novel AES microarchitecture with 32-bit datapath optimized for low-power and low-energy consumption targeting IoT applications. The proposed design uses simple shift registers for key/data storage and permutation to minimize the area, and the power/energy consumption. These shift registers also minimize the control logics in the key expansion and the encryption path. The proposed architecture is further optimized for area and/or power/energy consumption by selecting a suitable implementation of S-boxes and applying the clock gating technique. The implementation results in TSMC 65nm technology show that our design can save 20% of area or 20% of energy per bit at the same area when compared with the current 32-bit datapath designs. Our design also occupies smaller core area with lower energy per bit and at least 4 times higher in throughput in comparison with other 8-bit …",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:u5HHmVD_uO8C,,1
2451,Hardware implementation for entropy coding and byte stream packing engine in H. 264/AVC,"Entropy coding and data packing are the major phases in video coding. The new video coding standard, H.264 Advanced Video Coding (H.264/AVC), has adopted Exp-Golomb and Context-Adaptive coding methods to increase data compression ratio. In this paper, we propose hardware architecture of entropy encoding and byte stream data packing engines for the H.264/AVC. Our entropy coding engine, that contains Exp-Golomb and Context-Adaptive Variable Length Coding (CAVLC), supports baseline and main profile of the standard. The proposed architecture is implemented using 180nm technology from AMS. The design consumes only 1.56mW at the operating frequency of 100MHz.",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:Y0pCki6q_DkC,,1
2452,A Novel Hardware Architecture for Human Detection using HOG-SVM Co-Optimization,"Histogram of Oriented Gradient (HOG) in combination with Supported Vector Machine (SVM) has been used as an efficient method for object detection in general and human detection in particular. Human detection using HOG-SVM in hardware shows high classification rate at higher throughput when compared with deep learning methods. However, data dependencies and complicated arithmetic in HOG feature generation and SVM classification limit the maximum throughput of these applications. In this paper, we propose a novel high-throughput hardware architecture for human detection by co-optimizing HOG feature generation and SVM classification. The throughput is improved by using a fast, highly-parallel and low-cost HOG feature generation in combination with a modified datapath for parallel computation of SVM and HOG feature normalization. The proposed architecture has been implemented in TSMC …",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:8k81kl-MbHgC,,1
2453,High-performance adaption of ARM processors into Network-on-Chip architectures,"The demand for system scalability, reusability, and the decoupling between computation and communication have motivated the growth of Network-on-Chip (NoC) paradigm in the recent years. The system design has changed from the computation centric design to the communication centric design. Researchers have proposed a number of NoC architectures. Most of these works focus on network architectures and routing algorithms, however, the interfaces between network architectures and processing units also need to be addressed to improve the overall performance of the system. This paper presents an efficient AXI (Advanced eXtensible Interface) compliant network adapter for 2D mesh Wormhole-based NoC architectures, named AXI-NoC adapter. The proposed network adapter achieves high frequency of 650MHz with a low area footprint (952 cells, approximate to 2; 793μm 2  with a CMOS 45nm …",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:eQOLeE2rZwMC,,1
2454,An Efficient Hardware Implementation of Artificial Neural Network based on Stochastic Computing,"Recently, Artificial Neural Network (ANN) has emerged as the main driving force behind the rapid developments of many applications. Although ANN provides high computing capabilities, its prohibitive computational complexity, together with the large area footprints of ANN hardware implementations, has made it unsuitable for embedded applications with real-time constraints. Stochastic Computing (SC), an unconventional computing technique which could offer low-power and area-efficient hardware implementations, has shown promising results when applied to ANN hardware circuits. In this paper, efficient hardware implementations of ANN with conventional binary radix computation and SC technique are proposed. The system's performance is benchmarked with a handwritten digit recognition application. Simulation results show that, on the MNIST dataset, the 10-bit binary implementation of the system only …",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:roLk4NBRz8UC,,1
2455,H. 264/AVC hardware encoders and low-power features,"Because of significant bit rate reduction in comparison to the previous video compression standards, the H.264/AVC has been successfully used in a wide range of applications. In hardware design for H.264/AVC video encoders, power reduction is currently a tremendous challenge. This paper presents a survey of different H.264/AVC hardware encoders focusing on power features and power reduction techniques to be applied. A new H.264/AVC hardware encoder, named VENGME, is proposed. This low power encoder is a four-stage architecture with memory access reduction, in which, each module has been optimized. The actual total power consumption, estimated at RTL level, is 19.1mW.",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:W7OEmFMy1HYC,,1
2456,A hardware architecture for intra prediction in H. 264/AVC encoder,"MPEG-4 AVC, so called H. 264/AVC, is the latest video compression standard focusing on  network transport and storage of digital multimedia. In H. 264 system, the intra prediction  module is one of the most essential parts and it is different from the previous video  compression standards. With the new prediction techniques, intra prediction in H. 264  improves the bit rate but it also increases the memory bandwidth and the computational  complexity with many prediction modes. In this work, the intra prediction procedure is fully ...",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:2osOgNQ5qMEC,,1
2457,Efficient Binary Arithmetic Encoder for HEVC with multiple bypass bin processing,"The increasing amount of digital video with supreme quality requires more efficient compression. As the complexity of video coding algorithm is rising, there are more demands for hardware accelerators and customized hardware. Context-based Adaptive Binary Arithmetic Coding (CABAC) is the only entropy coding method adopted in the latest video compression standard, High Efficiency Video Coding (HEVC). Binary Arithmetic Encoder (BAE) is an essential component in CABAC, where the compression process happens. Because of the high data dependency and sequential coding characteristic, it is challenging to parallelize BAE. In this work, we proposed a low-cost and high-throughput hardware architecture for one core of BAE in HEVC. Our 4-stage pipelined BAE architecture is capable of processing one regular bin and up to 4 bypass bins per clock cycle with 30% reduction in terms of area when compared …",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:Tyk-4Ss8FVUC,,1
2458,An Efficient Hardware Implementation of Residual Data Binarization in HEVC CABAC Encoder,"HEVC-standardized encoders employ the CABAC (context-based adaptive binary arithmetic coding) to achieve high compression ratios and video quality that supports modern real-time high-quality video services. Binarizer is one of three main blocks in a CABAC architecture, where binary symbols (bins) are generated to feed the binary arithmetic encoder (BAE). The residual video data occupied an average of 75% of the CABAC’s work-load, thus its performance will significantly contribute to the overall performance of whole CABAC design. This paper proposes an efficient hardware implementation of a binarizer for CABAC that focuses on low area cost, low power consumption while still providing enough bins for high-throughput CABAC. On the average, the proposed design can process upto 3.5 residual syntax elements (SEs) per clock cycle at the maximum frequency of 500 MHz with an area cost of 9.45 Kgates (6.41 Kgates for the binarizer core) and power consumption of 0.239 mW (0.184 mW for the binarizer core) with NanGate 45 nm technology. It shows that our proposal achieved a high overhead-efficiency of 1.293 Mbins/Kgate/mW, much better than the other related high performance designs. In addition, our design also achieved a high power-efficiency of 8288 Mbins/mW; this is important factor for handheld applications.",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:Zph67rFs4hoC,,1
2459,AXI-NoC: High-Performance Adaptation Unit for ARM Processors in Network-on-Chip Architectures,"The increasing demand on scalability and reusability of system-on-chip design as well as the decoupling between computation and communication has motivated the growth of the Network-on-Chip (NoC) paradigm in the last decade. In NoC-based systems, the computational resources (ie IPs) communicate with each other using a network infrastructure. Many works have focused on the development of NoC architectures and routing mechanisms, while the interfacing between network and associated IPs also needs to be considered. In this paper, we present a novel efficient AXI (AMBA eXtensible Interface) compliant network adapter for NoC architectures, which is named an AXI-NoC adapter. The proposed network adapter achieves high communication throughput of 20.8 Gbits/s and consumes 4.14 mW at the operating frequency of 650 MHz. It has a low area footprint (952 gates, approximate to 2, 793 µm2 with CMOS 45 nm technology) thanks to its effective hybrid micro-architectures and with zero latency thanks to the proposed mux-selection method.",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:u-x6o8ySG0sC,,1
2460,Reducing temporal redundancy in MJPEG using Zipfian estimation techniques,"Motion JPEG simply compresses each frame in a video sequence using JPEG still image compression standard. This video codec satisfies power consumption and real-time properties of embedded systems while having very low complexity. However, the downside is its inefficiency in reducing the bit-rate because there are still redundancies between encoded frames. In this paper, we focus on reducing temporal redundancies in MJPEG using a motion detection algorithm, called Zipfian estimation. The Zipfian estimation helps MJPEG extract separately the moving blocks and the stationary blocks then removes temporal redundancies by minimizing encoded bit-stream of stationary blocks and encoding only the residuals of the moving blocks between adjacent frames. Experimental results show that the proposed method can provide twice compression ratio as much as the conventional MJPEG, and an approximate …",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:_FxGoFyzp5QC,,1
2461,A Lightweight AEAD encryption core to secure IoT applications,"The Internet of Things (IoT) with the advancements of many technologies opens a wide range of new applications such as smart appliances, smart cities and smart grids. Despite its popularity and usability, it also creates a new attack surface for the hackers especially on highly constrained devices which have limited memory footprints and processing power. These constrained devices often use Authenticated Encryption with Associated Data (AEAD) to secure data stored in the devices and transmitted over the network. In this work, we design a lightweight data encryption core in hardware with the support for AEAD to secure IoT applications on highly constrained devices. The design achieves a low area cost with only 23kGEs in TSMC 65nm technology and an encryption throughput of 123Mbps at 60MHz.",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:ULOm3_A8WrAC,,1
2462,An Efficient Event-driven Neuromorphic Architecture for Deep Spiking Neural Networks,"Deep Neural Networks (DNNs) have been successfully applied to various real-world machine learning applications. However, performing large DNN inference tasks in real-time remains a challenge due to its substantial computational costs. Recently, Spiking Neural Networks (SNNs) have emerged as an alternative way of processing DNN'fs task. Due to its eventbased, data-driven computation, SNN reduces both inference latency and complexity. With efficient conversion methods from traditional DNN, SNN exhibits similar accuracy, while leveraging many state-of-the-art network models and training methods. In this work, an efficient neuromorphic hardware architecture for image recognition task is presented. To preserve accuracy, the analog-to-spiking conversion algorithm is adopted. The system aims to minimize hardware area cost and power consumption, enabling neuromorphic hardware processing in edge devices …",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:3fE2CSJIrl8C,,1
2463,Accurate and low complex cell histogram generation by bypass the gradient of pixel computation,"Histogram of Oriented Gradient (HOG) is a popular feature description for the purpose of object detection. However, HOG algorithm requires a high performance system because of its complex operation set. In HOG algorithm, the cell histogram generation is one of the most complex part, it uses inverse tangent, square, square root, floating point multiplication. In this paper, we propose an accurate and low complex cell histogram generation by bypass the gradient of pixel computation. It employs the bin's boundary angle method to determine the two quantized angles. However, instead of choosing an approximate value of tan, the nearest greater and the nearest smaller of each tan value from the ratios between pixel's derivative in y and x direction are used. The magnitudes of two bins are the solutions of a system of two equations, which represents the equality of the gradient of a pixel and its two bins in both vertical …",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:UeHWp8X0CEIC,,1
2464,An Overview of H. 264 Hardware Encoder Architectures Including Low-Power Features,"H.264 is the most popular video coding standard with high potent coding performance. For its efficiency, the H.264 is expected to encode real-time and/or high-definition video. However, the H.264 standard also requires highly complex and long lasting computation. To overcome these difficulties, many efforts have been deployed to increase encoding speed. Besides, with the revolution of portable devices, multimedia chips for mobile environments are more and more developed. Thus, power-oriented design for H.264 video encoders is currently a tremendous challenge. This paper discusses these trends and presents an overview of the state of the art on power features for different H.264 hardware encoding architectures. We also propose the VENGME's design, a particular hardware architecture of H.264 encoder that enables applying low-power techniques and developing power-aware ability. This low power encoder is a four-stage architecture with memory access reduction, in which, each module has been optimized. The actual total power consumption, estimated at Register-Transfer-Level (RTL), is only 19.1mW.",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:d1gkVwhDpl0C,,1
2465,Reducing Bitrate and Increasing the Quality of Inter Frame by Avoiding Quantization Errors in Stationary Blocks,"In image compression and video coding, quantization error helps to reduce the amount of information of the high frequency components. However, in temporal prediction the quantization error contributes its value as noise in the total residual information. Therefore, the residual signal of the inter-picture prediction is greater than the expected one and always differs zero value even input video contains only homogeneous frames. In this paper, we reveal negative effects of quantization errors in inter prediction and propose a video encoding scheme which is able to avoid side effects of quantization errors in the stationary parts. We propose to implement a motion detection algorithm as the first stage of video encoding to separate the video into two parts: motion and static. The motion information allows us to force residual data of non-changed part to zero and keep the residual signal of motion regularly. Beside, we design block-based filters which improve motion results and filter those results fit into block encode size well. Fixed residual data of static information permits us to pre-calculate its quantized coefficient and create a bypass encoding path for it. Experimental results with the JPEG compression (MJPEG-DPCM) showed that the proposed method produces lower bitrate than the conventional MJPEG-DPCM at the same quantization parameter and a lower computational complexity.",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:LkGwnXOMwfcC,,1
2466,An innovative lightweight cryptography system for Internet-of-Things ULP applications,"The Internet of Things (IoT) has been fostered by accelerated advancements in communication technologies, computation technologies,sensor technologies, artificial intelligence, cloud computing, and semiconductor technologies. In general, IoT contains cloud computing to do data processing, communication infrastructure including the Internet, and sensor nodes which can collect data, send them through the network infrastructure to the Internet, and receive controls to react to the environment. During its operations, IoT may collect, transmit and process secret data, which raise security problems. Implementing security mechanisms for IoT is challenging because IoT organizations include millions of devices integrated at multiple layers, whereas each layer has different computation capabilities and security requirements. Furthermore, sensor nodes in IoT are intended to be battery-based constrained devices with limited power budget, limited computation, and limited memory footprint to reduce costs. Implementing security mechanisms on these devices even encounters more challenges. This work is therefore motivated to focus on implementing data encryption to protect IoT sensor nodes and systems with the consideration of hardware cost, throughput and power/energy consumption. To begin with, a ultra-low-power block cipher crypto-accelerator with configurable parameters is proposed and implemented in ST 28nm FDSOI technology in SNACk test chip with two cryptography modules: AES and PRESENT. AES is a widely used data encryption algorithm for the Internet and currently used for new IoT proposals, while PRESENT is a lightweight …",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:hqOjcs7Dif8C,,1
2467,Low Cost Inter-prediction Architecture in H. 264/AVC Encoders with an Efficient Data Reuse Strategy,"Advances in Engineering Research. Volume 40 first presents the characteristics of the laser-plasma extreme ultraviolet radiation from solid rare gas targets composed of Xe, Kr and Ar, along with the performances of the radiation sources developed using these targets. Following this, the authors consider the most important issues related to creating a universal system of adaptive applications for use in the Internet of Things and Internet of People systems. Previous techniques and recent advances in circuit techniques are reviewed, and a comparison of the reported techniques in the context of low-pass continuous-time Delta-Sigma modulators is presented. A comprehensive overview of the properties of aggregates used on roads is provided, particularly focusing on their influence on the mechanical and skid resistance of road surfaces. An analytical approach that allows for a rough prediction of the of the acoustic parameters of road surface is also presented. The attenuation depends on frequency, propagation distance, angle of incidence and geometric configuration of sources and sensors. Due to computation complexity, the VLSI implementation of Inter-Prediction in the H.264/Advanced Video Coding imposes latency, memory bandwidth, and area cost challenges. To tackle these obstacles, the authors discuss a design methodology which exploits the relationship between the main processes in inter-prediction to enhance the performance while keeping an affordable design cost. The penultimate study focuses on the way we can interpret linguistic algebra to understand and reverse translation formulas’ linguistic algebra into natural language …",193,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=7kLiMi8AAAAJ&citation_for_view=7kLiMi8AAAAJ:_kc_bZDykSQC,,1
2469,Author profiling for English emails,"This paper reports on some aspects of a project aimed at automating the analysis of texts for the purpose of author profiling and identification. The complete analysis provides probabilities for the author’s basic demographic traits (gender, age, geographic origin, level of education and native language) as well as for five psychometric traits. We describe the email data which was collected for the project, the ways this data is processed and analysed, and the experimental setup used for classification with the Text Attribution Tool (TAT) before presenting our results for the demographic and psychometric traits using English email. Results are very promising for all ten traits examined.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:u-x6o8ySG0sC,,1
2470,Omnidirectional locomotion for quadruped robots,"Competing at the RoboCup 2000 Sony legged robot league, the UNSW team won both the challenge competition and all their soccer matches, emerging the outright winners for this league against eleven other international teams. The main advantage that the UNSW team had was speed. A major contributor to the speed was a novel omnidirectional locomotion method developed for the quadruped Sony ERS-110 robot used in the competition. It is believed to be the fastest walk style known for this type of robot. In this paper we describe the parameterised omnidirectional walk in detail. The walk also made a positive contribution to other robot tasks such as ball tracking and localisation while playing soccer. The authors believe that this omnidirectional locomotion could be applied more generally in other legged robots.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:u5HHmVD_uO8C,,1
2471,A review of unmet needs in obesity management,"The prevalence of obesity continues to escalate in the USA; however, there is no consensus regarding the optimal therapy for obesity. For the vast majority of severely obese patients, conventional medical therapies (i.e., diet, exercise, behavioral counseling) often fail over the long term. Existing pharmacotherapy adjunctive to behavioral therapy has limited effectiveness and an imperfect safety record. In contrast, bariatric surgery has a high degree of weight loss efficacy, yet only a small fraction of the qualifying obese population undergoes these procedures because of the associated perioperative risks and potential late complications. In addition, the role of bariatric surgery is unclear in certain patient populations, such as patients with lower body mass index (BMI, 30–35 kg/m2), the high-risk super-super obese patients (BMI > 60), the morbidly obese adolescent, and obese patients requiring weight …",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:5ugPr518TE4C,,1
2472,JAIST: Combining multiple features for answer selection in community question answering,"In this paper, we describe our system for SemEval-2015 Task 3: Answer Selection in Community Question Answering. In this task, the systems are required to identify the good or potentially good answers from the answer thread in Community Question Answering collections. Our system combines 16 features belong to 5 groups to predict answer quality. Our final model achieves the best result in subtask A for English, both in accuracy and F1-score.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:70eg2SAEIzsC,,1
2473,A new approach for scientific citation classification using cue phrases,"This paper introduces a new method for the rapid development of complex rule bases involving cue phrases for the purpose of classifying text segments. The method is based on Ripple-Down Rules, a knowledge acquisition method that proved very successful in practice for building medical expert systems and does not require a knowledge engineer. We implemented our system KAFTAN and demonstrate the applicability of our method to the task of classifying scientific citations. Building cue phrase rules in KAFTAN is easy and efficient. We demonstrate the effectiveness of our approach by presenting experimental results where our resulting classifier clearly outperforms previously built classifiers in the recent literature.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:d1gkVwhDpl0C,,1
2474,RDRPOSTagger: A ripple down rules-based part-of-speech tagger,"This paper describes our robust, easyto-use and language independent toolkit namely RDRPOSTagger which employs an error-driven approach to automatically construct a Single Classification Ripple Down Rules tree of transformation rules for POS tagging task. During the demonstration session, we will run the tagger on data sets in 15 different languages.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:pqnbT2bcN3wC,,1
2475,Tat: an author profiling tool with application to arabic emails,"This paper reports on the application of the Text Attribution Tool (TAT) to profiling the authors of Arabic emails. The TAT system has been developed for the purpose of language-independent author profiling and has now been trained on two email corpora, English and Arabic. We describe the overall TAT system and the Machine Learning experiments resulting in classifiers for the different author traits. Predictions for demographic and psychometric author traits show improvements over the baseline for some of the author traits with both the English and the Arabic data. Arabic presents particular challenges for NLP and this paper describes more specifically the text processing components developed to handle Arabic emails.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:W7OEmFMy1HYC,,1
2476,A robust transformation-based learning approach using ripple down rules for part-of-speech tagging,"In this paper, we propose a new approach to construct a system of transformation rules for the Part-of-Speech (POS) tagging task. Our approach is based on an incremental knowledge acquisition method where rules are stored in an exception structure and new rules are only added to correct the errors of existing rules; thus allowing systematic control of the interaction between the rules. Experimental results on 13 languages show that our approach is fast in terms of training time and tagging speed. Furthermore, our approach obtains very competitive accuracy in comparison to state-of-the-art POS and morphological taggers.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:ns9cj8rnVeAC,,1
2477,Document processor and associated method,"A computer implemented method of processing a digitally encoded document having a text composed by an author by using a processor to analyse the segmentation, punctuation and linguistics of text and storing the results in a digitally accessible format. Author traits are then predicted using a machine learning system based on the results of the segmentation, punctuation and linguistics analysis of the text.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:4DMP91E08xMC,,1
2479,Author profiling for Vietnamese blogs,"This paper presents the first work in the task of author profiling for Vietnamese blogs. This task is important in threat identification and marketing intelligence. We have developed a Vietnamese Blog Profiling framework to automatically predict age, gender, geographic origin and occupation of weblogs' authors purely based on language use. The experiments on the blogs corpus we collected show very promising results with accuracy of around 80% across all traits.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:5nxA0vEk-isC,,1
2480,A hybrid approach to vietnamese word segmentation using part of speech tags,"Word segmentation is one of the most important tasks in NLP. This task, within Vietnamese language and its own features, faces some challenges, especially in words boundary determination. To tackle the task of Vietnamese word segmentation, in this paper, we propose the WS4VN system that uses a new approach based on Maximum matching algorithm combining with stochastic models using part-of-speech information. The approach can resolve word ambiguity and choose the best segmentation for each input sentence. Our system gives a promising result with an F-measure of 97%, higher than the results of existing publicly available Vietnamese word segmentation systems.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:zYLM7Y9cAGgC,,1
2481,Sentiment analysis for Vietnamese,"Sentiment analysis is one of the most important tasks in Natural Language Processing. Research in sentiment analysis for Vietnamese is relatively new and most of current work only focus in document level. In this paper, we address this problem at the sentence level and build a rule-based system using the Gate framework. Experimental results on a corpus of computer products reviews are very promising. To the best of our knowledge, this is the first work that analyzes sentiment at sentence level in Vietnamese.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:mVmsd5A6BfQC,,1
2482,From treebank conversion to automatic dependency parsing for Vietnamese,"This paper presents a new conversion method to automatically transform a constituent-based Vietnamese Treebank into dependency trees. On a dependency Treebank created according to our new approach, we examine two state-of-the-art dependency parsers: the MSTParser and the MaltParser. Experiments show that the MSTParser outperforms the MaltParser. To the best of our knowledge, we report the highest performances published to date in the task of dependency parsing for Vietnamese. Particularly, on gold standard POS tags, we get an unlabeled attachment score of 79.08% and a labeled attachment score of 71.66%.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:RYcK_YlVTxYC,,1
2483,Bioactive compounds in anti-diabetic plants: From herbal medicine to modern drug discovery,"Natural products, including organisms (plants, animals, or microorganisms) have been shown to possess health benefits for animals and humans. According to the estimation of the World Health Organization, in developing countries, 80% of the population has still depended on traditional medicines or folk medicines which are mostly prepared from the plant for prevention or treatment diseases. Traditional medicine from plant extracts has proved to be more affordable, clinically effective and relatively less adverse effects than modern drugs. Literature shows that the attention on the application of phytochemical constituents of medicinal plants in the pharmaceutical industry has increased significantly. Plant-derived secondary metabolites are small molecules or macromolecules biosynthesized in plants including steroids, alkaloids, phenolic, lignans, carbohydrates and glycosides, etc. that possess a diversity of biological properties beneficial to humans, such as their antiallergic, anticancer, antimicrobial, anti-inflammatory, antidiabetic and antioxidant activities Diabetes mellitus is a chronic disease result of metabolic disorders in pancreas β-cells that have hyperglycemia. Hyperglycemia can be caused by a deficiency of insulin production by pancreatic (Type 1 diabetes mellitus) or insufficiency of insulin production in the face of insulin resistance (Type 2 diabetes mellitus). The current medications of diabetes mellitus focus on controlling and lowering blood glucose levels in the vessel to a normal level. However, most modern drugs have many side effects causing some serious medical problems during a period of treating. Therefore, traditional …",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:V3AGJWp-ZtQC,,1
2484,Sentiment classification on polarity reviews: an empirical study using rating-based features,"We present a new feature type named rating-based feature and evaluate the contribution of this feature to the task of document-level sentiment analysis. We achieve state-of-the-art results on two publicly available standard polarity movie datasets: on the dataset consisting of 2000 reviews produced by Pang and Lee (2004) we obtain an accuracy of 91.6% while it is 89.87% evaluated on the dataset of 50000 reviews created by Maas et al. (2011). We also get a performance at 93.24% on our own dataset consisting of 233600 movie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:M3NEmzRMIkIC,,1
2485,A vietnamese question answering system,"Question answering systems aim to produce exact answers to users' questions instead of a list of related documents as used by current search engines. In this paper, we propose an ontology-based Vietnamese question answering system that allows users to express their questions in natural language. To the best of our knowledge, this is the first attempt to enable users to query an ontological knowledge base using Vietnamese natural language. Experiments of our system on an organizational ontology show promising results.",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:WF5omc3nYNoC,,1
2486,Email document parsing method and apparatus,"A preferred example of the process flow of the inventive method (1) is depicted in FIG. 1). The first step (2) of the method (1) is to import an email document (3) to be parsed. In the preprocessing step (10) the email (3) is processed to determine the presence of any header text (5)(excluding any header text that may be within the embedded reply chain) or attachments 4, including attached email documents, if any. Once the header text (5), attachments (4) or other forwarded materials have been identified in the preprocessing step (10), these components of the email (3) are categorized by the computer (51) as non-author composed text. Next the process flow of the parsing computer (51) moves to the step of normalization (11). This entails processing the email document (3) to ascertain whether it is in a preferred format and, if the email document (3) is not in the preferred format, converting at least some of the …",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:j3f4tGmQtD8C,,1
2487,Email document parsing method and apparatus,"A preferred example of the process flow of the inventive method (1) is depicted in FIG. 1). The first step (2) of the method (1) is to import an email document (3) to be parsed. In the preprocessing step (10) the email (3) is processed to determine the presence of any header text (5)(excluding any header text that may be within the embedded reply chain) or attachments 4, including attached email documents, if any. Once the header text (5), attachments (4) or other forwarded materials have been identified in the preprocessing step (10), these components of the email (3) are categorized by the computer (51) as non-author composed text. Next the process flow of the parsing computer (51) moves to the step of normalization (11). This entails processing the email document (3) to ascertain whether it is in a preferred format and, if the email document (3) is not in the preferred format, converting at least some of the …",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:hC7cP41nSMkC,,1
2488,Email document parsing method and apparatus,"A preferred example of the process flow of the inventive method (1) is depicted in FIG. 1). The first step (2) of the method (1) is to import an email document (3) to be parsed. In the preprocessing step (10) the email (3) is processed to determine the presence of any header text (5)(excluding any header text that may be within the embedded reply chain) or attachments 4, including attached email documents, if any. Once the header text (5), attachments (4) or other forwarded materials have been identified in the preprocessing step (10), these components of the email (3) are categorized by the computer (51) as non-author composed text. Next the process flow of the parsing computer (51) moves to the step of normalization (11). This entails processing the email document (3) to ascertain whether it is in a preferred format and, if the email document (3) is not in the preferred format, converting at least some of the …",194,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=xJrraAcAAAAJ&citation_for_view=xJrraAcAAAAJ:ZeXyd9-uunAC,,1
2489,Constructing a Bayesian belief network to generate learning path in adaptive hypermedia system.,"There are many methods and techniques which have been promoted to develop adaptive hypermedia systems [1]. Our model approach [2], generating adaptive courses based on learner’s profile which learner’s includes background, skills, style... etc. One of important steps in our model is to generate learning path adaptive for each learner. In this paper, we promote an algorithm based on shortest path search algorithm to evaluate learning object (LO) based on its attributes [3] and constructed a Bayesian Belief Network (BBN) to generate learning path for each learner.",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:u5HHmVD_uO8C,,1
2490,Collaborative learning model for cyberattack detection systems in iot industry 4.0,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can …",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:g5m5HwL7SMYC,,1
2491,On domain driven design using annotation-based domain specific language,"The aim of object-oriented domain-driven design (DDD) is to iteratively develop software around a realistic domain model. Recent work in DDD use an annotation-based extension of object-oriented programming language to build the domain model. This model becomes the basis for a ubiquitous language and is used as input to generate software. However, the annotation-based extensions of these work do not adequately address the primitive and essential structural and behavioural modelling requirements of practical software. Further, they do not precisely characterise the software that is generated from the domain model. In this paper, we propose a DSL-based DDD method to address these limitations. We make four contributions:  (1) an annotation-based domain-specific language (DSL) named DCSL, whose annotation extension expresses a set of essential structural constraints and the essential behaviour …",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:YFjsv_pBGBYC,,1
2492,Analyzing RBAC security policy of implementation using AST,"Security policy is a critical property in software applications which require high levels of safety and security. It has to be clearly specified in requirement documents and its implementation must be conformed to the specification. In this paper, we propose an approach to check if the implementation is in accordance with its security policy specification. We use the abstract syntax tree (AST), another manner of expressing the program, to analyze the source code and specify user permission policy in software systems by role-based access control (RBAC).",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:d1gkVwhDpl0C,,1
2493,Developing adaptive hypermedia system based on learning design level B with rules for adaptive learning activities,"For recently years the research of adaptation of computer education has been an important topic. Although Adaptive Educational Hypermedia Systems (AEHS) are different disciplines with IMS Learning Design (IMS LD), they have the same goal is to create the best possible environment for a learner to perform his/her learning activities in. How IMS LD addresses many requirements for computer based adaptation and personalized e-Leaming is one of ửie main concerns for researcher in this field. This paper represents an approach to learning design for adaptive learning system for adaptation of learning activities. Consữiicting set of rules for learning activities adaptation represented in first order logic, and mapping them into IMS LD specification. In addition, an adaptive course of Computer science domain in online context is implemented using IMS LD design.",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:u-x6o8ySG0sC,,1
2494,Domain-driven design using meta-attributes: a DSL-based approach,"Applying object-oriented domain-driven design in practice requires bridging the gaps that exist among the perceived domain class models of the key stakeholders involved. In this paper, we propose a domain-driven design method that uses meta-attributes with an aim to fill these gaps. Our method extends and generalises a previous work to use meta-attributes to build the domain class model. The meta-attributes are designed to not only make it easier for the designer and domain expert to collaboratively capture the domain-specific requirements in the model, but to ease the translation of the model to design specification. This specification is written in an object-oriented, internal DSL. To increase productivity, we define a generator function that realises the meta-mapping between the state and behaviour spaces of a domain class to automatically generate its behavioural specification. We demonstrate our method …",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:LkGwnXOMwfcC,,1
2496,Domain-driven design patterns: A metadata-based approach,"Design pattern is the most common form of object oriented software reuse. In object oriented domain driven design, a number of high-level patterns have been identified and applied for over a decade. However, no concrete design patterns for domain modeling in this method have been published in the literature. A primary challenge in defining these design patterns is how to express their form in a way that eases their application to a specific domain that uses a specific object oriented programming platform. In this work, we propose a set of concrete design patterns, whose form is expressed in a domain class modeling language (DCML). DCML is based on UML and uses implementation-aware meta-attributes to define design metadata. We extend this language with new meta-attributes to support the proposed design patterns. Further, we discuss how domain-specific examples of these patterns are translated to …",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:4TOpqqG69KYC,,1
2497,A Method for Automated Test Data Generation from Sequence Diagrams and Object Constraint Language,"This paper proposes an automated test data generation method from the information embedded in model elements such as Unified Modeling Language (UML) sequence diagrams, class diagrams, and Object Constraint Language (OCL). The method supports UML 2.0 sequence diagrams including eight kinds of combined fragments describing control flow of systems. Comparing with some approaches by using depth first search (DFS) or breadth first search (BFS) algorithms, the proposed method generates all possible test scenarios with the higher error uncover capability. Test data for testing loop fragment is also generated. Therefore, it helps to detect errors in testing loops and the concurrency errors such as safety and liveness property of the systems.",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:eQOLeE2rZwMC,,1
2498,Checking interface interaction protocols using aspect-oriented programming,"We propose an approach based on aspect-oriented programming to specify and to verify interaction protocols in the component interfaces of Java programs. First, based on method patterns of AspectJ we introduce a simple protocol specification language that specifies temporal orders of operations of an interface. We then develop an algorithm that takes a protocol specification and produces aspect code that will check for protocol conformance of programs that use the interface at runtime. Finally, we propose several extensions that enable the integration of our approach with static techniques and allow checking protocols over multiple components.",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:2osOgNQ5qMEC,,1
2500,On optimization of minimized assumption generation method for component-based software verification,"The minimized assumption generation has been recognized as an important improvement of the assume-guarantee verification method in order to generate minimal assumptions. The generated minimal assumptions can be used to recheck the whole component-based software at a lower computational cost. The method is not only fitted to component-based software but also has a potential to solve the state space explosion problem in model checking. However, the computational cost for generating the minimal assumption is very high so the method is difficult to be applied in practice. This paper presents an optimization as a continuous work of the minimized assumption generation method in order to reduce the complexity of the method. The key idea of this method is to find a smaller assumption in a sub-tree of the search tree containing the candidate assumptions using the depth-limited search strategy. With this …",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:IjCSPb-OGe4C,,1
2501,An improvement of minimized assumption generation method for component-based software verification,"The minimized assumption generation has been recognized as an improved method of the assume-guarantee verification for generating minimal assumptions. This method is not only fitted to component-based software but also has a potential to solve the state space explosion problem in model checking. However, the computational cost for generating the minimal assumption is very high so the method is difficult to be applied in practice. This paper presents an optimization as a continuous work of the minimized assumption generation method in order to reduce the complexity of the method. The key idea of this method is to find a smaller assumption in a sub-tree of the search tree containing the candidate assumptions using the depth-limited search strategy. With this approach, the improved method can generate smaller assumptions with a lower computational cost and consumption memory than the minimized …",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:W7OEmFMy1HYC,,1
2502,Checking protocol-conformance in component models using Aspect oriented programming,"CHECKING PROTOCOL CONFORMANCE IN COMPONENT MODELS USING ASPECT ORIENTED PROGRAMMING Thanh-Binh Trinh1 , Anh-Hoang Truong1 and Viet-Ha Nguyen1,2 1 College of Technology, 2 Information Technology Institute Vietnam National University, Hanoi 144 Xuan Thuy, Cau Giay, Hanoi Email:{binhtt.di07,hoangta,hanv}@vnu.edu.vn ABSTRACT Protocol state machines (PSM) in UML 2.0 allows us to specify communication protocols or expected method call sequences among a number of objects or software compo- nents. As the implementation of objects or components can go wrong with respect to the speciﬁcation, one needs a method to check for the correctness of the implementation. We propose an approach based on aspect oriented program- ming (AOP) to check for the conformance between the im- plementation and the PSM speciﬁcation. Taking a PSM speciﬁcation as input we",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:Y0pCki6q_DkC,,1
2503,Generative software module development: A domain-driven design perspective,"Object-oriented domain-driven design (DDD) aims to develop software (iteratively) around a realistic model of the application domain, which both thoroughly captures the domain requirements and is technically feasible for implementation. The main focus of existing work in DDD has been on using an annotation-based extension of object-oriented programming language to build the domain model. However, these work do not consider software modules as first-class objects and thus lack a method for their development. In this paper, we propose a generative software module development method to bridge this gap. More specifically, we make the following contributions: (1) An annotation-based internal DSL named MCCL for configuring the modules. This language uses a reflective module configuration class design. (2) A module configuration class generator that generates a module configuration class from a …",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:k_IJM867U9cC,,1
2504,A method for automated unit testing of C programs,"This research proposes an automated test case generation method for C functions. In this method, the source code is transformed into a control flow graph corresponding to the given coverage criterion. After that, a list of feasible test paths are discovered by traversing the control flow graph using backtracking algorithm, symbolic execution, and Z3 solver. We also generate test cases for functions containing one loop or two-nested loop. A tool supporting the proposed method has been developed and applied to test on some C functions. The experimental results show the high coverage with the minimum number of test cases, the ability to improve the total time of the test case generation with a specified coverage criterion, and the increasing precision of checking the feasibility of test paths if comparing with the random technique. The experimental results display the potential usefulness of this tool for automated test …",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:aqlVkmm33-oC,,1
2505,Assume-guarantee tools for component-based software verification,"This paper presents a minimized assumption generation method and its associated tools for L*-based assume-guarantee verification of component-based software by model checking. The method is not only fitted to component-based software but also has a potential to solve the state space explosion problem in model checking. In the proposed method, a verification target is decomposed into components so that we can model check each of them separately. The key idea of this method is finding the minimal assumptions in the search spaces of the candidate assumptions. The minimal assumptions generated by the proposed method can be used to recheck the whole system at much lower computational cost. Our experience so far indicates that the implemented tools are potential for verifying practical component-based software.",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:9yKSN-GCB0IC,,1
2506,Coordinated consensus analysis of multi-agent systems using Event-B,"SOCK is a process calculus for the modeling of service oriented systems recently extended with primitives for dynamic fault and compensation handling. In this paper we investigate the relationships between the sagas calculi for compensable flow composition and SOCK. First, we present an encoding of parallel sagas (with interruption and centralized compensation) into SOCK. Then, we discuss a new semantics for parallel sagas that we consider more adequate to the dynamic approach to fault and compensation handling.",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:UeHWp8X0CEIC,,1
2507,Generative software module development for domain-driven design with annotation-based domain specific language,"Context Object-oriented domain-driven design (DDD) aims to iteratively develop software around a realistic model of the application domain, which both thoroughly captures the domain requirements and is technically feasible for implementation. The main focus of recent work in DDD has been on using a form of annotation-based domain specific language (aDSL), internal to an object-oriented programming language, to build the domain model. However, these work do not consider software modules as first-class objects and thus lack a method for their development. Objective In this paper, we tackle software module development with the DDD method by adopting a generative approach that uses aDSL. To achieve this, we first extend a previous work on module-based software architecture with three enhancements that make it amenable to generative development. We then treat module configurations as first-class …",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:35N4QoGY0k4C,,1
2508,"Preliminary Result of 3D City Modelling For Hanoi, Vietnam","Hanoi is the one of the fastest-growing cities in Vietnam, which sets the target to turn into a smart city in 2030. Nowadays, 3D city models are being increasingly employed for many domains and tasks beyond visualization, then it will take an important role in smart city. In order to develop 3D city models, 2D geographic data such as building footprint and building height attribute are required. However, the lack of the height attribute for various types of building and low performance of rendering and visualizing 3D city models are two big remaining problems. In this paper, available data from open sources is used to predict the building height. The prediction has carried out with machine learning techniques using the combination of different attributes. After that, the models will be created using 3D tiles specification to improve the visualization performance. The preliminary results of the proposed method highlight the …",195,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=z1B3S74AAAAJ&citation_for_view=z1B3S74AAAAJ:ldfaerwXgEUC,,1
2509,A semi–supervised tensor regression model for siRNA efficacy prediction,"Short interfering RNAs (siRNAs) can knockdown target genes and thus have an immense impact on biology and pharmacy research. The key question of which siRNAs have high knockdown ability in siRNA research remains challenging as current known results are still far from expectation. This work aims to develop a generic framework to enhance siRNA knockdown efficacy prediction. The key idea is first to enrich siRNA sequences by incorporating them with rules found for designing effective siRNAs and representing them as enriched matrices, then to employ the bilinear tensor regression to predict knockdown efficacy of those matrices. Experiments show that the proposed method achieves better results than existing models in most cases. Our model not only provides a suitable siRNA representation but also can predict siRNA efficacy more accurate and stable than most of state–of–the–art models. Source …",197,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=BcrsrbwAAAAJ&citation_for_view=BcrsrbwAAAAJ:d1gkVwhDpl0C,,1
2510,A novel framework to improve sirna efficacy prediction,"Short interfering RNA sequences (siRNAs) can knockdown target genes and thus have an immense impact on biology and pharmacy research. The key question of which siRNAs have high knockdown ability in siRNA research remains challenging as current known results are still far from expectation. This work aims to develop a generic framework to enhance siRNA knockdown efficacy prediction. The key idea is first to enrich siRNA sequences by incorporating them with rules found for designing effective siRNAs and representing them as transformed matrices, then to employ the bilinear tensor regression to do prediction on those matrices. Experiments show that the proposed method achieves results better than existing models in most cases.",197,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=BcrsrbwAAAAJ&citation_for_view=BcrsrbwAAAAJ:9yKSN-GCB0IC,,1
2511,A new FPGA implementation of four-switch three-phase inverter,"This paper is concerned on FPGA design for control implementations of four switch three phase inverters (B4, FSTPI). This paper is to present a space vector PWM algorithm for four switch three phase inverters (B4, FSTPI) based on the one for six switch three phase inverters (B6, SSTPI) (principle of similarity) where the αβ plan is divided into 6 sectors and the formation of the required reference voltage space vector is done in the same way as for B6 by using effective (mean) vectors. An SVPWM technique has been developed using the ready-to-use field-programmable gate array (FPGA) technology. High speed, very large number of components, large number of supported protocols, and addition of ready-to-use intellectual property cores make programmable devices the preferred choice of implementation and even deployment mass production quantities of Power Electronics. Matlab/Simulink is used for the …",197,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=BcrsrbwAAAAJ&citation_for_view=BcrsrbwAAAAJ:qjMakFHDy7sC,,1
2512,A Descriptive method for generating siRNA design rules,"Short-interfering RNAs (siRNAs) suppress gene expression through a process called RNA interference (RNAi). Current research focuses on finding design principles or rules for siRNAs and using them to artificially generate siRNAs with high efficiency of gene knockdown ability. Design rules have been reported by analyzing biology experiments and applying learning methods. However, possible good design rules or hidden characteristics remain undetected. In contribution to computational methods for finding design rules which are mostly employed by discriminative learning techniques, in this paper we propose a novel descriptive method to discover two design rules for effective siRNA sequences with 19 nucleotides (nt) and 21 nt in length that have important characteristics of previous design rules and contain new characteristics of highly effective siRNA. The key idea of the method is first to transform …",197,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=BcrsrbwAAAAJ&citation_for_view=BcrsrbwAAAAJ:2osOgNQ5qMEC,,1
2513,On Rectifying the Mapping between Articles and Institutions in Bibliometric Databases,"Today, bibliometric databases are indispensable sources for researchers and research institutions. The main role of these databases is to find research articles and estimate the performance of researchers and institutions. Regarding the evaluation of the research performance of an organization, the accuracy in determining institutions of authors of articles is decisive. However, current popular bibliometric databases such as Scopus and Web of Science have not addressed this point efficiently. To this end, we propose an approach to revise the authors’ affiliation information of articles in bibliometric databases. We build a model to classify articles to institutions with high accuracy by assembling the bag of words and n-grams techniques for extracting features of affiliation strings. After that, these features are weighted to determine their importance to each institution. Affiliation strings of articles are transformed into the new feature space by integrating weights of features and local characteristics of words and phrases contributing to the sequences. Finally, on the feature space, the support vector classifier method is applied to learn a predictive model. Our experimental result shows that the proposed model’s accuracy is about 99.1%.",197,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=BcrsrbwAAAAJ&citation_for_view=BcrsrbwAAAAJ:UeHWp8X0CEIC,,1
2514,Genomedics: Whole exome analysis system for clinical studies,"Whole exome sequencing (WES) is a widely used technique in both medical studies and clinical practice. However, a number of studies show that the results produced by different WES analysis pipelines are not always homogeneous. To this end, we propose a method (called Genomedics) using a consensus approach to expand the list of variants by combining results called from six separate pipelines with sensitive options. To evaluate the performance of the proposed method, Gemomedics was compared to seven existing methods when they were tested on two datasets and F1-score was used as an indicator of accuracy. The results showed that Genomedics has the highest score among seven methods. We also applied Genomedics to analyze whole exomes from Multiple Myeloma and Dravet syndrome patients and found interesting results. The results demonstrate the promising applications of Genomedics in …",197,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=BcrsrbwAAAAJ&citation_for_view=BcrsrbwAAAAJ:u5HHmVD_uO8C,,1
2515,MVRM: A Hybrid Approach to Predict siRNA Efficacy,"The discovery of RNA interference (RNAi) leads to design novel drugs for different diseases. Selecting short interfering RNAs (siRNAs) that can knockdown target genes efficiently is one of the key tasks in studying RNAi. A number of predictive models have been proposed to predict knockdown efficacy of siRNAs, however, their performance is still far from the expectation. This work aims to develop a predictive model to enhance siRNA knockdown efficacy prediction. The key idea is to combine both the rule -- based and the model -- based approaches. To this end, views of siRNAs that integrate available siRNA design rules are first learned using an adaptive Fuzzy C Means (FCM) algorithm. The learned views and other properties of siRNAs are combined to final representations of siRNAs. The elastic net regression method is employed to learn a predictive model from these final representations. Experiments on …",197,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=BcrsrbwAAAAJ&citation_for_view=BcrsrbwAAAAJ:u-x6o8ySG0sC,,1
2516,An approach for the verification of UML models using B,We describe the formal verification of UML models using B abstract machines and a support tool (AtelierB). We transform the UML metamodel to B and automatically check proof obligations generated by using the B proven The correctness of the properties of UML models is ensured by the well-formedness rules in the UML semantics which are transformed to B as the invariants of abstract machines. We address the class diagram and study the Core Package (backbone and relationships) of the UML metamodel as well as the well-formedness rules of these packages.,198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:3fE2CSJIrl8C,,1
2517,Analyzing RBAC security policy of implementation using AST,"Security policy is a critical property in software applications which require high levels of safety and security. It has to be clearly specified in requirement documents and its implementation must be conformed to the specification. In this paper, we propose an approach to check if the implementation is in accordance with its security policy specification. We use the abstract syntax tree (AST), another manner of expressing the program, to analyze the source code and specify user permission policy in software systems by role-based access control (RBAC).",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:ULOm3_A8WrAC,,1
2518,Modeling and verifying ws-cdl using event-b,"The Web Services Choreography Description Language (WS-CDL) is an XML-based language that describes web service composition in the view point of choreography by defining their common and complementary observable behavior, where ordered message exchanges result in accomplishing a common business goal [3]. However, WS-CDL does not come with formal specification, nor with official vefication tools. In this paper, we present an approach to formalize and verify choreography composition described in WS-CDL. In the first phase, we propose to use Event-B as a formal method to model choreography interactions by transforming WS-CDL entities to Event-B elements. We use the Rodin platform, in the next phase, to verify some properties of the translated model. Finally, we run an example to illustrate our approach in detail.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:L8Ckcad2t8MC,,1
2519,A fast algorithm to compute heap memory bounds of Java Card applets,"In this paper, we present an approach to find upper bounds of heap space for Java Card applets. Our method first transforms an input bytecode stream into a control flow graph (CFG), and then collapses cycles of the CFG to produce a directed acyclic graph (DAG). Based on the DAG, we propose a linear-time algorithm to solve the problem of finding the single-source largest path in it. We also have implemented a prototype tool, tested it on several sample applications, and then compared the bounds found by our tool with the actual heap bounds of the programs. The experiment shows that our tool returns good estimation of heap bounds, runs fast, and has a small memory footprint.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:YOwf2qJgpHMC,,1
2520,Verification of behavioural elements of UML models using B,"This paper describes the formal verification of behavioural elements of UML models using B abstract machines. We transform the UML metamodel of behavioural diagrams to B and automatically check proof obligations generated by using the B prover. The correctness of the properties of behavioural elements of UML models is ensured by the well-formedness rules in the UML semantics which are transformed to B as the invariants of abstract machines. We address collaboration diagrams, state-chart diagrams of UML models and study the Behavioural Elements package (Collaboration and State Machine) of the UML metamodel as well as well-formedness rules of these packages. We illustrate our approach by a case study.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:kNdYIx-mwKoC,,1
2522,Verification of uml model elements using b,"This paper describes the formal verification of UML model elements using B abstract machines. We study the UML metamodel of class diagrams, collaboration diagrams and state-chart diagrams as well as their well-formedness rules. Each element of UML models which is an instance of a metaclass, is transformed into a B abstract machine. The relationship between abstract machines is organised using the abstract syntax of UML class diagram of the UML metamodel. B specifications are proved by a B prover which generates automatically proof obligations, allowing UML model elements to be verified. The correctness of the UML model elements is ensured by the well-formedness rules which are transformed to B invariants. We illustrate our approach by a simple case study, the printing system.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:aqlVkmm33-oC,,1
2523,An approach to modeling and estimating power consumption of mobile applications,"Recently, the number of mobile users grow enormously. Even though hardware technologies have been taken to many advantages, which aim at increasing the battery capacity, people are still facing with the problem of battery shortage. An application that runs functionally correct and has a friendly graphic user interfaces still causes users frustrated if it drains the battery. In parallel with increasing the energy storage capability of mobile device, optimizing the source code to reduce power consumption is an emerging topic. This paper presents a new approach to modeling and evaluating power consumption of mobile applications. First, we introduce new definitions of power states and a power consumption automaton (PCA) for a hardware component. In the next step, we propose algorithms to optimize and merge these into an unified automaton. In order to estimate the power consumption amount, the power …",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:35N4QoGY0k4C,,1
2524,Modeling and verifying imprecise requirements of systems using Event-B,"Formal methods are mathematical techniques for describing system model properties. Such methods providing frameworks to specify and verify the correctness of systems which are usually described by precise requirements. In fact, system requirements are sometimes described with vague, imprecise, uncertain, ambiguous, or probabilistic terms. In this paper, we propose an approach to model and verify software systems with imprecise requirements using a formal method, e.g. Event-B. In the first step, we generalize our approach by representing some fuzzy concepts in the classical set theory. We then use such definitions to formalize the fuzzy requirements in Event-B and finally verify its properties such as safety, inconsistency and redundancy by using the Rodin tool. We also take a case study to illustrate the approach in detail.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:qUcmZB5y_30C,,1
2525,Coordinated consensus analysis of multi-agent systems using Event-B,"SOCK is a process calculus for the modeling of service oriented systems recently extended with primitives for dynamic fault and compensation handling. In this paper we investigate the relationships between the sagas calculi for compensable flow composition and SOCK. First, we present an encoding of parallel sagas (with interruption and centralized compensation) into SOCK. Then, we discuss a new semantics for parallel sagas that we consider more adequate to the dynamic approach to fault and compensation handling.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:4TOpqqG69KYC,,1
2526,Utilisation de B pour la vérification de spécifications UML et le développement formel orienté objet,"Le couplage des approches orientées objets avec la méthode B est une piste pour l'amélioration de l'activité de spécification et de développement de logiciels. La méthode B fournit des notations et des outils supports puissants permettant de modéliser et de vérifier des modèles. Les approches objets fournissent des mécanismes intéressants pour la structuration et le développement de gros systèmes. L'apport de notre travail de thèse contribue aux activités de couplage entre ces deux formalismes en utilisant le prouveur de B pour valider et vérifier des spécifications UML. En étendant les schémas de dérivation d'UML vers B proposés dans des travaux précédents réalisés dans l'équipe de recherche Dédale, nous proposons une approche de dérivation en B de méta-modèles UML, de diagrammes statiques et de diagrammes dynamiques. L'objectif de cette proposition est de vérifier la sémantique et la cohérence entre différents diagrammes de spécifications UML. Notre thèse apporte aussi une contribution au développement de spécifications objets en utilisant la méthode B. La première proposition concerne la prise en compte de certains types d'associations entre classes lors de la dérivation en B. La deuxième proposition concerne la validation de spécifications orientées objets décrites à l'aide de diagrammes de séquence UML2. 0.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:4DMP91E08xMC,,1
2527,Checking the compliance of timing constraints in software applications,"Timing constraints play an important role in software development, particularly in real time systems. This paper proposes an approach for checking the compliance of execution of tasks in an application with their timing constraints. In this approach, timing constraints are specified by UML timing diagrams. Aspect oriented programming technology is used to observe the execution of the program and to check if the execution of tasks satisfies their timing constraint specification.The approach is illustrated by a case study of an automatic teller machine system.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:_kc_bZDykSQC,,1
2528,Using b to verify the weaving of aspects,"Aspect J is an aspect-oriented extension of the Java language that enables a modular implementation of crosscutting concerns. Despite this, aspects lack support for formal specification and verification. This paper expresses the base class and some related aspects of Aspect J model in B notation. It aims to benefit from proof obligations generated by B tools to ensure the correctness of Aspect J component composition. Static crosscuts of aspects are guaranteed by proof obligations of relation clauses between B abstract machines and dynamic crosscuts are proved by proof obligations of B refinement machines. This approach is illustrated by verifying a simple example.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:bEWYMUwI8FkC,,1
2529,Validation of UML static diagrams using B,"We present an approach for the validation of UML models and OCL constraints based on the B method. Class diagrams, object diagrams and OCL constraints are transformed into B abstract machines which are then combined into a single B specification. UML object diagrams provide test cases to verify association constraints in class diagrams and OCL constraints. Proof obligations automatically generated by the support tool (AtelierB) are used to validate the specified system.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:KlAtU1dfN6UC,,1
2530,Verifying eventuality properties of imprecise system requirements using Event-B,"Formally checking properties of functional requirements needs precise descriptions. However, the stakeholders sometimes describe the system with ambiguous, vague or fuzzy terms, hence formal frameworks for modeling and verifying such requirements are desirable. As one such, the Fuzzy If-Then rules have been used, but analyzing their functional properties still needs new methods. In this paper, we propose a refinement-based modeling approach for specification and verification of such requirements which are represented by Fuzzy If-Then rules. First, we make use of Event-B refinement where we provide a set of translation rules from Fuzzy If-Then rules to Event-B notations, then show how to verify both safety and eventuality properties with RODIN/Event-B. Finally, we illustrate the proposed method on an example case of Crane Controller.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:k_IJM867U9cC,,1
2531,Formal modeling and verification of context-aware systems using event-b,"Context awareness is a computing paradigm that makes applications responsive and adaptive with their environment. Formal modeling and verification of context-aware systems are challenging issues in the development as they are complex and uncertain. In this paper, we propose an approach to use a formal method Event-B to model and verify such systems. First, we specify a context aware system’s components such as context data entities, context rules, context relations by Event-B notions. In the next step, we use the Rodin platform to verify the system’s desired properties such as safety properties. It aims to benefit from natural representation of context awareness concepts in Event-B and proof obligations generated by refinement mechanism to ensure the correctness of systems. We illustrate the use of our approach on a simple example.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:4JMBOYKVnBMC,,1
2532,Consistency between UCM and PSMs in component models,"In software engineering, the earlier the detection of faults, the less expensive the correction of faults. This paper proposes an approach for detecting the conformability in component models. It focuses on verifying the internal specification of component interfaces with the maps described the execution of tasks between components. Firstly, protocol state machines (PSMs) associating component interfaces are expressed by B abstract machines. Then, the UCM (Use Case Maps) describing a scenario execution in component interaction is formalised by a B implementation machine. The consistency between UCM and PSMs thus can be automatically validated by B provers.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:iH-uZ7U-co4C,,1
2533,Formal Analysis of Database Trigger Systems Using Event-B,"Most modern relational database systems use triggers to implement automatic tasks in response to specific events happening inside or outside a system. A database trigger is a human readable block code without any formal semantics. Frequently, people can check if a trigger is designed correctly after it is executed or by manual checking. In this article, the authors introduce a new method to model and verify database trigger systems using Event-B formal method at design phase. First, the authors make use of similar mechanism between triggers and Event-B events to propose a set of rules translating a database trigger system into Event-B constructs. Then, the authors show how to verify data constraint preservation properties and detect infinite loops of trigger execution with RODIN/Event-B. The authors also illustrate the proposed method with a case study. Finally, a tool named Trigger2B which partly supports the …",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:zA6iFVUQeVQC,,1
2534,Modeling and verifying DML triggers using Event-B,"Database trigger is a block code that automatically executes in response to changes of table or view in the database system. The correctness of a trigger usually can be verified when it is executed. It is apparently useful if we can detect the trigger system’s errors in the design phase. In this paper, we introduce an approach to model and verify data manipulation language (DML) triggers in the database system by a formal method. In the first phase, we formalize a database trigger system by an Event-B model. After that, we use the Rodin tool to verify some properties of the system such as termination, preservation of constraint rules. We also run an example to illustrate the approach in detail.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:-f6ydRqryjwC,,1
2535,Validation of UML scenarios using the B Prover,"We propose an approach to validate object-based specifications by checking that sequence diagrams can be executed in the B world and do not conflict with safety and dynamic properties. The process begins by a UML specification in the form of a class diagram and sequence diagrams which express scenarios modelling the system's behaviour. These diagrams are transformed into a B specification which is completed by the definition of the operations (messages in the sequence diagrams corresponding to the methods in the class diagram), safety and dynamic properties on the system. The validation of scenarios and the satisfaction of the properties is done by means of a theorem prover. The approach is illustrated on a simplified case study: the access control of persons to a building.",198,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=R_D1SGAAAAAJ&citation_for_view=R_D1SGAAAAAJ:qxL8FJ1GzNcC,,1
2536,"Carbon storage of a tropical mangrove forest in Mui Ca Mau National Park, Vietnam","Mangrove forests constitute the most important sink of carbon (C) in the tropics, the conservation of which is an essential mean in offsetting C emissions and climate change. Mangrove forests are therefore suggested to be an important component of reducing emissions from deforestation and degradation (REDD +) schemes, which require scrupulous quantification of ecosystem C storage in order to monitor temporal C sequestration and emissions. Despite this, proportionally less is known about ecosystem C storage of mangrove forests in Vietnam, where these systems constitute a large proportion of its coastline. In this study, ecosystem C storage of a tropical mangrove forest in Mui Ca Mau National Park, Vietnam (CMNP) was quantified by measuring biomass of trees, roots, and downed woody debris, and sediment organic C and overall depth. Results showed that above- and below-ground C stock ranged from …",204,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6Tdv9T8AAAAJ&citation_for_view=6Tdv9T8AAAAJ:q3CdL3IzO_QC,,1
2537,"A cross-system analysis of sedimentary organic carbon in the mangrove ecosystems of Xuan Thuy National Park, Vietnam","A cross-system analysis of bulk sediment composition, total organic carbon (TOC), atomic C/N ratio, and carbon isotope composition (δ13C) in 82 surface sediment samples from natural and planted mangrove forests, bank and bottom of tidal creeks, tidal flat, and the subtidal habitat was conducted to examine the roles of mangroves in sedimentation and organic carbon (OC) accumulation processes, and to characterize sources of sedimentary OC of the mangrove ecosystem of Xuan Thuy National Park, Vietnam. Sediment grain sizes varied widely from 5.4 to 170.2 μm (mean 71.5 μm), with the fine sediment grain size fraction (< 63 μm) ranging from 11 to 99.3% (mean 72.5%). Bulk sediment composition suggested that mangroves play an important role in trapping fine sediments from river outflows and tidal water by the mechanisms of tidal current attenuation by vegetation and the ability of fine roots to bind …",204,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6Tdv9T8AAAAJ&citation_for_view=6Tdv9T8AAAAJ:2osOgNQ5qMEC,,1
2538,"Carbon storage in a restored mangrove forest in Can Gio Mangrove Forest Park, Mekong Delta, Vietnam","Mangrove forests are considered to be the most important carbon (C) sink in the tropics. Evaluation of ecosystem C storage in restored mangrove forests will provide important information for management and conservation practices, climate change mitigation strategies and reduction of emissions from deforestation and forest degradation (REDD+) schemes. Despite this, a gap remains in the understanding of ecosystem C storage in Can Gio Mangrove Forest Park (CGM), the largest restored mangrove forest area in the Mekong Delta, Vietnam. In the present study, ecosystem C storage was quantified in CGM in growing mangrove forests, a typhoon disturbed forest, and a mudflat by measuring the biomass of trees, roots, downed woody debris, sediment organic C, and overall depth. The mean above-ground C storage was 102 ± 24.7, 298.1 ± 14.1 and 243.6 ± 40.4 MgC ha−1 for fringe, transition, and interior forests …",204,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6Tdv9T8AAAAJ&citation_for_view=6Tdv9T8AAAAJ:9pM33mqn1YgC,,1
2539,Food sources of macro-invertebrates in an important mangrove ecosystem of Vietnam determined by dual stable isotope signatures,"Dual stable isotope signatures (δ13C and δ15N) were applied to determine the contribution of mangrove materials and other organic carbon sources to the invertebrate community in an ecologically important mangrove ecosystem of Vietnam. We have analyzed 181 specimens of 30 invertebrate species and found δ13C and δ15N ranging from − 14.5 to − 26.8‰ and from 1.3 to 12.1‰, respectively. From taxa measured for stable isotopes, polychaete, gastropods, bivalves, and grapsid crabs living in mangrove forest showed relative low δ13C values, while fiddler crabs inhabiting in the land–water ecotone showed the highest δ13C values. The δ13C showed that just a few mangrove inhabitants directly relied on the mangrove materials. The wide ranges of δ13C and δ15N signatures indicated that the invertebrates utilized heterogeneous diets, comprising benthic microalgae, marine phytoplankton, particulate organic …",204,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6Tdv9T8AAAAJ&citation_for_view=6Tdv9T8AAAAJ:W7OEmFMy1HYC,,1
2540,"The application of δ13C and C/N ratios as indicators of organic carbon sources and paleoenvironmental change of the mangrove ecosystem from Ba Lat Estuary, Red River, Vietnam","In this study, two sediment cores (~70 cm) were collected from separate mangrove forests straddling the Ba Lat Estuary, Red River of northern Vietnam, to examine the origins of sedimentary organic carbon (SOC) and reconstruct the paleoenvironment. In addition, mangrove leaves and particulate organic matter were collected and measured for δ13C to trace the origins of SOC. The cores were analyzed by high-resolution sections for δ13C, TOC, C/N ratios, sediment grain size, water content, and porosity, with values of δ13C, TOC, and C/N ratios ranging from −28.19 to −22.5‰, 2.14–30.94 mg/g, and 10.29–18.32, respectively. The δ13C and TOC relationship indicated that there were some small residual effects of diagenetic processes on TOC and δ13C values in mangrove sediments. However, the shifts of δ13C and C/N ratios from the bottom to the surface sediment of the cores explained the change in …",204,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6Tdv9T8AAAAJ&citation_for_view=6Tdv9T8AAAAJ:d1gkVwhDpl0C,,1
2541,"Historical Profiles of Trace Element Concentrations in Mangrove Sediments from the Ba Lat Estuary, Red River, Vietnam","Historical profiles of trace element concentrations were reconstructed from two mangrove sediment cores collected within the Ba Lat Estuary (BLE), Red River, Vietnam. Chronologies of sediment cores were determined by the 210Pb method, which showed that each respective sediment core from the south and north entrances of BLE provided a record of sediment accumulation spanning approximately 100 and 60 years. The profiles of Pb, Zn, Cu, Cr, V, Co, Sb, and Sn concentrations markedly increased from the years of the 1920s–1950s, and leveled out from 1950s–1980s, and then gradually decreased from 1980s to present. The profiles of Cd and Ag concentrations increased from 1920s–1940s, and then decreased from 1940s to present. The profile of Mo concentrations progressively increased from 1920s–1980s, then decreased to present. The Mn concentrations failed to show a clear trend in both …",204,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6Tdv9T8AAAAJ&citation_for_view=6Tdv9T8AAAAJ:9yKSN-GCB0IC,,1
2542,"An indicator-based approach to quantifying the adaptive capacity of urban households: the case of Da Nang city, Central Vietnam","Municipal governments have to contend with climate change dynamics with household-level adaptive capacity becoming an increasingly important component to the urban setting. However, methods in the quantification of adaptation, and what factors may drive adaptation at the urban household level, remain poorly resolved. In this study, we proposed an empirical approach and a set of 17 adaptive indicators to assess the adaptive capacity of urban households to climate change. The adaptive indicators were tested from three urban districts of the Da Nang city, Vietnam. A household survey using socioeconomic data was conducted in 617 urban households. The empirical results showed that inherent capacity of urban households was relatively high in term of wealth, housing condition and durable assets, and low for livelihood diversity and knowledge to climate change. Six components, consisting of household …",204,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6Tdv9T8AAAAJ&citation_for_view=6Tdv9T8AAAAJ:ClCfbGk0d_YC,,1
2543,"Assessment of Vietnam Coastal Wetland Vulnerability for Sustainable Use (Case Study in Xuanthuy Ramsar Site, Vietnam)","For sustainable use of coastal wetland ecosystems, vulnerability of the Xuan Thuy Ramsar site in the northern coast of Vietnam, an internationally important area for feeding and roosting of migratory water birds, is assessed. The assessment tools include 1) a geographical information system database, which describes physical, socio-economic, and environmental conditions; 2) spatial multi-criteria decision analysis, and 3) evaluating criteria by a pair-wise comparison method. Consequently, coastal vulnerability criteria were determined following three periods of a year with different density of migrating birds: the highest density period–from September to March of the following year; the average density period–April and from August to October; the lowest density period–the remaining time in year. For each period, 5 levels of vulnerability (from 1-lowest to 5-highest level) were determined. High vulnerability level is concentrated in a great area, belonging to the ecological restored area and the south of communes Giao Thien, Giao Xuan; Low vulnerability level is distributed in communes of the buffer zone. Correspondingly to each vulnerability level, recommendations are given for the risk management, resource use, environment protection and human activities.",204,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6Tdv9T8AAAAJ&citation_for_view=6Tdv9T8AAAAJ:u-x6o8ySG0sC,,1
2544,"Sources and Exchange of Particulate Organic Matter in an Estuarine Mangrove Ecosystem of Xuan Thuy National Park, Vietnam","The spatio-temporal variations in stable isotope signatures (δ13C and δ15N) and C/N ratios of particulate organic matter (POM), and physicochemical parameters in a creek water column were examined in an estuarine mangrove ecosystem of Xuan Thuy National Park, Vietnam. The objective was to examine the factors influencing creek water properties, and the sources and exchange of POM in this important mangrove ecosystem. The diel and seasonal variations in water temperature, flow velocity, pH, dissolved oxygen, and salinity demonstrated that tidal level, season, and biological factors affected the creek water properties. Mangroves had relatively low δ15N and very low δ13C values, with respective average values of 1.5 ± 0.9‰ and −28.1 ± 1.4‰. The low mangrove leaf δ15N indicated minor anthropogenic nitrogen loading to the mangrove forests. A significant positive correlation between POM …",204,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6Tdv9T8AAAAJ&citation_for_view=6Tdv9T8AAAAJ:_FxGoFyzp5QC,,1
2545,Detection of potentially novel paramyxovirus and coronavirus viral RNA in bats and rats in the Mekong Delta region of southern Viet Nam,"Bats and rodents are being increasingly recognized as reservoirs of emerging zoonotic viruses. Various studies have investigated bat viruses in tropical regions, but to date there are no data regarding viruses with zoonotic potential that circulate in bat and rat populations in Viet Nam. To address this paucity of data, we sampled three bat farms and three wet markets trading in rat meat in the Mekong Delta region of southern Viet Nam. Faecal and urine samples were screened for the presence of RNA from paramyxoviruses, coronaviruses and filoviruses. Paramyxovirus RNA was detected in 4 of 248 (1%) and 11 of 222 (4.9%) bat faecal and urine samples, respectively. Coronavirus RNA was detected in 55 of 248 (22%) of bat faecal samples; filovirus RNA was not detected in any of the bat samples. Further, coronavirus RNA was detected in 12 of 270 (4.4%) of rat faecal samples; all samples tested negative for …",204,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6Tdv9T8AAAAJ&citation_for_view=6Tdv9T8AAAAJ:LdasjJ6CEcoC,,1
2546,Will restored mangrove forests enhance sediment organic carbon and ecosystem carbon storage?,"Mangrove forests are among the largest carbon (C) sinks in (sub) tropical areas, playing a vital role in reducing C emissions. However, these forests have degraded rapidly in recent decades due to various human activities and natural disturbances. Thus, mangrove restoration programs have been widely initiated to prevent further forest loss and to mitigate climate change. The measurement of ecosystem C storage in restored mangrove forests will provide crucial information for the temporal reporting of C emissions in reducing emissions from deforestation and forest degradation (REDD+) schemes. The question whether restored mangrove forests enhance sediment organic carbon (OC) and ecosystem C storage has not been resolved for mangrove restoration programs. The present study, therefore, aimed to quantify ecosystem C storage and the contribution of organic matter sources to sedimentary OC in Kandelia …",204,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6Tdv9T8AAAAJ&citation_for_view=6Tdv9T8AAAAJ:XvxMoLDsR5gC,,1
2548,Nonlinear thermomechanical buckling and post-buckling response of porous FGM plates using Reddy's HSDT,"This work presents an analytical approach to investigate buckling and post-buckling behavior of FGM plate with porosities resting on elastic foundations and subjected to mechanical, thermal and thermomechanical loads. The formulations are based on Reddy's higher-order shear deformation plate theory taking into consideration Von Karman nonlinearity, initial geometrical imperfections, and Pasternak type of elastic foundations. By applying Galerkin method, closed-form relations of buckling loads and post-buckling equilibrium paths for simply supported plates are determined. Numerical results are carried out to show the effects of porosity distribution characteristics (Porosity-I and Porosity-II), geometrical parameters, material properties and elastic foundations on the mechanical, thermal and thermomechanical buckling loads and post-buckling resistance capacity of the porous FGM plates.",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:AHdEip9mkN0C,,1
2549,Thermal and mechanical stability of functionally graded carbon nanotubes (FG CNT)-reinforced composite truncated conical shells surrounded by the elastic foundations,"The thermal and mechanical stability of a functionally graded composite truncated conical shell reinforced by carbon nanotube fibers and surrounded by the elastic foundations are studied in this paper. Distribution of reinforcements across the shell thickness is assumed to be uniform or functionally graded. The equilibrium and linearized stability equations for the shells are derived based on the classical shell theory. Using Galerkin method, the closed – form expression for determining the linear thermal and mechanical buckling load is obtained. The paper also analyzed and discussed the effects of semi-vertex angle, shell length, volume fraction of fibers, distribution pattern of fibers, temperature, elastic foundations on the linear thermal and mechanical buckling loads of the functionally graded carbon nanotube fibers-reinforced composite (FG CNTRC) truncated conical shell in thermal environment.",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:EYYDruWGBe4C,,1
2550,Nonlinear analysis of stability for functionally graded plates under mechanical and thermal loads,"This paper presents a simple analytical approach to investigate the stability of functionally graded plates under in-plane compressive, thermal and combined loads. Material properties are assumed to be temperature-independent, and graded in the thickness direction according to a simple power law distribution in terms of the volume fractions of constituents. Equilibrium and compatibility equations for functionally graded plates are derived by using the classical plate theory taking into account both geometrical nonlinearity in von Karman sense and initial geometrical imperfection. The resulting equations are solved by Galerkin procedure to obtain explicit expressions of postbuckling load–deflection curves. Stability analysis of a simply supported rectangular functionally graded plate shows the effects of the volume fraction index, plate geometry, in-plane boundary conditions, and imperfection on postbuckling behavior …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:u5HHmVD_uO8C,,1
2551,Nonlinear dynamic response and vibration of sandwich composite plates with negative Poisson’s ratio in auxetic honeycombs,"Auxetic cellular solids in the forms of honeycombs under blast load have great potential in a diverse range of applications, including core material in sandwich plates composite components. Based on Reddy’s first-order shear deformation plate theory, this paper presents an analysis of the nonlinear dynamic response and vibration of sandwich plates with negative Poisson’s ratio in auxetic honeycombes on elastic foundations subjected to blast and mechanical loads. A three-layer sandwich plate is considered discretized in the thickness direction by using analytical methods (stress function method, approximate solution), Galerkin method, and fourth-order Runge-Kutta method. The results show the effects of geometrical parameters, material properties, mechanical and elastic foundations on the nonlinear dynamic response, and vibration of sandwich plates.",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:anf4URPfarAC,,1
2552,Mechanical and thermal postbuckling of higher order shear deformable functionally graded plates on elastic foundations,"This paper presents an analytical investigation on the buckling and postbuckling behaviors of thick functionally graded plates resting on elastic foundations and subjected to in-plane compressive, thermal and thermomechanical loads. Material properties are assumed to be temperature independent, and graded in the thickness direction according to a simple power law distribution in terms of the volume fractions of constituents. The formulations are based on higher order shear deformation plate theory taking into account Von Karman nonlinearity, initial geometrical imperfection and Pasternak type elastic foundation. By applying Galerkin method, closed-form relations of buckling loads and postbuckling equilibrium paths for simply supported plates are determined. Analysis is carried out to show the effects of material and geometrical properties, in-plane boundary restraint, foundation stiffness and imperfection on the …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:d1gkVwhDpl0C,,1
2553,Nonlinear thermal dynamic analysis of eccentrically stiffened S-FGM circular cylindrical shells surrounded on elastic foundations using the Reddy's third-order shear …,"This paper presents an analytical investigation on nonlinear thermal dynamic behavior of imperfect functionally graded circular cylindrical shells eccentrically reinforced by outside stiffeners and surrounded on elastic foundations using the Reddy's third order shear deformation shell theory in thermal environment. Material properties are graded in the thickness direction according to Sigmoid power law distribution (S-FGM) in terms of the volume fractions of constituents with metal–ceramic–metal layers. The shells are affected by mechanical, damping loads and temperature. The stress function and the Bubnov–Galerkin method are applied. Unlike previous publications, we propose a general formulation for forces and moments which allow the non-linear dynamic of shear deformable eccentrically stiffened shell to be studied taking into account the thermal stress in both the shells and the stiffeners. Numerical results …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:nb7KW1ujOQ8C,,1
2554,Analysis of bi-directional functionally graded plates by FEM and a new third-order shear deformation plate theory,"Modern structures and components may require advanced materials whose properties vary continuously not only in one specified direction, but also different other directions. In particular, the bi-directional functionally graded materials (2D-FGMs) introduced are expected to have more effective properties, consequently eliminating commonly awkward problems such as local stress concentrations and delamination. In this paper, buckling and bending behaviors of 2D-FGM plates, which are of great importance in the design and development of engineering applications, are numerically analyzed by a finite element model. The plate kinematics are described using a new third-order shear deformation plate theory (TSDT), without the need for special treatment of shear-locking effect and shear correction factors. The present TSDT theory based on rigorous kinematic of displacements, which is shown to be dominated over …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:6ZxmRoH8BuwC,,1
2555,Dynamic response and vibration of composite double curved shallow shells with negative Poisson's ratio in auxetic honeycombs core layer on elastic foundations subjected to …,"The purpose of the present study is to investigate dynamic response and vibration of composite double curved shallow shells with negative Poisson's ratios in auxetic honeycombs core layer on elastic foundations subjected to blast and damping loads using analytical solution. This study considers composite double curved shallow shells with auxetic core which have three layers in which the top and bottom outer skins are isotropic aluminum materials; the central layer has honeycomb structure using the same aluminum material. Based on the first order shear deformation theory (FSDT) with the geometrical nonlinear in von Karman and using Airy stress functions method, Galerkin method and the fourth-order Runge–Kutta method, the resulting equations are solved to obtain expressions for nonlinear motion equations. The effects of geometrical parameters, material properties, elastic foundations Winkler and …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:WJVC3Jt7v1AC,,1
2556,New approach to investigate nonlinear dynamic response and vibration of imperfect functionally graded carbon nanotube reinforced composite double curved shallow shells …,"This paper presents a new approach – using analytical solution to investigate nonlinear dynamic response and vibration of imperfect functionally graded carbon nanotube reinforced composite (FG-CNTRC) double curved shallow shells. The double curved shallow shells are reinforced by single-walled carbon nanotubes (SWCNTs) which vary according to the linear functions of the shell thickness. The shells are resting on elastic foundations and subjected to blast load and temperature. The shell's effective material properties are assumed to depend on temperature and estimated through the rule of mixture. By applying higher order shear theory, Galerkin method and fourth-order Runge–Kutta method and the Airy stress function, nonlinear dynamic response and natural frequency for thick imperfect FG-CNTRC double curved shallow shells are determined. In numerical results, the influences of geometrical …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:OTTXONDVkokC,,1
2557,Static response and free vibration of functionally graded carbon nanotube-reinforced composite rectangular plates resting on Winkler–Pasternak elastic foundations,"In the present article, static response and free vibration of functionally graded carbon nanotube reinforced composite (FG-CNTRC) rectangular plate resting on Winkler–Pasternak elastic foundations using an analytical approach are studied. The rectangular plates are reinforced by single-walled carbon nanotubes (SWCNTs) which are assumed to be graded through the thickness direction with four types of distributions. The mathematical model of the FG-CNTRC plate is developed based on the first-order shear deformation plate theory (FSDT) and Hamilton principle. By using Navier solution, the governing equations are solved to obtain the central deflection and the natural frequency parameters. Several examples are verified to have higher accuracy than those from the previous method in the literature. Also, the effects of different parameters on static response and natural frequency of FG-CNTRC plate are …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:lmc2jWPfTJgC,,1
2558,Nonlinear thermo-electro-mechanical dynamic response of shear deformable piezoelectric sigmoid functionally graded sandwich circular cylindrical shells on elastic foundations,"In this paper, we study the nonlinear dynamic response of higher order shear deformable sandwich functionally graded circular cylindrical shells with outer surface-bonded piezoelectric actuator on elastic foundations subjected to thermo-electro-mechanical and damping loads. The sigmoid functionally graded material shells are made of the metal–ceramic–metal layers with temperature-dependent material properties. The governing equations are established based on Reddy’s third-order shear deformation theory using the stress function, the Galerkin method and the fourth-order Runge–Kutta method. Numerical results are given to demonstrate the influence of geometrical parameters, material properties, imperfection, elastic foundations, and thermo-electro-mechanical and damping loads on the nonlinear dynamic response of the shells. Accuracy of the present formulation is shown by comparing the results of …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:86PQX7AUzd4C,,1
2559,Nonlinear dynamic response of imperfect eccentrically stiffened FGM double curved shallow shells on elastic foundation,"This paper presents an analytical investigation on the nonlinear dynamic response of eccentrically stiffened functionally graded double curved shallow shells resting on elastic foundations and being subjected to axial compressive load and transverse load. The formulations are based on the classical shell theory taking into account geometrical nonlinearity, initial geometrical imperfection and the Lekhnitsky smeared stiffeners technique with Pasternak type elastic foundation. The non-linear equations are solved by the Runge-Kutta and Bubnov-Galerkin methods. Obtained results show effects of material and geometrical properties, elastic foundation and imperfection on the dynamical response of reinforced FGM shallow shells. Some numerical results are given and compared with ones of other authors.",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:LkGwnXOMwfcC,,1
2560,Hybrid phase field simulation of dynamic crack propagation in functionally graded glass-filled epoxy,"Numerical simulation of dynamic crack propagation in functionally graded glass-filled epoxy (FG) beams using a regularized variational formulation is presented. The Griffith’s theory based hybrid phase field approach for diffusive fracture is taken, which is able to accurately simulate complex behaviors of dynamic crack growth in FGMs. The FG beams under impact loads experimented by Kirugulige and Tippur (Exper. Mech. 2006; 46:269–281) are considered, taking the same configurations, material property, crack location, and other relevant assumptions. The crack paths, crack length, crack velocity, energies, etc., computed through the hybrid phase field model are numerically analyzed, and some of those results are directly compared with the experimental data. Due to lack of necessary information regarding impact loading profiles and boundary conditions in setting the tests, the simulations become difficult as an …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:tOudhMTPpwUC,,1
2561,Nonlinear dynamic analysis and vibration of shear deformable piezoelectric FGM double curved shallow shells under damping-thermo-electro-mechanical loads,"Nonlinear dynamic analysis and vibration of imperfect functionally graded materials (FGM) thick double curved shallow shells with piezoelectric actuators on elastic foundations subjected to the combination of electrical, thermal, mechanical and damping loading are investigated in this paper. Material properties of FGM shells are assumed to be temperature dependent and graded in the thickness direction according to a simple power-law distribution in terms of the volume fractions of the constituents. The governing equations are established based on Reddy’s higher order shear deformation theory that includes thermo-piezoelectric effects and are solved by the stress function, the Galerkin method and fourth-order Runge–Kutta method. In numerical results, the effects of geometrical parameters; the material properties; imperfections; elastic foundations; electrical, mechanical, thermal and damping loads on the …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:zA6iFVUQeVQC,,1
2562,Nonlinear dynamic response and vibration of functionally graded carbon nanotube-reinforced composite (FG-CNTRC) shear deformable plates with temperature-dependent material …,"Based on Reddy’s third-order shear deformation plate theory, the nonlinear dynamic response and vibration of imperfect functionally graded carbon nanotube-reinforced composite (FG-CNTRC) plates on elastic foundations subjected to dynamic loads and temperature are presented. The plates are reinforced by single-walled carbon nanotubes which vary according to the linear functions of the plate thickness. The plate’s effective material properties are assumed to depend on temperature and estimated through the rule of mixture. By applying the Airy stress function, Galerkin method and fourth-order Runge–Kutta method, nonlinear dynamic response and natural frequency for imperfect FG-CNTRC plates are determined. In numerical results, the influences of geometrical parameters, elastic foundations, initial imperfection, dynamic loads, temperature increment, and nanotube volume fraction on the nonlinear …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:4hFrxpcac9AC,,1
2563,New approach to study nonlinear dynamic response and vibration of sandwich composite cylindrical panels with auxetic honeycomb core layer,"The main goal of this study is using analytical solution to investigate the nonlinear dynamic response and vibration of sandwich auxetic composite cylindrical panels. The sandwich composite panels have three layers in which the top and bottom outer skins are isotropic aluminum materials, the central auxetic core layer – honeycomb structures with negative Poisson's ratio using the same aluminum material. The panels are resting on elastic foundations and subjected to mechanical, blast and damping loads. Based on Reddy's first order shear deformation theory (FSDT) with the geometrical nonlinear in von Karman and using the Airy stress functions method, Galerkin method and fourth-order Runge–Kutta method, the resulting equations are solved to obtain expressions for nonlinear motion equations. The effects of geometrical parameters, material properties, elastic Winkler and Pasternak foundations, mechanical …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:2VqYfGB8ITEC,,1
2564,Nonlinear response of shear deformable FGM curved panels resting on elastic foundations and subjected to mechanical and thermal loading conditions,"This paper presents an analytical investigation on the nonlinear response of thick functionally graded doubly curved shallow panels resting on elastic foundations and subjected to some conditions of mechanical, thermal, and thermomechanical loads. Material properties are assumed to be temperature independent, and graded in the thickness direction according to a simple power law distribution in terms of the volume fractions of constituents. The formulations are based on higher order shear deformation shell theory taking into account geometrical nonlinearity, initial geometrical imperfection and Pasternak type elastic foundation. By applying Galerkin method, explicit relations of load-deflection curves for simply supported curved panels are determined. Effects of material and geometrical properties, in-plane boundary restraint, foundation stiffness and imperfection on the buckling and postbuckling loading capacity …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:mB3voiENLucC,,1
2565,New approach to investigate nonlinear dynamic response of sandwich auxetic double curves shallow shells using TSDT,"Nonlinear dynamic behavior of double curved shallow shells with negative Poisson’s ratios in auxetic honeycombs on elastic foundations subjected blast, mechanical and damping loads is investigated in the present article. This study considers double curved shallow shells with auxetic core which have three layers in which the top and the bottom outer skins are isotropic aluminum materials; the central layer has honeycomb structure using the same aluminum material. Based on the analytical solution, Reddy’s third order shear deformation theory (TSDT) with the geometrical nonlinear in von Karman and Airy stress functions method, Galerkin method and the fourth-order Runge-Kutta method, the resulting equations are solved to obtain expressions for nonlinear motion equations. The effects of geometrical parameters, material properties, elastic foundations, imperfections, blast loads, mechanical and damping loads …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:fFSKOagxvKUC,,1
2566,Nonlinear response of pressure-loaded functionally graded cylindrical panels with temperature effects,"This paper presents an analytical approach to investigate nonlinear response of functionally graded cylindrical panels under uniform lateral pressure with temperature effects are incorporated. Material properties are assumed to be temperature-independent, and graded in the thickness direction according to a simple power law distribution in terms of the volume fractions of constituents. Equilibrium and compatibility equations for cylindrical panels are derived by using the classical shell theory with both geometrical nonlinearity in von Karman–Donnell sense and initial geometrical imperfection are taken into consideration. The resulting equations are solved by Galerkin method to determine explicit expressions of nonlinear load-deflection curves. Stability analysis for a simply supported panel shows the effects of material and geometric parameters, in-plane restraint and temperature conditions, and imperfection on the …",206,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=bD9RE7AAAAAJ&citation_for_view=bD9RE7AAAAAJ:qxL8FJ1GzNcC,,1
2567,Reconstruction of Low Degree B-spline Surfaces with Arbitrary Topology Using Inverse Subdivision Scheme,"Multivariate B-spline surfaces over triangular parametric domain have many interesting properties in the construction of smooth free-form surfaces. This paper introduces a novel approach to reconstruct triangular B-splines from a set of data points using inverse subdivision scheme. Our proposed method consists of two major steps. First, a control polyhedron of the triangular B-spline surface is created by applying the inverse subdivision scheme on an initial triangular mesh. Second, all control points of this B-spline surface, as well as knotclouds of its parametric domain are iteratively adjusted locally by a simple geometric fitting algorithm to increase the accuracy of the obtained B-spline. The reconstructed B-spline having the low degree along with arbitrary topology is interpolative to most of the given data points after some fitting steps without solving any linear system. Some concrete experimental examples are also provided to demonstrate the effectiveness of the proposed method. Results show that this approach is simple, fast, flexible and can be successfully applied to a variety of surface shapes.",217,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:W7OEmFMy1HYC,,1
2568,B-spline surface reconstruction by inverse subdivisions,"This paper presents a method to reconstruct a B-spline surface from a quadrangular mesh, using an inverse Catmull-Clark subdivision. We want to minimize the surface contraction due to the approximating subdivision scheme. We introduce geometrical operations which minimize the impact of the subdivision approximation and can be used in the parametric surface reconstruction. The quality of the method is evaluated by criteria of distances, curvatures or computing time on experimental results.",217,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:u5HHmVD_uO8C,,1
2569,Reconstructing low degree triangular parametric surfaces based on inverse Loop subdivision,"In this paper, we present an efficient local geometric approximate method for reconstruction of a low degree triangular parametric surface using inverse Loop subdivision scheme. Our proposed technique consists of two major steps. First, using the inverse Loop subdivision scheme to simplify a given dense triangular mesh and employing the result coarse mesh as a control mesh of the triangular Bézier surface. Second, fitting this surface locally to the data points of the initial triangular mesh. The obtained parametric surface is approximate to all data points of the given triangular mesh after some steps of local surface fitting without solving a linear system. The reconstructed surface has the degree reduced to at least of a half and the size of control mesh is only equal to a quarter of the given mesh. The accuracy of the reconstructed surface depends on the number of fitting steps k, the number of reversing …",217,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:d1gkVwhDpl0C,,1
2570,Handwriting Recognition Using B-Spline Curve,"This paper aims at presenting novel approach for curve matching and character recognition such as printed writing, handwriting, signatures, etc. based on B-Spline curve. The advantages of the B-Spline that are continuous curve representation and affine invariant, and the robustness. The recognition process is composed of two main steps: sample training and recognition. The computer must be trained with data from bitmap image file. The next step is pre-processing input data from the binary image and finding its skeleton. The reconstruction of a B-Spline curve representing the sample character is applied to find out the control points. Then the sample B-spline curve of each character is stored in a database. For the test character, it has the same process with the sample character. The matching is done by computing the Euclidean distance between the control points of test curve with those of all sample …",217,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:9yKSN-GCB0IC,,1
2572,B-spline surface reconstruction by inverse subdivisions,"B-spline surface reconstruction by inverse subdivisions - Archive ouverte HAL Accéder directement au contenu Accéder directement à la navigation Toggle navigation CCSD HAL HAL HALSHS TEL MédiHAL Liste des portails AURéHAL API Data Documentation Episciences.org Episciences.org Revues Documentation Sciencesconf.org Support HAL - Archives Ouvertes Accueil Dépôt Consultation Les derniers dépôts Par type de publication Par discipline Par année de publication Par structure de recherche Les portails de l'archive Recherche Documentation hal-01311452, version 1 Communication dans un congrès B-spline surface reconstruction by inverse subdivisions Khoi Nguyen Tan 1 Romain Raffin, 1 Marc Daniel 1 Le Cung. Détails 1 LSIS - Laboratoire des Sciences de l'Information et des Systèmes 2 DUT - University of Science and Technology [Danang] Abstract : no abstract Type de document : …",217,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:IjCSPb-OGe4C,,1
2573,A Method to Determine the Characteristic of Object Based on 2D/3D Correspondance,"This paper presents a method to retrieve semantic using combination of images and the corresponding 3D model reconstructed using photogrammetry. Our proposed method for determining the characteristics consists of two major steps. First, recognize characteristics based on the curvature of the point cloud and image processing of input photographies. Second, determine the characteristics of both spaces (2D and 3D) simultaneously. Our proposed method has the advantage of a combination of additional missing information on images and 3D model, to enhance detection and extraction of features. Experimental examples are presented to present the occurrely and the flexitricity of the method that can be applied to a large variety of objects.",217,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:eQOLeE2rZwMC,,1
2574,Approximation of triangular B-spline surfaces by local geometric fitting algorithm,"Surfaces over triangular domain are a powerful and flexible tool for modeling of complex objects with non-rectangular topology. Due to the particular advantages of triangular parametric surfaces, they have wide application and prospect in computer aided design and reverse engineering. The interpolation or approximation problem for reconstructing an arbitrary topological parametric surface from scattered data points or polygonal mesh is one of the significant research areas. This paper deals with the approximation of triangular B-spline surfaces based on a local geometric fitting algorithm and an inverse Loop subdivision scheme. The reconstructed triangular B-spline with the low degree crosses through most of the given data points of an initial triangular mesh. The accuracy of the obtained triangular B-spline surfaces can be carried out by changing the position of control points in the local geometric algorithm as …",217,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:Tyk-4Ss8FVUC,,1
2575,Matching correspondence between images and 3D model in a reconstruction process,"3D reconstruction from photographies is an active research trend. The resolution of the sensors is increasing and the data processing is more accurate (not only restricted to calibrated stereo vision). In archaeological research it becomes a common way to safeguard some views of an ancient site, coupled with a manner to describe in 3D the artifacts with the same set of photographies. Archaeological scientist are now facing a complex problem to handle these digital data. An important usage is to describe semantically the artifacts. It is generally made ""by hand"", supplied by the knowledge of the scientists. We propose a solution that can perform a part of this work automatically, to generate descriptions of the obtained geometry. It combines image processing, geometry processing, 3D reconstruction. This paper aims at presenting an algorithm for 2D/3D point matching. The 3D reconstruction process of model from multiple views based on SIFT algorithm. The matching process uses a 2D mask pattern to lookup the 3D corresponding point. Experimental results show that our matching algorithm is precise, highly flexible, and can be successfully applied to a variety of 3D shapes.",217,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:Y0pCki6q_DkC,,1
2576,Poster: Developing an Intrusion Detection System for Cloud Computing,"Intrusion Detection System (IDS) has been used widely in network systems to detect malicious behaviors which can harm system or computers. However, in practical implementations, an IDS is a complex work and needs investing in the new infrastructure or in training new personnel. Therefore, it is inconsistent with the users who has a small network system. In this paper, we introduce an IDSCloud model as a solution to implementing a network IDS based on cloud computing. The IDSCloud provides network IDS as a service over the internet which can be simple in deployment, maintenance, scalability without investing in the new infrastructure.",217,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:3fE2CSJIrl8C,,1
2577,Reconstruction of Planar Cam Profile Function and its Follower Displacement using B-Spline Curve based on Inverse Subdivision Method and Theory of Contact Relations-Application …,"For controlling the precision of a manufacturing planar cam mechanism, one can base on a set of digital points",217,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:2osOgNQ5qMEC,,1
2578,Analysis of geometrical features of 3D model based on the surface curvature of a set of point cloud,"In this paper, we present a method to analyze features of an unstructured 3D set of points. The method proposed consists in two main stages: 1) From the 3D point input data set, apply an algorithm of sparsity based on octree to subdivide the data surface; 2) Make a 3D reconstruction model from the set of data points after the density reducing, calculate the geometric curvature of the object mesh surface. From the calculated curvature, grouping the points with their similar curvature will be performed to analyze the surface curvature of the 3D object set of points. The results after separating by the parts on the 3D reconstruction model will be used to analyze and evaluate the reconstructed object, separate and identify the parts on the object. The research results can be applied in many fields such as: restoration of archaeological sites, analysis and identification of medical objects, CAD objects. Our work uses the data …",217,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:kNdYIx-mwKoC,,1
2579,An integrated data collection and remote monitoring of power transmission grid using Internet of Things (IoT),"The Internet of Things (IoT) refers to a heterogeneous network of physical and virtual objects embedded. Most of the data collection solution today relies on specialized electronic devices located far apart, then perform data collection and analysis based on a fixed schedule. This paper proposes an improved IoT architecture takes into consideration the environment surrounding the devices, including the objects and fires to be detected, and other factors affecting the remote management and monitoring. We present the results of research and build an Integrated Monitoring System (IMS) based on IoT technology. The collected data in IMS can be used to cater to the different requirements from the various environment. The analysis results indicate that the designed platform is suitable for the resource-constrained IoT architecture and is scalable to be extended in various IoT scenarios.",217,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:5nxA0vEk-isC,,1
2590,Statistical implicative similarity measures for user-based collaborative filtering recommender system,"This paper proposes a new similarity measures for User-based collaborative filtering recommender system. The similarity measures for two users are based on the Implication intensity measures. It is called statistical implicative similarity measures (SIS). This similarity measures is applied to build the experimental framework for User-based collaborative filtering recommender model. The experiments on MovieLense dataset show that the model using our similarity measures has fairly accurate results compared with User-based collaborative filtering model using traditional similarity measures as Pearson correlation, Cosine similarity, and Jaccard.",218,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=igmb0QwAAAAJ&citation_for_view=igmb0QwAAAAJ:WF5omc3nYNoC,,1
2591,Collaborative recommenderation based on statistical implication rules,"In recent researches, many approaches based on association rules have been proposed to improve the accuracy of recommender systems. These approaches are primarily based on Apriori data mining algorithm in order to generate the association rules and apply them to improving the recommendation results. However, these approaches also reveal some disadvantages of the system, such as taking a longer time for generating association rules; applying the Apriori algorithm on rating sparse matrix resulting in irrelevant information and causing poor recommendation results to target users and association rules generated primarily relying on given threshold of Support and Confidence measures leading to the focus on the majority of rules and ignoring the astonishment of rules to affect the recommendation results. In this study, we propose a new model for collaborative filtering recommender systems: The collaborative recommendation is based on statistical implication rules (IIR); Differently from collaborative recommendation based on association rules (AR), the IIR predicts the items for users based on statistical implication rules generated from rating matrix and Implication intensity measures measuring the surprisingness of rules. To evaluate the effectiveness of the model, in the experimental section, we implement the model on three real datasets and compare the results with some different effective models. The results show that the IIR has higher precision on the experimental datasets.",218,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=igmb0QwAAAAJ&citation_for_view=igmb0QwAAAAJ:eQOLeE2rZwMC,,1
2594,Multidomain Supervised Aspect-based Sentiment Analysis using CNN_Bidirectional LSTM model,"Sentiment analysis or opinion mining used to capture the community’s attitude who have experienced the specific service/product. Sentiment analysis usually concentrates to classify the opinion of whole document or sentence. However, in most comments, users often express their opinions on different aspects of the mentioned entity rather than express general sentiments on entire document. In this case, using aspect-based sentiment analysis (ABSA) is a solution. ABSA emphases on extracting and synthesizing sentiments on particular aspects of entities in opinion text. The previous studies have difficulty working with aspect extraction and sentiment polarity classification in multiple domains of review. We offer an innovative deep learning approach with the integrated construction of bidirectional Long Short Term Memory (BiLSTM) and Convolutional Neural Network (CNN) for multidomain ABSA in this article. Our …",218,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=igmb0QwAAAAJ&citation_for_view=igmb0QwAAAAJ:Se3iqnhoufwC,,1
2595,Adaptive testing model approach based on Birnbaum model and Markov model,"Computerized adaptive testing is widely used in the testing and assessment of the level of learners' competency. Popular computerized adaptive testing systems now use the mathematical models of Item Response Theory based on the relationship between the ability of examinees and item parameters. However, Item Response Theory does not take into account the impact between previous answers and the next item selection. The item parameters principally rely on probability methods of Classical Test Theory. This article proposes the combination of Item Response Theory (Birnbaum model) and Markov chain to calculate the dependency of answer set during the quiz process. Concurrently, using Hooke-Jeeves direct search method within the limited range of parameters to assess the set of item parameters.",218,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=igmb0QwAAAAJ&citation_for_view=igmb0QwAAAAJ:ufrVoPGSRksC,,1
2596,Lựa chọn mô hình và tham số cho bài toán tư vấn lọc cộng tác dựa trên đồ thị đánh giá,"Recommender system is considered one of the most effective solutions that can cope with information explosion due to the rapid development of Internet services and is widely applied in many fields. However, to design a recommender system can meet the needs of users, the selection of suitable models for the recommender system and choosing the appropriate value of parameters for the model are always big challenges of designers. This study proposes solutions to choose models and value of parameters suitable for specific collaborative filtering recommender systems. To evaluate the proposed solutions, experiments on three standard datasets of MovieLens, MSWeb, and Jester5k are conducted. Experimental results show that the proposed solutions can assist designers and researchers to quickly identify model and the value parameters model for their specific collaborative filtering recommender systems …",218,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=igmb0QwAAAAJ&citation_for_view=igmb0QwAAAAJ:zYLM7Y9cAGgC,,1
2604,Measurement and prediction of the density and viscosity of biodiesel blends,"Biodiesel has been considered as the potential fuel type with many advantages such as environmental pollution reduction, no sulfur production, and biodegradation. However, disadvantages of biodiesel such as high viscosity and high density affected diesel engines and fuel systems negatively. Thus, it is necessary to reduce the viscosity and density of biodiesel fuel in unmodified diesel engines. Until now, a large number of empirical correlations have been used to predict the viscosity and density of biodiesel–fossil diesel fuel blend This study was conducted to predict the kinematic viscosity and density of blends of biodiesel and fossil diesel fuel. Three types of biodiesel were examined: Coconut oil-based biodiesel (COB), Jatropha oil-based biodiesel (JOB), and Waste oil-based biodiesel (WOB). Twenty-four samples of the three types of biodiesel–diesel fuel blends were created by blending 5%(B5), 10%(B10), 20%(B20), 40%(B40), 50%(B50), 60%(B60), 75%(B75), and 100%(B100) of biodiesel with conventional diesel fuel to produce the corresponding blends for experimental purposes. Experimental correlations and mathematical equations for predicting the relationship between the kinematic viscosity and the density of the biodiesel–fossil diesel fuel blends, the dependence of the kinematic viscosity and the density of the biodiesel–fossil diesel fuel blends on biodiesel fractions, and the effects of temperature on the kinematic viscosity and density of pure biodiesel were developed. The results of the experimental correlation data were near the predicted mathematical equation with a confidence level of 95%.",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:r0BpntZqJG4C,,1
2605,"Influences of heating temperatures on physical properties, spray characteristics of bio-oils and fuel supply system of a conventional diesel engine","Alternative fuels need to satisfy the strict requirements of the use for diesel engines aiming at enhancing the performance and reducing pollutant emissions. The use of straight bio-oils for diesel engines entails improving their disadvantages such as high density, high surface tension and kinematic viscosity (tri-physical parameters). There have been some as-used methods for reduction of the above-mentioned negative effects related to straight bio-oil disadvantage, however, the adequately-heating method may be considered as a simple one helping the physical parameters of straight bio-oils to reach stable and highly-confident values which are close to those of traditional diesel fuel. As a consequence, the spray and atomization, combustion, performance, and emissions of diesel engines fueled with preheated bio-oils are improved. In this work, a study of the dependence of the density, surface tension and kinematic viscosity of coconut oil (a type of bio-oils) on temperatures (from 40-110oC) within a wide variety are conducted. In the first stage, the influence study of temperature on tri-physical parameters is carried out on the basis of experimental correlation and asdescribed mathematical equation. In the second stage, the influence study of tri-physical parameters on spray and atomization parameters including penetration length (Lb) and Sauter mean diameter (SMD), and the influence of tri-physical parameters on fuel supply system are investigated. The optimal range of temperature for the as-used bio-oils is found after analyzing and evaluating the obtained results regarding the physical properties and spray characteristics, as well as …",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:j3f4tGmQtD8C,,1
2606,Power generation characteristics of a thermoelectric modules-based power generator assisted by fishbone-shaped fins: Part II–Effects of cooling water parameters,"Operation control combined with the optimization of the structure for thermoelectric generators (TEGs) based on thermoelectric modules (TEMs) is an effective method to improve the power generation characteristics of TEGs. The present study is intended to experimentally investigate the power generation characteristics including voltage and power output, and conversion efficiency of a TEM-based TEG, which is added fishbone-fins aiming to enhance heat exchange of hot inlet air to the hot side of TEG. The effects of mass flow rate (MFR) of cooling water on the power generation characteristics of TEGs are analyzed in the case of the hot inlet air temperatures of 200–500°C and a constant MFR of hot inlet air. Compared to TEM-based TEG without fishbone-shaped fins, TEM-based TEG with fishbone-shaped fins has a maximum increase by 31.37 V of voltage output and 16.49 W of power output. Conversion …",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:TFP_iSt0sucC,,1
2607,A Gaussian wavelet network-based robust adaptive tracking controller for a wheeled mobile robot with unknown wheel slips,"In this paper, a robust adaptive tracking controller is proposed for a nonholonomic wheeled mobile robot (WMR) in the presence of unknown wheel slips. The role of the Gaussian wavelet network in this proposed controller is to approximate unknown smooth nonlinear dynamic functions due to no prior knowledge of the dynamic parameters of the WMR. In addition, one robust law is employed at the kinematic level so as to compensate the harmful effects of the unknown wheel slips, and another robust law is used at the dynamic level to overcome total uncertainties caused by dynamic parameter variations, external disturbances, etc. The stability of the whole closed-loop control system is proved in accordance with Lyapunov theory and Barbalat's lemma. Ultimately, the simulation results are shown in comparison with those of another control method under the same condition to confirm the validity and efficiency of this …",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:YFjsv_pBGBYC,,1
2608,"Modeling and feedback linearization control of a nonholonomic wheeled mobile robot with longitudinal, lateral slips","This paper presents a systematic method to build the kinematic model and dynamic model of a nonholonomic wheeled mobile robot (WMR) with longitudinal and lateral slip, followed by the design of a control law using the input-output feedback linearization method to drive the mobile robot to track a given trajectory while longitudinal, and lateral slip exist. The asymptotical stability of the system is verified by solving second-order differential linear equations to find solutions for history time. Matlab-Simulink simulation results show the correctness and performances of the control law.",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:M3ejUd6NZC8C,,1
2609,Classification and clustering of spatial patterns with geometric algebra,"In fields of classification and clustering of patterns most conventional methods of feature extraction do not pay much attention to the geometric properties of data, even in cases where the data have spatial features. This paper proposes to use geometric algebra to systematically extract geometric features from data given in a vector space. We show the results of classification of handwritten digits and those of clustering of consumers’ impression with the proposed method.",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:d1gkVwhDpl0C,,1
2610,Influence of Various Basin Types on Performance of Passive Solar Still: A Review.,"Passive solar still is the simplest design for distilling seawater by harnessing solar energy. Although it is undeniable that solar still is a promising device to provide an additional freshwater source for global increasing water demand, low thermal efficiency along with daily distillate yield are its major disadvantages. A conventional solar still can produced 2 to 5 L/m²day. Various studies have been carried out to improve passive solar stills in terms of daily productivity, thermal efficiency, and economic effectiveness. Most of the researches that relate to the daily output improvement of passive solar still concentrates on enhancing evaporation or/and condensation processes. While the condensation process is influenced by wind velocity and characteristics of the condensed surface, the evaporation process is mainly affected by the temperature of basin water. Different parameters affect the brackish water temperature such …",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:e5wmG9Sq2KIC,,1
2611,Visual control of integrated mobile robot-pan tilt-camera system for tracking a moving target,"In this paper, in order to track a moving target, we propose a new control law for an integrated mobile robot- pan tilt-camera system. Our controller consists of two control loops, i.e., a kinematic and dynamic control loop, respectively. The kinematic control loop performs three tasks, i.e., allowing an image feature of the target to converge to the center of the image plane asymptotically, designing a trajectory for the mobile robot, and allowing the mobile robot to track the desired position and direction. In the dynamic control loop, the torques are determined; the actual angular velocities of the system, i.e., angular velocities of pan and tilt axis, angular velocities of right and left wheels, track the desired angular velocities which are the outputs of the kinematic controller of the kinematic control loop. According to the Lyapunov theory and Barbalat's theorem, the asymptotic stability of the whole system is proven. Simulation …",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:4DMP91E08xMC,,1
2612,Robust feature extractions from geometric data using geometric algebra,"Most conventional methods of feature extraction for pattern recognition do not pay sufficient attention to inherent geometric properties of data, even in the case where the data have spatial features. This paper introduces geometric algebra to extract invariant geometric features from spatial data given in a vector space. Geometric algebra is a multidimensional generalization of complex numbers and of quaternions, and it ables to accurately describe oriented spatial objects and relations between them. This paper proposes to combine several geometric features using Gaussian mixture models. It applies the proposed method to the classification of hand-written digits.",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:Tyk-4Ss8FVUC,,1
2613,Feature extractions with geometric algebra for classification of objects,"Most conventional methods of feature extraction do not pay much attention to the geometric properties of data, even in cases where the data have spatial features. In this study we introduce geometric algebra to undertake various kinds of feature extraction from spatial data. Geometric algebra is a generalization of complex numbers and of quaternions, and it is able to describe spatial objects and relations between them. This paper proposes to use geometric algebra to systematically extract geometric features from data given in a vector space. We show the results of classification of hand-written digits, which were classified by feature extraction with the proposed method.",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:u5HHmVD_uO8C,,1
2614,"A review on ignition delay times of 2, 5-Dimethylfuran","The rapid depletion of fossil fuels and the strengthening of emission regulations have been considered as the primary issues in the global energy and environment field, suggesting that the increase in the percentage of alternative fuels in the transport sector is believed to be a prospective pathway. Among the renewable biofuels that have attracted great interest, 2,5-dimethylfuran (DMF) is found to offer several attractive combustion features, of which the ignition behavior is known as a global parameter. Therefore, several experimental studies and prediction models have been used to determine the ignition behavior of DMF. However, the ignition behavior of DMF in comparison with other fuels, which is important for further model development and engine applications, has not been completely understood as well as the relationship between the reaction mechanism of DMF and its ignition behavior. Moreover, the …",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:_Qo2XoVZTnwC,,1
2615,A clustering method for geometric data based on approximation using conformal geometric algebra,"Clustering is one of the most useful methods for understanding similarity among data. However, most conventional clustering methods do not pay sufficient attention to the geometric properties of data. Geometric algebra (GA) is a generalization of complex numbers and quaternions able to describe spatial objects and the relations between them. This paper uses conformal GA (CGA), which is a part of GA, to transform a vector in a real vector space into a vector in a CGA space and presents a proposed new clustering method using conformal vectors. In particular, this paper shows that the proposed method was able to extract the geometric clusters which could not be detected by conventional methods.",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:W7OEmFMy1HYC,,1
2616,A review of the indirect solar dryer with sensible heat storage mediums,"The quality of agricultural products will have deteriorated as the preserved technology is inadequate and the storage process is not guaranteed. postharvest losses have been overcome with advanced techniques in production processes. Drying is applied to preserve food as well as agricultural goods. Utilizing the sun’s energy for drying has been performed historically. Different designs of solar dryers with a variety of capacities are available in the agricultural sector. Among them, an Indirect solar dryer (ISD) is potential for food products however its drawback is unable to work after sunset. Techniques extend its operation at off sunshine hours for performance enhancement have not been reviewed in detail. This review aims to assess the features along with the advantages of sensible heat storage material (SHSM) in ISD. Common types of ISD with SHSM are presented and the review, discussion, and tabulation of obtained findings on ISD with various sensible heat material have been carried out. It is potential to develop a solar dryer system that employed thermal energy storage materials which have high thermal efficiency and appropriate cost and substitute possibly to fossil fuel in both the developing and developed countries. The storage unit usage, Drying process in ISD for agricultural food can be conducted at off sunshine hours, which was impossible with a conventional solar dryer. As a result, solar drying agricultural products with sensible heat storage materials benefit the energy conservation as well as the performance of solar systems.",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:RHpTSmoSYBkC,,1
2617,Logistics Revolution for e-commerce in Vietnam: A Brief Review,"For the whole process of circulation, distribution, application of the logistics system is a higher development step of transportation technology. Multimodal transport has linked all modes of transport together to serve the supply and consumption needs of the manufacturer. Currently, when e-commerce (EC) is on the rise, quality logistics and delivery services are the essential stages to achieve success in transactions. e-commerce today is no longer a fashion trend; it is an essential part of the retail industry. EC is changing the game of Logistics. The e-commerce world has been introducing many risks and complex issues that logistics and supply chain experts have never faced. E-commerce is the main impact on terminal logistics (last mile) and warehousing. However, it is also quite interesting to see how e-commerce is impacting airports and seaports. E-commerce is growing actively in Vietnam at a rate of about 25% per year, which is an excellent opportunity for business logistics services to get more orders. Moreover, the Vietnamese logistics market is small but has a high growth rate (20% - 25% / year). In recent times, this market is expected to change the face and grow faster due to the development of electronic retail in Vietnam and e-Logistics logistics trends in the world. However, to meet the requirements, to grasp the opportunities, logistics enterprises need to link to get a closed and synchronized service chain. So logistics companies need to increase information, respond faster, enhance technology to meet flexible processes, establish networks, and business links to meet changing needs. The article shows that the inadequacies and lack of …",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:NMxIlDl6LWMC,,1
2618,The DOMJudge Based Online Judge System with Plagiarism Detection,"There were many online judge systems which were created to automatically judge programming code. Among the development of online judge systems, the open-source online judge systems are also developing gradually, like for example Sharif Online Judge and DOMjudge. This research modified the DOMjudge by creating additional functions and improved the graphic user interface of DOMjudge in the user-friendly way to students and teachers. Besides, the research also developed a plagiarism detection system, which helps teachers to find out who cheated during the contest. This paper showed that proposed preprocessing and using k-Grams, Hashing and Suffix Array to compare two source codes is better than conventional methods.",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:5nxA0vEk-isC,,1
2619,An adaptive algorithm for restoring image corrupted by mixed noise,"Image denoising is one of the fundamental problems in image processing. Digital images are often contaminated by noise due to the image acquisition process under poor conditions. In this paper, we propose an effective approach to remove mixed Poisson-Gaussian noise in digital images. Particularly, we propose to use a spatially adaptive total variation regularization term in order to enhance the ability of edge preservation. We also propose an instance of the alternating direction algorithm to solve the proposed denoising model as an optimization problem. The experiments on popular natural images demonstrate that our approach achieves superior accuracy than other recent state-of-the-art techniques.",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:_kc_bZDykSQC,,1
2620,A conformal geometric algebra based clustering method and its applications,"Clustering is one of the most useful methods for understanding similarity among data. However, most conventional clustering methods do not pay sufficient attention to the geometric distributions of data. Geometric algebra (GA) is a generalization of complex numbers and quaternions able to describe spatial objects and the geometric relations between them. This paper uses conformal GA (CGA), which is a part of GA. This paper transforms data from a real Euclidean vector space into a CGA space and presents a new clustering method using conformal vectors. In particular, this paper shows that the proposed method was able to extract the geometric clusters which could not be detected by conventional methods.",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:IjCSPb-OGe4C,,1
2621,Trajectory tracking control of a mobile robot by computed torque method with on-line learning neural network,"This paper proposes a novel control algorithm for the mobile robot with nonholonomic constraint. The algorithm consists of two control loops: one is based on the kinematics and Lyapunov theory to derive the control laws for the tangent and angular velocities to control the robot to follow a target trajectory, the other controls the robot dynamic based on the moment method in which a neural network namely RBFNN is introduced to compensate the uncertainty of dynamic parameters. The convergence of the estimators based on RBFNN of Stone-Weierstrass is proven. The asymptotically stabilization of the whole system is confirmed by direct Lyapunov stabilization theory. The effectiveness of the method is verified by simulations in Matlab.",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:iH-uZ7U-co4C,,1
2622,Feature extraction using conformal geometric algebra for AdaBoost algorithm based in-plane rotated face detection,"In this paper, we propose a novel face detection method based on the AdaBoost algorithm. In the past few years, a variety of variant AdaBoost approaches has been proposed and obtained increasing success in both performance and robustness. However, those approaches have not focused much on the geometric characteristics of face images. A new way to apply AdaBoost is introduced in this paper with the utilization of Conformal Geometric Algebra for extracting features from input samples. By analysis and experiments, using Conformal Geometric Algebra, we can find the hyper-spheres (-planes) that mostly fit data points and yield very low error for classification in both frontal and in-plane rotated face detection. Haar-like patterns are used as well but in a more pertinent approach to achieve more informative features. In comparison with the state-of-the-art face detection method developed by Paul Viola …",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:YOwf2qJgpHMC,,1
2623,Feature extraction for classification method using principal component based on conformal geometric algebra,"This paper discusses feature extraction methods. The feature extraction methods such principal component analysis and multiple discriminant analysis are very important techniques in machine learning research areas. The characteristic of feature extraction is to transform the data from a difficultly classified space to a easily classified space. There are many conventional machine learning methods including transformation such as artificial neural network and support vector machines. However, extracting the good features before applying machine learning methods will lead to better classification results. This paper focuses on the principal component regression (PCR). The PCR finds the approximation with hyper-planes where the data distributed on. The problem now is that it has a case of the data do not distribute on hyper-planes, for example they distribute on hyper-spheres such as rotation objects, the PCR can …",219,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:WF5omc3nYNoC,,1
2627,Automatic Plant Image Identification of Vietnamese species using Deep Learning Models,"It is complicated to distinguish among thousands of plant species in the natural ecosystem, and many efforts have been investigated to address the issue. In Vietnam, the task of identifying one from 12,000 species requires specialized experts in flora management, with thorough training skills and in-depth knowledge. Therefore, with the advance of machine learning, automatic plant identification systems have been proposed to benefit various stakeholders, including botanists, pharmaceutical laboratories, taxonomists, forestry services, and organizations. The concept has fueled an interest in research and application from global researchers and engineers in both fields of machine learning and computer vision. In this paper, the Vietnamese plant image dataset was collected from an online encyclopedia of Vietnamese organisms, together with the Encyclopedia of Life, to generate a total of 28,046 environmental images of 109 plant species in Vietnam. A comparative evaluation of four deep convolutional feature extraction models, which are MobileNetV2, VGG16, ResnetV2, and Inception Resnet V2, is presented. Those models have been tested on the Support Vector Machine (SVM) classifier to experiment with the purpose of plant image identification. The proposed models achieve promising recognition rates, and MobilenetV2 attained the highest with 83.9%. This result demonstrates that machine learning models are potential for plant species identification in the natural environment, and future works need to examine proposing higher accuracy systems on a larger dataset to meet the current application demand.",222,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=T6VJndoAAAAJ&citation_for_view=T6VJndoAAAAJ:VOx2b1Wkg3QC,,1
2628,Recognition of Plant Species using Deep Convolutional Feature Extraction,"There are more than 391,000 plant species currently known to global science, and it is challenging to distinguish among them. The identification of plant species requires in-depth surveyors and botanists who possess a tremendous amount of knowledge on native plant species. Therefore, plant recognition has become an interdisciplinary concentration in both botanical taxonomy and machine learning for a faster identification process. In this paper, a convolutional neural network system has been proposed to perform feature extraction using different deep learning models in large-scale plant classification methods. The plant image dataset was collected from the PlantCLEF2003 dataset, which consists of 51,273 images from 609 plant species. Four deep convolutional feature extraction methods, including Resnet50V2, Inception Resnet V2, MobilenetV2, and VGG16, are used to extract features from the images. A comparative evaluation of four deep learning models using two classification methods, Support Vector Machine (SVN) and k-nearest neighbor (KNN), is presented. With the highest accuracy of 95.6%, MobilenetV2 performed better than the other deep learning models for plant recognition in both SVM and KNN classification methods. Moreover, the SVM classifier has outperformed the KNN in terms of accuracy in the plant image recognition system. The outcomes are promising for further applications and future work gears towards experiments on a larger dataset with high-performance computing facilities to propose a higher accuracy system of plant image identification in natural environments.",222,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=T6VJndoAAAAJ&citation_for_view=T6VJndoAAAAJ:eJXPG6dFmWUC,,1
2629,Artificial Neural Network and Fuzzy Logic Approach to diagnose Autism Spectrum Disorder,"Autism Spectrum Disorder (ASD) is becoming a big issue in numerous countries around the world which can even negatively affect human natural evolution. Even though autism can be diagnosed early-before 2 years old, most children were not diagnosed with ASD until the age of 4 because of its complex symptoms and ambiguous manifestation in infant’s disorders. Applying science and technology into early autism diagnosis is of vital importance, especially when data mining branches and decision-making support systems are developing and achieving many accomplishments in various fields, medicine included. Contributing to those developments, the combination between the Artificial Neural Network (ANN) and Fuzzy logic has triggered a huge revolution in data mining and is able to solve a variety of problems. This paper is the elaboration on the method of employing this combination to facilitate the early diagnosis of ASD. The result of the paper shows that the aforementioned approach has the potential to be the fundamental basis of the supporting decision-making system in ASD researching and diagnosing.",222,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=T6VJndoAAAAJ&citation_for_view=T6VJndoAAAAJ:OU6Ihb5iCvQC,,1
2630,Artwork style transfer model using deep learning approach,"Abstract Art in general and fine arts, in particular, play a significant role in human life, entertaining and dispelling stress and motivating their creativeness in specific ways. Many well-known artists have left a rich treasure of paintings for humanity, preserving their exquisite talent and creativity through unique artistic styles. In recent years, a technique called’style transfer’allows computers to apply famous artistic styles into the style of a picture or photograph while retaining the shape of the image, creating superior visual experiences. The basic model of that process, named’Neural Style Transfer,’has been introduced promisingly by Leon A. Gatys; however, it contains several limitations on output quality and implementation time, making it challenging to apply in practice. Based on that basic model, an image transform network was proposed in this paper to generate higher-quality artwork and higher abilities to perform on a larger image amount. The proposed model significantly shortened the execution time and can be implemented in a real-time application, providing promising results and performance. The outcomes are auspicious and can be used as a referenced model in color grading or semantic image segmentation, and future research focuses on improving its applications.",222,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=T6VJndoAAAAJ&citation_for_view=T6VJndoAAAAJ:t6usbXjVLHcC,,1
2631,Web Crawler: Design And Implementation For Extracting Article-Like Contents,"Abstract The World Wide Web is a large, wealthy, and accessible information system whose users are increasing rapidly nowadays. To retrieve information from the web as per users’ requests, search engines are built to access web pages. As search engine systems play a significant role in cybernetics, telecommunication, and physics, many efforts were made to enhance their capacity. However, most of the data contained on the web are unmanaged, making it impossible to access the entire network at once by current search engine system mechanisms. Web Crawler, therefore, is a critical part of search engines to navigate and download full texts of the web pages. Web crawlers may also be applied to detect missing links and for community detection in complex networks and cybernetic systems. However, template-based crawling techniques could not handle the layout diversity of objects from web pages. In this paper, a web crawler module was designed and implemented, attempted to extract article-like contents from 495 websites. It uses a machine learning approach with visual cues, trivial HTML, and text-based features to filter out clutters. The outcomes are promising for extracting article-like contents from websites, contributing to the search engine systems development and future research gears towards proposing higher performance systems.",222,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=T6VJndoAAAAJ&citation_for_view=T6VJndoAAAAJ:mvPsJ3kp5DgC,,1
2632,A pessimistic approach for solving a multi-criteria decision making,"An extension of the DS/AHP method in the paper. The extension assumes that expert judgments concerning the criteria are often imprecise and incomplete. The proposed extension also uses groups of experts or decision makers for comparing decision alternatives and criteria. However, it does not require assigning favorable values for different groups of decision alternatives and criteria. The computation procedure for processing and aggregating the incomplete information about criteria and decision alternatives is reduced to solving a finite set of linear programming problems. Main results are explained and illustrated by numerical examples.",222,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=T6VJndoAAAAJ&citation_for_view=T6VJndoAAAAJ:kRWSkSYxWN8C,,1
2633,Keyphrase Extraction Model: A New Design and Application on Tourism Information,"Keyphrase extraction has recently become a foundation for developing digital library applications, especially in semantic information retrieval techniques. From that context, in this paper, a keyphrase extraction model was formulated in terms of Natural Language Processing, applied explicitly in extracting information and searching techniques in tourism. The proposed process includes collecting and processing data from tourism sources such as Tripadvisor. com, Agoda. com, and vietnam-guide. com. Then, the raw data was analyzed and pre-processed with labeling keyphrase and fed data forward to Pretrained BERT model and Bidirectional Long Short-Term Memory with Conditional Random Field. The model performed the combination of Bidirectional Long Short-Term Memory with Conditional Random Field in order to solve keyphrase extraction tasks. Furthermore, the model integrated the Elasticsearch technique to enhance performance and time of looking up tourism destinations' information. The outcome extracted key phrases produce high accuracy and can be applied for extraction problems and textual content summaries.",222,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=T6VJndoAAAAJ&citation_for_view=T6VJndoAAAAJ:WA5NYHcadZ8C,,1
2634,"Assessment of the current distribution, dispersal trends and impacts of invasive species in Bana-Nui Chua Nature Reserve, Vietnam","Invasive species threaten the biodiversity and the function of ecosystems. Drone image, satellite images, and image analysis software were used to create the map of invasive distribution and the potential spreading of invasive plants. 13 most invasive plants were identified with 11 species listed as invasive species in Southeast Asia and 5 of them in the 100 world’s invasive species by IUCN. Three species Merremia boisiana (Gagn.) van Ooststr., Ipomoea eberhardtii Gagn, and Mimosa pigra were identified as the species with high-ranking impacts on biodiversity and ecosystem biodiversity in Ba Na-Nui Chua Nature Reserve (BNNR). Ipomoea eberhardtii Gagn shows the highest spreading rate at 0.65±0.06 ha/month, followed by Merremia boisiana (Gagn.) van Ooststr) and Mimosa pigra at 0.12±0.01 ha/month and 0.01±0.001 ha/month respectively. Fresh biomass of Ipomoea eberhardtii Gagn; Merremia boisiana (Gagn.); Mimosa pigra and Sphagnetola trilobata (L.)",222,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=T6VJndoAAAAJ&citation_for_view=T6VJndoAAAAJ:wbdj-CoPYUoC,,1
2635,Cải tiến phương pháp phân tích thứ bậc sử dụng thuyết Dempster-Shafer,"The Analytic Hierarchy Process of Thomas Saaty plays a very important role in information processing to make selection decisions and to decide the best and most reasonable course of action. However, this method cannot be used in many cases where the expert judgments concerning the criteria are imprecise and incomplete. This paper proposes a method for improving the Analytic Hierarchy of Thomas Saaty. The proposes method also uses group of experts for comparing alternatives and criteria. However, it does not require assigning favorability values for different groups of decision alternatives and criteria. In addition, it uses the Maximin approach for combining the criteria. Efficient algorithms are developed for computing the optimal solution. The main results of this research are explained and illustrated by nummerical examples.",222,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=T6VJndoAAAAJ&citation_for_view=T6VJndoAAAAJ:B3FOqHPlNUQC,,1
2639,Quasi-periodic action recognition from monocular videos via 3D human models and cyclic HMMs,"This paper proposes a system to recognize quasi-periodic human actions from monocular video sequences. First, each input video frame is analyzed and estimated to generate the best 3D human model pose which consists of a set of 3D coordinates of specific human joints. Next, these 3D coordinates for each frame are converted into corresponding 3D geometric relational features (GRFs), which describe the geometric relations among body joints of a pose. Finally, we train a cyclic hidden Markov model (CHMM) for each action based on the vector quantized 3D GRFs, and the trained CHMMs are used to classify different quasi-periodic human actions. The experimental results indicate the effectiveness of the proposed system in terms of the view point invariance, the low -dimensional feature vectors, and the encouraging recognition rates.",224,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J-m5dVMAAAAJ&citation_for_view=J-m5dVMAAAAJ:UeHWp8X0CEIC,,1
2640,Mining Frequent Fuzzy Itemsets Using Node-List,"Data mining plays an important in knowledge discovery in databases; many types of knowledge and technology have been proposed for data mining. Among them, association rule mining is the problem important not only in data mining task but also in many practical applications in different areas of life. These previous studies mostly focused on showing the transaction data with binary values. However, in real-world applications, transactions also contain uncertain and imprecise data. To solve the above-mentioned problem, fuzzy association rule mining algorithms are developed to handle quantitative data using fuzzy set. In this paper, we present proposed algorithm NFFP, an improved fuzzy version of PPV algorithm for discovering frequent fuzzy itemsets using Node-List structure.",224,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J-m5dVMAAAAJ&citation_for_view=J-m5dVMAAAAJ:WF5omc3nYNoC,,1
2641,Digit detection from digital devices in multiple environment conditions,"This paper proposes two automatic segmentation and detection methods to recognize seven segments digits in images of temperature and humidity screen from multiple types of devices. Since there is a lack of methods working in seven segments recognition in comparison with hand-written digits, it is important to dive deep into this area. Reading digits from screen is a difficult computer vision problem that is important for a range of real world applications. This paper focuses on the problem of segmentation and recognition digits on multiple types of digital screen, many points of views, and noisy images. Since most cameras in recent years are becoming more and more popular, users can use them to take a picture or real time tracking the screen of the device. After frame taken, computer vision techniques can be used to recognize the digits on the picture and thus record the data. In this case, specific model trained …",224,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J-m5dVMAAAAJ&citation_for_view=J-m5dVMAAAAJ:Tyk-4Ss8FVUC,,1
2642,Clustering and Query Optimization in Fuzzy Object-Oriented Database,"The purpose of the clustering method is to provide some meaningful partitioning of the data set. In general, finding separate clusters with similar members is essential. A problem in clustering is how to determine the number of optimal clusters that best fits the data set. Most clustering algorithms generate a partition based on input parameters (for example, cluster number, minimum density) which results in limiting the number of clusters. Therefore, the article proposes an improved EMC clustering algorithm that is more flexible in handling and manipulating those clusters, where input parameter values are assumed to be different clusters for different partitions of a data set. In addition, based on the above partitioning results, this article proposes a new approach to processing and optimizing fuzzy queries to improve efficiency in the manipulation and processing of specific data such as (less time consuming, less resource …",224,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J-m5dVMAAAAJ&citation_for_view=J-m5dVMAAAAJ:W7OEmFMy1HYC,,1
2643,A New Approach for Query Processing and Optimization in Fuzzy Object-Oriented Database,"For enhancing the efficiency of processing users’ queries, all database management systems (DBMSs) must conduct query preprocessing, or query optimizing. This paper proposes a new model for the Fuzzy Object-Oriented DBMS (FOO-DBMS), which optimizes the query statements and processes the data before returning back to users based on fuzzy object algebra and equivalent transformation rules. Discussions on this model are also presented with computation and analysis.",224,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J-m5dVMAAAAJ&citation_for_view=J-m5dVMAAAAJ:ufrVoPGSRksC,,1
2645,A Fuzzy Association Rules Mining Algorithm with Fuzzy Partitioning Optimization for Intelligent Decision Systems,"Fuzzy association rule mining is one of the important problems in fuzzy machine learning. The fuzzy association rule is intended to handle quantitative data that was indecisive in the classical association rule. However, the fuzzy set of quantitative attributes and their membership functions depends on expert opinion, subjective or available. In this paper, we propose a Node-list Pre-order Size Fuzzy Frequent (NPSFF) algorithm in fuzzy association rule mining based on Node-list data structure combined with the Pre-order Size Code structure suitable for the organization of fuzzy data and the manipulation of this structure. This organization speeds up tree building and finds frequent fuzzy item sets. To increase the efficiency of the NPSFF algorithm, we performed the data preprocessing by applying affinity propagation clustering (AP) technique to specify the suitable number of clusters. Next, we convert the quantitative …",224,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J-m5dVMAAAAJ&citation_for_view=J-m5dVMAAAAJ:eQOLeE2rZwMC,,1
2647,A New Approach for Query Processing and Optimization Base on the Fuzzy Object Algebra and Equivalent Transformation Rules,"For enhancing the effeciency of processing users’ queries, all Database Management Systems (DBMSs) must conduct query pre-processing, or query optimizing. This paper proposes a new model for the Fuzzy Object Oriented DBMS (FOO-DBMS), which optimizes the query statements and processes the data before returning back to users based on Fuzzy-Object-Algebra and equivalant transformation rules. Discussions on this model are also presented with computation and analysis",224,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J-m5dVMAAAAJ&citation_for_view=J-m5dVMAAAAJ:YsMSGLbcyi4C,,1
2648,Tối ưu biểu thức điều kiện tìm kiếm trong các truy vấn đối tượng sử dụng các chỉ mục lồng,"In large databases in general and Object-Oriented Database in particular, the queries on databases often contain conditional expression finding including complicated conjunctions and disjunctions. This article studies and proposes ways to optimize complicated finding conditional expression, which have container path expressions using nested indexes.",224,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J-m5dVMAAAAJ&citation_for_view=J-m5dVMAAAAJ:Y0pCki6q_DkC,,1
2649,Optimization technique for finding conditional expression using nested indexes,"In large databases in general and Object-Oriented Database in particular, the queries on databases often contain conditional expression finding including complicated conjunctions and disjunctions. This article studies and proposes ways to optimize complicated finding conditional expression, which have container path expressions using nested indexes.",224,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J-m5dVMAAAAJ&citation_for_view=J-m5dVMAAAAJ:2osOgNQ5qMEC,,1
2653,Gom cụm các đối tượng trong cơ sở dữ liệu hướng đối tượng sử dụng ma trận khoảng cách,"1. We hereby assign copyright of our article (the Work) in all forms of media, whether now known or hereafter developed, to the Journal of Computer Science and Cybernetics. We understand that the Journal of Computer Science and Cybernetics will act on my/our behalf to publish, reproduce, distribute and transmit the Work.",224,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=J-m5dVMAAAAJ&citation_for_view=J-m5dVMAAAAJ:u-x6o8ySG0sC,,1
2654,Optimizing Parameters of Software Effort Estimation Models using Directed Artificial Bee Colony Algorithm,"Effective software effort estimation is one of the challenging tasks in software engineering. There has been various alternatives introduced to enhance the accuracy in predictions. In this respect, estimation approaches based on algorithmic models have been widely used. These models consider modeling software effort as a function of the size of the developed project. However, most approaches sharing a common thread of complex mathematical models face the difficulties in parameters calibration and tuning. This study proposes using a directed artificial bee colony algorithm in order to tune the values of model parameters based on past actual effort. The proposed methods were verified with NASA software dataset and the obtained results were compared to the existing models in other literatures. The results indicated that our proposal has significantly improved the performance of the estimations. Effective software effort estimation is one of the challenging tasks in software engineering. There has been various alternatives introduced to enhance the accuracy in predictions. In this respect, estimation approaches based on algorithmic models have been widely used. These models consider modeling software effort as a function of the size of the developed project. However, most approaches sharing a common thread of complex mathematical models face the difficulties in parameters calibration and tuning. This study proposes using a directed artificial bee colony algorithm in order to tune the values of model parameters based on past actual effort. The proposed methods were verified with NASA software dataset and the obtained results were …",225,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=uYFPpDwAAAAJ&citation_for_view=uYFPpDwAAAAJ:0EnyYjriUFMC,,1
2656,A Novel Hybrid ABC-PSO Algorithm for Effort Estimation of Software Projects Using Agile Methodologies,"In modern software development processes, software effort estimation plays a crucial role. The success or failure of projects depends greatly on the accuracy of effort estimation and schedule results. Many studies focused on proposing novel models to enhance the accuracy of predicted results; however, the question of accurate estimation of effort has been a challenging issue with regards to researchers and practitioners, especially when it comes to projects using agile methodologies. This study aims at introducing a novel formula based on team velocity and story point factors. The parameters of this formula are then optimized by employing swarm optimization algorithms. We also propose an improved algorithm combining the advantages of the artificial bee colony and particle swarm optimization algorithms. The experimental results indicated that our approaches outperformed methods in other studies in terms of …",225,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=uYFPpDwAAAAJ&citation_for_view=uYFPpDwAAAAJ:5nxA0vEk-isC,,1
2657,An application of artificial neural networks and fuzzy logic on the stock price prediction problem,"The financial industry has been becoming more and more dependent on advanced computing technologies in order to maintain competitiveness in a global economy. Hence, the stock price prediction problem using data mining techniques is one of the most important issues in finance. This field has attracted great scientific interest and has become a crucial research area to provide a more precise prediction process. Fuzzy logic (FL) and Artificial Neural Network (ANN) present an exciting and promising technique with a wide scope for the applications of prediction. There is a growing interest in both fields of fuzzy logic computing and the financial world in the use of fuzzy logic to predict future changes in prices of stocks, exchange rates, commodities, and other financial time series. Fuzzy logic provides a way to draw definite conclusions from vague, ambiguous or imprecise information. Artificial Neural Network is one of data mining techniques being widely accepted in the business area due to its ability to learn and detect relationships among nonlinear variables. The ANN outperforms statistical regression models and also allows deeper analysis of large data sets, especially those that have the tendency to fluctuate within a short of time period. In this paper, we investigate the ability of Fuzzy logic and multilayer perceptron (MLP), which is a kind of the ANN, to tackle the financial time series stock forecasting problem. The proposed approaches were tested on the historical price data collected from Yahoo Finance with different companies. Furthermore, the comparison between those techniques is performed to examine their effectiveness.",225,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=uYFPpDwAAAAJ&citation_for_view=uYFPpDwAAAAJ:8k81kl-MbHgC,,1
2659,Mutation-based Test Data Generation for Simulink Models using Genetic Algorithm and Simulated Annealing,"Software testing is costly, labor intensive, and time consuming. Modern testing requires faults to be discovered at the earliest possible stages to decrease the cost of fixing errors in software development process. Thus, high level models such as Simulink models have become the focus of much verification effort and research. Mutation testing is a powerful and effective testing technique in terms of process automation and faults detection. Test case generation for Simulink that achieves a high mutation score is complicated. In this paper, we propose the automated test data generation approach based on mutation testing for Simulink models by using Genetic Algorithm (GA) and Simulated Annealing (SA) in order to improve the quality of test data. The approach has been applied to some different case studies and the obtained results are very promising.",225,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=uYFPpDwAAAAJ&citation_for_view=uYFPpDwAAAAJ:9yKSN-GCB0IC,,1
2660,A Novel Fitness Function of Metaheuristic Algorithms for Test Data Generation for Simulink Models based on Mutation Analysis,"Testing is one of the crucial activities to assure the software quality. The main objective of testing is to generate test data uncovering faults in software modules. There are a variety of testing techniques in which mutation testing is a popular approach to generate test sets and evaluate their fault detection ability. Simulink is an environment widely used in industry to design and simulate critical systems. Testing such a system at the design phase could help to detect faults earlier. This study aims to propose a novel fitness function of metaheuristic algorithms to generate test data based on the mutation technique for the Simulink models. The fitness function is designed by analyzing each mutation operator and the features of blocks in the Simulink environment in order to guide the search process to reach the test data killing mutants more easily. Then, this fitness function is used in the multi-parent crossover genetic …",225,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=uYFPpDwAAAAJ&citation_for_view=uYFPpDwAAAAJ:eQOLeE2rZwMC,,1
2662,A genetic algorithm with multi-parent crossover using quaternion representation for numerical function optimization,"Finding optimal solutions of a numerical function of more than one independent variable is an important problem with many practical applications including process control systems, data fitting, and engineering designs. Over the last few decades, techniques for solving unconstrained optimization problems have been proposed. Evolutionary Algorithms have emerged as one of the most popular selections for tackling these problems, among which Genetic Algorithms (GAs) are widely used in practice. In recent literature on GAs, a Genetic Algorithm with multi-parent crossover (GA-MPC) was found to be superior over other algorithms. Nevertheless, the GA-MPC still has some difficulties when dealing with separable test issues and convergence to global optima in the high-dimensional search space. Meanwhile, quaternions, which are an extension of complex numbers, can allow algorithms to expand the …",225,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=uYFPpDwAAAAJ&citation_for_view=uYFPpDwAAAAJ:UebtZRa9Y70C,,1
2663,An effort estimation approach for agile software development using fireworks algorithm optimized neural network,"Software effort estimation is one of the most critical steps in the software development process. The success or failure of projects relies greatly on the accuracy of effort estimation and schedule results. Agile software development process has become prevalent in the industry and replacing the conventional approaches of software development. Nevertheless, the question of accurate estimation of effort for this novel method has still been a challenging problem with regard to researchers and practitioners. This study aims to propose a novel method to ameliorate the accuracy of agile software effort prediction process using Artificial Neural Network (ANN) optimized by Fireworks Algorithm (FWA). The performance of the proposed approach is compared to the various types of neural networks and the regression model. In addition, the role of Fireworks Algorithm in optimizing the weights and biases of the ANN is also compared with other optimization algorithms.",225,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=uYFPpDwAAAAJ&citation_for_view=uYFPpDwAAAAJ:Se3iqnhoufwC,,1
2664,"Combining feature selection, feature learning and ensemble learning for software fault prediction","This paper studies a combination of feature selection and ensemble learning to address the feature redundancy and class imbalance problems in software fault prediction. Also, a deep learning model is used to generate deep representation from defect data to improve the performance of fault prediction models. The proposed method, GFsSDAEsTSE, is evaluated on 12 NASA datasets, and the results show that GFsSDAEsTSE outperforms state-of-the-art methods in both small and large datasets.",225,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=uYFPpDwAAAAJ&citation_for_view=uYFPpDwAAAAJ:dhFuZR0502QC,,1
2665,Ensemble learning for software fault prediction problem with imbalanced data,"Fault prediction problem has a crucial role in the software development process because it contributes to reducing defects and assisting the testing process towards fault-free software components. Therefore, there are a lot of efforts aiming to address this type of issues, in which static code characteristics are usually adopted to construct fault classification models. One of the challenging problems influencing the performance of predictive classifiers is the high imbalance among patterns belonging to different classes. This paper aims to integrate the sampling techniques and common classification techniques to form a useful ensemble model for the software defect prediction problem. The empirical results conducted on the benchmark datasets of software projects have shown the promising performance of our proposal in comparison with individual classifiers.",225,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=uYFPpDwAAAAJ&citation_for_view=uYFPpDwAAAAJ:_kc_bZDykSQC,,1
2666,A novel test data generation approach based upon Mutation testing by using Artificial Immune System for Simulink models,"Software testing is costly, labor intensive, and time consuming activity. Test data generation is one of the most important steps in testing process in terms of revealing faults in software. A set of test data is considered as good quality if it is highly capable of discovering possible faults. Mutation analysis is an effective way to assess the quality of a test set. Nowadays, high level models such as Simulink are widely used to reduce the time of software development in many industrial fields. This also allows faults to be detected at the earlier stages. Verification and validation of Simulink models are becoming vital to users. In this paper, we propose the automated test data generation approach based on mutation testing for Simulink models by using Artificial Immune System (AIS) in order to evolve test data. The approach was integrated into the MuSimulink tool [15]. It has been applied to some different case studies …",225,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=uYFPpDwAAAAJ&citation_for_view=uYFPpDwAAAAJ:MXK_kJrjxJIC,,1
2667,Geometry-based static hand gesture recognition using support vector machine,"Static hand gesture recognition plays an important role in developing a system for human-computer interaction. Besides, such systems can be also used by the deaf community in order to convey information through gestures instead of words. A vision-based processing of hand gesture recognition consists of three main stages: preprocessing, feature extraction and identification. In this paper, the first stage involves following two sub-stages: segmentation which locates hand using color information and extracts its silhouette; separation that separates arm, the part with less information, based on geometrical properties. In the second stage, features which extracted from hand-without-arm are general (ratio of width to height, wrist angle and number of fingers) and detailed (calculated based on fingertips and cross sections) characteristics. Finally, support vector machine model with ""max-wins"" voting strategy is used to …",227,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=pN0M57kAAAAJ&citation_for_view=pN0M57kAAAAJ:zYLM7Y9cAGgC,,1
2668,The role of magnetic resonance imaging in the preoperative evaluation of anal fistulas,"This study aimed to determine the role of magnetic resonance imaging (MRI) in diagnosing and describing the characteristics of fistula-in-ano, and the agreement between MRI and operative findings. We conducted a retrospective study in 367 patients with fistula-in-ano who were diagnosed and had an operation at the University Medical Center between January 2016 and January 2018. MRI findings were evaluated and compared with surgical findings using the kappa coefficient (k) method. 367 patients (327 male and 40 female, mean age 39.3±12.4 years). A total of 411 primary fistulas were found during surgery. There was a strong agreement between MRI and surgery for classifying primary tracts (k= 0.89) and detecting secondary tracts (k= 0.94). While the sensitivity and specificity of MRI for detecting internal openings were 99% and 85.2% respectively; these rates were 100% for abscesses. Both T2-weighted …",227,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=pN0M57kAAAAJ&citation_for_view=pN0M57kAAAAJ:4TOpqqG69KYC,,1
2669,Dynamic gesture classification for Vietnamese sign language recognition,"This paper presents an approach of feature extraction and classification for recognizing continuous dynamic gestures corresponding to Vietnamese Sign Language (VSL). Input data are captured by the depth sensor of a Microsoft Kinect, which is almost not affected by the light of environment. In detail, each gesture is represented by a volume corresponding to a sequence of depth images. The feature extraction stage is performed by dividing such volume into a 3D grid of same-size blocks in which each one is then converted into a scalar value. This step is followed by the process of classification. The wellknown method Support Vector Machine (SVM) is employed in this work, and the Hidden Markov Model (HMM) technique is also applied in order to provide a comparison on recognition accuracy. Besides, a dataset of 3000 samples corresponding to 30 dynamic gestures in VSL was created by 5 volunteers. The experiments on this dataset to validate the approach and that shows the promising results with average accuracy up to 95%.",227,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=pN0M57kAAAAJ&citation_for_view=pN0M57kAAAAJ:WF5omc3nYNoC,,1
2670,Recognizing vietnamese sign language based on rank matrix and alphabetic rules,"Sign language plays an important role in communication in hard-of-hearing community. Hand gesture recognition is an issue which is being researched widely. In this paper, we propose an approach, which can perform in real-time, to solve such problem for Vietnamese sign language. Instead of RGB data as many other solutions, the input of our system is depth images captured by Microsoft Kinect. We also propose a novel technique, called rank-order correlation matrix (ROCM), to describe hand gestures. Based on properties of Vietnamese alphabet and the captured gesture, the classification stage is applied on different sets of gestures. Multiple support vector machines (SVMs) is combined with ""max-wins"" voting strategy to perform the recognition task. Experiments are conducted on three datasets of the D-VSL database and receive promising accuracy.",227,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=pN0M57kAAAAJ&citation_for_view=pN0M57kAAAAJ:Y0pCki6q_DkC,,1
2671,Automatic hand gesture segmentation for recognition of Vietnamese sign language,"In this paper, we propose a new solution to identify hand gestures corresponding to alphabetic characters of Vietnamese Sign Language (VSL) via a sequence of images (video) collected from the depth sensor in a Microsoft Kinect. First, a preprocessing is performed to localize and separate the hand from each image and then remove possible noise. In the next stage, the object is extracted to select key frames, which support to represent a segment of the video. Each determined key frame is then converted to a binary image and estimate some biological information such as the hand boundary, finger positions and the palm center. The position of palm centre and fingertips are also localized in 3D space. The process of recognition is performed using Support Vector Machine (SVM) method. The experiments show that the proposed approach is promising since the recognition accuracy is about 91%.",227,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=pN0M57kAAAAJ&citation_for_view=pN0M57kAAAAJ:W7OEmFMy1HYC,,1
2672,Second-order total generalized variation based model for restoring images with mixed Poisson-Gaussian noise,"A common problem in image restoration is image denoising. Among many noise models, the mixed Poisson - Gaussian model has recently aroused considerable interest.
Purpose:
Development of a model for denoising images corrupted by mixed Poisson - Gaussian noise, along with an algorithm for solving the resulting minimization problem.
Results:
We proposed a new total variation model for restoring an image with mixed Poisson - Gaussian noise, based on second-order total generalized variation. In order to solve this problem, an efficient alternating minimization algorithm is used. To illustrate its comparison with related methods, experimental results are presented, demonstrating the high efficiency of the proposed approach.
Practical relevance:
The proposed model allows you to remove mixed Poisson - Gaussian noise in digital images, preserving the edges. The presented numerical results demonstrate the competitive features of the proposed model.",227,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=pN0M57kAAAAJ&citation_for_view=pN0M57kAAAAJ:qxL8FJ1GzNcC,,1
2673,A hybrid regularizers model for multiplicative noise removal,"In this paper, we propose a variational method for restoring images corrupted by multiplicative noise. Computationally, we employ the alternating minimization method to solve our minimization problem. We also study the existence and uniqueness of the proposed problem. Finally, experimental results are provided to demonstrate the superiority of our proposed hybrid model and algorithm for image denoising in comparison with state-of-the-art methods.",227,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=pN0M57kAAAAJ&citation_for_view=pN0M57kAAAAJ:aqlVkmm33-oC,,1
2674,Short Survey on Static Hand Gesture Recognition,"This paper presents a survey of methods which have been recently proposed for recognizing static hand gestures. These approaches are first summarized and then are assessed based on a common dataset. Because mentioned methods employ different types of input, the survey focuses on stages of feature extraction and classification. Other former steps, such as pre-processing and hand segmentation, are slightly modified. In experiments, this work does not only consider the recognition accuracy but also suggests suitable scenarios for each method according to its advantages and limitations.",227,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=pN0M57kAAAAJ&citation_for_view=pN0M57kAAAAJ:Se3iqnhoufwC,,1
2675,Geometry-based dynamic hand gesture recognition,"Hand gestures play an important role in communication in the hard-of-hearing community. They are used to convey information instead of words. Besides, a system which is developed to identify gestures can be also used for human-computer interaction. In this paper, we propose a vision-based approach for recognizing dynamic hand gestures. Our processing consists of three main stages: pre-processing, feature extraction and recognition. The first stage involves two sub-stages: segmentation which locates the hand and extracts the corresponding silhouette using color information; separation that removes the arm based on geometrical properties. Some characteristics which describe the hand posture are then extracted. Finally, the recognition is performed using two popular algorithms, which are Dynamic Time Warping and Hidden Markov Model. The experiment is conducted on SKIG dataset with a comparison of classification accuracies corresponding to the two mentioned methods.",227,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=pN0M57kAAAAJ&citation_for_view=pN0M57kAAAAJ:YsMSGLbcyi4C,,1
2676,Modeling dynamic hand gesture based on geometric features,"Hand gesture identification is one of problems being widely studied. There are two research trends corresponding to two data types, which are static and dynamic gestures. The static gesture is recognized based on the hand shape, while motion is the main feature in identifying dynamic gestures. In this paper, we propose an approach for modeling the dynamic hand gestures based on a combination of two mentioned information. At first, the hand silhouette is extracted using a skin-color filter. A sequence of geometric manipulations is then performed to remove the possible arm. The characteristics which describe the hand shape and motion orientation are estimated. Finally, the k-means clustering technique is combined with hidden Markov model to model each dynamic gesture. The experiments are performed on human-computer interaction dataset and obtain high efficiency.",227,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=pN0M57kAAAAJ&citation_for_view=pN0M57kAAAAJ:Tyk-4Ss8FVUC,,1
2677,Role of magnetic resonance imaging in diagnosing ovarian teratoma,"Aims and objectives
To describe characteristics of ovarian teratoma on MR imaging (MRI). To evaluate the role of MRI in diagnosing benign and malignant ovarian teratoma. It is helpful to differentiate between a benign and malignant teratoma before surgery.",227,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=pN0M57kAAAAJ&citation_for_view=pN0M57kAAAAJ:M3ejUd6NZC8C,,1
2679,SURVEY ON SKELETAL GAIT NORMALITY ASSESSMENT,"In this paper, we present a survey on recent approaches for gait normality assessment based on 3D skeleton. Such methods can be applied or adapted in various applications such as gait analysis in healthcare systems and person identification in surveillance. Some studies working on other input data that can be extracted and/or approximated from 3D skeletal information are also considered in this paper. Their experimental results on a huge benchmark dataset are also provided and analyzed to give useful suggestions for further works of developing realistic health care systems.",227,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=pN0M57kAAAAJ&citation_for_view=pN0M57kAAAAJ:YOwf2qJgpHMC,,1
2683,Statistical implicative similarity measures for user-based collaborative filtering recommender system,"This paper proposes a new similarity measures for User-based collaborative filtering recommender system. The similarity measures for two users are based on the Implication intensity measures. It is called statistical implicative similarity measures (SIS). This similarity measures is applied to build the experimental framework for User-based collaborative filtering recommender model. The experiments on MovieLense dataset show that the model using our similarity measures has fairly accurate results compared with User-based collaborative filtering model using traditional similarity measures as Pearson correlation, Cosine similarity, and Jaccard.",229,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=igmb0QwAAAAJ&citation_for_view=igmb0QwAAAAJ:WF5omc3nYNoC,,1
2684,Collaborative recommenderation based on statistical implication rules,"In recent researches, many approaches based on association rules have been proposed to improve the accuracy of recommender systems. These approaches are primarily based on Apriori data mining algorithm in order to generate the association rules and apply them to improving the recommendation results. However, these approaches also reveal some disadvantages of the system, such as taking a longer time for generating association rules; applying the Apriori algorithm on rating sparse matrix resulting in irrelevant information and causing poor recommendation results to target users and association rules generated primarily relying on given threshold of Support and Confidence measures leading to the focus on the majority of rules and ignoring the astonishment of rules to affect the recommendation results. In this study, we propose a new model for collaborative filtering recommender systems: The collaborative recommendation is based on statistical implication rules (IIR); Differently from collaborative recommendation based on association rules (AR), the IIR predicts the items for users based on statistical implication rules generated from rating matrix and Implication intensity measures measuring the surprisingness of rules. To evaluate the effectiveness of the model, in the experimental section, we implement the model on three real datasets and compare the results with some different effective models. The results show that the IIR has higher precision on the experimental datasets.",229,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=igmb0QwAAAAJ&citation_for_view=igmb0QwAAAAJ:eQOLeE2rZwMC,,1
2687,Multidomain Supervised Aspect-based Sentiment Analysis using CNN_Bidirectional LSTM model,"Sentiment analysis or opinion mining used to capture the community’s attitude who have experienced the specific service/product. Sentiment analysis usually concentrates to classify the opinion of whole document or sentence. However, in most comments, users often express their opinions on different aspects of the mentioned entity rather than express general sentiments on entire document. In this case, using aspect-based sentiment analysis (ABSA) is a solution. ABSA emphases on extracting and synthesizing sentiments on particular aspects of entities in opinion text. The previous studies have difficulty working with aspect extraction and sentiment polarity classification in multiple domains of review. We offer an innovative deep learning approach with the integrated construction of bidirectional Long Short Term Memory (BiLSTM) and Convolutional Neural Network (CNN) for multidomain ABSA in this article. Our …",229,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=igmb0QwAAAAJ&citation_for_view=igmb0QwAAAAJ:Se3iqnhoufwC,,1
2688,Adaptive testing model approach based on Birnbaum model and Markov model,"Computerized adaptive testing is widely used in the testing and assessment of the level of learners' competency. Popular computerized adaptive testing systems now use the mathematical models of Item Response Theory based on the relationship between the ability of examinees and item parameters. However, Item Response Theory does not take into account the impact between previous answers and the next item selection. The item parameters principally rely on probability methods of Classical Test Theory. This article proposes the combination of Item Response Theory (Birnbaum model) and Markov chain to calculate the dependency of answer set during the quiz process. Concurrently, using Hooke-Jeeves direct search method within the limited range of parameters to assess the set of item parameters.",229,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=igmb0QwAAAAJ&citation_for_view=igmb0QwAAAAJ:ufrVoPGSRksC,,1
2689,Lựa chọn mô hình và tham số cho bài toán tư vấn lọc cộng tác dựa trên đồ thị đánh giá,"Recommender system is considered one of the most effective solutions that can cope with information explosion due to the rapid development of Internet services and is widely applied in many fields. However, to design a recommender system can meet the needs of users, the selection of suitable models for the recommender system and choosing the appropriate value of parameters for the model are always big challenges of designers. This study proposes solutions to choose models and value of parameters suitable for specific collaborative filtering recommender systems. To evaluate the proposed solutions, experiments on three standard datasets of MovieLens, MSWeb, and Jester5k are conducted. Experimental results show that the proposed solutions can assist designers and researchers to quickly identify model and the value parameters model for their specific collaborative filtering recommender systems …",229,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=igmb0QwAAAAJ&citation_for_view=igmb0QwAAAAJ:zYLM7Y9cAGgC,,1
2697,A Speaker-Adaptive HMM-based Vietnamese Text-to-Speech System,"This paper describes the first attempt in developing a Vietnamese HMM-based Text-to-Speech system using the speaker-adaptive approach. Although speaker-dependent systems have been built widely, no speaker-adaptive system has been developed for Vietnamese so far. We collected speech data from several Vietnamese native speakers and employed state-of-the-art speech analysis, model training and speaker adaptation techniques to develop the system. Besides, we performed perceptual experiments to compare the quality of speaker-adapted (SA) voices built on the average voice model and speaker-dependent (SD) voices built on SD models, and to confirm the effects of contextual features including word boundary (WB) and part-of-speech (POS) on the quality of synthetic speech. Evaluation results show that SA voices have significantly higher naturalness than SD voices when the same limited …",232,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_Aiphz0AAAAJ&citation_for_view=_Aiphz0AAAAJ:YsMSGLbcyi4C,,1
2698,F0 parameterization of glottalized tones in HMM-based speech synthesis for Hanoi Vietnamese,"SUMMARY A conventional HMM-based speech synthesis system for Hanoi Vietnamese often suffers from hoarse quality due to incomplete F0 parameterization of glottalized tones. Since estimating F0 from glottalized waveform is rather problematic for usual F0 extractors, we propose a pitch marking algorithm where pitch marks are propagated from regular regions of a speech signal to glottalized ones, from which complete F0 contours for the glottalized tones are derived. The proposed F0 parameterization scheme was confirmed to significantly reduce the hoarseness whilst slightly improving the tone naturalness of synthetic speech by both objective and listening tests. The pitch marking algorithm works as a refinement step based on the results of an F0 extractor. Therefore, the proposed scheme can be combined with any F0 extractor.",232,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_Aiphz0AAAAJ&citation_for_view=_Aiphz0AAAAJ:2osOgNQ5qMEC,,1
2699,Representing context in abbreviation expansion using machine learning approach,"Text normalization is an essential problem in applications involving natural language processing since the input text often contains non-standard words such as abbreviations, numbers, and foreign words. This paper deals with the problem of normalizing abbreviations in Vietnamese text when there are several possible expansions for an abbreviation. To disambiguate the expansions for an abbreviation, a machine learning approach is proposed in which contextual information of the abbreviation is represented by either of the two models: Bag-of-words or Doc2vec. Experiments with Naïve Bayes classifier on a dataset of abbreviations collected by us shows that the average ratios of expanding correctly for Bag-of-words and Doc2vec are 86.0% and 79.7%, respectively. Experimental results also show that information on the context plays an important role in the correct expansion of an abbreviation.",232,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_Aiphz0AAAAJ&citation_for_view=_Aiphz0AAAAJ:IjCSPb-OGe4C,,1
2700,F0 parameterization of glottalized tones for HMM-based Vietnamese TTS,"A conventional HMM-based TTS system for Hanoi Vietnamese often suffers from the hoarse quality due to the incomplete F0 parameterization of glottalized tones. As estimating F0 in glottalization is rather problematic for usual F0 extractors, we propose a pitch marking algorithm where the pitch marks are propagated from regular regions of speech signal to glottalized one, from which the complete F0 contour of a glottalized tone is derived. The proposed F0 parameterization scheme was confirmed to significantly reduce the hoarseness whilst improve the tone naturalness of synthetic speech by both objective and listening tests. The pitch marking algorithm works as a refinement step based on the results of an F0 extractor. Therefore, the proposed scheme can be combined with any F0 extractor.",232,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_Aiphz0AAAAJ&citation_for_view=_Aiphz0AAAAJ:u5HHmVD_uO8C,,1
2701,A generation error function considering dynamic properties of speech parameters for minimum generation error training for hidden Markov model-based speech synthesis,"A minimum generation error (MGE) criterion has been proposed for model training in hidden Markov model (HMM)-based speech synthesis to minimize the error between generated and original static parameter sequences of speech. However, dynamic properties of speech parameters are ignored in the generation error definition. In this study, we incorporate these dynamic properties into MGE training by introducing the error component of dynamic features (ie, delta and delta-delta parameters) into the generation error function. We propose two methods for setting the weight associated with the additional error component. In the fixed weighting approach, this weight is kept constant over the course of speech. In the adaptive weighting approach, it is adjusted according to the degree of dynamicity of speech segments. An objective evaluation shows that the newly derived MGE criterion with the adaptive weighting method results in comparable performance for the static feature and better performance for the delta feature compared with the baseline MGE criterion. Subjective listening tests exhibit a small but statistically significant improvement in the quality of speech synthesized by the proposed technique. The newly derived criterion improves the capability of HMMs in capturing dynamic properties of speech without increasing the computational complexity of the training process compared with the baseline criterion.",232,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_Aiphz0AAAAJ&citation_for_view=_Aiphz0AAAAJ:u-x6o8ySG0sC,,1
2703,Fruit recognition based on near-infrared spectroscopy using deep neural networks,"Near-infrared (NIR) spectroscopy has been widely used to determine the varieties and chemical properties of agricultural and food products. The major advantage of NIR spectroscopy is that the analysis is carried out in a simple, fast, and non-destructive manner, making it suitable for food applications. As the first step in applying NIR spectroscopy for fruit recognition and analysis in Vietnam, this paper presents deep neural networks (DNNs) based solutions for automatic recognition of several kinds of fruits. We compared two proposed DNN architectures based on Convolutional Neural Network (CNN) and Residual Network (ResNet). Additionally, we proposed feature extraction methods using the first and second derivatives of the Savitzky-Golay (SG) filtered normalized NIR data. Experimental results show that the deep learning approach combined with reasonable feature extraction process can achive the …",232,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_Aiphz0AAAAJ&citation_for_view=_Aiphz0AAAAJ:LkGwnXOMwfcC,,1
2704,An Effective Vector Representation of Facebook Fan Pages and Its Applications,"Social networks have become an important part of human life. There have been recently several studies on using Latent Dirichlet Allocation (LDA) to analyze text corpora extracted from social platforms to discover underlying patterns of user data. However, when we wish to discover the major contents of a social network (e.g., Facebook) on a large scale, the available approaches need to collect and process published data of every person on the social network. This is against privacy rights as well as time and resource consuming. This paper tackles this problem by focusing on fan pages, a class of special accounts on Facebook that have much more impact than those of regular individuals. We proposed a vector representation for Facebook fan pages by using a combination of LDA-based topic distributions and interaction indices of their posts. The interaction index of each post is computed based on the …",232,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=_Aiphz0AAAAJ&citation_for_view=_Aiphz0AAAAJ:ufrVoPGSRksC,,1
2708,A Scalable Network-on-Chip Based Neural Network Implementation on FPGAs,"This paper presents a feed-forward neural network implementation using a network on chip (NoC) on a field-programmable gate array (FPGA). The design is intended for large neural networks with hundreds of neurons in each layer. It is one of the first publications that fully realizes a large neural network using NoC-based communication. A dedicated NoC is implemented to handle many interconnections among neurons. The design uses multiple processing elements (PE) to exploit the parallelism available in neural network layers. Different PE configurations are implemented and evaluated for area, power consumption and performance. Implementation and experimental results for a handwritten digit recognition system show that using a large neural network with NoC interconnect allows system scalability with scalable performance. The design achieves the maximum performance of 1339 FPOPs/cycle on a Xilinx …",233,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=atrA1oQAAAAJ&citation_for_view=atrA1oQAAAAJ:9yKSN-GCB0IC,,1
2709,Interconnect architectures for dynamically partially reconfigurable systems,"Dynamically partially reconfigurable FPGAs (Field-Programmable Gate Arrays) allow hardware modules to be placed and removed at runtime while other parts of the system keep working. With their potential benefits, they have been the topic of a great deal of research over the last decade. To exploit the partial reconfiguration capability of FPGAs, there is a need for efficient, dynamically adaptive communication infrastructure that automatically adapts as modules are added to and removed from the system. Many bus and network-on-chip (NoC) architectures have been proposed to exploit this capability on FPGA technology. However, few realizations have been reported in the public literature to demonstrate or compare their performance in real world applications. While partial reconfiguration can offer many benefits, it is still rarely exploited in practical applications. Few full realizations of partially reconfigurable systems in current FPGA technologies have been published. More application experiments are required to understand the benefits and limitations of implementing partially reconfigurable systems and to guide their further development. The motivation of this thesis is to fill this research gap by providing empirical evidence of the cost and benefits of different interconnect architectures. The results will provide a baseline for future research and will be directly useful for circuit designers who must make a well-reasoned choice between the alternatives. This thesis contains the results of experiments to compare different NoC and bus interconnect architectures for FPGA-based designs in general and dynamically partially reconfigurable systems …",233,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=atrA1oQAAAAJ&citation_for_view=atrA1oQAAAAJ:u5HHmVD_uO8C,,1
2711,An Efficient Total Variation Minimization Method for Image Restoration,"In this paper, we present an effective algorithm for solving the Poisson–Gaussian total variation model. The existence and uniqueness of solution for the mixed Poisson–Gaussian model are proved. Due to the strict convexity of the model, the split-Bregman method is employed to solve the minimization problem. Experimental results show the effectiveness of the proposed method for mixed Poisson–Gaussion noise removal. Comparison with other existing and well-known methods is provided as well.",234,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=S66t3CMAAAAJ&citation_for_view=S66t3CMAAAAJ:3s1wT3WcHBgC,,1
2712,Multi-quadratic dynamic programming procedure of edge-preserving denoising for medical images,"Low–level image processing is a functional step in almost every medical image analysis system. It is a prerequisite for proper data interpretation, diagnosis and suggestion the corresponding treatments. Medical images (example: Magnetic Resonance, Ultrasound, Computed Tomography, X–Ray), may be corrupted by a disruptive noise during acquisition and transmission process and the essential requirement for every noise reduction procedure is to preserve local image features for an accurate and effective diagnosis.",234,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=S66t3CMAAAAJ&citation_for_view=S66t3CMAAAAJ:KlAtU1dfN6UC,,1
2713,An algorithm for image restoration with mixed noise using total variation regularization,"We present here an effective scheme for image denoising based on total variation regularization. The proposed scheme allows to efficiently remove Poisson noise as well as Gaussian noise simultaneously with the help of a new kind of data fidelity term, suitable for the mixed Poisson-Gaussian noise model. The results show that the algorithm corresponding to our new scheme outperforms the existing methods for mixed Poisson-Gaussian noise removal.",234,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=S66t3CMAAAAJ&citation_for_view=S66t3CMAAAAJ:HoB7MX3m0LUC,,1
2714,AN ADAPTIVE ALGORITHM FOR RESTORING IMAGE CORRUPTED BY MIXED NOISE,"Image denoising is one of the fundamental problems in image processing. Digital images are often contaminated by noise due to the image acquisition process under poor conditions. In this paper, we propose an effective approach to remove mixed Poisson-Gaussian noise in digital images. Particularly, we propose to use a spatially adaptive total variation regularization term in order to enhance the ability of edge preservation. We also propose an instance of the alternating direction algorithm to solve the proposed denoising model as an optimization problem. The experiments on popular natural images demonstrate that our approach achieves superior accuracy than other recent state-of-the-art techniques.",234,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=S66t3CMAAAAJ&citation_for_view=S66t3CMAAAAJ:SeFeTyx0c_EC,,1
2715,Tree-serial parametric dynamic programming with flexible prior model for image denoising,"We consider here image denoising procedures, based on computationally effective tree-serial parametric dynamic programming procedures, different representations of an image lattice by the set of acyclic graphs and non-convex regularization of a new type which allows to flexibly set a priori preferences. Experimental results in image denoising, as well as comparison with related methods, are provided. A new extended version of multi quadratic dynamic programming procedures for image denoising, proposed here, shows an improved accuracy for images of a different type.",234,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=S66t3CMAAAAJ&citation_for_view=S66t3CMAAAAJ:maZDTaKrznsC,,1
2718,An Adaptive Variational Model for Medical Images Restoration,"Image denoising is one of the important tasks required by medical imaging analysis. In this work, we investigate an adaptive variation model for medical images restoration. In the proposed model, we have used the first-order total variation combined with Laplacian regularizer to eliminate the staircase effect in the first-order TV model while preserve edges of object in the piecewise constant image. We also propose an instance of Split Bregman method to solve the proposed denoising model as an optimization problem. Experimental results from mixed Poisson-Gaussian noise are given to demonstrate that our proposed approach outperforms the related methods",234,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=S66t3CMAAAAJ&citation_for_view=S66t3CMAAAAJ:pqnbT2bcN3wC,,1
2719,An Algorithm for Hybrid Regularizers Based Image Restoration with Poisson Noise,"In this paper, a hybrid regularizers model for Poissonian image restoration is introduced. We study existence and uniqueness of minimizer for this model. To solve the resulting minimization problem, we employ the alternating minimization method with rigorous convergence guarantee. Numerical results demonstrate the efficiency and stability of the proposed method for suppressing Poisson noise.",234,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=S66t3CMAAAAJ&citation_for_view=S66t3CMAAAAJ:yD5IFk8b50cC,,1
2720,A hybrid regularizers model for multiplicative noise removal,"In this paper, we propose a variational method for restoring images corrupted by multiplicative noise. Computationally, we employ the alternating minimization method to solve our minimization problem. We also study the existence and uniqueness of the proposed problem. Finally, experimental results are provided to demonstrate the superiority of our proposed hybrid model and algorithm for image denoising in comparison with state-of-the-art methods.",234,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=S66t3CMAAAAJ&citation_for_view=S66t3CMAAAAJ:pyW8ca7W8N0C,,1
2722,Image Processing Procedures Based on Multi-Quadratic Dynamic Programming,"This paper summarizes the doctoral dissertation [1] of the author. The main subject of this thesis is the study and development of a method for edge preserving in image smoothing, which is developed based on multi-quadratic dynamic programming procedure for maximum a posteriori probability estimation. Additionally, a new non-convex type regularization is proposed, with ability to flexibly set a priori preferences, using different penalties for various ranges of differences between the values of adjacent image elements. Procedures of image processing, as presented here, consider heterogeneities and discontinuities in the source data, while retaining high computational efficiency of the dynamic programming procedure and Kalman filter-interpolator. Comparative study shows, that proposed algorithms has high accuracy to speed ratio, especially in the case of high-resolution images.",234,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=S66t3CMAAAAJ&citation_for_view=S66t3CMAAAAJ:L8Ckcad2t8MC,,1
2723,Parametric procedures for image denoising with flexible prior model,"In this work, we present procedures for image denoising based on dynamic programming procedure for maximum a posteriori probability estimation. A new non-convex type regularization is used, with ability to flexibly set a priori preferences, using different penalties for various ranges of differences between the values of adjacent image elements. Proposed procedures can take into account heterogeneities and discontinuities in the source data.",234,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=S66t3CMAAAAJ&citation_for_view=S66t3CMAAAAJ:mVmsd5A6BfQC,,1
2727,A hybrid regularizers approach based model for restoring image corrupted by Poisson noise,"Image denoising is one of the fundamental problems in digital image processing. This problem usually refers to the reconstruction of an image from an observed image degraded by noise. There are many factors that cause this degradation such as transceiver equipment, or environmental influences, etc. In order to obtain higher quality images, many methods have been proposed for image denoising problem. Most image denoising method are based on total variation (TV) regularization to develop efficient algorithms for solving the related optimization problem. TV-based models have become a standard technique in image restoration with the ability to preserve image sharpness.",234,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=S66t3CMAAAAJ&citation_for_view=S66t3CMAAAAJ:f2IySw72cVMC,,1
2728,EDGE–PRESERVING DENOISING BASED ON DYNAMIC PROGRAMMING ON THE FULL SET OF ADJACENCY GRAPHS,"The ability of a denoising procedure to preserve fine image structures when suppressing unwanted noise has crucial importance for an accurate and effective medical diagnosis. We introduce here a new procedure of edge-preserving denoising for medical images, that combines the flexibility in prior assumptions, and computational effectiveness of parametric multi-quadratic dynamic programming with the increased accuracy of a tree-like representation of a discrete lattice based on the full set of possible adjacency graphs of image elements. Proposed procedure can effectively remove an additive white Gaussian noise with high quality. We provide experimental results in image denoising as well as comparison with related methods.",234,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=S66t3CMAAAAJ&citation_for_view=S66t3CMAAAAJ:7PzlFSSx8tAC,,1
2731,Skeleton-based abnormal gait detection,"Human gait analysis plays an important role in musculoskeletal disorder diagnosis. Detecting anomalies in human walking, such as shuffling gait, stiff leg or unsteady gait, can be difficult if the prior knowledge of such a gait pattern is not available. We propose an approach for detecting abnormal human gait based on a normal gait model. Instead of employing the color image, silhouette, or spatio-temporal volume, our model is created based on human joint positions (skeleton) in time series. We decompose each sequence of normal gait images into gait cycles. Each human instant posture is represented by a feature vector which describes relationships between pairs of bone joints located in the lower body. Such vectors are then converted into codewords using a clustering technique. The normal human gait model is created based on multiple sequences of codewords corresponding to different gait cycles. In the detection stage, a gait cycle with normality likelihood below a threshold, which is determined automatically in the training step, is assumed as an anomaly. The experimental results on both marker-based mocap data and Kinect skeleton show that our method is very promising in distinguishing normal and abnormal gaits with an overall accuracy of 90.12%.",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:WF5omc3nYNoC,,1
2732,3D reconstruction with time-of-flight depth camera and multiple mirrors,"In order to extract more detailed features, many recent practical applications work with 3-D models instead of 2-D images. However, 3-D reconstruction usually requires either multiple cameras or a depth sensor and a turntable. This paper proposes an approach for performing a 3-D reconstruction using only one depth camera together with two or more mirrors. Mirrors are employed as virtual depth cameras placed at different positions. All measured depth data are provided in only one frame at each time. Significant depth distortion behind a mirror, which occurred with a standard time-of-flight depth sensor, is reduced by removing unreliable points and/or re-estimating better positions for these points. The experiments on easy-to-evaluate geometric objects show that the proposed approach could play a basic role in reconstructing intermediate 3-D object models in practical applications using only cheap devices.",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:Fu2w8maKXqMC,,1
2733,Static hand gesture recognition using artificial neural network,"Computers are widely used in all fields. However, the interaction between human and machine is done mainly through the traditional input devices like mouse, keyboard etc. To satisfy the requirements of users, computers need other ways to interact more conveniently, such as using speech or body language (eg gestures, posture). In this paper, we propose a new method supporting hand gesture recognition in the static form, using artificial neural network. The proposed solution has been tested with high accuracy (98%) and is promising.",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:tKAzc9rXhukC,,1
2734,"Real time detection, tracking and recognition of medication intake","Abstract 2 In this paper, the detection and tracking of face, mouth, hands and medication bottles in the context of medication intake monitoring with a camera is presented. This is aimed at recognizing medication intake for elderly in their home setting to avoid an inappropriate use. Background subtraction is used to isolate moving objects, and then, skin and bottle segmentations are done in the RGB normalized color space. We use a minimum displacement distance criterion to track skin color regions and the R/G ratio to detect the mouth. The color-labeled medication bottles are simply tracked based on the color space distance to their mean color vector. For the recognition of medication intake, we propose a three-level hierarchal approach, which uses activity-patterns to recognize the normal medication intake activity. The proposed method was tested with three persons, with different medication intake scenarios, and gave an overall precision of over 98%.",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:5awf1xo2G04C,,1
2735,Geometry-based static hand gesture recognition using support vector machine,"Static hand gesture recognition plays an important role in developing a system for human-computer interaction. Besides, such systems can be also used by the deaf community in order to convey information through gestures instead of words. A vision-based processing of hand gesture recognition consists of three main stages: preprocessing, feature extraction and identification. In this paper, the first stage involves following two sub-stages: segmentation which locates hand using color information and extracts its silhouette; separation that separates arm, the part with less information, based on geometrical properties. In the second stage, features which extracted from hand-without-arm are general (ratio of width to height, wrist angle and number of fingers) and detailed (calculated based on fingertips and cross sections) characteristics. Finally, support vector machine model with ""max-wins"" voting strategy is used to …",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:zYLM7Y9cAGgC,,1
2736,Static hand gesture recognition using principal component analysis combined with artificial neural network,,235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:tkaPQYYpVKoC,,1
2737,Extracting silhouette-based characteristics for human gait analysis using one camera,"With the strong development of computer vision, health care and in-home monitoring systems are widely applied. Gait analysis is one of the main problems, which needs to be solved in such systems. Most of the recent researches implemented on 3D information of each walking person extracted from stereo cameras or devices with sensors, thus it leads to an increase in the computational cost and price. Therefore, we propose an approach for performing gait analysis using only one normal camera. This paper presents how characteristics are extracted from the walking person's silhouette for gait analysis, in detail, detecting abnormal gaits. Experiments are performed with normal gaits and three different types of anomaly, which consist of hunched back, left-right asymmetry, and sudden motion variation. The obtained results show that there is no case of omission or false detection, and our solution can be integrated …",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:9yKSN-GCB0IC,,1
2738,Interestingnesslab: A framework for developing and using objective interestingness measures,"The objective interestingness measures play an important role in data mining because they are used for mining, filtering and ranking the patterns. However, there is no research that collects the measures fully as well as there is no tool that can: automatically calculate the interestingness values of the patterns by using those measures, and is the framework for rapidly developing the applications related to objective interestingness measures. This paper describes Interestingnesslab - a tool of the objective interestingness measures is developed in the R language. The main functions of the tool are: mining a set of association rules and presenting them by the cardinalities (), calculating the interestingness value of an association rule according to 1 of 109 collected measures; calculating the interestingness values of the whole rule set in many measures selected by the user; discovering the …",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:VL0QpB8kHFEC,,1
2739,Estimating skeleton-based gait abnormality index by sparse deep auto-encoder,"This paper proposes an approach estimating a gait abnormality index based on skeletal information provided by a depth camera. Differently from related works where the extraction of hand-crafted features is required to describe gait characteristics, our method automatically performs that stage with the support of a deep auto-encoder. In order to get visually interpretable features, we embedded a constraint of sparsity into the model. Similarly to most gait-related studies, the temporal factor is also considered as a post-processing in our system. This method provided promising results when experimenting on a dataset containing nearly one hundred thousand skeleton samples.",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:tzM49s52ZIMC,,1
2740,Skeleton-based gait index estimation with lstms,"In this paper, we propose a method that estimates a gait index for a sequence of skeletons. Our system is a stack of an encoder and a decoder that are formed by Long Short-Term Memories (LSTMs). In the encoding stage, the characteristics of an input are automatically determined and are compressed into a latent space. The decoding stage then attempts to reconstruct the input according to such intermediate representation. The reconstruction error is thus considered as a weak gait index. By combining such weak indices over a long-time movement, our system can provide a good estimation for the gait index. Our experiments on a large dataset (nearly one hundred thousand skeletons) showed that the index given by the proposed method outperformed some recent works on gait analysis.",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:ZfRJV9d4-WMC,,1
2741,Association-based recommender system using statistical implicative cohesion measure,"The strength of the association rule-based approach compared to other approaches in building recommender systems is that it can provide the deep explanations. Besides, evaluating the quality of generated rules to obtain the better recommendations is also necessary. This can be completed by using the statistical implicative cohesion measure - a measure used for finding the rules with strong implicative relationships. The higher the cohesion value of a rule is, the better the quality of that rule is. This paper proposes a new approach based on the association rules and the cohesion measure to discover the tendencies in a data set and recommend the top items to a user. The proposed recommender system is tested on the data sets Groceries and CourseRegistration. Depending on the purpose of users, they can change the thresholds on the measure to observe the tendencies as well as to get the top …",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:PR6Y55bgFSsC,,1
2742,Human gait symmetry assessment using a depth camera and mirrors,"It is proposed in this paper a reliable approach for human gait symmetry assessment using a Time-of-Flight (ToF) depth camera and two mirrors. The setup formed from these devices provides a sequence of 3D point clouds that is the input of our system. A cylindrical histogram is estimated for describing the posture in each point cloud. The sequence of such histograms is then separated into two sequences of sub-histograms representing two half-bodies. A cross-correlation technique is finally applied to provide values describing gait symmetry indices. The evaluation was performed on 9 different gait types to demonstrate the ability of our approach in assessing gait symmetry. A comparison between our system and related methods, that employ different input data types, is also provided.",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:NJ774b8OgUMC,,1
2743,Dynamic gesture classification for Vietnamese sign language recognition,"This paper presents an approach of feature extraction and classification for recognizing continuous dynamic gestures corresponding to Vietnamese Sign Language (VSL). Input data are captured by the depth sensor of a Microsoft Kinect, which is almost not affected by the light of environment. In detail, each gesture is represented by a volume corresponding to a sequence of depth images. The feature extraction stage is performed by dividing such volume into a 3D grid of same-size blocks in which each one is then converted into a scalar value. This step is followed by the process of classification. The wellknown method Support Vector Machine (SVM) is employed in this work, and the Hidden Markov Model (HMM) technique is also applied in order to provide a comparison on recognition accuracy. Besides, a dataset of 3000 samples corresponding to 30 dynamic gestures in VSL was created by 5 volunteers. The experiments on this dataset to validate the approach and that shows the promising results with average accuracy up to 95%.",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:dTyEYWd-f8wC,,1
2744,Recognizing vietnamese sign language based on rank matrix and alphabetic rules,"Sign language plays an important role in communication in hard-of-hearing community. Hand gesture recognition is an issue which is being researched widely. In this paper, we propose an approach, which can perform in real-time, to solve such problem for Vietnamese sign language. Instead of RGB data as many other solutions, the input of our system is depth images captured by Microsoft Kinect. We also propose a novel technique, called rank-order correlation matrix (ROCM), to describe hand gestures. Based on properties of Vietnamese alphabet and the captured gesture, the classification stage is applied on different sets of gestures. Multiple support vector machines (SVMs) is combined with ""max-wins"" voting strategy to perform the recognition task. Experiments are conducted on three datasets of the D-VSL database and receive promising accuracy.",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:W7OEmFMy1HYC,,1
2745,Enhancing the recognition of medication intake using a stereo camera,"In this paper, we propose a simple method for estimating the depth of objects with a stereo camera in a medication intake recognition context. A template for each detected object in the left image is matched against the right image, for calculating its horizontal disparity. Then the disparity is used for estimating the distance from camera, so that we can decide whether two objects in occlusion are in genuine contact or not. Experiments show that, the proposed approach is simple and efficient for detecting a true contact between objects in occlusion, a necessary step to further improve a video surveillance system for detection and recognition of medication intake.",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:d1gkVwhDpl0C,,1
2746,Assessment of gait normality using a depth camera and mirrors,This paper presents an initial work on assessment of gait normality in which the human body motion is represented by a sequence of enhanced depth maps. The input data is provided by a system consisting of a Time-of-Flight (ToF) depth camera and two mirrors. This approach proposes two feature types to describe characteristics of localized points of interest and the level of posture symmetry. These two features are processed on a sequence of enhanced depth maps with the support of a sliding window to provide two corresponding scores. The gait assessment is finally performed based on a weighted combination of these two scores. The evaluation is performed by experimenting on 6 simulated abnormal gaits.,235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:SdhP9T11ey4C,,1
2747,User based recommender systems using implicative rating measure,"This paper proposes the implicative rating measure developed on the typicality measure. The paper also proposes a new recommendation model presenting the top N items to the active users. The proposed model is based on the user-based collaborative filtering approach using the implicative intensity measure to find the nearest neighbors of the active users, and the proposed measure to predict users’ ratings for items. The model is evaluated on two datasets MovieLens and CourseRegistration, and compared to some existing models such as: the item based collaborative filtering model using the Jaccard measure, the user based collaborative filtering model using Jaccard measure, the popular items based model, the latent factor based model, and the association rule based model using the confidence measure. The experimental results show that the performance of the proposed model is better when compared to other five models.",235,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=SP_1vdQAAAAJ&citation_for_view=SP_1vdQAAAAJ:LjlpjdlvIbIC,,1
2748,Measurement and prediction of the density and viscosity of biodiesel blends,"Biodiesel has been considered as the potential fuel type with many advantages such as environmental pollution reduction, no sulfur production, and biodegradation. However, disadvantages of biodiesel such as high viscosity and high density affected diesel engines and fuel systems negatively. Thus, it is necessary to reduce the viscosity and density of biodiesel fuel in unmodified diesel engines. Until now, a large number of empirical correlations have been used to predict the viscosity and density of biodiesel–fossil diesel fuel blend This study was conducted to predict the kinematic viscosity and density of blends of biodiesel and fossil diesel fuel. Three types of biodiesel were examined: Coconut oil-based biodiesel (COB), Jatropha oil-based biodiesel (JOB), and Waste oil-based biodiesel (WOB). Twenty-four samples of the three types of biodiesel–diesel fuel blends were created by blending 5%(B5), 10%(B10), 20%(B20), 40%(B40), 50%(B50), 60%(B60), 75%(B75), and 100%(B100) of biodiesel with conventional diesel fuel to produce the corresponding blends for experimental purposes. Experimental correlations and mathematical equations for predicting the relationship between the kinematic viscosity and the density of the biodiesel–fossil diesel fuel blends, the dependence of the kinematic viscosity and the density of the biodiesel–fossil diesel fuel blends on biodiesel fractions, and the effects of temperature on the kinematic viscosity and density of pure biodiesel were developed. The results of the experimental correlation data were near the predicted mathematical equation with a confidence level of 95%.",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:r0BpntZqJG4C,,1
2749,"Influences of heating temperatures on physical properties, spray characteristics of bio-oils and fuel supply system of a conventional diesel engine","Alternative fuels need to satisfy the strict requirements of the use for diesel engines aiming at enhancing the performance and reducing pollutant emissions. The use of straight bio-oils for diesel engines entails improving their disadvantages such as high density, high surface tension and kinematic viscosity (tri-physical parameters). There have been some as-used methods for reduction of the above-mentioned negative effects related to straight bio-oil disadvantage, however, the adequately-heating method may be considered as a simple one helping the physical parameters of straight bio-oils to reach stable and highly-confident values which are close to those of traditional diesel fuel. As a consequence, the spray and atomization, combustion, performance, and emissions of diesel engines fueled with preheated bio-oils are improved. In this work, a study of the dependence of the density, surface tension and kinematic viscosity of coconut oil (a type of bio-oils) on temperatures (from 40-110oC) within a wide variety are conducted. In the first stage, the influence study of temperature on tri-physical parameters is carried out on the basis of experimental correlation and asdescribed mathematical equation. In the second stage, the influence study of tri-physical parameters on spray and atomization parameters including penetration length (Lb) and Sauter mean diameter (SMD), and the influence of tri-physical parameters on fuel supply system are investigated. The optimal range of temperature for the as-used bio-oils is found after analyzing and evaluating the obtained results regarding the physical properties and spray characteristics, as well as …",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:j3f4tGmQtD8C,,1
2750,Power generation characteristics of a thermoelectric modules-based power generator assisted by fishbone-shaped fins: Part II–Effects of cooling water parameters,"Operation control combined with the optimization of the structure for thermoelectric generators (TEGs) based on thermoelectric modules (TEMs) is an effective method to improve the power generation characteristics of TEGs. The present study is intended to experimentally investigate the power generation characteristics including voltage and power output, and conversion efficiency of a TEM-based TEG, which is added fishbone-fins aiming to enhance heat exchange of hot inlet air to the hot side of TEG. The effects of mass flow rate (MFR) of cooling water on the power generation characteristics of TEGs are analyzed in the case of the hot inlet air temperatures of 200–500°C and a constant MFR of hot inlet air. Compared to TEM-based TEG without fishbone-shaped fins, TEM-based TEG with fishbone-shaped fins has a maximum increase by 31.37 V of voltage output and 16.49 W of power output. Conversion …",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:TFP_iSt0sucC,,1
2751,A Gaussian wavelet network-based robust adaptive tracking controller for a wheeled mobile robot with unknown wheel slips,"In this paper, a robust adaptive tracking controller is proposed for a nonholonomic wheeled mobile robot (WMR) in the presence of unknown wheel slips. The role of the Gaussian wavelet network in this proposed controller is to approximate unknown smooth nonlinear dynamic functions due to no prior knowledge of the dynamic parameters of the WMR. In addition, one robust law is employed at the kinematic level so as to compensate the harmful effects of the unknown wheel slips, and another robust law is used at the dynamic level to overcome total uncertainties caused by dynamic parameter variations, external disturbances, etc. The stability of the whole closed-loop control system is proved in accordance with Lyapunov theory and Barbalat's lemma. Ultimately, the simulation results are shown in comparison with those of another control method under the same condition to confirm the validity and efficiency of this …",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:YFjsv_pBGBYC,,1
2752,"Modeling and feedback linearization control of a nonholonomic wheeled mobile robot with longitudinal, lateral slips","This paper presents a systematic method to build the kinematic model and dynamic model of a nonholonomic wheeled mobile robot (WMR) with longitudinal and lateral slip, followed by the design of a control law using the input-output feedback linearization method to drive the mobile robot to track a given trajectory while longitudinal, and lateral slip exist. The asymptotical stability of the system is verified by solving second-order differential linear equations to find solutions for history time. Matlab-Simulink simulation results show the correctness and performances of the control law.",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:M3ejUd6NZC8C,,1
2753,Classification and clustering of spatial patterns with geometric algebra,"In fields of classification and clustering of patterns most conventional methods of feature extraction do not pay much attention to the geometric properties of data, even in cases where the data have spatial features. This paper proposes to use geometric algebra to systematically extract geometric features from data given in a vector space. We show the results of classification of handwritten digits and those of clustering of consumers’ impression with the proposed method.",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:d1gkVwhDpl0C,,1
2754,Influence of Various Basin Types on Performance of Passive Solar Still: A Review.,"Passive solar still is the simplest design for distilling seawater by harnessing solar energy. Although it is undeniable that solar still is a promising device to provide an additional freshwater source for global increasing water demand, low thermal efficiency along with daily distillate yield are its major disadvantages. A conventional solar still can produced 2 to 5 L/m²day. Various studies have been carried out to improve passive solar stills in terms of daily productivity, thermal efficiency, and economic effectiveness. Most of the researches that relate to the daily output improvement of passive solar still concentrates on enhancing evaporation or/and condensation processes. While the condensation process is influenced by wind velocity and characteristics of the condensed surface, the evaporation process is mainly affected by the temperature of basin water. Different parameters affect the brackish water temperature such …",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:e5wmG9Sq2KIC,,1
2755,Visual control of integrated mobile robot-pan tilt-camera system for tracking a moving target,"In this paper, in order to track a moving target, we propose a new control law for an integrated mobile robot- pan tilt-camera system. Our controller consists of two control loops, i.e., a kinematic and dynamic control loop, respectively. The kinematic control loop performs three tasks, i.e., allowing an image feature of the target to converge to the center of the image plane asymptotically, designing a trajectory for the mobile robot, and allowing the mobile robot to track the desired position and direction. In the dynamic control loop, the torques are determined; the actual angular velocities of the system, i.e., angular velocities of pan and tilt axis, angular velocities of right and left wheels, track the desired angular velocities which are the outputs of the kinematic controller of the kinematic control loop. According to the Lyapunov theory and Barbalat's theorem, the asymptotic stability of the whole system is proven. Simulation …",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:4DMP91E08xMC,,1
2756,Robust feature extractions from geometric data using geometric algebra,"Most conventional methods of feature extraction for pattern recognition do not pay sufficient attention to inherent geometric properties of data, even in the case where the data have spatial features. This paper introduces geometric algebra to extract invariant geometric features from spatial data given in a vector space. Geometric algebra is a multidimensional generalization of complex numbers and of quaternions, and it ables to accurately describe oriented spatial objects and relations between them. This paper proposes to combine several geometric features using Gaussian mixture models. It applies the proposed method to the classification of hand-written digits.",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:Tyk-4Ss8FVUC,,1
2757,Feature extractions with geometric algebra for classification of objects,"Most conventional methods of feature extraction do not pay much attention to the geometric properties of data, even in cases where the data have spatial features. In this study we introduce geometric algebra to undertake various kinds of feature extraction from spatial data. Geometric algebra is a generalization of complex numbers and of quaternions, and it is able to describe spatial objects and relations between them. This paper proposes to use geometric algebra to systematically extract geometric features from data given in a vector space. We show the results of classification of hand-written digits, which were classified by feature extraction with the proposed method.",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:u5HHmVD_uO8C,,1
2758,"A review on ignition delay times of 2, 5-Dimethylfuran","The rapid depletion of fossil fuels and the strengthening of emission regulations have been considered as the primary issues in the global energy and environment field, suggesting that the increase in the percentage of alternative fuels in the transport sector is believed to be a prospective pathway. Among the renewable biofuels that have attracted great interest, 2,5-dimethylfuran (DMF) is found to offer several attractive combustion features, of which the ignition behavior is known as a global parameter. Therefore, several experimental studies and prediction models have been used to determine the ignition behavior of DMF. However, the ignition behavior of DMF in comparison with other fuels, which is important for further model development and engine applications, has not been completely understood as well as the relationship between the reaction mechanism of DMF and its ignition behavior. Moreover, the …",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:_Qo2XoVZTnwC,,1
2759,A clustering method for geometric data based on approximation using conformal geometric algebra,"Clustering is one of the most useful methods for understanding similarity among data. However, most conventional clustering methods do not pay sufficient attention to the geometric properties of data. Geometric algebra (GA) is a generalization of complex numbers and quaternions able to describe spatial objects and the relations between them. This paper uses conformal GA (CGA), which is a part of GA, to transform a vector in a real vector space into a vector in a CGA space and presents a proposed new clustering method using conformal vectors. In particular, this paper shows that the proposed method was able to extract the geometric clusters which could not be detected by conventional methods.",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:W7OEmFMy1HYC,,1
2760,A review of the indirect solar dryer with sensible heat storage mediums,"The quality of agricultural products will have deteriorated as the preserved technology is inadequate and the storage process is not guaranteed. postharvest losses have been overcome with advanced techniques in production processes. Drying is applied to preserve food as well as agricultural goods. Utilizing the sun’s energy for drying has been performed historically. Different designs of solar dryers with a variety of capacities are available in the agricultural sector. Among them, an Indirect solar dryer (ISD) is potential for food products however its drawback is unable to work after sunset. Techniques extend its operation at off sunshine hours for performance enhancement have not been reviewed in detail. This review aims to assess the features along with the advantages of sensible heat storage material (SHSM) in ISD. Common types of ISD with SHSM are presented and the review, discussion, and tabulation of obtained findings on ISD with various sensible heat material have been carried out. It is potential to develop a solar dryer system that employed thermal energy storage materials which have high thermal efficiency and appropriate cost and substitute possibly to fossil fuel in both the developing and developed countries. The storage unit usage, Drying process in ISD for agricultural food can be conducted at off sunshine hours, which was impossible with a conventional solar dryer. As a result, solar drying agricultural products with sensible heat storage materials benefit the energy conservation as well as the performance of solar systems.",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:RHpTSmoSYBkC,,1
2761,Logistics Revolution for e-commerce in Vietnam: A Brief Review,"For the whole process of circulation, distribution, application of the logistics system is a higher development step of transportation technology. Multimodal transport has linked all modes of transport together to serve the supply and consumption needs of the manufacturer. Currently, when e-commerce (EC) is on the rise, quality logistics and delivery services are the essential stages to achieve success in transactions. e-commerce today is no longer a fashion trend; it is an essential part of the retail industry. EC is changing the game of Logistics. The e-commerce world has been introducing many risks and complex issues that logistics and supply chain experts have never faced. E-commerce is the main impact on terminal logistics (last mile) and warehousing. However, it is also quite interesting to see how e-commerce is impacting airports and seaports. E-commerce is growing actively in Vietnam at a rate of about 25% per year, which is an excellent opportunity for business logistics services to get more orders. Moreover, the Vietnamese logistics market is small but has a high growth rate (20% - 25% / year). In recent times, this market is expected to change the face and grow faster due to the development of electronic retail in Vietnam and e-Logistics logistics trends in the world. However, to meet the requirements, to grasp the opportunities, logistics enterprises need to link to get a closed and synchronized service chain. So logistics companies need to increase information, respond faster, enhance technology to meet flexible processes, establish networks, and business links to meet changing needs. The article shows that the inadequacies and lack of …",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:NMxIlDl6LWMC,,1
2762,The DOMJudge Based Online Judge System with Plagiarism Detection,"There were many online judge systems which were created to automatically judge programming code. Among the development of online judge systems, the open-source online judge systems are also developing gradually, like for example Sharif Online Judge and DOMjudge. This research modified the DOMjudge by creating additional functions and improved the graphic user interface of DOMjudge in the user-friendly way to students and teachers. Besides, the research also developed a plagiarism detection system, which helps teachers to find out who cheated during the contest. This paper showed that proposed preprocessing and using k-Grams, Hashing and Suffix Array to compare two source codes is better than conventional methods.",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:5nxA0vEk-isC,,1
2763,An adaptive algorithm for restoring image corrupted by mixed noise,"Image denoising is one of the fundamental problems in image processing. Digital images are often contaminated by noise due to the image acquisition process under poor conditions. In this paper, we propose an effective approach to remove mixed Poisson-Gaussian noise in digital images. Particularly, we propose to use a spatially adaptive total variation regularization term in order to enhance the ability of edge preservation. We also propose an instance of the alternating direction algorithm to solve the proposed denoising model as an optimization problem. The experiments on popular natural images demonstrate that our approach achieves superior accuracy than other recent state-of-the-art techniques.",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:_kc_bZDykSQC,,1
2764,A conformal geometric algebra based clustering method and its applications,"Clustering is one of the most useful methods for understanding similarity among data. However, most conventional clustering methods do not pay sufficient attention to the geometric distributions of data. Geometric algebra (GA) is a generalization of complex numbers and quaternions able to describe spatial objects and the geometric relations between them. This paper uses conformal GA (CGA), which is a part of GA. This paper transforms data from a real Euclidean vector space into a CGA space and presents a new clustering method using conformal vectors. In particular, this paper shows that the proposed method was able to extract the geometric clusters which could not be detected by conventional methods.",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:IjCSPb-OGe4C,,1
2765,Trajectory tracking control of a mobile robot by computed torque method with on-line learning neural network,"This paper proposes a novel control algorithm for the mobile robot with nonholonomic constraint. The algorithm consists of two control loops: one is based on the kinematics and Lyapunov theory to derive the control laws for the tangent and angular velocities to control the robot to follow a target trajectory, the other controls the robot dynamic based on the moment method in which a neural network namely RBFNN is introduced to compensate the uncertainty of dynamic parameters. The convergence of the estimators based on RBFNN of Stone-Weierstrass is proven. The asymptotically stabilization of the whole system is confirmed by direct Lyapunov stabilization theory. The effectiveness of the method is verified by simulations in Matlab.",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:iH-uZ7U-co4C,,1
2766,Feature extraction using conformal geometric algebra for AdaBoost algorithm based in-plane rotated face detection,"In this paper, we propose a novel face detection method based on the AdaBoost algorithm. In the past few years, a variety of variant AdaBoost approaches has been proposed and obtained increasing success in both performance and robustness. However, those approaches have not focused much on the geometric characteristics of face images. A new way to apply AdaBoost is introduced in this paper with the utilization of Conformal Geometric Algebra for extracting features from input samples. By analysis and experiments, using Conformal Geometric Algebra, we can find the hyper-spheres (-planes) that mostly fit data points and yield very low error for classification in both frontal and in-plane rotated face detection. Haar-like patterns are used as well but in a more pertinent approach to achieve more informative features. In comparison with the state-of-the-art face detection method developed by Paul Viola …",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:YOwf2qJgpHMC,,1
2767,Feature extraction for classification method using principal component based on conformal geometric algebra,"This paper discusses feature extraction methods. The feature extraction methods such principal component analysis and multiple discriminant analysis are very important techniques in machine learning research areas. The characteristic of feature extraction is to transform the data from a difficultly classified space to a easily classified space. There are many conventional machine learning methods including transformation such as artificial neural network and support vector machines. However, extracting the good features before applying machine learning methods will lead to better classification results. This paper focuses on the principal component regression (PCR). The PCR finds the approximation with hyper-planes where the data distributed on. The problem now is that it has a case of the data do not distribute on hyper-planes, for example they distribute on hyper-spheres such as rotation objects, the PCR can …",237,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=hde_T-MAAAAJ&citation_for_view=hde_T-MAAAAJ:WF5omc3nYNoC,,1
2768,Flying ad-hoc network for emergency based on IEEE 802.11 p multichannel MAC protocol,"Flying ad-hoc networks are widely used in various fields, especially in searching and rescuing of people by using unmanned aerial systems, which includes one or more mobile base stations and mission-oriented UAVs. Thanks to the mobility of UAVs, we can create a communication of Flying Network for Emergencies to support quickly and ensure strict conditions of the time in searching and rescuing. In this paper, we propose an architecture that supports the communication among rescuers or between rescuers with victims or between victims with their relatives by using the flying network for emergency over satellite systems. We particularly propose a MAC protocol based on IEEE 802.11p and IEEE 1609.4 protocols called Cluster-based Multichannel MAC IEEE 802.11p protocol to support communication in flying ad-hoc network for emergency.",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:_FxGoFyzp5QC,,1
2769,A multi-criteria priority-based V2I communication for information dissemination at RSU in VANET,"In vehicular networks (VANETs), the priority of safety applications is always important. However, many cases such as the priority of vehicle type in the green wave coordination or the time to leave coverage zone in vehicle-to-infrastructure (V2I) communication are also essential and should be considered. In this paper, we propose a multipolling mechanism for channel access for V2I communication based on a multi-criteria priority taking into account the priority of special vehicles. The simulation results show that the proposed mechanism can solve some inherent problems in VANET, reduce network delay and packet loss ratio.",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:WF5omc3nYNoC,,1
2770,Evaluation of Routing Protocols for Multi-hop Communication in LPWAN,"Nowadays, the low-power long-range networks are required for the Internet of Things applications, in which devices can transmit data over long distances and consume low energy. Low-Power Wide-Area Networks (LPWANs) have emerged to focus on a group of networks providing solutions for long-range communication and saving power consumption. However, most such networks rely on star topology networks, where the end nodes send data directly to the gateway, which leads to an issue when many nodes are far from the gateway. Aiming for no need for additional gateways, we consider multi-hop communication in LPWANs in this paper. Routing protocols are used to help some nodes that can become relay nodes and find the optimal route to forward the data to the gateway. In the simulated LoRa network, we evaluated the routing protocols in considering the delivery latency and packet loss ratio in …",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:UebtZRa9Y70C,,1
2771,A Behavior-Based Malware Spreading Model for Vehicle-to-Vehicle Communications in VANET Networks,"Network attacking using malware has become very popular on the Internet and in many other networks, namely Vehicular Ad-hoc Network (VANET) networks. It is required to have the model describing the malware spreading based on factors, which directly affect this process to limit its influences. In this paper, we propose a mathematical model called SEIR-S (Susceptible–Exposed–Infectious–Recovered–Susceptible) based on the characteristics of the VANET network and the well-known disease-spreading model SIR (Susceptible–Infectious–Recovered). We take into account possible behaviors of malware and provide the corresponding states to vehicles: Susceptible (S), Exposed (E), Infectious (I), Recovered (R). We evaluate the basic reproduction number R0 of the model and perform a stability analysis of the proposed model. The results show that, when R0 < 1, the malware spreading will gradually decrease, and, when R0 > 1, that spreading cannot be extinguished. We also point out the condition that we can control the endemic in the VANET network. In addition, the correctness of the proposed model is verified using both numerical analysis and agent-based simulation on NetLogo.",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:8k81kl-MbHgC,,1
2772,Positioning methods based on flying network for emergencies,"Recently, unmanned aerial systems have emerged in different fields such as entertainment, military, Internet of Things, emergencies, etc. In these systems, unmanned aerial vehicles (UAVs) communicate with each other based on a flying network to provide services to customers such as live streaming, high-speed access point, etc. The mobility of UAVs can bring many advantages in emergency cases such as rescuing and searching victims in areas where the network infrastructure is not available. However, the crucial task needed to be done first is to locate the victims' coordinates. In this paper, we propose the positioning methods based on a flying network for emergencies by detecting the WiFi signals generated from victims' phones. A structure of UAV-swarms is considered to optimize the searching time. Considering proposed methods, the influence of the error of distance measurement and the number of …",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:LkGwnXOMwfcC,,1
2775,DeepPlace: Deep reinforcement learning for adaptive flow rule placement in Software-Defined IoT Networks,"In this paper, we propose a novel and adaptive flow rule placement system based on deep reinforcement learning, namely DeepPlace, in Software-Defined Internet of Things (SDIoT) networks. DeepPlace can provide a fine-grained traffic analysis capability while assuring QoS of traffic flows and proactively avoiding the flow-table overflow issue in the data plane. Specifically, we first investigate the traffic forwarding process in an SDIoT network, i.e., routing and flow rule placement tasks. We design a cost function for the routing to set up traffic flow paths in the data plane. Next, we propose an adaptive flow rule placement approach to maximize the number of match-fields in a flow rule at SDN switches. To deal with the dynamics of IoT traffic flows, we model the system operation by using the Markov decision process (MDP) with a continuous action space and formulate its optimization problem. Subsequently, we …",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:MXK_kJrjxJIC,,1
2776,Research on Using the AODV Protocol for a LoRa Mesh Network,"In this paper, we consider the LoRa technology to expand sensor network coverage in smart sustainable cities. A model of a LoRa mesh network is proposed using the AODV protocol in packet routing. With a simulation model developed based on OMNET++, a series of computer experiments was carried out with changing various parameters. In the experiments results, the end-to-end delay and packet loss ratio were analyzed in the dependence on the number of nodes and packet size in the network. The simulation results show that the latency is relatively high in the LoRa mesh network, but it might be accepted for some applications.",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:hqOjcs7Dif8C,,1
2777,A Priority-Based Multichannel Mac to Support the Non-Safety Applications in SCH Interval at RSU in V2I Communication,"The transmission of safety applications in VANET is clearly the top concern. However, the non-safety applications, which improve the quality of experience, as well as the efficiency of the traffic, also need to be paid more attention. In VANET, the safety applications are transmitted in control channel interval and the non-safety applications can be transmitted only in service channel interval. In this paper, we propose a priority-based multichannel MAC to support the non-safety applications in service channel interval in Vehicle-to-Infrastructure communication with the presence of roadside unit. The novel MAC also allocates service channel according to the priority and divides the vehicles, which have the same required service channel, into four priority groups to enhance the EDCA mechanism. We use the mathematical model and the simulation for evaluating the performance under the influence of velocity with a different number of vehicles in the network.",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:ufrVoPGSRksC,,1
2778,EMATMM: эффективный метод планирования трафика для механизма мульти-опроса в высокоплотных WLAN,"An effective method to assign TXOP for the multipolling mechanism (EMATMM) in high-density WLAN is proposed by changing the TXOP and SI values in IEEE 802.11 e. In the method, the TXOP values are assigned based on Queue Size (QS) values in the QoS frames to send to the access point the actual resource requirement. At the same time, it was possible to overcome the limitations that were proposed in other methods based on estimating the number of incoming packets. If EMATMM is used, the multipolling mechanism can provide QoS support for both the CBR and VBR traffics.",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:Y0pCki6q_DkC,,1
2780,АНАЛИЗ ПРОБЛЕМ ОБЕСПЕЧЕНИЯ QOS В ВЫСОКОПЛОТНОЙ WI-FI СЕТИ,"Крупномасштабное развертывание WLAN сетей с высокой плотностью точек доступа (APs) привлекает значительное внимание из-за внедрения решений IoT в домашних сетях, стандартом в которых фактически стала технология Wi-Fi. При этом интерференция между BSSs (Basic Service Set) является серьёзной проблемой. Wi-Fi работает в нелицензируемом диапазоне частот с 11 или 13 каналами, и существует несколько групп трех непересекающихся каналов, например, 1, 6, 11. Эта нехватка частотных ресурсов приводит к возникновению ряда проблем: перекрывающиеся базовые наборы (OBSS-Overlapping Basic Service Set), проблема скрытого узла (HNP-Hidden node problem), проблема незащищенного узла (ENP-Exposed node problems), интерференции между узлами: AP–AP, AP–STA (станция), STA–STA.",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:UeHWp8X0CEIC,,1
2782,An Efficient Feature Extraction Method for Attack Classification in IoT Networks,"Numerous attacks on the Internet of Things (IoTs) necessitate detection and prevention technologies, such as Intrusion Detection Systems (IDSs). Anomaly detection is a time-consuming and challenging task in intrusion detection systems. It requires the development of a robust classifier model capable of intelligently detecting multiple types of attacks. When different input features are provided, IDS' detection performance will fluctuate substantially. Additionally, the volume of network traffic and its multidimensional characteristics will result in a lengthy classification process. In this paper, we focus on data preprocessing and propose another feature extraction method for classifying attacks in IoT networks. Additionally, we also point out the features that have the most significant impact on the accuracy of the classification model for malicious IoT traffics.",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:Zph67rFs4hoC,,1
2783,CARS: Dynamic Cyber-attack Reaction in SDN-based Networks with Q-learning,"In this paper, we propose a dynamic cyber-attack reaction system based on Q-learning, namely CARS, to effectively defeat cyber-attacks in Software-Defined Networks (SDN). In particular, we first examine a cyber-attack reaction system that operates at the SDN control plane. Then, we propose a dynamic cyber-attack reaction solution to maximize the attack defense performance while minimizing the negative influence on benign traffic forwarding in the data plane. Next, we model the cyber-attack reaction system based on a Markov decision process (MDP) and formulate its optimization problem. Afterward, we develop a Q-learning based cyber-attack reaction control algorithm to solve the optimization problem, obtaining the optimal cyber-attack reaction policy. As our case study on denial-of-service (DoS) attacks, the obtained results verify that CARS can effectively prevent malicious packets from reaching the victim …",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:kNdYIx-mwKoC,,1
2784,Determination of Subscribers Coordinates using Flying Network for Emergencies,"In the last decade, unmanned aerial systems have emerged in different fields, such as entertainment, military, Internet of Things, emergencies. In these systems, unmanned aerial vehicles (UAVs) communicate with each other based on a flying network to provide services to customers such as live streaming, high-speed access point, etc. The mobility of UAVs can bring numerous advantages in emergency cases, for example, rescuing and searching victims in areas where the network infrastructure is not available. However, the crucial task needed to be done first is to locate the victims' coordinates. In this paper, we propose a method for detecting the coordinates of subscribers with the Wi-Fi signals generated from victims' phones in the absence of network infrastructure of communication operators using a flying network for emergencies based on UAV-swarms. We develop a new protocol for communication between …",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:0EnyYjriUFMC,,1
2785,The 1-millisecond challenge-tactile internet: From concept to standardization,"In recent years, Tactile Internet (TI) has become a familiar concept to humankind. It is expected to have the potential to create many new opportunities and applications that reshape our life and economy. However, the biggest challenge for recognizing the TI - the ""1- millisecond challenge"" remains unchanged, and it requires additional research efforts. In this paper, we will dissect what has been done and what needs to be done for the ""TI ecosystem"". We will also investigate the TI concept from the perspective of the ""network latency evolution"", as well as analyzing the architecture and the emerging technologies, which are needed to meet the strict requirements of the TI.",238,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=6nGP2ysAAAAJ&citation_for_view=6nGP2ysAAAAJ:roLk4NBRz8UC,,1
2790,Adaptive testing model approach based on Birnbaum model and Markov model,"Computerized adaptive testing is widely used in the testing and assessment of the level of learners' competency. Popular computerized adaptive testing systems now use the mathematical models of Item Response Theory based on the relationship between the ability of examinees and item parameters. However, Item Response Theory does not take into account the impact between previous answers and the next item selection. The item parameters principally rely on probability methods of Classical Test Theory. This article proposes the combination of Item Response Theory (Birnbaum model) and Markov chain to calculate the dependency of answer set during the quiz process. Concurrently, using Hooke-Jeeves direct search method within the limited range of parameters to assess the set of item parameters.",239,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=CkJDcfQAAAAJ&citation_for_view=CkJDcfQAAAAJ:u-x6o8ySG0sC,,1
2791,Improving the reliability of system for secure information trasmission,"By using masking noise-like signals, modern Secure Information Transmission Systems (SITS) use auto-oscillating systems in order to generate deterministic chaos. Dynamic chaos has properties which absolutely comply with the secure communication system. These properties include a high speed data transmission of stable broadband signals to multipath fading and the possibility to confidential communication in real time. In this chaotic process, it can be obtained by using fairly simple dynamical systems which utilize reproducible numerical calculations.[1, 2].",239,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=CkJDcfQAAAAJ&citation_for_view=CkJDcfQAAAAJ:d1gkVwhDpl0C,,1
2793,Reconstruction of Low Degree B-spline Surfaces with Arbitrary Topology Using Inverse Subdivision Scheme,"Multivariate B-spline surfaces over triangular parametric domain have many interesting properties in the construction of smooth free-form surfaces. This paper introduces a novel approach to reconstruct triangular B-splines from a set of data points using inverse subdivision scheme. Our proposed method consists of two major steps. First, a control polyhedron of the triangular B-spline surface is created by applying the inverse subdivision scheme on an initial triangular mesh. Second, all control points of this B-spline surface, as well as knotclouds of its parametric domain are iteratively adjusted locally by a simple geometric fitting algorithm to increase the accuracy of the obtained B-spline. The reconstructed B-spline having the low degree along with arbitrary topology is interpolative to most of the given data points after some fitting steps without solving any linear system. Some concrete experimental examples are also provided to demonstrate the effectiveness of the proposed method. Results show that this approach is simple, fast, flexible and can be successfully applied to a variety of surface shapes.",240,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:W7OEmFMy1HYC,,1
2794,B-spline surface reconstruction by inverse subdivisions,"This paper presents a method to reconstruct a B-spline surface from a quadrangular mesh, using an inverse Catmull-Clark subdivision. We want to minimize the surface contraction due to the approximating subdivision scheme. We introduce geometrical operations which minimize the impact of the subdivision approximation and can be used in the parametric surface reconstruction. The quality of the method is evaluated by criteria of distances, curvatures or computing time on experimental results.",240,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:u5HHmVD_uO8C,,1
2795,Reconstructing low degree triangular parametric surfaces based on inverse Loop subdivision,"In this paper, we present an efficient local geometric approximate method for reconstruction of a low degree triangular parametric surface using inverse Loop subdivision scheme. Our proposed technique consists of two major steps. First, using the inverse Loop subdivision scheme to simplify a given dense triangular mesh and employing the result coarse mesh as a control mesh of the triangular Bézier surface. Second, fitting this surface locally to the data points of the initial triangular mesh. The obtained parametric surface is approximate to all data points of the given triangular mesh after some steps of local surface fitting without solving a linear system. The reconstructed surface has the degree reduced to at least of a half and the size of control mesh is only equal to a quarter of the given mesh. The accuracy of the reconstructed surface depends on the number of fitting steps k, the number of reversing …",240,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:d1gkVwhDpl0C,,1
2796,Handwriting Recognition Using B-Spline Curve,"This paper aims at presenting novel approach for curve matching and character recognition such as printed writing, handwriting, signatures, etc. based on B-Spline curve. The advantages of the B-Spline that are continuous curve representation and affine invariant, and the robustness. The recognition process is composed of two main steps: sample training and recognition. The computer must be trained with data from bitmap image file. The next step is pre-processing input data from the binary image and finding its skeleton. The reconstruction of a B-Spline curve representing the sample character is applied to find out the control points. Then the sample B-spline curve of each character is stored in a database. For the test character, it has the same process with the sample character. The matching is done by computing the Euclidean distance between the control points of test curve with those of all sample …",240,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:9yKSN-GCB0IC,,1
2799,A Method to Determine the Characteristic of Object Based on 2D/3D Correspondance,"This paper presents a method to retrieve semantic using combination of images and the corresponding 3D model reconstructed using photogrammetry. Our proposed method for determining the characteristics consists of two major steps. First, recognize characteristics based on the curvature of the point cloud and image processing of input photographies. Second, determine the characteristics of both spaces (2D and 3D) simultaneously. Our proposed method has the advantage of a combination of additional missing information on images and 3D model, to enhance detection and extraction of features. Experimental examples are presented to present the occurrely and the flexitricity of the method that can be applied to a large variety of objects.",240,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:eQOLeE2rZwMC,,1
2800,Approximation of triangular B-spline surfaces by local geometric fitting algorithm,"Surfaces over triangular domain are a powerful and flexible tool for modeling of complex objects with non-rectangular topology. Due to the particular advantages of triangular parametric surfaces, they have wide application and prospect in computer aided design and reverse engineering. The interpolation or approximation problem for reconstructing an arbitrary topological parametric surface from scattered data points or polygonal mesh is one of the significant research areas. This paper deals with the approximation of triangular B-spline surfaces based on a local geometric fitting algorithm and an inverse Loop subdivision scheme. The reconstructed triangular B-spline with the low degree crosses through most of the given data points of an initial triangular mesh. The accuracy of the obtained triangular B-spline surfaces can be carried out by changing the position of control points in the local geometric algorithm as …",240,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:Tyk-4Ss8FVUC,,1
2801,Matching correspondence between images and 3D model in a reconstruction process,"3D reconstruction from photographies is an active research trend. The resolution of the sensors is increasing and the data processing is more accurate (not only restricted to calibrated stereo vision). In archaeological research it becomes a common way to safeguard some views of an ancient site, coupled with a manner to describe in 3D the artifacts with the same set of photographies. Archaeological scientist are now facing a complex problem to handle these digital data. An important usage is to describe semantically the artifacts. It is generally made ""by hand"", supplied by the knowledge of the scientists. We propose a solution that can perform a part of this work automatically, to generate descriptions of the obtained geometry. It combines image processing, geometry processing, 3D reconstruction. This paper aims at presenting an algorithm for 2D/3D point matching. The 3D reconstruction process of model from multiple views based on SIFT algorithm. The matching process uses a 2D mask pattern to lookup the 3D corresponding point. Experimental results show that our matching algorithm is precise, highly flexible, and can be successfully applied to a variety of 3D shapes.",240,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:Y0pCki6q_DkC,,1
2802,Poster: Developing an Intrusion Detection System for Cloud Computing,"Intrusion Detection System (IDS) has been used widely in network systems to detect malicious behaviors which can harm system or computers. However, in practical implementations, an IDS is a complex work and needs investing in the new infrastructure or in training new personnel. Therefore, it is inconsistent with the users who has a small network system. In this paper, we introduce an IDSCloud model as a solution to implementing a network IDS based on cloud computing. The IDSCloud provides network IDS as a service over the internet which can be simple in deployment, maintenance, scalability without investing in the new infrastructure.",240,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:3fE2CSJIrl8C,,1
2803,Reconstruction of Planar Cam Profile Function and its Follower Displacement using B-Spline Curve based on Inverse Subdivision Method and Theory of Contact Relations-Application …,"For controlling the precision of a manufacturing planar cam mechanism, one can base on a set of digital points",240,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:2osOgNQ5qMEC,,1
2804,Analysis of geometrical features of 3D model based on the surface curvature of a set of point cloud,"In this paper, we present a method to analyze features of an unstructured 3D set of points. The method proposed consists in two main stages: 1) From the 3D point input data set, apply an algorithm of sparsity based on octree to subdivide the data surface; 2) Make a 3D reconstruction model from the set of data points after the density reducing, calculate the geometric curvature of the object mesh surface. From the calculated curvature, grouping the points with their similar curvature will be performed to analyze the surface curvature of the 3D object set of points. The results after separating by the parts on the 3D reconstruction model will be used to analyze and evaluate the reconstructed object, separate and identify the parts on the object. The research results can be applied in many fields such as: restoration of archaeological sites, analysis and identification of medical objects, CAD objects. Our work uses the data …",240,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:kNdYIx-mwKoC,,1
2805,An integrated data collection and remote monitoring of power transmission grid using Internet of Things (IoT),"The Internet of Things (IoT) refers to a heterogeneous network of physical and virtual objects embedded. Most of the data collection solution today relies on specialized electronic devices located far apart, then perform data collection and analysis based on a fixed schedule. This paper proposes an improved IoT architecture takes into consideration the environment surrounding the devices, including the objects and fires to be detected, and other factors affecting the remote management and monitoring. We present the results of research and build an Integrated Monitoring System (IMS) based on IoT technology. The collected data in IMS can be used to cater to the different requirements from the various environment. The analysis results indicate that the designed platform is suitable for the resource-constrained IoT architecture and is scalable to be extended in various IoT scenarios.",240,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=A-lHhK0AAAAJ&citation_for_view=A-lHhK0AAAAJ:5nxA0vEk-isC,,1
2814,Using Pacemaker to achieve VM Cluster High Availability for NFV,"High Availability is an important requirement for Network Function Virtualization. To guarantee high availability, every layer’components must be extremely reliable. This paper proposes to use Pacemaker to ensure high availability for Virtual Machine Cluster which is High Availability based on the use of redundant virtual machines organized into a structure.",243,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=8xEDBHAAAAAJ&citation_for_view=8xEDBHAAAAAJ:u-x6o8ySG0sC,,1
2815,Method of using Redundancy and Cluster to VNFs for Achieving VNF HA,"High Availability in Network Function Virtualization requires high availability for each layer: hardware, NFVI/VIM, Application/VM and service layer. This paper proposes VNF redundancy and cluster methods to achieving VNF High Availability (in Application/VM layer). Besides, we consider two separate cases for VNF high availability: VNF failure and VNF overload.",243,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=8xEDBHAAAAAJ&citation_for_view=8xEDBHAAAAAJ:u5HHmVD_uO8C,,1
2816,Mutation operators for Simulink models,"Mutation testing is a fault-based technique widely used for testing software. Particularly, it allows the effectiveness of a set of test data to be evaluated in terms of the ability to reveal faults. Nowadays, many industrial complex systems are more and more developed. Such systems require more testing activities to ensure a good quality. Simulink is one of the most popular tools to develop this kind of systems. In this paper, we focus on studying the application of mutation testing technique to Simulink models. We propose a set of mutation operators by basing on investigating common faults in Simulink models. A process of mutation testing for Simulink models is also presented. Some first experimentations show the promising outcomes.",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:eQOLeE2rZwMC,,1
2817,Mutation-based test data generation for simulink models using genetic algorithm and simulated annealing,"Software testing is costly, labor intensive, and time consuming. Modern testing requires faults to be discovered at the earliest possible stages to decrease the cost of fixing errors in software development process. Thus, high level models such as Simulink models have become the focus of much verification effort and research. Mutation testing is a powerful and effective testing technique in terms of process automation and faults detection. Test case generation for Simulink that achieves a high mutation score is complicated. In this paper, we propose the automated test data generation approach based on mutation testing for Simulink models by using Genetic Algorithm (GA) and Simulated Annealing (SA) in order to improve the quality of test data. The approach has been applied to some different case studies and the obtained results are very promising.",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:YsMSGLbcyi4C,,1
2818,Testability analysis for software components,"In this paper, we propose to use the static single assignment form, which was originally proposed for code optimization in compilation techniques, in order to transform software components into a data-flow representation. Thus, hardware testability concepts can be used to analyze the testability of components that are described by C or Ada programs. Such a testability analysis helps designers during the specification phases of their components and testers during the testing phases to evaluate and eventually to modify the design.",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:Tyk-4Ss8FVUC,,1
2819,A novel fitness function of metaheuristic algorithms for test data generation for simulink models based on mutation analysis,"Testing is one of the crucial activities to assure the software quality. The main objective of testing is to generate test data uncovering faults in software modules. There are a variety of testing techniques in which mutation testing is a popular approach to generate test sets and evaluate their fault detection ability. Simulink is an environment widely used in industry to design and simulate critical systems. Testing such a system at the design phase could help to detect faults earlier. This study aims to propose a novel fitness function of metaheuristic algorithms to generate test data based on the mutation technique for the Simulink models. The fitness function is designed by analyzing each mutation operator and the features of blocks in the Simulink environment in order to guide the search process to reach the test data killing mutants more easily. Then, this fitness function is used in the multi-parent crossover genetic …",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:3fE2CSJIrl8C,,1
2820,Survey on mutation-based test data generation,"The critical activity of testing is the systematic selection of suitable test cases, which is able to reveal highly the faults. Therefore, mutation coverage is an effective criterion for generating test data. Since the test data generation process is very labor intensive, time-consuming and error-prone when done manually, the automation of this process is highly aspired. The researches about automatic test data generation contributed a set of tools, approaches, development and empirical results. This paperanalyzes and conducts a comprehensive survey on generating test data based on mutation. The paper also analyzes the trends in this field.",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:KlAtU1dfN6UC,,1
2821,Testability analysis of data-flow software,"This paper is about testability analysis for data-flow software. We describe an application of the SATAN method, which allows testability of data-flow designs to be measured, to analyze testability of the source code of critical data-flow software, such as avionics software. We first propose the transformation of the source code generated from data-flow designs into the Static Single Assignment (SSA) form; then we describe the algorithm to automatically translate the SSA form into a testability model. Thus, analyzing the testability model can allow the detection of the software parts which induce a testability weakness.",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:Y0pCki6q_DkC,,1
2822,Testability analysis applied to embedded data-flow software,"Testability is an important quality factor of software, particularly embedded data-flow software such as avionics software. A lack of testability of such software can badly affect test costs and software dependability. Testability analysis can be used to identify parts of software which are difficult for testing. In this paper, we propose the use of the static single assignment (SSA) form to transform source code generated from data-flow designs into a data-flow representation, and then we describe some algorithms to automatically translate the SSA form into a testability model. Thus, some metrics can be applied to the testability model in order to locate the software parts which induce a weakness of the testability.",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:W7OEmFMy1HYC,,1
2823,Mutation testing applied to hardware: The mutants generation,"The Al-Hayek «s testing approach consists in adapting the mutation testing, a software test method, to hardware. To apply this approach, in this paper, we present a mutants generator and a test data generator for circuit descriptions in VHDL. In the mutants generator, we implement the enhancement process that allows design validation to be efficiently reused for hardware testing, and we implement the selective mutation to reduce the test cost. We also present both random and deterministic test data generator.",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:Zph67rFs4hoC,,1
2824,Comparing rule evaluation metrics for the evolutionary discovery of multi-relational association rules in the semantic web,"We carry out a comparison of popular asymmetric metrics, originally proposed for scoring association rules, as building blocks for a fitness function for evolutionary inductive programming. In particular, we use them to score candidate multi-relational association rules in an evolutionary approach to the enrichment of populated knowledge bases in the context of the Semantic Web. The evolutionary algorithm searches for hidden knowledge patterns, in the form of SWRL rules, in assertional data, while exploiting the deductive capabilities of ontologies.",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:u_35RYKgDlwC,,1
2825,"Combining feature selection, feature learning and ensemble learning for software fault prediction","This paper studies a combination of feature selection and ensemble learning to address the feature redundancy and class imbalance problems in software fault prediction. Also, a deep learning model is used to generate deep representation from defect data to improve the performance of fault prediction models. The proposed method, GFsSDAEsTSE, is evaluated on 12 NASA datasets, and the results show that GFsSDAEsTSE outperforms state-of-the-art methods in both small and large datasets.",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:b0M2c_1WBrUC,,1
2826,An evolutionary algorithm for discovering multi-relational association rules in the semantic web,"In the Semantic Web context, OWL ontologies represent the conceptualization of domains of interest while the corresponding assertional knowledge is given by RDF data referring to them. Because of its open, distributed, and collaborative nature, such knowledge can be incomplete, noisy, and sometimes inconsistent. By exploiting the evidence coming from the assertional data, we aim at discovering hidden knowledge patterns in the form of multi-relational association rules while taking advantage of the intensional knowledge available in ontological knowledge bases. An evolutionary search method applied to populated ontological knowledge bases is proposed for finding rules with a high inductive power. The proposed method, EDMAR, uses problem-aware genetic operators, echoing the refinement operators of ILP, and takes the intensional knowledge into account, which allows it to restrict and guide the search …",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:zA6iFVUQeVQC,,1
2827,A novel test data generation approach based upon mutation testing by using artificial immune system for Simulink models,"Software testing is costly, labor intensive, and time consuming activity. Test data generation is one of the most important steps in testing process in terms of revealing faults in software. A set of test data is considered as good quality if it is highly capable of discovering possible faults. Mutation analysis is an effective way to assess the quality of a test set. Nowadays, high level models such as Simulink are widely used to reduce the time of software development in many industrial fields. This also allows faults to be detected at the earlier stages. Verification and validation of Simulink models are becoming vital to users. In this paper, we propose the automated test data generation approach based on mutation testing for Simulink models by using Artificial Immune System (AIS) in order to evolve test data. The approach was integrated into the MuSimulink tool [15]. It has been applied to some different case studies …",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:roLk4NBRz8UC,,1
2828,Automatic generation of test cases in regression testing for lustre/SCADE programs,"Lustre is a formal synchronous declarative language widely used for modeling and specifying safety critical applications in the fields of avionics, transportation, and energy production. In such applications, the testing activity to ensure correctness of the system plays a crucial role. During the development process, Lustre programs (or SCADE) are often upgraded, so regression test should be performed to detect bugs. However, regression test is generally costly, because the number of test cases is usually very large. In this paper, we present the solution to automatically generating test cases in regression testing of Lustre/SCADE programs. We apply this solution to regression testing for case study Uturn System.",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:WF5omc3nYNoC,,1
2829,Applying the meta-heuristic algorithms for mutation-based test data generation for Simulink models,"Test data generation is one of the most important steps in testing process in order to reveal the faults in software. This activity is time-consuming and labor intensive. With the development of modeling tools such as Simulink, testing is able to early realize at design level. Therefore, it is desirable to seek the effective techniques for automating the testing process for Simulink models in order to make sure that the accurateness of systems which are built from these models. Mutation testing could be used as criterion to generate test data for Simulink models. In this paper, we evaluate the application of different meta-heuristic algorithms, like genetic algorithm, simulated annealing and artificial immune system, to optimize mutation-based test data generation in terms of killing the number of generated mutants for Simulink models. We discuss the effectiveness of these approaches and propose also an improvement of the …",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:LkGwnXOMwfcC,,1
2831,A new test modeling language for interactive applications based on task trees,"Designing interactive applications is a complex and error-prone activity, because of the importance of the human-computer interaction aspect. For the same reason, thoroughly testing such applications is particularly important and requires a lot of effort. Automating this activity can result to significant development cost reduction and quality improvement. This paper presents a research work aiming at defining a test modeling language for interactive applications based on task trees, called TTT. The objective of the TTT language is to provide a means for expressing abstract test scenarios for interactive applications, including non-deterministic choices and action occurrence probabilities that can be used to automate the test generation.",246,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=Wnu-9cUAAAAJ&citation_for_view=Wnu-9cUAAAAJ:Se3iqnhoufwC,,1
2836,Automatic generation of test cases in regression testing for lustre/SCADE programs,"Lustre is a formal synchronous declarative language widely used for modeling and specifying safety critical applications in the fields of avionics, transportation, and energy production. In such applications, the testing activity to ensure correctness of the system plays a crucial role. During the development process, Lustre programs (or SCADE) are often upgraded, so regression test should be performed to detect bugs. However, regression test is generally costly, because the number of test cases is usually very large. In this paper, we present the solution to automatically generating test cases in regression testing of Lustre/SCADE programs. We apply this solution to regression testing for case study Uturn System.",247,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=97Q0BrYAAAAJ&citation_for_view=97Q0BrYAAAAJ:d1gkVwhDpl0C,,1
2837,LusRegTes: A Regression Testing Tool for Lustre Programs.,"Lustre is a synchronous data-flow declarative language widely used for safety-critical applications (avionics, energy, transport...). In such applications, the testing activity for detecting errors of the system plays a crucial role. During the development and maintenance processes, Lustre programs are often evolving, so regression testing should be performed to detect bugs. In this paper, we present a tool for automatic regression testing of Lustre programs. We have defined an approach to generate test cases in regression testing of Lustre programs. In this approach, a Lustre program is represented by an operator network, then the set of paths is identified and the path activation conditions are symbolically computed for each version. Regression test cases are generated by comparing paths between versions. The approach was implemented in a tool, called LusRegTes, in order to automate the test process for Lustre programs.",247,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=97Q0BrYAAAAJ&citation_for_view=97Q0BrYAAAAJ:u-x6o8ySG0sC,,1
2838,A regression testing approach for Lustre/SCADE programs,"Software maintenance is an activity which includes enhancements, error corrections, optimization and deletion of existing features. These modifications may cause the system to work incorrectly. Therefore, regression testing becomes necessary. Regression testing is any type of software testing that seeks to uncover new software bugs, or regressions, in existing functional and non-functional areas of a system after changes, such as enhancements, patches or configuration changes, have been made to them. During the software maintenance phase, regression testing is certainly an expensive but necessary activity to make sure the new versions of the system do not"" regress"". As software evolves, chances are that not only the implementation changes, but that the specification of the system changes too. We argue that guiding regression testing by the system specifications will be more accurate and cost effective …",247,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=97Q0BrYAAAAJ&citation_for_view=97Q0BrYAAAAJ:9yKSN-GCB0IC,,1
2848,A RESEARCH ON ARCHITECTURE AND CONSTRUCTION OF CENTRAL AUTHENTICATION SYSTEM FOR THE UNIVERSITY OF DANANG,"User authentication is the precondition in the security software. For the applications which are different in many details but use the same user data, Central Authentication System is an optimal technique which supports account management without data redundancy. In this paper, we present concept, structure of central authentication and analyze disadvantages of software product which uses separate data. Then, we research on architecture and propose the model for The University of Danang. The proposed model has been applied to some software products (operations management system, scientific research time management system for The University of Danang) and the results show the effectiveness for developing and managing data.",247,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=97Q0BrYAAAAJ&citation_for_view=97Q0BrYAAAAJ:hqOjcs7Dif8C,,1
2856,DNA sequences representation derived from discrete wavelet transformation for text similarity recognition,"Recognizing text similarity, also known as duplicated documents, is considered as the most important solution for plagiarism detection which is a rising dramatically in the era of digital revolution recently. With the aim to contribute an efficient plagiarism system, we investigate a new approach for in text similarity mining via DNA sequences representation derived from Discrete Wavelet Transformation (DWT). Consequently, the contribution of the paper is classified as threefold. Firstly, we convert the raw source materials into a unique set of floating-number series called a DeoxyriboNucleic Acid (DNA) sequences using DWT. The DNA-based structure then is also required for the testing documents input at the second step. Lastly, text similarity discovery algorithm is performed for those given input DNA strings via computing the Euclidean distance. The experimental result demonstrates the advantages of the …",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:IjCSPb-OGe4C,,1
2857,Intelligent Fruit Recognition System Using Deep Learning,"Industrial Revolution 4.0 has made us people more professional, automating all production stages from office work to project work on farms. In the precision agriculture, it is very urgent to bring new and effective solutions to using artificial intelligence for people to use and improve the manual steps gradually, and increase the automation feature. So, automatic fruit recognition technique is the latest trend and effective technique in precision agriculture. This paper proposes a technical solution for fruit classification using deep learning. Automatic fruit identification using computer vision is considered a challenging task. This is because there are similarities between fruits and changes in the external environment such as light affect the fruit recognition model. Most previously implemented techniques have some limitations since their testing and evaluation is done using a limited set of data sets. Some implementations …",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:0EnyYjriUFMC,,1
2858,Data warehouse designing for Vietnamese textual document-based plagiarism detection system,"In this paper, the significance role of data warehouse designing for textual anti-plagiarism system is investigated. The paper covers the central issues of data warehousing modeling including: (1) formulating the data representation, (2) establishing the foundations of storage structure, (3) proposing corresponding architecture allowing to store, update and manage data. Consequently, two levels are considered in this paper to address the above mentioned research axes. First, at a theoretical level, the objective is to introduce novel and practical contributions in the area of textual document-based plagiarism system. The chosen approach is proposed to collect, analysis and store textual dataset. Secondly, at an implementation level, the paper focuses on the platform for processing the data, calling to modeling exhibits promising capabilities such as support for real-time, new sources of data, and self-service capabilities …",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:UeHWp8X0CEIC,,1
2859,Comparison of the Documents Based On Vector Model: A Case Study of Vietnamese Documents,"In this paper, we present the result of the study related to the comparability of two documents. This comparison aims to determine the similarity of a text/document with other one. Our method is converting a document into vector. Each element of vector is a weight corresponding to the index term that appears in the text. The similarity comparison of the two texts are transformed into angle created by two vectors. This angle represents the similarity/difference between the two documents. We have developed a tool that compares an analysis document with two or set of documents. The results reflect exactly the similarity/difference and achievement of the objectives.",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:zYLM7Y9cAGgC,,1
2861,Power Minimization for Data Collection in UAV-Assisted IoT Wireless Sensor Networks,"Power consumption is being an interest for the IoT wireless sensor networks (WSNs). Unmanned aerial vehicle (UAV) assisting data collection has been a potential candidate to meet the high demands from WSN, while reducing the power consumption. This paper considers a WSN where a UAV supports to collect the sensed data from sensors, such that the power consumption for sensors in data transmission is minimized. The resulting problem is non-convex, and thus it is hard to be directly solved. By exploiting the approximation method, we can solve the problem efficiently and achieve at least a local optimal solution. The numerical results verify the effectiveness of the proposed method.",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:YOwf2qJgpHMC,,1
2862,A Novel Approach for UAV-Aided Vehicle-to-Everything Networks,"Vehicle-to-everything (V2X) communications has been a promising solution under the demand of massive connectivity in vehicular devices. However, the mobility of vehicle is still challenging with the pressure on a higher throughput demand. Unmanned aerial vehicle (UAV) with a capability of flexible movement has been investigated to support the V2X networks. This paper proposes a novel system where a UAV can assist V2X communication under the realistic channel model of aerial devices. Furthermore, we derive a design problem for spectral efficiency maximization under power control, UAV’s trajectory to support the V2X service.",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:ULOm3_A8WrAC,,1
2863,Joint UAV Trajectory and Data Demand for Scheduling in Wireless Sensor Networks,"Energy reduction for sireless sensor networks (WSNs) is considered in many works. Sensor scheduling for data collection where the sensors in a WSN are set to wake-up and sleeping mode is a promising candidate. Unmanned aerial vehicle (UAV) utilizing the mobility is investigated to support the data collection for WSNs. This paper proposes a novel system and timing model where a UAV flies to hover at sensors and collect the sensed data. Therefore, a design problem for operation",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:Zph67rFs4hoC,,1
2864,Intelligent Fruit Recognition System Using Deep Learning,"Industrial Revolution 4.0 has made us people more professional, automating all production stages from office work to project work on farms. In the precision agriculture, it is very urgent to bring new and effective solutions to using artificial intelligence for people to use and improve the manual steps gradually, and increase the automation feature. So, automatic fruit recognition technique is the latest trend and effective technique in precision agriculture. This paper proposes a technical solution for fruit classification using deep learning. Automatic fruit identification using computer vision is considered a challenging task. This is because there are similarities between fruits and changes in the external environment such as light affect the fruit recognition model. Most previously implemented techniques have some limitations since their testing and evaluation is done using a limited set of data sets. Some implementations, does not consider changes to the external environment for the image are considered in this implementation. In this paper, exploring part of the deep learning algorithms was achieved and discovered strengths and weaknesses for these algorithms. The knowledge was gained on deep learning and a model was built that could recognize fruits from images.",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:KlAtU1dfN6UC,,1
2867,Similarity Detection for Higher-Order Structure of DNA Sequences,"With the advances in data collection and storage capabilities, large amount of multidimensional dataset, known as higher-order data representation, has been generated on bioinformatics applications recently, especially in DNA sequences recognition. This paper thus proposes a mathematical model that could be capable of the multidimensional problem of DNA similarity detection with high accuracy and reliability. To this end, the paper covers the central issues of multidimensional DNA gene expression data, including:(1) formulating multidimensional DNA data into higher-order representation;(2) recovering missing values;(3) decomposing high-order DNA data directly from their tensorial representation to extracted useful information for classification. Consequently, an exploring a novel type of third-order microarray expression, termed as genesample-time (GST), is presented for biological sample classification. The contributions will be distributed along two main thrusts of effectiveness; including latent modeling setting for imputing missing values based on the High-Order Kalman Filter and feature extraction based on Tensor Discriminative Feature Extraction. The experimental performance on real dataset of DNA sequences corroborates the advantages of the proposed approaches upon those of the matrix-based algorithms and recent tensor-based, discriminant-decomposition, in terms of missing values completion, classication accuracy and computation time.",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:W7OEmFMy1HYC,,1
2877,Automatic generation of test cases in regression testing for lustre/SCADE programs,"Lustre is a formal synchronous declarative language widely used for modeling and specifying safety critical applications in the fields of avionics, transportation, and energy production. In such applications, the testing activity to ensure correctness of the system plays a crucial role. During the development process, Lustre programs (or SCADE) are often upgraded, so regression test should be performed to detect bugs. However, regression test is generally costly, because the number of test cases is usually very large. In this paper, we present the solution to automatically generating test cases in regression testing of Lustre/SCADE programs. We apply this solution to regression testing for case study Uturn System.",247,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=97Q0BrYAAAAJ&citation_for_view=97Q0BrYAAAAJ:d1gkVwhDpl0C,,1
2878,LusRegTes: A Regression Testing Tool for Lustre Programs.,"Lustre is a synchronous data-flow declarative language widely used for safety-critical applications (avionics, energy, transport...). In such applications, the testing activity for detecting errors of the system plays a crucial role. During the development and maintenance processes, Lustre programs are often evolving, so regression testing should be performed to detect bugs. In this paper, we present a tool for automatic regression testing of Lustre programs. We have defined an approach to generate test cases in regression testing of Lustre programs. In this approach, a Lustre program is represented by an operator network, then the set of paths is identified and the path activation conditions are symbolically computed for each version. Regression test cases are generated by comparing paths between versions. The approach was implemented in a tool, called LusRegTes, in order to automate the test process for Lustre programs.",247,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=97Q0BrYAAAAJ&citation_for_view=97Q0BrYAAAAJ:u-x6o8ySG0sC,,1
2879,A regression testing approach for Lustre/SCADE programs,"Software maintenance is an activity which includes enhancements, error corrections, optimization and deletion of existing features. These modifications may cause the system to work incorrectly. Therefore, regression testing becomes necessary. Regression testing is any type of software testing that seeks to uncover new software bugs, or regressions, in existing functional and non-functional areas of a system after changes, such as enhancements, patches or configuration changes, have been made to them. During the software maintenance phase, regression testing is certainly an expensive but necessary activity to make sure the new versions of the system do not"" regress"". As software evolves, chances are that not only the implementation changes, but that the specification of the system changes too. We argue that guiding regression testing by the system specifications will be more accurate and cost effective …",247,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=97Q0BrYAAAAJ&citation_for_view=97Q0BrYAAAAJ:9yKSN-GCB0IC,,1
2889,A RESEARCH ON ARCHITECTURE AND CONSTRUCTION OF CENTRAL AUTHENTICATION SYSTEM FOR THE UNIVERSITY OF DANANG,"User authentication is the precondition in the security software. For the applications which are different in many details but use the same user data, Central Authentication System is an optimal technique which supports account management without data redundancy. In this paper, we present concept, structure of central authentication and analyze disadvantages of software product which uses separate data. Then, we research on architecture and propose the model for The University of Danang. The proposed model has been applied to some software products (operations management system, scientific research time management system for The University of Danang) and the results show the effectiveness for developing and managing data.",247,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=97Q0BrYAAAAJ&citation_for_view=97Q0BrYAAAAJ:hqOjcs7Dif8C,,1
2897,DNA sequences representation derived from discrete wavelet transformation for text similarity recognition,"Recognizing text similarity, also known as duplicated documents, is considered as the most important solution for plagiarism detection which is a rising dramatically in the era of digital revolution recently. With the aim to contribute an efficient plagiarism system, we investigate a new approach for in text similarity mining via DNA sequences representation derived from Discrete Wavelet Transformation (DWT). Consequently, the contribution of the paper is classified as threefold. Firstly, we convert the raw source materials into a unique set of floating-number series called a DeoxyriboNucleic Acid (DNA) sequences using DWT. The DNA-based structure then is also required for the testing documents input at the second step. Lastly, text similarity discovery algorithm is performed for those given input DNA strings via computing the Euclidean distance. The experimental result demonstrates the advantages of the …",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:IjCSPb-OGe4C,,1
2898,Intelligent Fruit Recognition System Using Deep Learning,"Industrial Revolution 4.0 has made us people more professional, automating all production stages from office work to project work on farms. In the precision agriculture, it is very urgent to bring new and effective solutions to using artificial intelligence for people to use and improve the manual steps gradually, and increase the automation feature. So, automatic fruit recognition technique is the latest trend and effective technique in precision agriculture. This paper proposes a technical solution for fruit classification using deep learning. Automatic fruit identification using computer vision is considered a challenging task. This is because there are similarities between fruits and changes in the external environment such as light affect the fruit recognition model. Most previously implemented techniques have some limitations since their testing and evaluation is done using a limited set of data sets. Some implementations …",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:0EnyYjriUFMC,,1
2899,Data warehouse designing for Vietnamese textual document-based plagiarism detection system,"In this paper, the significance role of data warehouse designing for textual anti-plagiarism system is investigated. The paper covers the central issues of data warehousing modeling including: (1) formulating the data representation, (2) establishing the foundations of storage structure, (3) proposing corresponding architecture allowing to store, update and manage data. Consequently, two levels are considered in this paper to address the above mentioned research axes. First, at a theoretical level, the objective is to introduce novel and practical contributions in the area of textual document-based plagiarism system. The chosen approach is proposed to collect, analysis and store textual dataset. Secondly, at an implementation level, the paper focuses on the platform for processing the data, calling to modeling exhibits promising capabilities such as support for real-time, new sources of data, and self-service capabilities …",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:UeHWp8X0CEIC,,1
2900,Comparison of the Documents Based On Vector Model: A Case Study of Vietnamese Documents,"In this paper, we present the result of the study related to the comparability of two documents. This comparison aims to determine the similarity of a text/document with other one. Our method is converting a document into vector. Each element of vector is a weight corresponding to the index term that appears in the text. The similarity comparison of the two texts are transformed into angle created by two vectors. This angle represents the similarity/difference between the two documents. We have developed a tool that compares an analysis document with two or set of documents. The results reflect exactly the similarity/difference and achievement of the objectives.",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:zYLM7Y9cAGgC,,1
2902,Power Minimization for Data Collection in UAV-Assisted IoT Wireless Sensor Networks,"Power consumption is being an interest for the IoT wireless sensor networks (WSNs). Unmanned aerial vehicle (UAV) assisting data collection has been a potential candidate to meet the high demands from WSN, while reducing the power consumption. This paper considers a WSN where a UAV supports to collect the sensed data from sensors, such that the power consumption for sensors in data transmission is minimized. The resulting problem is non-convex, and thus it is hard to be directly solved. By exploiting the approximation method, we can solve the problem efficiently and achieve at least a local optimal solution. The numerical results verify the effectiveness of the proposed method.",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:YOwf2qJgpHMC,,1
2903,A Novel Approach for UAV-Aided Vehicle-to-Everything Networks,"Vehicle-to-everything (V2X) communications has been a promising solution under the demand of massive connectivity in vehicular devices. However, the mobility of vehicle is still challenging with the pressure on a higher throughput demand. Unmanned aerial vehicle (UAV) with a capability of flexible movement has been investigated to support the V2X networks. This paper proposes a novel system where a UAV can assist V2X communication under the realistic channel model of aerial devices. Furthermore, we derive a design problem for spectral efficiency maximization under power control, UAV’s trajectory to support the V2X service.",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:ULOm3_A8WrAC,,1
2904,Joint UAV Trajectory and Data Demand for Scheduling in Wireless Sensor Networks,"Energy reduction for sireless sensor networks (WSNs) is considered in many works. Sensor scheduling for data collection where the sensors in a WSN are set to wake-up and sleeping mode is a promising candidate. Unmanned aerial vehicle (UAV) utilizing the mobility is investigated to support the data collection for WSNs. This paper proposes a novel system and timing model where a UAV flies to hover at sensors and collect the sensed data. Therefore, a design problem for operation",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:Zph67rFs4hoC,,1
2905,Intelligent Fruit Recognition System Using Deep Learning,"Industrial Revolution 4.0 has made us people more professional, automating all production stages from office work to project work on farms. In the precision agriculture, it is very urgent to bring new and effective solutions to using artificial intelligence for people to use and improve the manual steps gradually, and increase the automation feature. So, automatic fruit recognition technique is the latest trend and effective technique in precision agriculture. This paper proposes a technical solution for fruit classification using deep learning. Automatic fruit identification using computer vision is considered a challenging task. This is because there are similarities between fruits and changes in the external environment such as light affect the fruit recognition model. Most previously implemented techniques have some limitations since their testing and evaluation is done using a limited set of data sets. Some implementations, does not consider changes to the external environment for the image are considered in this implementation. In this paper, exploring part of the deep learning algorithms was achieved and discovered strengths and weaknesses for these algorithms. The knowledge was gained on deep learning and a model was built that could recognize fruits from images.",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:KlAtU1dfN6UC,,1
2908,Similarity Detection for Higher-Order Structure of DNA Sequences,"With the advances in data collection and storage capabilities, large amount of multidimensional dataset, known as higher-order data representation, has been generated on bioinformatics applications recently, especially in DNA sequences recognition. This paper thus proposes a mathematical model that could be capable of the multidimensional problem of DNA similarity detection with high accuracy and reliability. To this end, the paper covers the central issues of multidimensional DNA gene expression data, including:(1) formulating multidimensional DNA data into higher-order representation;(2) recovering missing values;(3) decomposing high-order DNA data directly from their tensorial representation to extracted useful information for classification. Consequently, an exploring a novel type of third-order microarray expression, termed as genesample-time (GST), is presented for biological sample classification. The contributions will be distributed along two main thrusts of effectiveness; including latent modeling setting for imputing missing values based on the High-Order Kalman Filter and feature extraction based on Tensor Discriminative Feature Extraction. The experimental performance on real dataset of DNA sequences corroborates the advantages of the proposed approaches upon those of the matrix-based algorithms and recent tensor-based, discriminant-decomposition, in terms of missing values completion, classication accuracy and computation time.",248,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=KDXbzakAAAAJ&citation_for_view=KDXbzakAAAAJ:W7OEmFMy1HYC,,1
2917,Reuse of free online MT engines to develop a meta-system of multilingual machine translation,"We propose a method to develop a meta system of multilingual machine translation by reusing free online automatic translation engines. This system can process and translate a homogeneous or heterogeneous document (multilingual and multicoding). This system’s purpose is to identify the language(s) and the coding(s) of the input text, to segment a heterogeneous text into several homogeneous zones, and to call a better MT engine for the target and source language pair and to retrieve the translated results in the desired language. This system can be used in several different applications, such as multilingual research, translation of the electronic mails, construction of multilingual Web sites, etc.",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:zYLM7Y9cAGgC,,1
2918,"An Isolated Picochlorum Species for Aquaculture, Food, and Biofuel","More than 500 marine algal strains in Vietnam were screened for their ability to produce high lipids. Among these, a Picochlorum species of Trebuxiophyceae emerged as the species that had the highest total lipid content with a value of 48.6% dry weight (DW), including 27.84% docosahexaenoic acid (DHA). The remaining lipid was mostly C16 and C18 fatty acids, which is appropriate for biofuel production. In addition, 20 different amino acids were identified and included a high ratio of essential amino acids. Subsequently, the effect of environmental conditions for growth, such as salinity, temperature, and media, on the oleogenic potential of this species was investigated. The alga grew better (μ = 0.25 divisions per day) at a salinity of 0.5 M NaCl in enriched seawater medium (MD1) and at high temperature, but the lipid production was higher at 2 M NaCl in artificial medium (MD2) and at low temperature …",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:RYcK_YlVTxYC,,1
2919,Text classification based on semi-supervised learning,"In this paper, we present our solution and experimental results of the application of semi-supervised machine learning techniques and the improvement of SVM algorithm to build text classification applications. Firstly, we create a features model which is based on labeled data, and then we will be improved it by the unlabeled data. The technique that is to be added a label into new data is based on binary classification. Our experiment is implemented on three data layers which are extracted from papers in three topics sports, entertainment and education on VNEXPRESS.NET. We experimented and compared the accuracy of the classification results between before and after improve features model through semi-supervised machine learning method and classification algorithm based on SVM model. Experiments show that classification quality is enhanced after improvement features model.",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:UeHWp8X0CEIC,,1
2920,Towards fairer evaluations of commercial MT systems on Basic Travel Expressions Corpora,"We compare the performance of several SYSTRAN systems on the BTEC corpus. Two language pairs: Chinese to English and Japanese to English are used. Whenever it is possible the system will be used “off the shelf” and then tuned. The first system we use is freely available on the web. The second system, SYSTRAN Premium, is commercial. It is used in two ways:(1) choosing and ordering available original dictionaries and setting parameters,(2) same+ user dictionaries.",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:d1gkVwhDpl0C,,1
2921,UVDict-a machine translation dictionary for Vietnamese language in UNL system,"In this paper, we introduce our solution to build UNL - Vietnamese dictionary using in UNL multilingual translation system. UNL (Universal networking language) is a computer language that enables computers to process information and knowledge across language barriers. It is an artificial language that replicates, in the cyber world, the functions of natural languages in human communication. Here, we focus on structure of UNL dictionary, and how to build UNL - Vietnamese dictionary from existent dictionaries as UNL-French dictionary, FEV dictionary, and Vdict dictionary.",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:Tyk-4Ss8FVUC,,1
2922,"Méthodes et outils pour utilisateurs, développeurs et traducteurs de logiciels en contexte multilingue","Dans cette thèse, nous nous intéressons d'abord à ce qui peut être fait pour aider un utilisateur de l'informatique telle qu'elle est aujourd'hui. Nous avons construit un outil, SANDOH, qui identifie les plages homogènes en langue et codage dans un document éventuellement hétérogène. Nous pouvons alors transcoder le document en fonction des ressources disponibles, de façon qu'il devienne visualisable. Un autre outil, TRADOH, permet d'obtenir une traduction dans sa langue, par mise en œuvre automatique d'un ou plusieurs systèmes de TA disponibles en local ou à distance, avec composition éventuelle. Tout n'étant pas possible du côté utilisateur, nous avons ensuite travaillé sur l'amélioration des techniques de programmation de logiciels multilingues. Nous introduisons l'outil GetAMsg, une généralisation de GETTEXT de GNU, qui permet de traiter les messages avec variables et variantes. Un message est décrit dans une syntaxe très simple comme un"" automate de messages""(AdM), ou automate fini contrôlé sans boucle. La librairie GetAMsg peut être utilisée depuis de nombreux langages de programmation usuels. Enfin, nous avons mis au point une méthodologie de traduction d'un catalogue de messages GetAMsg d'une langue dans une autre. Nous générons en langue source une instance du message pour chaque"" cas de figure"" possible en langue cible, le faisons traduire (automatiquement ou manuellement), et factorisons les résultats obtenus en un AdM en langue cible. Nous avons aussi commencé à étudier une nouvelle approche, consistant à représenter un AdM par un graphe UNL.",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:kNdYIx-mwKoC,,1
2923,Medium improvement for higher growth and longer stationary phase of Dunaliella,"Beta-carotene is a valuable carotenoid in high demand as a natural food coloring agent, provitamin A, additive to cosmetics, and health food. It can be accumulated up to more than 10% of cellular dry weight of Dunaliella salina under carotenogenic conditions such as high irradiance, high temperature, high salt concentration and nutrient deficiency. High beta-carotene productivity in Dunaliella is best achieved in a two-phase culture system through biomass optimization and beta-carotene induction. A low-cost enriched natural seawater medium (MD4) was previously investigated for biomass optimization (Tran et al., 2014); However, the culture declined rapidly after reaching the stationary phase. Thus the present study is to further improve the effectiveness of this enriched natural seawater medium (MD4) for higher growth and longer stationary phase in order to avoid quick crash phase. Algal culture in MD4 medium used as control medium was subjected to 13 different feeding treatments (TM1→ TM13) using a matrix of concentrations of various compounds (NPK, KNO3, KH2PO4). Dunaliella growth was determined based on chlorophyll concentration, cell density. Results revealed the best feeding treatment was TM4 (NPK 0.15 g/l) with cell density doubling one week compared with cell density in the control medium, and is recommended for use in the first phase biomass optimization of Dunaliella.",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:NaGl4SEjCO4C,,1
2924,"Message automata for messages with variants, and methods for their translation","In messages with variables and variants, such as “the imag.fr $n-[st|nd|rd|th] trial was successful, and the $p file[|s] found [is|are] satisfactory.”, variable types are specific (cardinal, ordinal, politeness...) and induce different “variant cases” in each language. Controlled loop-free FSAs, called here “message automata” (MAs), are proposed to model such messages. To translate a MA, one generates an instance of it for each possible variant in the target language. After translation, the values used in the instances are discarded and a target language MA is built by factorization (not classical minimization), using an original dynamic programming algorithm. A library for handling catalogues of MAs, GetAMsg, has been implemented in C, and can be used from many usual programming languages. A still speculative idea is to use a UNL graph conform to the official specifications, but with some special conventions …",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:ULOm3_A8WrAC,,1
2925,DNA sequences representation derived from discrete wavelet transformation for text similarity recognition,"Recognizing text similarity, also known as duplicated documents, is considered as the most important solution for plagiarism detection which is a rising dramatically in the era of digital revolution recently. With the aim to contribute an efficient plagiarism system, we investigate a new approach for in text similarity mining via DNA sequences representation derived from Discrete Wavelet Transformation (DWT). Consequently, the contribution of the paper is classified as threefold. Firstly, we convert the raw source materials into a unique set of floating-number series called a DeoxyriboNucleic Acid (DNA) sequences using DWT. The DNA-based structure then is also required for the testing documents input at the second step. Lastly, text similarity discovery algorithm is performed for those given input DNA strings via computing the Euclidean distance. The experimental result demonstrates the advantages of the …",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:5nxA0vEk-isC,,1
2926,Building Structured Query in Target Language for Vietnamese–English Cross Language Information Retrieval Systems,"Query translation is the most important component in Cross Language Information Retrieval systems using dictionary-based approach. In this paper, we present a method to build structured query in target language from a given query in source language. The method is based on constructing bi-lingual dictionaries, keyword extraction from source query, getting translation candidates for each keyword using mutual information and finally building structured query in target language. By combining several translations for each query term in target language, we overcome shortcomings of selecting only one translation and therefore improve system performance. Results for Vietnamese-English cross-lingual retrieval show improvements of building structured query over other methods using dictionary to produce query translation in target language by combining single selection translation for each keyword.",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:2osOgNQ5qMEC,,1
2927,Effects of salinity and light on growth of Dunaliella isolates,"Dunaliella salina, halotorelant unicellular green algae, is the main natural source of beta-carotene. Several strains of local Dunaliella salina were isolated. Together with Dunaliella bardawil DCCBC 15 and Dunaliella salina CCAP 19/18, the strains were examined for their growth under the effects of salinities (1 M, 1.5 M and 2 M) and light intensities (50, 100 and 150 µmol photon/m2/s). The result showed optimal growth for Dunaliella was obtained at 1.5 M and 2 M salinities and 50 µmol photon/m2/s light intensity. Data of this study will be further applied for carotenogenic induction experiments using salinity and light stresses on these Dunaliella salina strains.",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:vV6vV6tmYwMC,,1
2928,Identification of Dunaliella viridis using its markers,"The phylogenetic position and taxonomic status of the green alga Dunaliella viridis was investigated based on internal transcribed spacer (ITS) markers. The alga was isolated from saltern in Vinh Hao, Binh Thuan province, Vietnam. Independent phylogenetic trees of ITS1 and ITS2 sequences revealed that the alga belongs to the clade of Dunaliella viridis. The salinity for optimal growth of the alga was 2M NaCl, which was much lower than the original sampling site (4M NaCl). This tolerance to a wide range of salinity may provide distinct advantages to Dunaliella viridis over its competitors in natural environments. Further physiological and biochemical characteristics of this strain will need to be investigated in order to assess its potential for algal biomass production and other applications such as beta-carotene, carbohydrate, lipid and protein for feed, food, aquaculture and biofuels, including opening new search for other Dunaliella species.",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:lSLTfruPkqcC,,1
2932,"Willingness to pay for mangrove preservation in Xuan Thuy National Park, Vietnam: do household knowledge and interest play a role?","Xuan Thuy National Park, a special nature reserve with mangrove swamps located in the Red River Delta in North Vietnam, plays an important role in combating coastal erosion and provides a habitat for many endangered bird species. This study applied double-bounded dichotomous choice contingent valuation method to directly estimate how much locals are willing to pay for mangrove conservation at Xuan Thuy National Park. In particular, the technique was used to provide better assess to the non-use value of biodiversity and ecosystem support of mangroves. Survey respondents from 350 households in the buffer zone were presented with a hypothetical scenario describing a policy that quantifies the environmental change to be achieved by 2030, and specifying a lump sum payment. Non-parametric estimate of mean WTP was found at 511,090 VND per household (22.03 USD) whereas parametric estimate of …",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:UxriW0iASnsC,,1
2933,Automatic translation for Vietnamese based on UNL language,"In the field of Natural Language Processing, Universal Networking Language (UNL) has been used by various researchers as an interlingual approach for automated machine translation. The UNL system consists of two main components, namely, EnConverter used for converting the text from a source language to UNL and DeConverter used for converting from UNL to a target language. Currently, many projects are researching to apply UNL in different languages except Vietnamese. In this paper, we introduce the tools that are UNL's applications and how to reuse it's for the process of encoding a Vietnamese sentence into UNL expression and decoding an UNL expression into Vietnamese. The testing tools, we is done with about 1,000 Vietnamese sentence (with the help of about 4573 dictionary entries and 3161 rules) to compare the proportion of sentences was changed between the direct method and …",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:IjCSPb-OGe4C,,1
2934,Experiments with query translation and re-ranking methods in Vietnamese-English bilingual information retrieval,"Using bilingual dictionaries is a common way for query translation in Cross Language Information Retrieval. In this article, we focus on Vietnamese-English Bilingual Information Retrieval and present algorithms for query segmentation, word disambiguation and re-ranking to improve the dictionary-based query translation approach. An evaluation environment is implemented to verify and compare the application of proposed algorithms with the baseline method using manual translation.",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:9yKSN-GCB0IC,,1
2935,Results Comparison of machine translation by Direct translation and by Through intermediate language,"Accurate evaluation of machine translation is a very interesting problem in the field of automatic translation. The translation tools are mainly supported language pairs having been used in the world and very limited for the poor language pairs about bilingual corpus or linguistics resources. However, we can use Interlingua language which has a large bilingual corpus (eg English) to translate. In this paper, we use the free online Google Translation tool for testing the two methods: direct translation from English→ Vietnamese and interlingual translation using French, German. We evaluated and compared results of the two translation methods by BLUE and NIST scores.",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:Y0pCki6q_DkC,,1
2936,Réutilisation de traducteurs gratuits pour développer des systèmes multilingues,"Nous présentons ici une méthode de réutilisation de systèmes de traduction automatique gratuits en ligne pour développer des applications multilingues et évaluer ces mêmes systèmes. Nous avons développé un outil de traitement et de traduction de documents hétérogènes (multilingues et multicodage). Cet outil permet d’identifier la langue et le codage du texte, de segmenter un texte hétérogène en zones homogènes, d’appeler un traducteur correspondant avec une paire de langue source et cible, et de récupérer les résultats traduits dans la langue souhaitée. Cet outil est utilisable dans plusieurs applications différentes comme la recherche multilingue, la traduction des courriers électroniques, la construction de sites web multilingues, etc.",250,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=5f8Ny4IAAAAJ&citation_for_view=5f8Ny4IAAAAJ:WF5omc3nYNoC,,1
2937,Experimental investigation on machinability of polypropylene reinforced with miscanthus fibers and biochar,"The machinability of composite materials depends on reinforcements, matrix properties, cutting parameters, and on the cutting tool used (material, coating, and geometry). For new composites, experimental studies must be performed in order to understand their machinability, and thereby help manufacturers establishing appropriate cutting data. In this study, investigations are conducted to analyze the effects of cutting parameters and drill bit diameter on the thrust force, surface roughness, specific cutting energy, and dust emission during dry drilling of a new hybrid biocomposite consisting of polypropylene reinforced with miscanthus fibers and biochar. A full factorial design was used for the experimental design. It was found that the feed rate, the spindle speed, and the drill bit diameter have significant effects on the thrust force, the surface roughness, and the specific cutting energy. The effects of the machining parameters and the drill bit diameter on ultrafine particles emitted were not statistically significant, while the feed rate and drill bit diameter had significant effects on fine particle emission.",252,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zW67Pz0AAAAJ&citation_for_view=zW67Pz0AAAAJ:zYLM7Y9cAGgC,,1
2938,Effects of reinforcements and cutting parameters on machinability of polypropylene-based biocomposite reinforced with biocarbon particles and chopped miscanthus fibers,"New biocomposites are increasingly being developed and applied in engineering and manufacturing. The reinforcement type used in biocomposite is one of the major factors that greatly affect the machinability of biocomposite. This study investigated the drilling of two biocomposites (polypropylene-based biocomposite reinforced with biocarbon M1 (30 wt% biocarbon + PP/POE/MAPP) and chopped miscanthus fibers M2 (30 wt% miscanthus + PP/POE/MAPP)). A full factorial design was used for this study. The effects of drilling parameters and reinforcement types on surface roughness, thrust force, specific cutting energy, fine particles, and ultrafine particle generation were measured and analyzed. The results show that the thrust force and specific cutting energy during drilling of biocomposite M1 is bigger than that of biocomposite M2 under the same cutting conditions. However, the surface roughness and fine dust …",252,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zW67Pz0AAAAJ&citation_for_view=zW67Pz0AAAAJ:YsMSGLbcyi4C,,1
2939,Regression and ANFIS-based models for predicting of surface roughness and thrust force during drilling of biocomposites,"Biocomposites are increasingly being applied in the automotive industry. For assembly purpose, drilling process is often performed in these biocomposites parts to produce holes. The machining behavior of biocomposites is affected by the matrix, the reinforcements (nature and size), by the cutting conditions and the mechanical properties of biocomposites. Some of the newly developed biocomposites are made of a polypropylene/polyolefin blend matrix, which has low thermal conductivity and low softening temperature. Therefore, the selection of cutting parameters should be done with care to avoid thermal-induced damages occurred during drilling biocomposites. Also, due to the heterogeneous and anisotropic properties of biocomposites, it is essential to develop models for predicting the effects of these factors on the machining process performance indicators. In this research work, a full factorial design …",252,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=zW67Pz0AAAAJ&citation_for_view=zW67Pz0AAAAJ:eQOLeE2rZwMC,,1
2947,Developing database of Vietnamese abbreviations and some applications,"Abbreviations (CVT) in documents are widely used in various fields and in many languages including Vietnamese. In fact, currently, abbreviations are often repeatedly and unclearly used, demand for abbreviation use is increasing that requests a database of plentiful abbreviations which is saved and used conveniently, is easy to update and consistently exploited. In this article, we propose an opening solution in order to develop a database of Vietnamese abbreviations for many purposes of use during processing language and exploiting database.  ",253,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:W7OEmFMy1HYC,,1
2948,Building a spelling checker for documents in khmer language,"The practice of Khmer language teaching at Tra Vinh University shows that students often deal with many difficulties when making reports on subjects of Khmer language. The need to print, edit Khmer text with MS Word by using conventional typing on the ASCII keyboard and the Unicode fonts available made students have many spelling errors, ranging from syllable level to vocabulary level, the syntax of the sentence. Students have not correctly identified the various syllables in word structures, encountered errors in pronunciation and spellings in Khmer. In fact, Khmer-Vietnamese community in Vietnam has few people, scattered, culturally and economically limited. Moreover, Khmer writing system is complex, from the rich alphabet to the multi-syllable structure of the syllable, with no whitespace separated from the sentences. There are many borrowed words which have changed much from the spoken words in …",253,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:M3ejUd6NZC8C,,1
2949,New Automatic Search and Update Algorithms of Vietnamese Abbreviations.,"Abbreviations in documents are widely used in various fields and in many languages including Vietnamese. In fact, currently, abbreviations are regularly repeated and unclearly used, demand for abbreviation use is increasing, which requests a plentiful source of abbreviations which is conveniently saved and used, easily updated and consistently exploited. In this article, we propose some abbreviation search algorithms on the Internet in order to automatically update into database of Vietnamese abbreviations for many purposes during language processing and database exploitation.",253,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:eQOLeE2rZwMC,,1
2950,Building a machine translation system in a restrict context from Ka-Tu Language into Vietnamese,"The paper introduces some basic characteristics of Ka Tu language, from that we built the machine translation system from Ka Tu language into Vietnamese in a restrict context for translating the weather forecast bulletins, supporting the communication of Ka Tu language of Vietnamese voice emission. The system was tested at the Quang Nam Radio and Television station. The initial Results are Positive for the test of the system. The machine translation system from Ka Tu language into Vietnamese in a restrict context contributed to solving the serious lack of information in aboriginal languages and can be expanded into other areas such as warming fire alarm, broadcasting policies and laws of the State, farming experience, animal husbandry, natural disaster prevetion... Because the corpus with open structure, the optical axis is in Vietnamese language, a multilingual corpus can be easily built by adding other ethnic …",253,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:u5HHmVD_uO8C,,1
2951,"Panel: Good Spelling of Vietnamese Texts, One Aspect of Computational Linguistics in Vietnam","There are many challenging problems for Vietnamese language processing. It will be a long time before these challenges are met. Even some apparently simple problems such as spelling correction are quite difficult and have not been approached systematically yet. In this paper, we will discuss one aspect of this type of work: designing the so-called Vietools to detect and correct spelling of Vietnamese texts by using a spelling database based on TELEX code. Vietools is also extended to serve many purposes in Vietnamese language processing.",253,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:u-x6o8ySG0sC,,1
2952,Multilinguisation d’un editeur de documents structures. Application a un dictionnaire trilingue,"En informatique, les probl6mes du multilinguisme se posent actuellement avec acuit6. Tant les mat6riels que les logiciels sont h~ t6rog~ nes et incomplets. Certes, il existe un"" documenteur"" multilingue, le Star de Xerox [1]. Mais il s' agit d'une solution ferm6e, sur mat6riel spEcifique. Quant aux outils dits"" multilingues"", comme le texteur WinText [16], ou le SGBD 4D, tous deux sur Macintosh, ou encore les diverses extensions du formateur TEX [5, 6, 7, 15], il s' agit en fait de"" localisations"", qui h6ritent leurs possibilitEs et leurs limites du syst~ me d'exploitation sous-jacent. I1 est encore impossible, avec ces outils, d'6crire et d'utiliser en m~ me temps deux (ou plus de deux) syst6mes d'6criture non"" classiques""(chinois, japonais, arabe, par exemple), en sus des syst~ mes classiques, traitables par simple ajout de polices ad6quates.",253,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:qUcmZB5y_30C,,1
2953,Abbreviations Applicationin 108 VNPT Service Exploitation in Da Nang City,"Looking up and searching for information are always the basic functions of every database (CSDL) management and exploitation application. Information search performance depends not only on resource capacity of systems or searching algorithms but also on time of manipulation and processing on computers of users (NSD). The fact shows that time of typing keywords for information searching of users always take a lot of time, especially for Vietnamese words. It is necessary to give solutions to help users in order to shorten time of keyboard events and enhance operation performance of systems. In this article, we present a solution by developing Abbreviation generating functions (Abbreviation: CVT), reestablishing customer database (CSDL) in the station 108 VNPT Da Nang. The Switchboard 108 VNPT Da Nang has been received practical benefits in information searching for customers owing to practical application of the solution.",253,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:zYLM7Y9cAGgC,,1
2954,Building a Vietnamese-Ede Machine Translation Based on the Bilingual Corpus,"Solving the serious lack of information in ethnic minority languages, the paper proposes building a machine translation system from Vietnamese into ethnic minority language in a restrict context such as bulletins about weather forcast, forest fire warnings, policies and laws of the State, cultivation experience, husbandry experience, natural disaster prevention methods etc. Translation method of system bases on Vietnamese-ethnic minority corpus in which the entries are indicated a domain for ethnic minority language terminology. This method has also contributed to solving the ambiguity of the word boundary, homonym word and polysemous word. Applying this method, a machine translation system from Vietnamese into Ede language has been built for translating the weather bulletins. A Vietnamese-Ede bilingual corpus with Ede language terminology in natural, geographical, hydrometeorology, weather forecast is used in translation system. The corpus has open structure and the major axis is in Vietnamese. The multilingual corpus is easily built by adding other ethnic minority languages such as: Cham, Ka Tu, Jarai, Muong etc.",253,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:L8Ckcad2t8MC,,1
2961,SOLUTIONS TO BUILDING THE VIET-EDE MUTILINGUAL CORPUS WITH THE CONTEXTUAL LABEL,"In the natural language processing (NLP), the multilingual corpus is a necessary resource. The quality of multilingual corpus plays a decisive role in the output quality of the translational system. The translational system will not produce a good output, if the the quality of multilingual corpus in the training process is not good, though the most advanced machine learning methods are applied. Currently, there is no Vietnamese-EDe multilingual corpus using Unicode fonts, which has been officially announced and allows the research community to share and use for research purposes. For this reason, the propose of this paper is to develop a solution to building a Vietnamese-EDe multilingual corpus using the Unicode font which can process the ambiguity and multi-meaning words by labeling each word with the context in the educational field such as education in animal husbandry, cultivation, forest preservation, health care, etc.... for the ethnic minorities (EM) in Vietnam.",253,https://scholar.google.com/citations?view_op=view_citation&hl=en&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:-f6ydRqryjwC,,1
2974,Developing database of Vietnamese abbreviations and some applications,"Abbreviations (CVT) in documents are widely used in various fields and in many languages including Vietnamese. In fact, currently, abbreviations are often repeatedly and unclearly used, demand for abbreviation use is increasing that requests a database of plentiful abbreviations which is saved and used conveniently, is easy to update and consistently exploited. In this article, we propose an opening solution in order to develop a database of Vietnamese abbreviations for many purposes of use during processing language and exploiting database.  ",254,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:W7OEmFMy1HYC,,1
2975,Building a spelling checker for documents in khmer language,"The practice of Khmer language teaching at Tra Vinh University shows that students often deal with many difficulties when making reports on subjects of Khmer language. The need to print, edit Khmer text with MS Word by using conventional typing on the ASCII keyboard and the Unicode fonts available made students have many spelling errors, ranging from syllable level to vocabulary level, the syntax of the sentence. Students have not correctly identified the various syllables in word structures, encountered errors in pronunciation and spellings in Khmer. In fact, Khmer-Vietnamese community in Vietnam has few people, scattered, culturally and economically limited. Moreover, Khmer writing system is complex, from the rich alphabet to the multi-syllable structure of the syllable, with no whitespace separated from the sentences. There are many borrowed words which have changed much from the spoken words in …",254,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:M3ejUd6NZC8C,,1
2976,New Automatic Search and Update Algorithms of Vietnamese Abbreviations.,"Abbreviations in documents are widely used in various fields and in many languages including Vietnamese. In fact, currently, abbreviations are regularly repeated and unclearly used, demand for abbreviation use is increasing, which requests a plentiful source of abbreviations which is conveniently saved and used, easily updated and consistently exploited. In this article, we propose some abbreviation search algorithms on the Internet in order to automatically update into database of Vietnamese abbreviations for many purposes during language processing and database exploitation.",254,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:eQOLeE2rZwMC,,1
2977,Building a machine translation system in a restrict context from Ka-Tu Language into Vietnamese,"The paper introduces some basic characteristics of Ka Tu language, from that we built the machine translation system from Ka Tu language into Vietnamese in a restrict context for translating the weather forecast bulletins, supporting the communication of Ka Tu language of Vietnamese voice emission. The system was tested at the Quang Nam Radio and Television station. The initial Results are Positive for the test of the system. The machine translation system from Ka Tu language into Vietnamese in a restrict context contributed to solving the serious lack of information in aboriginal languages and can be expanded into other areas such as warming fire alarm, broadcasting policies and laws of the State, farming experience, animal husbandry, natural disaster prevetion... Because the corpus with open structure, the optical axis is in Vietnamese language, a multilingual corpus can be easily built by adding other ethnic …",254,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:u5HHmVD_uO8C,,1
2978,"Panel: Good Spelling of Vietnamese Texts, One Aspect of Computational Linguistics in Vietnam","There are many challenging problems for Vietnamese language processing. It will be a long time before these challenges are met. Even some apparently simple problems such as spelling correction are quite difficult and have not been approached systematically yet. In this paper, we will discuss one aspect of this type of work: designing the so-called Vietools to detect and correct spelling of Vietnamese texts by using a spelling database based on TELEX code. Vietools is also extended to serve many purposes in Vietnamese language processing.",254,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:u-x6o8ySG0sC,,1
2979,Multilinguisation d’un editeur de documents structures. Application a un dictionnaire trilingue,"En informatique, les probl6mes du multilinguisme se posent actuellement avec acuit6. Tant les mat6riels que les logiciels sont h~ t6rog~ nes et incomplets. Certes, il existe un"" documenteur"" multilingue, le Star de Xerox [1]. Mais il s' agit d'une solution ferm6e, sur mat6riel spEcifique. Quant aux outils dits"" multilingues"", comme le texteur WinText [16], ou le SGBD 4D, tous deux sur Macintosh, ou encore les diverses extensions du formateur TEX [5, 6, 7, 15], il s' agit en fait de"" localisations"", qui h6ritent leurs possibilitEs et leurs limites du syst~ me d'exploitation sous-jacent. I1 est encore impossible, avec ces outils, d'6crire et d'utiliser en m~ me temps deux (ou plus de deux) syst6mes d'6criture non"" classiques""(chinois, japonais, arabe, par exemple), en sus des syst~ mes classiques, traitables par simple ajout de polices ad6quates.",254,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:qUcmZB5y_30C,,1
2980,Abbreviations Applicationin 108 VNPT Service Exploitation in Da Nang City,"Looking up and searching for information are always the basic functions of every database (CSDL) management and exploitation application. Information search performance depends not only on resource capacity of systems or searching algorithms but also on time of manipulation and processing on computers of users (NSD). The fact shows that time of typing keywords for information searching of users always take a lot of time, especially for Vietnamese words. It is necessary to give solutions to help users in order to shorten time of keyboard events and enhance operation performance of systems. In this article, we present a solution by developing Abbreviation generating functions (Abbreviation: CVT), reestablishing customer database (CSDL) in the station 108 VNPT Da Nang. The Switchboard 108 VNPT Da Nang has been received practical benefits in information searching for customers owing to practical application of the solution.",254,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:zYLM7Y9cAGgC,,1
2981,Building a Vietnamese-Ede Machine Translation Based on the Bilingual Corpus,"Solving the serious lack of information in ethnic minority languages, the paper proposes building a machine translation system from Vietnamese into ethnic minority language in a restrict context such as bulletins about weather forcast, forest fire warnings, policies and laws of the State, cultivation experience, husbandry experience, natural disaster prevention methods etc. Translation method of system bases on Vietnamese-ethnic minority corpus in which the entries are indicated a domain for ethnic minority language terminology. This method has also contributed to solving the ambiguity of the word boundary, homonym word and polysemous word. Applying this method, a machine translation system from Vietnamese into Ede language has been built for translating the weather bulletins. A Vietnamese-Ede bilingual corpus with Ede language terminology in natural, geographical, hydrometeorology, weather forecast is used in translation system. The corpus has open structure and the major axis is in Vietnamese. The multilingual corpus is easily built by adding other ethnic minority languages such as: Cham, Ka Tu, Jarai, Muong etc.",254,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:L8Ckcad2t8MC,,1
2988,SOLUTIONS TO BUILDING THE VIET-EDE MUTILINGUAL CORPUS WITH THE CONTEXTUAL LABEL,"In the natural language processing (NLP), the multilingual corpus is a necessary resource. The quality of multilingual corpus plays a decisive role in the output quality of the translational system. The translational system will not produce a good output, if the the quality of multilingual corpus in the training process is not good, though the most advanced machine learning methods are applied. Currently, there is no Vietnamese-EDe multilingual corpus using Unicode fonts, which has been officially announced and allows the research community to share and use for research purposes. For this reason, the propose of this paper is to develop a solution to building a Vietnamese-EDe multilingual corpus using the Unicode font which can process the ambiguity and multi-meaning words by labeling each word with the context in the educational field such as education in animal husbandry, cultivation, forest preservation, health care, etc.... for the ethnic minorities (EM) in Vietnam.",254,https://scholar.google.com/citations?view_op=view_citation&hl=vi&oe=ASCII&user=N0AmMgMAAAAJ&citation_for_view=N0AmMgMAAAAJ:-f6ydRqryjwC,,1
2993,Solving Reduction Problems in Cover Lattice Based Decision Tables.,"Covering based rough set is an important extension of Pawlak's traditional rough set. Reduction is a typical application of rough sets, including traditional, covering based and other rough set extensions. Although this task has several proposals, it is still an open problem to decision systems (tables) in covering based rough set. This paper focuses on the reduction problem for the condition lattice, and fitting problem for the decision lattice in the decision table based on cover lattice. A corresponding algorithm is proposed for each problem. Two examples to illustrate a covering based decision table and two related problems show the applications of these concepts and problems.",170,https://doi.org/10.1007/978-981-16-1685-3_5,1,
2994,An Adversarial Learning and Canonical Correlation Analysis Based Cross-Modal Retrieval Model.,"The key of cross-modal retrieval approaches is to find a maximally correlated subspace among multiple datasets. This paper introduces a novel Adversarial Learning and Canonical Correlation Analysis based Cross-Modal Retrieval (ALCCA-CMR) model. For each modality, the ALCCA phase finds an effective common subspace and calculates the similarity by canonical correlation analysis embedding for cross-modal retrieval. We demonstrate an application of ALCCA-CMR model implemented for the dataset of two modalities. Experimental results on real music data show the efficacy of the proposed method in comparison with other existing ones.",170,https://doi.org/10.1007/978-3-030-14799-0_13,1,
2995,Node-aware convolution in Graph Neural Networks for Predicting molecular properties.,"Molecular property prediction is a challenging task which aims to solve various issues of science namely drug discovery, materials discovery. It focuses on understanding the structure-property relationship between atoms in a molecule. Previous approaches have to face difficulties dealing with the various structure of the molecule as well as heavy computational time. Our model, in particular, utilizes the idea of message passing neural network and Schnet on the molecular graph with enhancement by adding the Node-aware Convolution and Edge Update layer in order to acquire the local information of the graph and to propagate interaction between atoms. Through experiments, our model has been shown the outperformance with previous deep learning methods in predicting quantum mechanical, calculated molecular properties in the QM9 dataset and magnetic interaction of two atoms in molecules approaches.",176,https://doi.org/10.1109/KSE50997.2020.9287744,1,
2996,ViWOZ: A Multi-Domain Task-Oriented Dialogue Systems Dataset For Low-resource Language.,"Most of the current task-oriented dialogue systems (ToD), despite having interesting results, are designed for a handful of languages like Chinese and English. Therefore, their performance in low-resource languages is still a significant problem due to the absence of a standard dataset and evaluation policy. To address this problem, we proposed ViWOZ, a fully-annotated Vietnamese task-oriented dialogue dataset. ViWOZ is the first multi-turn, multi-domain tasked oriented dataset in Vietnamese, a low-resource language. The dataset consists of a total of 5,000 dialogues, including 60,946 fully annotated utterances. Furthermore, we provide a comprehensive benchmark of both modular and end-to-end models in low-resource language scenarios. With those characteristics, the ViWOZ dataset enables future studies on creating a multilingual task-oriented dialogue system.",180,https://doi.org/10.48550/arXiv.2203.07742,1,
2997,Fully Automated Machine Learning Pipeline for Echocardiogram Segmentation.,"Nowadays, cardiac diagnosis largely depends on left ventricular function assessment. With the help of the segmentation deep learning model, the assessment of the left ventricle becomes more accessible and accurate. However, deep learning technique still faces two main obstacles: the difficulty in acquiring sufficient training data and time consuming in developing quality models. In the ordinary data acquisition process, the dataset was selected randomly from a large pool of unlabeled images for labeling, leading to massive labor time to annotate those images. Besides that, hand-designed model development is strenuous and also costly. This paper introduces a pipeline that relies on Active Learning to ease the labeling work and utilizes Neural Architecture Search's idea to design the adequate deep learning model automatically. We called this Fully automated machine learning pipeline for echocardiogram segmentation. The experiment results show that our method obtained the same IOU accuracy with only two-fifths of the original training dataset, and the searched model got the same accuracy as the hand-designed model given the same training dataset.",180,https://doi.org/10.1109/KSE53942.2021.9648607,1,
2998,UET-Headpose: A sensor-based top-view head pose dataset.,"Head pose estimation is a challenging task that aims to solve problems related to predicting three dimensions vector, that serves for many applications in human-robot interaction or customer behavior. Previous researches have proposed some precise methods for collecting head pose data. But those methods require either expensive devices like depth cameras or complex laboratory environment setup. In this research, we introduce a new approach with efficient cost and easy setup to collecting head pose images, namely UET-Headpose dataset, with top-view head pose data. This method uses an absolute orientation sensor instead of Depth cameras to be set up quickly and small cost but still ensure good results. Through experiments, our dataset has been shown the difference between its distribution and available dataset like CMU Panoptic Dataset [6]. Besides using the UET-Headpose dataset and other head pose datasets, we also introduce the full-range model called FSANet-Wide, which significantly outperforms head pose estimation results by the UET-Headpose dataset, especially on top-view images. Also, this model is very lightweight and takes small size images.",184,https://doi.org/10.1109/KSE53942.2021.9648656,1,
2999,Simultaneous face detection and 360 degree head pose estimation.,"With many practical applications in human life, including manufacturing surveillance cameras, analyzing and processing customer behavior, many researchers are noticing face detection and head pose estimation on digital images. A large number of proposed deep learning models have state-of-the-art accuracy such as YOLO, SSD, MTCNN, solving the problem of face detection or HopeNet, FSA-Net, RankPose model used for head pose estimation problem. According to many state-of-the-art methods, the pipeline of this task consists of two parts, from face detection to head pose estimation. These two steps are completely independent and do not share information. This makes the model clear in setup but does not leverage most of the featured resources extracted in each model. In this paper, we proposed the Multitask-Net model with the motivation to leverage the features extracted from the face detection model, sharing them with the head pose estimation branch to improve accuracy. Also, with the variety of data, the Euler angle domain representing the face is large, our model can predict with results in the 360° Euler angle domain. Applying the multitask learning method, the Multitask-Net model can simultaneously predict the position and direction of the human head. To increase the ability to predict the head direction of the model, we change the representation of the human face from the Euler angle to vectors of the Rotation matrix.",184,https://doi.org/10.1109/KSE53942.2021.9648838,1,
3000,Prosodic Boundary Prediction Model for Vietnamese Text-To-Speech.,"This research aims to build a prosodic boundary prediction model for improving the naturalness of Vietnamese speech synthesis. This model can be used directly to predict prosodic boundaries in the synthesis phase of the statistical parametric or end-to-end speech systems. Beside conventional features related to Part-Of-Speech (POS), this paper proposes two efficient features to predict prosodic boundaries: syntactic blocks and syntactic links, based on a thorough analysis of a Vietnamese dataset. Syntactic blocks are syntactic phrases whose sizes are bounded in their constituent syntactic tree. A syntactic link of two adjacent words is calculated based on the distance between them in the syntax tree. The experimental results show that the two proposed predictors improve the quality of the boundary prediction model using a decision tree classification algorithm, about 36.4% (F1 score) higher than the model with only POS features. The final boundary prediction model with POS, syntactic block, and syntactic link features using the LightGBM algorithm gives the best F1-score results at 87.0% in test data. The proposed model helps the TTS systems, developed by either HMM-based, DNN-based, or End-to-end speech synthesis techniques, improve about 0.3 MOS points (i.e. 6 to 10%) compared to the ones without the proposed model.",190,https://doi.org/10.21437/Interspeech.2021-125,1,
3001,A Hypercuboid-Based Machine Learning Algorithm for Malware Classification.,"Malware attacks have been among the most serious threats to cyber security in the last decade. Antimalware software can help safeguard information systems and minimize their exposure to the malware. Most of anti-malware programs detect malware instances based on signature or pattern matching. Data mining and machine learning techniques can be used to automatically detect models and patterns behind different types of malware variants. However, traditional machine-based learning techniques such as SVM, decision trees and naive Bayes seem to be only suitable for detecting malicious code, not effective enough for complex problems such as classification. In this article, we propose a new prototype extraction method for non-traditional prototype-based machine learning classification. The prototypes are extracted using hypercuboids. Each hypercuboid covers all training data points of a malware family. Then we choose the data points nearest to the hyperplanes as the prototypes. Malware samples will be classified based on the distances to the prototypes. Experiments results show that our proposition leads to F1 score of 96.5% for classification of known malware and 97.7% for classification of unknown malware, both better than the original prototype-based classification method.",190,https://doi.org/10.1109/RIVF51545.2021.9642093,1,
3002,"Comparison of Machine Learning Methods for Estimating Mangrove Above-Ground Biomass Using Multiple Source Remote Sensing Data in the Red River Delta Biosphere Reserve, Vietnam.","This study proposes a hybrid intelligence approach based on an extreme gradient boosting regression and genetic algorithm, namely, the XGBR-GA model, incorporating Sentinel-2, Sentinel-1, and ALOS-2 PALSAR-2 data to estimate the mangrove above-ground biomass (AGB), including small and shrub mangrove patches in the Red River Delta biosphere reserve across the northern coast of Vietnam. We used the novel extreme gradient boosting decision tree (XGBR) technique together with genetic algorithm (GA) optimization for feature selection to construct and verify a mangrove AGB model using data from a field survey of 105 sampling plots conducted in November and December of 2018 and incorporated the dual polarimetric (HH and HV) data of the ALOS-2 PALSAR-2 L-band and the Sentinel-2 multispectral data combined with Sentinel-1 (C-band VV and VH) data. We employed the root-mean-square error (RMSE) and coefficient of determination (R2) to evaluate the performance of the proposed model. The capability of the XGBR-GA model was assessed via a comparison with other machine-learning (ML) techniques, i.e., the CatBoost regression (CBR), gradient boosted regression tree (GBRT), support vector regression (SVR), and random forest regression (RFR) models. The XGBR-GA model yielded a promising result (R2 = 0.683, RMSE = 25.08 Mg·ha−1) and outperformed the four other ML models. The XGBR-GA model retrieved a mangrove AGB ranging from 17 Mg·ha−1 to 142 Mg·ha−1 (with an average of 72.47 Mg·ha−1). Therefore, multisource optical and synthetic aperture radar (SAR) combined with the XGBR-GA model can be used to estimate the mangrove AGB in North Vietnam. The effectiveness of the proposed method needs to be further tested and compared to other mangrove ecosystems in the tropics.",190,https://doi.org/10.3390/rs12081334,1,
3005,Intonation issues in HMM-based speech synthesis for Vietnamese.,"In an HMM-based Text-To-Speech system, contextual features, including phonetic and prosodic factors have a significant influence to the spectrum, F0 and duration of the synthetic voice. This paper proposes prosodic features aiming at improving the naturalness of an HMM-based TTS system (VTed) for a tonal language, Vietnamese. The ToBI (Tones and Break Indices) features are used to learn two crucial prosodic cues i.e. intonation (boundary tones) and pause (break indices), concurrently with another set of features. The result of MOS test showed that the general quality of synthetic voice is rather good, 1.21 point lower than the natural voice. About 55% of the voice trained with ToBI boundary tone feature are perceived as similar to the voice trained without this feature, while a 10% difference in favour of the voice trained without this ToBI feature is observed. This may be linked with F0 contour lowering or raising regardless of lexical tones. This brought two main problems in the synthetic voice: discontinuity in spectrum and F0 or unexpected voice quality. This paper then concluded the need of much more work on intonation modeling that should take into account the Vietnamese tones. A new prosody model can be designed, which may consider the ToBI model, with respect to lexical tones and the syntactic structure of Vietnamese.",190,http://www.isca-speech.org/archive/sltu_2014/nguyen14_sltu.html,1,
3007,Responsive student model in an intelligent tutoring system and its evaluation.,"In this paper, we introduce a new student responsive model to support students who use an Intelligent Tutoring System (ITS) as an E-Learning tool. We proposed a weighted-based model to estimate and suggest learning materials for students who are pursuing a computer-based course. We have built a brand new ITS called WinITS with our proposed responsive student model and deployed it in Hanoi National University of Education-Vietnam (HNUE) in the second semester of the school year 2019-2020 with a computer science course. To compare the effectiveness of applying ITS to the students, we compare test results and analyze some other aspects related to the course. On the other hand, we conducted a survey between two groups: with and without using WinITS. 63 students are volunteers who participated in the case study. Before learning, 43 students from Group 1 will take a short survey of the Felder-Silverman questionnaire to identify learning styles, after that, they go through all the lessons from the course under the support of WinITS, the lessons will be chosen to satisfy student’s need. On another side, 18 students from Group 2 will make the same test to compare the result to Group 1. In the range of research, we illustrate that our implementation shows some encouraging results such as reducing learning time, improving test score by 1.13 standard deviations, and making the lesson more interesting and flexible. The results have revealed some advantages of studying with computer-added compared to the traditional class in various ways and showed the effectiveness of the proposed model in Intelligent Tutoring Systems.",193,https://doi.org/10.1007/s10639-021-10485-4,1,
3008,A deep wavelet sparse autoencoder method for online and automatic electrooculographical artifact removal.,"Electrooculographical (EOG) artifacts are problematic to electroencephalographical (EEG) signal analysis and degrade performance of brain–computer interfaces. A novel, robust deep wavelet sparse autoencoder (DWSAE) method is presented and validated for fully automated EOG artifact removal. DWSAE takes advantage of wavelet transform and sparse autoencoder to become a universal EOG artifact corrector. After being trained without supervision, the sparse autoencoder performs EOG correction on time–frequency coefficients collected after brain wave signal wavelet decomposition. Corrected coefficients are then used for wavelet reconstruction of uncontaminated EEG signals. DWSAE is compared with five other methods: second-order blind identification, information maximization, joint approximation diagonalization of eigen-matrices, wavelet neural network (WNN) and wavelet thresholding (WT). Experimental results on a visual attention task dataset, a mental state recognition dataset and a semi-simulated contaminated EEG dataset show that DWSAE is capable of suppressing EOG artifacts effectively, while preserving the nature of background EEG signals. The mean square error of signals before and after correction by DWSAE on a semi-simulated contaminated EEG segment of 30 s is the lowest (65.62) when compared to the results produced by WNN and WT. DWSAE addresses limitations posed by these methods in three ways. First, DWSAE can be performed automatically and online in a single channel of EEG data; this has advantages over independent component analysis-based methods. Second, its results are robust and stable in comparison with those of other wavelet-based methods. Third, as an unsupervised learning scheme, DWSAE does not require the off-line training that is necessary for WNN and other supervised learning machine learning-based methods.",193,https://doi.org/10.1007/s00521-020-04953-0,1,
3009,A Fast Template-based Approach to Automatically Identify Primary Text Content of a Web Page.,"Search engines have become an indispensable tool for browsing information on the Internet. The user, however, is often annoyed by redundant results from irrelevant Web pages. One reason is because search engines also look at non-informative blocks of Web pages such as advertisement, navigation links, etc. In this paper, we propose a fast algorithm called FastContentExtractor to automatically detect main content blocks in a Web page by improving the ContentExtractor algorithm. By automatically identifying and storing templates representing the structure of content blocks in a website, content blocks of a new Web page from the Website can be extracted quickly. The hierarchical order of the output blocks is also maintained which guarantees that the extracted content blocks are in the same order as the original ones.",193,http://arxiv.org/abs/1911.11473,1,
3010,Annotating Movement Phrases in Vietnamese Folk Dance Videos.,"This paper aims at the annotation of movement phrases in Vietnamese folk dance videos that were mainly gathered, stored and used in teaching at art schools and in preserving cultural intangible heritages (performed by different famous folk dance masters). We propose a framework of automatic movement phrase annotation, in which the motion vectors are used as movement phrase features. Movement phrase classification can be carried out, based on dancer’s trajectories. A deep investigation of Vietnamese folk dance gives an idea of using optical flow as movement phrase features in movement phrase detection and classification. For the richness and usefulness in annotation of Vietnamese folk dance, a lookup table of movement phrase descriptions is defined. In initial experiments, a sample movement phrase dataset is built up to train k-NN classification model. Experiments have shown the effectiveness of the proposed framework of automatic movement phrase annotation with classification accuracy at least 88%.",193,https://doi.org/10.1007/978-3-319-60045-1_1,1,
3011,Predicting students' performance based on learning style by using artificial neural networks.,"In recent years, many education researchers concentrated on learning styles and its implications. They relied that people who have different personality types tend to have different learning styles, which in turn influence to students' performance in each type of subjects. To measure the relationship between learning styles and student performance in a subject or entire course, we conducted an online survey with a participation of students in various courses to analysis and show the effects of different learning styles on students' performance. This test based on the Felder-Soloman questionnaire with 44 test questions divided into four dimensions. We have filtered data to select 316 undergraduate students for the analysis. According questionnaire results, we have applied statistical methods to analyze data and issued some important notes. Besides these, we have built an artificial neutral network to predict academic performance based on students' learning style. We have shown some significant relationships between learning style and performance of the testers, which have implications for the studying of a course.",193,https://doi.org/10.1109/KSE.2017.8119433,1,
3012,Speech classification using SIFT features on spectrogram images.,"Classification of speech is one of the most vital problems in speech processing. Although there have been many studies on the classification of speech, the results are still limited. Firstly, most of the speech classification approaches requiring input data have the same dimension. Secondly, all traditional methods must be trained before classifying speech signal and must be retrained when having more training data or new class. In this paper, we propose an approach for speech classification using Scale-invariant Feature Transform (SIFT) features on spectrogram images of speech signal combination with Local naïve Bayes nearest neighbor. The proposed approach allows using feature vectors to have different sizes. With this approach, the achieved classification results are satisfactory. They are 73, 96, 95, 97 %, and 97 % on the ISOLET, English Isolated Digits, Vietnamese Places, Vietnamese Digits, JVPD databases, respectively. Especially, in a subset of the TMW database, the accuracy is 100 %. In addition, in our proposed approach, non-retraining is needed for additional training data after the training phase. The experiment shows that the more features are added to the model, the more is the accuracy in performance.",193,https://doi.org/10.1007/s40595-016-0071-3,1,
3013,Learning relationship between speech and image.,"Currently, studies on learning relationship between objects focus on the text domain. There are a few researchers who focus on relationship learning between objects in other domains. In these researches, they have tried to represent the qualitative description of structure of objects, and the symbolic relationship between them. This output provides symbolic meaning to the inter-object relationships which are useful for subsequent common sense reasoning and decision making. In this paper, we propose an approach to learn the relationship between a speech signal and an image. In other words, this approach can classify objects based on speech input.",193,https://doi.org/10.1109/KSE.2016.7758037,1,
3014,Classifying human body postures by a two-neuron fuzzy neural network.,"Human behaviour analysis is an important step of developing a surveillance system. Classifying postures is a attractive topic of human behaviour analysis. Many classifiers and features extracting from human body are developed. However, complex computing is a difficult for implementing models and understanding extracted features. In this paper, we proposed a fuzzy neural network including two neurons to implement easily. Moreover, two features extracted by counting pixels of human body's silhouette are presented. Experiments classify four postures including standing, lying, sitting, and bending. To prove the effectiveness, our model is compared to competing models. Results show the proposed model is better than compared models and improve significantly the accuracy of classifying.",193,https://doi.org/10.1109/RIVF.2016.7800284,1,
3015,MapReduce based for speech classification.,"Speech classification is one of the most vital problems in speech processing as well as spoken word recognition. Although, there have been many studies on the classification of speech signals, the results are still limited on both accuracy and the size of the vocabulary. When classifying a huge volumes vocabulary, the speech classification becomes more and more difficult. Today, there are some frameworks that allow working with big data. One of these is a data mining utility. It can perform supervised classification procedures on very large amounts of data, usually named as big data, on a distributed infrastructure by using the MapReduce framework of Hadoop clusters. This tool has four classification approaches implemented. These are Random Forest, Naïve Bayes, Decision Trees and Support Vector Machines (SVM). All these approaches require input data having the same size, so the input data must be quantized before using. This leads to decrease the accuracy in the classification stage. In this paper, we propose an implementation of Local Naïve Bayes Nearest Neighbor based on Hadoop framework, which allows input data with different sizes and works well with huge training data.",193,https://doi.org/10.1145/3011077.3011090,1,
3016,Improving learning rule for fuzzy associative memory with combination of content and association.,"FAM is an associative memory that uses operators of fuzzy logic and mathematical morphology (MM). FAMs possess important advantages including noise tolerance, unlimited storage, and one pass convergence. An important property, deciding FAM performance, is the ability to capture content of each pattern, and association of patterns. Existing FAMs capture either content or association of patterns well, but not both of them. They are designed to handle either erosive or dilative noise in distorted inputs but not both. Therefore, they cannot recall distorted input patterns very well when both erosive and dilative noises are present. In this paper, we propose a new FAM called content-association associative memory (ACAM) that stores both content and association of patterns. The weight matrix is formed with the weighted sum of output pattern and the difference between input and output patterns. Our ACAM can handle inputs with both erosive and dilative noises better than existing models.",193,https://doi.org/10.1016/j.neucom.2014.01.063,1,
3017,Building 3D event logs for video investigation.,"In scene investigation, creating a video log captured using a handheld camera is more convenient and more complete than taking photos and notes. By introducing video analysis and computer vision techniques, it is possible to build a spatio-temporal representation of the investigation. Such a representation gives a better overview than a set of photos and makes an investigation more accessible. We develop such methods and present an interface for navigating the result. The processing includes (i) segmenting a log into events using novel structure and motion features making the log easier to access in the time dimension, and (ii) mapping video frames to a 3D model of the scene so the log can be navigated in space. Our results show that, using our proposed features, we can recognize more than 70 percent of all frames correctly, and more importantly find all the events. From there we provide a method to semi-interactively map those events to a 3D model of the scene. With this we can map more than 80 percent of the events. The result is a 3D event log that captures the investigation and supports applications such as revisiting the scene, examining the investigation itself, or hypothesis testing.",193,https://doi.org/10.1007/s11042-013-1826-9,1,
3018,A Vietnamese 3D taking face for embodied conversational agents.,"Conversational agents are receiving significant attention from multi-agent and human computer interaction research societies. Many techniques have been developed to enable these agents to behave in a human-like manner. In order to do so, they are simulated with similar communicative channels as humans. Moreover, they are also simulated with emotion and personality. In this work, we focus on issue of expressing emotions for embodied-agents. We present a three dimensional face with ability to speak emotional Vietnamese speech and naturally express emotions while talking. Our face can represent lip movements during emotionally pronouncing Vietnamese words, and at the same time it can show emotional facial expressions while speaking. The face's architecture consists of three parts: Vietnamese Emotional Speech Synthesis module, Emotions to Facial Expressions module, and Combination module which creates lip movements when pronouncing Vietnamese emotional speech and combines these movements with emotional facial expressions. We have tested the face in the football supporter domain in order to confirm its naturalness. The face is simulated as the face of a football supporter agent which experiences emotions and expresses emotional expressions in his voice as well as on his face.",193,https://doi.org/10.1109/RIVF.2015.7049881,1,
3019,A technique for extracting behavioral sequence patterns from GPS recorded data.,"The mobile wireless market has been attracting many customers. Technically, the paradigm of anytime-anywhere connectivity raises previously unthinkable challenges, including the management of million of mobile customers, their profiles, the profiles-based selective information dissemination, and server-side computing infrastructure design issues to support such a large pool of users automatically and intelligently. In this paper, we propose a data mining technique for discovering frequent behavioral patterns from a collection of trajectories gathered by Global Positioning System. Although the search space for spatiotemporal knowledge is extremely challenging, imposing spatial and temporal constraints on spatiotemporal sequences makes the computation feasible. Specifically, the mined patterns are incorporated with synthetic constraints, namely spatiotemporal sequence length restriction, minimum and maximum timing gap between events, time window of occurrence of the whole pattern, inclusion or exclusion event constraints, and frequent movement patterns predictive of one ore more classes. The algorithm for mining all frequent constrained patterns is named cAllMOP. Moreover, to control the density of pattern regions a clustering algorithm is exploited. The proposed method is efficient and scalable. Its efficiency is better than that of the previous algorithms AllMOP and GSP with respect to the compactness of discovered knowledge, execution time, and memory requirement.",193,https://doi.org/10.1007/s00607-013-0333-1,1,
3021,Toward a Rule-Based Synthesis of Vietnamese Emotional Speech.,"This paper presents a framework used to simulate four basic emotional styles of Vietnamese speech, by means of acoustic feature transplantation techniques applied to neutral utterances. First, it describes some analyses of acoustic features of Vietnamese emotional speech, accomplished to find the relations between prosodic, voice quality variations and emotional states in Vietnamese speech. Then the target pitch profiles together with duration, energy and spectrum constraints were obtained by applying rules which were inferred from the analysis results and based on the idea that when some emotional speech is synthesized from neutral speech, acoustic features are modified more in some syllables, instead of uniformly modified in all syllables. From there, neutral speech were morphed to produced synthesized speech with emotions. Results of perceptual tests show that emotional styles were well recognized.",193,https://doi.org/10.1007/978-3-319-11680-8_11,1,
3022,Emotional Facial Expression Analysis in the Time Domain.,"Emotions have been studied for a long time and results show that they play an important role in human cognitive functions. In fact, emotions play an extremely important role during the communication between people. And the human face is the most communicative part of the body for expressing emotions; it is recognized that a link exists between facial activity and emotional states. In order to make computer applications more believable and friendly, giving them the ability to recognize and/or express emotions are research fields which have been much focused on. Being able to perform these tasks, firstly, we need to have knowledge about the relationship between emotion and facial activity. Up to now, there have been proposed researches on this relationship. However, almost all these researches focused on analyzing the relationship without taking into account time factors. They analyzed the relationship but did not examined it in the time domain. In this paper, we propose a work on analyzing the relationship between emotions and facial activity in the time domain. Our goal is finding the temporal patterns of facial activity of six basic emotions (happy, sad, angry, fear, surprise, disgust). To perform this task, we analyzed a spontaneous video database in order to consider how facial activities which are related to the six basic emotions happen temporally. From there, we bring out the general temporal patterns for facial expressions of each of the six emotions.",193,https://doi.org/10.1007/978-3-319-11680-8_39,1,
3023,Improving Simulation of Continuous Emotional Facial Expressions by Analyzing Videos of Human Facial Activities.,"Conversational agents are receiving significant attention from multi-agent and human computer interaction research societies. In order to make conversational agents more believable and friendly, giving them the ability to express emotions is one of research fields which have drawn a lot of attention lately. In this paper, we propose a work on analysis of how emotional facial activities happen temporally. Our goal is to find the temporal patterns of facial activity of six basic emotions in order to improve the simulation of continuous emotional facial expressions on a 3D face of an embodied agent. Using facial expression recognition techniques, we first analyze a spontaneous video database in order to consider how facial activities are related to six basic emotions temporally. From there, we bring out the general temporal patterns for facial expressions of the six basic emotions. Then, based on the temporal patterns, we propose a scheme for displaying continuous emotional states of a conversational agent on a 3D face.",193,https://doi.org/10.1007/978-3-319-13191-7_18,1,
3024,Efficiency Improvements for Fuzzy Associative Memory.,"FAM is an Associative Memory that uses operators of Fuzzy Logic and Mathematical Morphology (MM). FAMs possess important advantages including noise tolerance, unlimited storage, and one pass convergence. An important property, deciding FAM performance, is the ability to capture contents of each pattern, and associations of patterns. Standard FAMs capture either contents or associations of patterns well, but not both of them. In this paper, we propose a novel FAM that effectively stores both contents and associations of patterns. We improve both learning and recalling processes of FAM. In learning process, the associations and contents are stored by mean of input and output patterns and they are generalised by erosion operator. In recalling process, a new threshold is added to output function to improve outputs. Experiments show that noise tolerance of the proposed FAM is better than standard FAMs with different types of noise.",193,https://doi.org/10.1007/978-3-642-39065-4_5,1,
3025,A max-min learning rule for Fuzzy ART.,"Fuzzy Adaptive Resonance Theory (Fuzzy ART) is an unsupervised neural network, which clusters data effectively based on learning from training data. In the learning process, Fuzzy ARTs update the weight vector of the wining category based on the current input pattern from training data. Fuzzy ARTs, however, only learn from patterns whose values are smaller than values of stored patterns. In this paper, we propose a max-min learning rule of Fuzzy ART that learns all patterns of training data and reduces effect of abnormal training patterns. Our learning rule changes the weight vector of the wining category based on the minimal difference between the current input pattern and the old weight vector of the wining category. We have also conducted experiments on seven benchmark datasets to prove the effectiveness of the proposed learning rule. Experiment results show that clustering results of Fuzzy ART with our learning rule (Max-min Fuzzy ART) is significantly higher than that of other models in complex datasets.",193,https://doi.org/10.1109/RIVF.2013.6719866,1,
3026,Shift Error Analysis in Image Based 3D Skull Feature Reconstruction.,"3D skull is crucial in skull-based 3D facial reconstruction [1, 2, 3, 4, 5, 6, 7, 8]. In 3D reconstruction, especially in skull-based 3D facial reconstruction, features usually play an important role. Because, the accuracy in feature detection strongly affects the accuracy of the 3D final model. In this paper, we concentrate on accuracy of 3D reconstructed skull, one important part in skull-based 3D facial reconstruction. We discuss a cause of errors called shift errors when taking sequence of skull images. In addition, we analysis the effect of shift error in 3D reconstruction and propose solution to limit the effect.",193,https://doi.org/10.1109/KSE.2012.38,1,
3027,A Study on Prosody of Vietnamese Emotional Speech.,"This paper describes the analyses of the prosody of Vietnamese emotional speech, accomplished to find the relations between prosodic variations and emotional states in Vietnamese speech. These relations were obtained by investigating the variations of prosodic features in Vietnamese emotional speech in comparison with prosodic features of neutral speech. The analyses were performed on a multi-style emotional speech database which consisted of Vietnamese sentences uttered in different styles. Specifically, four emotional styles were considered: happiness, sadness, cold anger, and hot anger. Speech data in the neutral style were also collected, and prosodic differences of each style with respect to this neutral baseline were quantified. The acoustic features related to prosody which were investigated were fundamental frequency, power, and duration. According to the analysis results, for each speaker of the database, a set of prosodic variation coefficients was produced for each emotional style. This will help for bringing emotions into Vietnamese synthesized speech, making them more natural.",193,https://doi.org/10.1109/KSE.2012.19,1,
3028,A New Learning Strategy of General BAMs.,"Bi-directional Associative Memory (BAM) is an artificial neural network that consists of two Hopfield networks. The most important advantage of BAM is the ability to recall a stored pattern from a noisy input, which depends on learning process. Between two learning types of iterative learning and non-iterative learning, the former allows better noise tolerance than the latter. However, interactive learning BAMs take longer to learn. In this paper, we propose a new learning strategy that assures our BAM converges in all states, which means that our BAM recalls perfectly all learning pairs. Moreover, our BAM learns faster, more flexibility and tolerates noise better. In order to prove the effectiveness of the model, we have compared our model to existing ones by theory and by experiments.",193,https://doi.org/10.1007/978-3-642-31537-4_17,1,
3029,A New Effective Learning Rule of Fuzzy ART.,"Unsupervised neural networks are known for their ability to cluster inputs into categories based on the similarity among inputs. Fuzzy Adaptive Resonance Theory (Fuzzy ART) is a kind of unsupervised neural networks that learns training data until satisfying a given need. In the learning process, weights of categories are changed to adapt to noisy inputs. In other words, learning process decides the quality of clustering. Thus, updating weights of categories is an important step of learning process. We propose a new effective learning rule for Fuzzy ART to improve clustering. Our learning rule modifies weights of categories based on the ratio of the input to the weight of chosen category and a learning rate. The learning rate presents the speed of increasing/decreasing the weight of chosen category. It is changed by the following rule: the number of inputs is larger, value is smaller. We have conducted experiments on ten typical data sets to prove the effectiveness of our novel model. Result from experiments shows that our novel model clusters better than existing models, including Original Fuzzy ART, Complement Fuzzy ART, K-mean algorithm, Euclidean ART.",193,https://doi.org/10.1109/TAAI.2012.60,1,
3030,A semi-interactive panorama based 3D reconstruction framework for indoor scenes.,"We present a semi-interactive method for 3D reconstruction specialized for indoor scenes which combines computer vision techniques with efficient interaction. We use panoramas, popularly used for visualization of indoor scenes, but clearly not able to show depth, for their great field of view, as the starting point. Exploiting user defined knowledge, in term of a rough sketch of orthogonality and parallelism in scenes, we design smart interaction techniques to semi-automatically reconstruct a scene from coarse to fine level. The framework is flexible and efficient. Users can build a coarse walls-and-floor textured model in five mouse clicks, or a detailed model showing all furniture in a couple of minutes interaction. We show results of reconstruction on four different scenes. The accuracy of the reconstructed models is quite high, around 1% error at full room scale. Thus, our framework is a good choice for applications requiring accuracy as well as application requiring a 3D impression of the scene.",193,https://doi.org/10.1016/j.cviu.2011.07.001,1,
3031,Facial Soft Tissue Thicknesses Prediction Using Anthropometric Distances.,"Predicting the face of an unidentified individual from its skeletal remains is a difficult matter. Obviously, if the soft tissue thicknesses at every location at the skull are known, we can easily rebuild the face from the skull model. Thus, the problem turns out to be predicting the soft tissue thicknesses for any given skull. With the rapid development of the computer, different techniques are being used in the community for prediction tasks and in recent years the concept of neural networks has emerged as one of them. The principal strength of the neural network is its ability to find patterns and irregularities as well as detecting multi-dimensional non-linear connections in data. In this paper, we propose a method of applying neural networks to predict the soft tissue thicknesses for facial reconstruction. We use the distances between anthropometric locations at the skull as input, and the soft tissue thicknesses as output, as this format is suitable for many machine learning mechanisms. These data is collected and measured from candidates using the Computed Tomography (CT) technique",193,https://doi.org/10.1007/978-3-642-19953-0_12,1,
3032,Features Detection on Industrial 3D CT Data.,"Features are significantly used as design elements to reconstruct a model in reverse engineering. This paper proposes a new method for detecting corner features and edge features in 3D from CT scanned data. Firstly, the level set method is applied on CT scanned data to segment the data in the form of implicit function having two values, which mean inside and outside of the boundary of the shape. Next, corners and sharp edges are detected and extracted from the boundary of the shape. The corners are detected based on Sobel-like mask convolution processing with a marching cube. The sharp edges are detected based on Canny-like mask convolution. In this step, a noisy removal module is included. In the paper, the result of detecting both features is presented.",193,https://doi.org/10.1007/978-3-642-27186-1_48,1,
3033,Motion Vector Refinement Algorithm for the High Definition Video Coding.,"In H.264/AVC, simplified rate distortion optimisation (RDO) is performed to estimate the motion vector (MV). As a result, the estimated MV is often suboptimal since the residual bitrate is not considered, which results in the deterioration of the coding efficiency. In this paper, we propose an MV refinement algorithm in which the final MV is selected from a set of candidate MVs by using the RDO including the residual bitrate. To further improve the performance of optimal MV selection, the RD calculation method in the RDO is modified using the deblocking filters. Experimental results show that the proposed algorithm outperforms the conventional H.264 in terms of coding efficiency",193,https://doi.org/10.1007/978-3-642-26010-0_43,1,
3034,3D Facial Reconstruction System from Skull for Vietnamese.,"Reconstructing the 3D facial model of an unidentified individual from his skull contributes considerable benefits to terms of archaeology, anthropology and forensic investigation but it is still significantly complicated matter. Computer aided system of 3D facial reconstruction based on skull has a great advantage of reduction in time consumption. Nevertheless, existing results either contain graphical artifacts or reflect incorrectly differences among reconstructed faces. In this paper, we propose a Vietnamese 3D facial reconstruction system from skull that can overcome existing problems. Machine learning is applied to CT image database of living Vietnamese to learn the relationship between skull measurements and soft tissue thicknesses. Then, the soft tissue thicknesses for each individual are calculated from his/her skull measurements. Moreover, instead of using a fixed number of dowels, extra dowels are added to deform a 3D template face, which ensures the smoothness of the 3D reconstructed face as well as represents best the characteristics of the face to be reconstructed.",193,https://doi.org/10.1109/KSE.2011.26,1,
3035,Improvement of Single-Trial EEG Classifier Accuracy Based on Combination of Optimal Spatial Filters and Time-Domain Features.,"Common spatial pattern (CSP) is a popular technique in feature extraction for brain-computer interface (BCI). However, CSP algorithm itself does not perform well since the estimation of covariance matrices is quite sensitive to training data. This causes over fitting in some cases, especially when the training set is small. Moreover, this method may result in poor outcomes because it just computes features on spatial domain but omit those on other domains. In this paper, we propose a simple yet effective approach. Through improving the CSP algorithm, optimal spatial filters with highest discriminative ability will be extracted. Concurrently, we also incorporate some time-domain information into feature vectors to make the signal presentation become more sufficient. Experiment results show that this is a promising method for an electroencephalography (EEG) - based brain-computer interface system.",193,https://doi.org/10.1109/KSE.2011.47,1,
3036,Blur Estimation for Barcode Recognition in Out-of-Focus Images.,"Recently, with the popularity of hand-held devices with camera, many applications requiring barcode identification in images have appeared. There are several problems with these applications because of the limitation of camera capability. Among them, heavily blur problems are the most significant and popular. In this paper, we propose a novel approach to estimate blur for recognising barcodes in heavily blurred images. With this approach, we are able to extract correctly the barcode from heavily blurred images, which existing methods cannot.",193,https://doi.org/10.1007/978-3-642-21786-9_21,1,
3037,Multi-agent Based Simulation of Traffic in Vietnam.,"There is always need for a good simulation for traffic in Vietnam in order to help transportation planners to improve the current traffic system. Over recent days, there has been severe traffic congestion in many streets of big cities in Vietnam such as Hanoi and HoChiMinh city. There is an urgent need for measures to deal with increasing congestions. The simulation of traffic in Vietnam is a hard problem due to two main reasons including: (1) the traffic participants in Vietnam do not give way according to the rule; (2) the participants do not consider that when waiting for the vehicles in front, stopping in the intersection is obstructing the traffic flow. In this paper, we propose a multi-agent based simulation system for traffic in Vietnam to help transportation planners to find treatments to the problem of congestion of the traffic system in Vietnam as well as to test new designs before committing resources to actually building the transportation infrastructure. By allowing a user to design different road systems as well as to create different simulation scenarios with different agent profiles, our system can simulate the dynamic of traffic in Vietnam in different situations.",193,https://doi.org/10.1007/978-3-642-25920-3_46,1,
3038,Supervising an Unsupervised Neural Network.,"Machine learning is the field that is dedicated to the design and development of algorithms and techniques that allow computers to ""learn"". Two common types of learning that are often mentioned are supervised learning and unsupervised learning. One often understands that in supervised learning, the system is given the desired output, and it is required to produce the correct output for the given input, while in unsupervised learning the system is given only the input and the objective is to find the natural structure inherent in the input data. We, however, suggest that even with unsupervised learning, the information inside the input, the structure of the input, and the sequence that the input is given to the system actually make the learning ""supervised"" in some way. Therefore, we recommend that in order to make the machine learn, even in a ""supervised"" manner, we should use an ""unsupervised learning"" model together with an appropriate way of presenting the input. We propose in this paper a simple plasticity neural network model that has the ability of storing information as well as storing the association between a pair of inputs. We then introduce two simple unsupervised learning rules and a framework to supervise our neural network.",193,https://doi.org/10.1109/ACIIDS.2009.92,1,
3039,Near-Duplicates Detection for Vietnamese Documents in Large Database.,"Near-duplicate documents exacerbate the problem of information overload. Research in detecting near-duplicates has attracted a lot of attention from both industry and academia. In this paper, we focus on addressing this problem for Vietnamese documents which, to the best of our knowledge, has not been done before. Most of the current algorithms have been designed for English which are not directly applicable to Vietnamese - a monosyllabic language. We propose to combine Charikarpsilas algorithm (Alexander Andoni et al., 2006) with a ldquoweighting schemerdquo and Vietnamese specific features to address the language intricacy. Experimental results indicate that our scheme is effective for detecting near-duplicates in a corpus of Vietnamese documents.",193,https://doi.org/10.1109/ALPIT.2008.76,1,
3040,Recognizing Vietnamese Online Handwritten Separated Characters.,"Vietnamese alphabet is based on the Latin alphabet with the addition of nine accent marks or diacritics - four of them to create additional sounds, and the other five to indicate the tone of each word. Because Vietnamese is a tonal language that uses tone to distinguish words, recognizing diacritics is an important part in recognizing Vietnamese word. However, in written form, diacritics are much smaller then the characters, which make very them hard to recognize. Previous works on Vietnamese characters recognition often pre-process input with a graph-based approach by trying to separate the main characters with their diacritics by determining connected regions at pixel level. This approach, however, only works well where the input contains only characters with separable diacritics, for example, scanned image of printed documents. We propose in this paper a robust method to recognize online Vietnamese characters with diacritics. Using cosine transformation with appropriated sampling algorithms, we represent multiple strokes of a character together in a single set of features. This set of features is then used as the input for a well designed machine learning based system. We have tested our system on the combination of Vietnamese characters with diacritics and Section 1c (isolated characters) of the Unipen data set, and have obtained very competitive results.",193,https://doi.org/10.1109/ALPIT.2008.58,1,
3041,A Fast and Distortion Tolerant Hashing for Fingerprint Image Authentication.,"Biometrics such as fingerprint, face, eye retina, and voice offers means of reliable personal authentication is now a widely used technology both in forensic and civilian domains. Reality, however, makes it difficult to design an accurate and fast biometric recognition due to large biometric database and complicated biometric measures. In particular, fast fingerprint indexing is one of the most challenging problems faced in fingerprint authentication system. In this paper, we present a specific contribution to advance the state of the art in this field by introducing a new robust indexing scheme that is able to fasten the fingerprint recognition process.",193,https://doi.org/10.1007/978-3-540-88181-0_34,1,
3042,On the problem of classifying Vietnamese online handwritten characters.,"Recognizing circumflexes and diacritics in a Latin-based tonal language of Vietnamese is important because they are used to distinguish words. There are two main approaches to solve the diacritics recognition problems. The first approach tries to separate diacritics from their main character and recognizes them independently. This approach, however, should only be applied with printed documents as the diacritics of printed characters are separable. With free-styled handwritten characters, diacritics are very hard to separate and recognize because they may overlap main characters. The second approach tries to recognize both main character and diacritics without separation step. In this paper, we want to show that our approach with an appropriate feature extraction method and subclass diving strategy, a robust recognition can be obtained with high recognition rate.",193,https://doi.org/10.1109/ICARCV.2008.4795620,1,
3043,A codeword-based indexing scheme for fingerprint identification.,"Fingerprint authentication system is now one of the most reliable personal identification methods. However, it is difficult to design a system such that it satisfies requirements in both accuracy and lookup speed due to large fingerprint database and complicated fingerprint measures. Therefore, fast and accurate fingerprint indexing plays very important role in fingerprint authentication system. In this paper, we present a new approach which is able to improve the performance of fingerprint indexing process. Moreover, this technique itself provides privacy property for fingerprint system which is not mentioned in previous indexing techniques.",193,https://doi.org/10.1109/ICARCV.2008.4795719,1,
3044,Dividing Agents on the Grid for Large Scale Simulation.,"Multi-agent based simulation is an important methodology that uses models incorporating agents to evaluate research conclusions. When the simulation involves a large number of agent, however, it requires extensively high computational power. In that case, all agents in the simulation model should be distributed in a way so that agents can be run in parallel on multiple computational nodes to gain the required performance speed up. In this paper, we present a framework for large scale multi-agent based simulation on grid. We have modified the desktop grid platform BOINC for multi-agent based simulation. Assuming that the agents interact locally with the environment, we proposed an approach to divide the agents for grid nodes so that we can keep load balancing for the distributed simulation while optimizing the communication between grid nodes and the grid server. We have implemented the food foraging simulation to evaluate the feasibility of the framework.",193,https://doi.org/10.1007/978-3-540-89674-6_25,1,
3046,Classifying Online Handwriting Characters under Cosine Representation.,"The natural way of handwriting to enter data into computer is still preferable in many tasks. However, handwriting character recognition is not a trivial task for computer. Based on the presentation of the input, handwriting recognition can be divided into two classes: offline and online. The main advantage of online handwritten data over offline data is the availability of stroke segmentation and order of writing. Utilizing this information rather than static image only can obtain higher recognition rate [11]. In this paper, we extend the method proposed in [13] to represent multiple strokes of a character together in a single set of features using cosine transformation. Using this representation, we have developed an online writer-independent character recognition system with MultiLayer Perceptron (MLP) classifiers, one classifier for each single character. We have tested our system on Section 1a (isolated digits) of the Unipen data set [7] and have obtained very competitive results.",193,https://doi.org/10.1109/ALPIT.2007.72,1,
3047,A 3D Conversational Agent for Presenting Digital Information for Deaf People.,"Sign languages appear to be the major communication tool for the deaf community. As a minor community, Vietnamese deaf people often have to rely on a small numbers of interpreters who are hearing people knowing sign language in order to communicate with hearing people. It would be very beneficial for deaf as well as hearing people to develop an automated translation system between spoken/written languages and sign language. In this paper, we present our attempt to graphically decompose the gestures in Vietnamese Sign Language so that they can be easily synthesized in a 3D conversational agent. In order to describe the signs in Vietnamese Sign Language, we proposed an animation-level markup language. Based on this, we presented our 3D conversational agent for presenting multi-modal information for deaf people. The 3D agent can assist deaf people easier and in a more natural way.",193,https://doi.org/10.1007/978-3-642-01639-4_28,1,
3048,When and How to Smile: Emotional Expression for 3D Conversational Agents.,"Conversational agents have become more and more common in the multimedia worlds of films, educative applications, e - business, computer games. Many techniques have been developed to enable these agents to behave in a human-like manner. In order to do so, conversational agents are simulated with emotion and personality as well as communicative channels such as voice, head and eye movement, manipulator and facial expression. Up to now, creating facial expression from emotions has received much attention. However, most of the work concentrates on producing static facial expressions from emotions. In this paper, we propose a scheme for displaying continuous emotional states of a conversational agent on a 3D face. The main idea behind the scheme is that an emotional facial expression happens for a few seconds only when there is a significant change in the emotional states. This makes the emotional facial expressions of the conversational agents more realistic due to the fact that a facial expression only stay on the face for a few seconds.",193,https://doi.org/10.1007/978-3-642-01639-4_31,1,
3049,Combination of Facial Movements on a 3D Talking Head.,"Facial movements play an important role in interpreting spoken conversations and emotions. There are several types of movements, such as conversational signals, emotion displays, etc. We call these channels of facial movement. Realistic animation of these movements improves the realism, liveliness of the interaction between human and computers using embodied conversational agents. To date, no appropriate methods have been proposed for integrating all facial movements. We propose in this paper a scheme of combining facial movements on a 3D talking head. First, we concatenate the movements in the same channel to generate smooth transitions between adjacent movements. This combination only applies to individual muscles. The movements from all channels are then combined taking into account the resolution of possible conflicting muscles",193,https://doi.org/10.1109/CGI.2004.1309223,1,
3050,Improvements on a Simple Muscle-Based 3D Face for Realistic Facial Expressions.,"Facial expressions play an important role in face-to-face communication. With the development of personal computers capable of rendering high quality graphics, computer facial animation has produced more and more realistic facial expressions to enrich human-computer communication. In this paper, we present a simple muscle-based 3D face model that can produce realistic facial expressions in real time. We extend Waters' (1987) muscle model to generate bulges and wrinkles and to improve the combination of multiple muscle actions. In addition, we present techniques to reduce the computation burden on the muscle model.",193,https://doi.org/10.1109/CASA.2003.1199301,1,
3052,Exporting Vector Muscles for Facial Animation.,"In this paper we introduce a method of exporting vector muscles from one 3D face to another for facial animation. Starting from a 3D face with an extended version of Waters’ linear muscle system, we transfer the linear muscles to a target 3D face. We also transfer the region division, which is used to increase the performance of the muscle as well as to control the animation. The human involvement is just as simple as selecting the faces which shows the most natural facial expressions in the animator’s view. The method allows the transfer of the animation to a new 3D model within a short time. The transferred muscles can then be used to create new animations.",193,https://doi.org/10.1007/3-540-37620-8_26,1,
3053,ParleE: An Adaptive Plan Based Event Appraisal Model of Emotions.,"We propose ParleE, a quantitative, flexible and adaptive model of emotions for a conversational agent in a multi-agent environment capable of multimodal communication. ParleE appraises events based on learning and aprobabilistic planning algorithm. ParleE also models personality and motivational states and their role in determining the way the agent experiences emotion.",193,https://doi.org/10.1007/3-540-45751-8_9,1,
3054,Generation of Facial Expressions from Emotion Using a Fuzzy Rule Based System.,"We propose a fuzzy rule-based system to map representations of the emotional state of an animated agent onto muscle contraction values for the appropriate facial expressions. Our implementation pays special attention to the way in which continuous changes in the intensity of emotions can be displayed smoothly on the graphical face. The rule system we have defined implements the patterns described by psychologists and researchers dealing with facial expressions of humans, including rules for displaying blends of expressions.",193,https://doi.org/10.1007/3-540-45656-2_8,1,
3055,Building Population-Specific Reference Genomes: A Case Study of Vietnamese Reference Genome.,"The human reference genome is an essential tool for studying human genomes. The standard reference genome is constructed from genomes of a few donors. The 1000 genomes project has revealed a huge amount of genetic differences between diverse populations. It is therefore naturally questioned whether the standard reference genome can work well for all human genome studies or population-specific reference genomes are needed accordingly. In this paper, we present a pipeline for constructing and evaluating a population-specific reference genome. The pipeline was examined on building the Vietnamese reference genome from 100 Kinh Vietnamese genomes obtained from the 1000 genomes project. Experiments showed that the resulting Vietnamese reference genome was better than the standard reference genome at analyzing Vietnamese genomic data. It helped improve the quality of short reads mapping and genotype calling for Vietnamese genomes. The pipeline is applicable for building and evaluating other population-specific reference genomes. For the first time the Vietnamese reference genome, which is now available for further Vietnamese genome studies, was successfully built.",194,https://doi.org/10.1109/KSE.2015.49,1,
3056,Collaborative Learning for Cyberattack Detection in Blockchain Networks.,"This article aims to study intrusion attacks and then develop a novel cyberattack detection framework for blockchain networks. Specifically, we first design and implement a blockchain network in our laboratory. This blockchain network will serve two purposes, i.e., generate the real traffic data (including both normal data and attack data) for our learning models and implement real-time experiments to evaluate the performance of our proposed intrusion detection framework. To the best of our knowledge, this is the first dataset that is synthesized in a laboratory for cyberattacks in a blockchain network. We then propose a novel collaborative learning model that allows efficient deployment in the blockchain network to detect attacks. The main idea of the proposed learning model is to enable blockchain nodes to actively collect data, share the knowledge learned from its data, and then exchange the knowledge with other blockchain nodes in the network. In this way, we can not only leverage the knowledge from all the nodes in the network but also do not need to gather all raw data for training at a centralized node like conventional centralized learning solutions. Such a framework can also avoid the risk of exposing local data's privacy as well as the excessive network overhead/congestion. Both intensive simulations and real-time experiments clearly show that our proposed collaborative learning-based intrusion detection framework can achieve an accuracy of up to 97.7% in detecting attacks.",195,https://doi.org/10.48550/arXiv.2203.11076,1,
3057,An Effective Framework of Private Ethereum Blockchain Networks for Smart Grid.,"A smart grid is an important application in Industry 4.0 with a lot of new technologies and equipment working together. Hence, sensitive data stored in the smart grid is vulnerable to malicious modification and theft. This paper proposes a framework to build a smart grid based on a highly effective private Ethereum network. Our framework provides a real smart grid that includes modern hardware and a smart contract to secure data in the blockchain network. To obtain high throughput but a low uncle rate, the difficulty calculation method used in the mining process of the Ethereum consensus mechanism is modified to adapt to the practical smart grid setup. The performance in terms of throughput and latency are evaluated by simulation and verified by the real smart grid setup. The enhanced private Ethereum-based smart grid has significantly better performance than the public one. Moreover, this framework can be applied to any system used to store data in the Ethereum network.",195,https://doi.org/10.48550/arXiv.2203.14633,1,
3058,A Hierarchical Long Short-Term Memory Encoder-Decoder Model for Abstractive Summarization.,"Abstractive summarization is the task of generating concise summary of a source text, which is a challenging problem in Natural Language Processing (NLP). Many recent studies have relied on encoder-decoder sequence-to-sequence deep neural networks to solve this problem. However, most of these models treat the input as a sequence of words at the same level during the encoding process. This will make the encoding inefficient, especially for long input texts. Addressing this issue, in this paper we propose a model to encode text in a hierarchical manner, which helps to encode information in a way that is consistent with the nature of the text: the text is synthesized from sentences, and each sentence is synthesized from words. Our proposed model is based on Long Short Term Memory model that we called HLSTM (Hierarchical Long Short Term Memory) and applied to the problem of abstractive summarization. We conducted extensive experiments on the two most popular corpora (Gigaword and Amazon Review) and obtain significant improvements in comparison with the baseline models.",195,https://doi.org/10.1109/KSE53942.2021.9648836,1,
3059,A framework for assume-guarantee regression verification of evolving software.,"This paper presents a framework for verifying evolving component-based software using assume-guarantee logic. The goal is to improve CDNF-based assumption generation method by having local weakest assumptions that can be used more effectively when verifying component-based software in the context of software evolution. For this purpose, we improve the technique for responding to membership queries when generating candidate assumptions. This technique is then integrated into a proposed backtracking algorithm to generate local weakest assumptions. These assumptions are effectively used in rechecking the evolving software by reducing time required for assumption regeneration within the proposed framework. The proposed framework can be applied to verify software that is continually evolving. An implemented tool and experimental results are presented to demonstrate the effectiveness and usefulness of the framework.",195,https://doi.org/10.1016/j.scico.2020.102439,1,
3060,Autoencoder based Friendly Jamming.,"Physical layer security (PLS) provides lightweight security solutions in which security is achieved based on the inherent random characteristics of the wireless medium. In this paper, we consider the PLS approach called friendly jamming (FJ), which is more practical thanks to its low computational complexity. State-of-the-art methods require that legitimate users have full channel state information (CSI) of their channel. Thanks to the recent promising application of the autoencoder (AE) in communication, we propose a new FJ method for PLS using AE without prior knowledge of the CSI. The proposed AE-based FJ method can provide good secrecy performance while avoiding explicit CSI estimation. We also apply the recently proposed tool for mutual information neural estimation (MINE) to evaluate the secrecy capacity. Moreover, we leverage MINE to avoid end-to-end learning in AE-based FJ.",195,https://doi.org/10.1109/WCNC45663.2020.9120554,1,
3061,An Attention-Based Long-Short-Term-Memory Model for Paraphrase Generation.,"Neural network based sequence-to-sequence models have shown to be the effective approach for paraphrase generation. In the problem of paraphrase generation, there are some words which should be ignored in the target text generation. The current models do not pay enough attention to this problem. To overcome this limitation, in this paper we propose a new model which is a penalty coefficient attention-based Residual Long-Short-Term-Memory (PCA-RLSTM) neural network for forming an end-to-end paraphrase generation model. Extensive experiments on the two most popular corpora (PPDB and WikiAnswers) show that our proposed model’s performance is better than the state-of-the-art models for paragraph generation problem.",195,https://doi.org/10.1007/978-3-319-75429-1_14,1,
3062,A Hierarchical Conditional Attention-Based Neural Networks for Paraphrase Generation.,"Sequence-to-Sequence (Seq2Seq) learning has immense interest in recent years. The prosperous approach of end-to-end training fashion using encoder-decoder neural networks like machine translation has sprouted active research in transduction tasks such as abstractive summarization or especially Paraphrase Generation (PG). Dealing with paraphrase generation problem, one of the most intrinsic obstruction of existing solutions do not pay enough attention to the fact that words and sentences in particular context own differential importance. Consequently, the loss of crucial information probably occurs and irrelevant paraphrasing components are generated. To overcome these barriers, an emerging Hierarchical Conditional Attention-based Neural Networks (HCANN) architecture to construct end-to-end text generation framework is proposed. More specifically, included method in that represents hierarchy of document along with conditional decoder for paraphrase generation processes. Quantitative evaluation of the method on several benchmark paraphrase datasets demonstrates its efficiency and performance capability by a significant margin.",195,https://doi.org/10.1007/978-3-030-03014-8_14,1,
3063,Generation of Test Data Using Genetic Algorithm and Constraint Solver.,"Search-based testing techniques using genetic algorithm (GA) can automatically generate test data that achieves high coverage on almost any given program under test. GA casts the path coverage test data generation as an optimization problem and applies efficient search-based algorithms to find suitable test cases. GA approaches scale well and can handle any source code and test criteria, but it still has some degrades when program under test has critical path clusters. This paper presents a method for improving GA efficiency by integrating a constraint solver to solve path conditions in which regular GA cannot generate test data for coverage. The proposed approach is also applied to some programs under test. Experimental results demonstrate that improved GA can generate suitable test data has higher path coverage than the regular one.",195,https://doi.org/10.1007/978-3-319-56660-3_43,1,
3064,A Method for Automated Test Cases Generation from UML Models with String Constraints.,"This paper proposes an automated test cases generation method from sequence diagrams and class diagrams with string constraints. The method supports UML 2.0 sequence diagrams including twelve combined fragments. An algorithm for generating test scenarios are developed to avoid test paths explosion without having data sharing points of threads in parallel fragments or weak sequencing fragments. Test data are also generated with solving constraints of string variables. We standardize string constraints and equations at the boundary of variables that are input formula of Z3-str solver. Comparing with the current approach of the solver, some preprocessing rules are extended for other operations such as charAt, lastindexOf, trim, startsWith and endsWith. If a result of the Z3-str is SAT, test data of each test scenario are generated to satisfy the constraints with boundary coverage. A tool is implemented to support the proposed method, and some experiments are also presented to illustrate the effectiveness of the tool.",195,https://doi.org/10.1007/978-3-319-56660-3_45,1,
3065,A knowledge representation for Vietnamese legal document system.,"Legal documents play an important role in the legal systems of every nation. In order to process information automatically and make use of knowledge from legal documents, we need an appropriate knowledge representation for them. In this research, we investigate properties of the Vietnamese legal document system and propose a representation for the documents and their interrelationships. The main challenges for this task are their unstability, limited validity period and complicated interrelationships. Properties and relationships which we design to represent in our knowlege base are selective in order to make use of information within legal documents. In order to represent our knowlege base, we compare three methods: Semantic Network, Production Rules, Frame Language and make use of their advantages in our representation. Moreover, we implement a service-oriented system providing knowledge querying service for other application systems based on our research result.",195,https://doi.org/10.1109/KSE.2017.8119430,1,
3066,A Technique for Generating Test Data Using Genetic Algorithm.,"Automatic test data generation for path coverage is an undecidable problem and genetic algorithm (GA) has been used as one good solution. This paper presents a method for optimizing GA efficiency by identifying the most critical path clusters in a program under test. We do this by using the static program analysis to find all the paths having the path conditions with low probability in generating coverage data, then basing on these path conditions to adjust the procedure of generating new populations in GA. The proposed approach is also applied some program under tests. Experimental results show that improved GA which can generate suitable test data has higher path coverage than the traditional GA.",195,https://doi.org/10.1109/ACOMP.2016.019,1,
3069,An Architecture for Web Services Mash-Up based on Mobile Agents.,"In the present day, there are many useful domain information scattered over the Internet. But domain users need to navigate different websites or information sources to get domain relevant information. Services mash-up can be used to integrate not only web content and also services to adapt the display on users' clients. However, the user's requests for contents generating the UI displays might consider participating in mash-up process as time-consuming, even a trouble of network bottleneck. To address this problem, we proposed an architecture for web service mash-up system based on mobile agents. By taking using the mobile agents, mash-up system can be adapt for current context to provide better visualization support to users. The experiments to support mobile agent base web service mash-up has been implemented.",195,https://doi.org/10.1145/2684200.2684331,1,
3070,Improving Simulation of Continuous Emotional Facial Expressions by Analyzing Videos of Human Facial Activities.,"Conversational agents are receiving significant attention from multi-agent and human computer interaction research societies. In order to make conversational agents more believable and friendly, giving them the ability to express emotions is one of research fields which have drawn a lot of attention lately. In this paper, we propose a work on analysis of how emotional facial activities happen temporally. Our goal is to find the temporal patterns of facial activity of six basic emotions in order to improve the simulation of continuous emotional facial expressions on a 3D face of an embodied agent. Using facial expression recognition techniques, we first analyze a spontaneous video database in order to consider how facial activities are related to six basic emotions temporally. From there, we bring out the general temporal patterns for facial expressions of the six basic emotions. Then, based on the temporal patterns, we propose a scheme for displaying continuous emotional states of a conversational agent on a 3D face.",195,https://doi.org/10.1007/978-3-319-13191-7_18,1,
3071,An Approach to Checking the Compliance of User Permission Policy in Software Development.,"The Security policy of a software system is a set of actions that the system should or should not do in given conditions. These actions can be considered as critical properties in many applications which require high level of safety, such as the military, bank or stock software systems. Security policy must be specified clearly in software requirements and then be followed strictly and correctly in implementations. User permission policy is one of the most important aspects in software security policy. This paper proposes an approach for checking the conformance between user permissions of an implementation and their given specifications. In this approach, the source code of a program is represented at an abstraction level called Abstract Syntax Tree, which are then checked against specification of user permissions expressed using Role-Based Access Control (RBAC). A checking tool has been developed and verified using several common examples.",195,https://doi.org/10.1142/S0218194013500344,1,
3072,A runtime approach for estimating resource usage.,"In the era of information explosion, a program is necessary to be scalable. Therefore, scalability analysis becomes very important in software verification and validation. However, current approaches to empirical scalability analysis remain limitations related to the number of supported models and performance. In this paper, we propose a runtime approach for estimating the program resource usage with two aims: evaluating the program scalability and revealing potential errors. In this approach, the resource usage of a program is first observed when it is executed on inputs with different scales, the observed results are then fitted on a model of the usage according to the program's input. Comparing to other approaches, ours supports diverse models to illustrate the resource usage, i.e., linear-log, power-law, polynomial, etc. We currently focus on the computation cost and stack frames usage as two representatives of resource usage, but the approach can be extended to other kinds of resource. The experimental result shows that our approach achieves more precise estimation and better performance than other state-of-the-art approaches.",195,https://doi.org/10.1145/2542050.2542091,1,
3073,Securing Data in Composite Web Services.,"Service-oriented architecture (SOA) provides a solution for building information systems with reusability, flexibility, and extensibility. In SOA, new services can be developed by using existing services. BPEL is considered as the main means for composing services. In service composition, a composite service may be built from element services which belong to different administrative domains. Therefore, in order to exploit the benefits of service composition, we must carefully consider security issues. One of them is to protect users' information. In this paper, we describe an approach for securing BPEL composite Web services. We assume that some users' data received by the composite services needs securing. The proposed approach makes sure that when the data is sent out from the composite service appropriate security policies will be applied.",195,https://doi.org/10.1109/KSE.2012.33,1,
3074,Verifying Java Object Invariants at Runtime.,"An object invariant consisting of a set of properties that must hold for all instances of a class at any time is usually used in object-oriented design. However, verifying object invariants at runtime is always a challenging task in software verification. This paper proposes a method for verifying invariants of Java objects at runtime using AOP. Suppose that a software application is designed using UML models and its constraints are specified in OCL expressions, the software is then implemented, by default, using the UML design. They propose to construct verifiable aspects which are automatically generated from OCL constraints. These aspects can be woven into Java code to check whether object invariants are violated at runtime. Benefiting from AOP in separation of crosscutting concerns and weaving mechanisms, generated aspects can do the verification task whenever values of objects' attributes are changed. A Verification Aspect Generator (VAG) tool has been developed allowing the automatic generation of verifying aspects from the UML/OCL constraints.",195,https://doi.org/10.1142/S0218194011005281,1,
3075,Towards scalable agent-based web service systems: performance evaluation.,"Web service technology is widely applied in building distributed information systems in order to provide flexibility and extensibility. As in the traditional web services, when the number of clients increases, the traffic problems of network may happen and the services may become overloaded. Distributed mobile agent platforms in Web service have become more mature in recent years, and may help reducing the load from Web servers as well as reduce the traffic for overall network. We have proposed an agent-based framework for Web services that is scalable and reliable. The framework uses mobile agents to solve the traffic problem on traditional Web service systems. In this paper, we study the performance of our proposed approach with the focus on response time. We simulate our framework by mathematical models on a network simulator tool. The simulation results showed that our proposed framework has ability to improve performance in comparison with traditional Web service systems.",195,https://doi.org/10.1145/2095536.2095635,1,
3076,Refining Undetermined Events for Specifying Concurrent Programs.,"Development of many system features in a robust specification is a challenge in reactive system design. This paper presents a formal approach for specifying concurrent programs by refining undetermined events in Event-B. This approach investigates the concurrent aspect in specification and provides a general structure of models using Event-B specification. According to this structure, we illustrate the specification of three concurrent mechanisms: critical section access, reader-writer and producer-consumer. It is shown that the concurrent aspects in programming can be specified coherently using Event-B refinement.",195,https://doi.org/10.1109/KSE.2011.29,1,
3077,Checking the Consistency between UCM and PSM Using a Graph-Based Method.,"Checking the consistency in component models at design phase is essential in component-based software engineering (CBSE). In our previous work, we proposed an approach for verifying automatically the matching between protocol state machines (PSMs) and the use case map (UCM), using the B method. Due to the expressive power of B notations, however, we cannot describe the parallel processing in the implementation machine, particularly we are not able to express all features (such as AND-forks/joins, OR-forks/joins) of UCMs in a B implementation machine. In this work, we propose an approach to solve the expression problem of UCM features using a graph-based algorithm. The UCM path which describes the interaction between components is extracted and then decomposed into sequential events' paths if it has AND-forks/joins and/or OR-forks/joins. Each of sequential eventspsila paths will be checked with the order of events of PSMs by the proposed algorithm.",195,https://doi.org/10.1109/ACIIDS.2009.66,1,
3078,Transformation Rule Learning without Rule Templates: A Case Study in Part of Speech Tagging.,"Part of speech (POS) tagging is an important problem and is one of the first steps included in many tasks in natural language processing. It affects directly on the accuracy of many other problems such as Syntax Parsing, WordSense Disambiguation, and Machine Translation. Stochastic models solve this problem relatively well, but they still make mistakes. Transformation-based learning (TBL) is a solution which can be used to improve stochastic taggers by learning a set of transformation rules. However, its rule learning algorithm has the disadvantages that rule templates must be prepared by hand and only rules are instances of rule templates can be generated. In this paper, we propose a model to learn transformation rules without rule templates. This model considers the rule learning problem as a feature selection problem. Experiments on PennTree Bank showed that the proposal model reduces errors of stochastic taggers with some tags.",195,https://doi.org/10.1109/ALPIT.2008.73,1,
3079,Using B to Verify the Weaving of Aspects.,"AspectJ is an aspect-oriented extension of the Java language that enables a modular implementation of crosscutting concerns. Despite this, aspects lack support for formal specification and verification. This paper expresses the base class and some related aspects of AspectJ model in B notation. It aims to benefit from proof obligations generated by B tools to ensure the correctness of AspectJ component composition. Static crosscuts of aspects are guaranteed by proof obligations of relation clauses between B abstract machines and dynamic crosscuts are proved by proof obligations of B refinement machines. This approach is illustrated by verifying a simple example.",195,https://doi.org/10.1109/APSEC.2007.99,1,
3080,A mechanism for inferring approximate solutions under incomplete knowledge based on rule similarity.,"This paper proposes an inference method which can obtain an approximate solution even if the knowledge stored in the problem-solving system is incomplete. When a rule needed for solving the problem does not exist, the problem can be solved by using rules similar to the existing rules. In an implementation using SLD procedure, a resolution is executed between a subgoal and a rule if an atom of the subgoal is similar to the consequence atom of the rule. Similarities between atoms are calculated using a knowledge base of words with account of the reasoning situation, and the reliability of the derived solution is calculated based on these similarities. If many solutions are obtained, they are grouped into classes of similar solutions and a representative solution is then selected for each class. The proposed method was verified experimentally by solving simple problems. © 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(9): 78–89, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.1154",195,https://doi.org/10.1002/scj.1154,1,
3081,Crack Identification on the Fresh Chilli (Capsicum) Fruit Destemmed System.,,196,https://doi.org/10.1155/2021/8838247,1,
3082,Measuring Semantic Similarity of Vietnamese Sentences Based on Lexical and Distribution Similarity.,"Measuring the semantic similarity of sentence pairs is an important natural language processing (NLP) problem and has many applications in many NLP systems. Sentence similarity is used to improve the performance of many systems such as machine translation, speech recognition, automatic question and answer, text summarization. However, accurately evaluate the semantic similarity between sentences is still a challenge. Up to now, there are not sentence similarity methods, which exploit Vietnamese specific characteristics, have been proposed. Moreover, there are not sentence similarity datasets for Vietnamese that have been published. In this paper, we propose a new method to measure the semantic similarity of Vietnamese sentence pairs based on combining lexical similarity score and distribution semantic similarity score of two sentences. The experimental results have shown that our proposed model has high performance for the Vietnamese semantic similarity problem.",196,https://doi.org/10.1007/978-3-030-92666-3_22,1,
3083,WEWD: A Combined Approach for Measuring Cross-lingual Semantic Word Similarity Based on Word Embeddings and Word Definitions.,"Cross-lingual semantic word similarity (CLSW) ad- dresses the task of estimating the semantic distance between two words across languages. This task is an important component in many natural language processing applications. Recent studies have proposed several effective CLSW models for resource- rich language pairs such as English-German, English-French. However, This task has not been effectively addressed for language pairs consisting of Vietnamese and another one. In this paper, we propose a neural network model that exploits cross- lingual lexical resources to learn high-quality cross-lingual word embedding models. Since our neural network model is language- independent, it can learn a truly multilingual space. Furthermore, we introduce a novel cross-lingual semantic word similarity measurement method based on Word Embeddings and Word Definitions (WEWD). Last but not least, we introduce a standard Vietnamese-English dataset for the cross-lingual semantic word similarity measurement task (VESim-1000). The experimental results show that our proposed method is more robust and outperforms current state-of-the-art methods that are only based on word embeddings or lexical resources.",196,https://doi.org/10.1109/RIVF51545.2021.9642084,1,
3084,ViNMT: Neural Machine Translation Tookit.,"Part of speech (POS) tagging is an important problem and is one of the first steps included in many tasks in natural language processing. It affects directly on the accuracy of many other problems such as Syntax Parsing, WordSense Disambiguation, and Machine Translation. Stochastic models solve this problem relatively well, but they still make mistakes. Transformation-based learning (TBL) is a solution which can be used to improve stochastic taggers by learning a set of transformation rules. However, its rule learning algorithm has the disadvantages that rule templates must be prepared by hand and only rules are instances of rule templates can be generated. In this paper, we propose a model to learn transformation rules without rule templates. This model considers the rule learning problem as a feature selection problem. Experiments on PennTree Bank showed that the proposal model reduces errors of stochastic taggers with some tags.",196,https://arxiv.org/abs/2112.15272,1,
3085,Vietnamese Antonyms Detection Based on Specialized Word Embeddings using Semantic Knowledge and Distributional Information.,"Antonymy is one of the fundamental relations shaping the organization of the semantic lexicon. Therefore, automatic detection of antonymy can be leveraged to make contributions to different NLP tasks, such as Machine Translation, Sentiment Analysis, and Information Retrieval. Currently, most prior studies just focus on discriminating between antonyms and synonyms. However, not only synonymy but other semantic relations, such as hypernymy, co-hyponyms, which also get high similarities thereby making it hard to discriminate. Therefore, it is necessary to make a thorough research on identifying antonyms from a wide variety of other semantic relations. In this paper, we aim to identify Vietnamese antonyms pairs according to the vector semantics approach. Specifically, we build up specialized word embedding models by incorporating lexical-semantic resource and distributional information. In addition, we propose specialized Vietnamese features and utilize mutual information between words in order to integrate with word embedding vectors. This aims to generate more meaningful feature vectors for supervised classifiers solving antonym detection problems. Furthermore, we construct three reliable Vietnamese testing datasets consisting of AntSynlOOO, AntHyplOOO, and AntMixlOOO, for this task. Experimental results conducted on the datasets demonstrated that our model performs effectively.",196,https://doi.org/10.1109/KSE50997.2020.9287542,1,
3086,Iterative Multilingual Neural Machine Translation for Less-Common and Zero-Resource Language Pairs.,"Research on providing machine translation
systems for unseen language pairs is gaining
increasing attention in recent years. However,
the quality of their systems is poor for most
language pairs, especially for less-common
pairs such as Khmer-Vietnamese. In this
paper, we show a simple iterative traininggenerating-filtering-training process that utilizes all available pivot parallel data to generate synthetic data for unseen directions.
In addition, we propose a filtering method
based on word alignments and the longest
parallel phrase to filter out noise sentence
pairs in the synthetic data. Experiment results on zero-shot Khmer→Vietnamese and
Indonesian→Vietnamese directions show that
our proposed model outperforms some strong
baselines and achieves a promising result
under the zero-resource condition on ALT
benchmarks. Besides, the results also indicate
that our model can easily improve their quality
with a small amount of real parallel data.",196,https://aclanthology.org/2020.paclic-1.24/,1,
3087,"Improving Multilingual Neural Machine Translation For Low-Resource Languages: French-, English- Vietnamese.","Prior works have demonstrated that a low-resource language pair can benefit from multilingual machine translation (MT) systems, which rely on many language pairs' joint training. This paper proposes two simple strategies to address the rare word issue in multilingual MT systems for two low-resource language pairs: French-Vietnamese and English-Vietnamese. The first strategy is about dynamical learning word similarity of tokens in the shared space among source languages while another one attempts to augment the translation ability of rare words through updating their embeddings during the training. Besides, we leverage monolingual data for multilingual MT systems to increase the amount of synthetic parallel corpora while dealing with the data sparsity problem. We have shown significant improvements of up to +1.62 and +2.54 BLEU points over the bilingual baseline systems for both language pairs and released our datasets for the research community.",196,https://arxiv.org/abs/2012.08743,1,
3088,Overcoming the Rare Word Problem for low-resource language pairs in Neural Machine Translation.,"Among the six challenges of neural machine translation (NMT) coined by (Koehn and Knowles, 2017), rare-word problem is considered the most severe one, especially in translation of low-resource languages. In this paper, we propose three solutions to address the rare words in neural machine translation systems. First, we enhance source context to predict the target words by connecting directly the source embeddings to the output of the attention component in NMT. Second, we propose an algorithm to learn morphology of unknown words for English in supervised way in order to minimize the adverse effect of rare-word problem. Finally, we exploit synonymous relation from the WordNet to overcome out-of-vocabulary (OOV) problem of NMT. We evaluate our approaches on two low-resource language pairs: English-Vietnamese and Japanese-Vietnamese. In our experiments, we have achieved significant improvements of up to roughly +1.0 BLEU points in both language pairs.",196,https://doi.org/10.18653/v1/D19-5228,1,
3089,How Transformer Revitalizes Character-based Neural Machine Translation: An Investigation on Japanese-Vietnamese Translation Systems.,"Antonymy and Synonymy are paradigmatic relations which are in the core problems of language. Distinguishing antonyms from synonyms is a key task to achieve high performance in natural language processing systems. Distinguishing between antonyms and synonyms is a hard problem because the co-occurrence distributions of the antonyms or synonyms tend to be highly similar. On the other hand, this issue has been thoroughly studied in English. However, it has not been effectively addressed for Vietnamese. Compared to English, Vietnamese has its own word-level characteristics that indicate the synonymous or antonymous relation. In this paper, we introduce a framework which exploits exhaustively special Vietnamese features to distinguish between antonyms from synonyms. We propose a deep neural network model (ViASNet) that can utilize not only lexico-syntactic information captured from the context of word pairs in a corpus but also its word-level features, and distribution features as well. The experimental results show that the proposed method is effective. Furthermore, our method achieved high performance in comparison to several the state of the art methods.",196,https://aclanthology.org/2019.iwslt-1.15,1,
3091,Combining Advanced Methods in Japanese-Vietnamese Neural Machine Translation.,"Neural machine translation (NMT) systems have recently obtained state-of-the-art in many machine translation systems between popular language pairs because of the availability of data. For low-resourced language pairs, there are few researches in this field due to the lack of bilingual data. In this paper, we attempt to build the first NMT systems for a low-resourced language pair: Japanese-Vietnamese. We have also shown significant improvements when combining advanced methods to reduce the adverse impacts of data sparsity and improve the quality of NMT systems. In addition, we proposed a variant of Byte-Pair Encoding algorithm to perform effective word segmentation for Vietnamese texts and alleviate the rare-word problem that persists in NMT systems.",196,https://doi.org/10.1109/KSE.2018.8573329,1,
3092,Enhancing Performance of Lexical Entailment Recognition for Vietnamese based on Exploiting Lexical Structure Features.,"The lexical entailment recognition problem aims to identify the is-a relation between words. The problem has recently been receiving research attention in the natural language processing field. In this study, we propose a novel method (VLER) for this problem on Vietnamese. For this purpose, we first exploit such lexical structure information of words as a feature, then combine this feature with vectors representation of words such as a unique feature for recognizing the relation. Moreover, we applied a number of methods based on word embedding and supervised learning, experimental results showed that our method achieves the best performance in the hypernymy detection task than other methods in terms of accuracy.",196,https://doi.org/10.1109/KSE.2018.8573391,1,
3093,Construction of a word similarity dataset and evaluation of word similarity techniques for Vietnamese.,"Measuring word similarity is a core issue because it has many applications in natural language processing. Although many studies have been reported and the techniques have been developed for addressing this issue for English, however, the study dealing with the applications, analyses and evaluation word similarity techniques to Vietnamese still has not reported yet. Especially, there is still lack of the benchmark Vietnamese dataset for evaluating these techniques. In this paper, we report three main topics including: firstly, construct a benchmark dataset for evaluation of similar techniques to the Vietnamese language; secondly, experiment with some similarity techniques based on WordNet and word embeddings; and finally, propose an extension for Lesk algorithm in order to improving the efficiency of similar measuring with Vietnamese language.",196,https://doi.org/10.1109/KSE.2017.8119436,1,
3094,Dealing with Out-Of-Vocabulary Problem in Sentence Alignment Using Word Similarity.,"Sentence alignment plays an essential role in
building bilingual corpora which are valuable
resources for many applications like statistical machine translation. In various approaches
of sentence alignment, length-and-word-based
methods which are based on sentence length
and word correspondences have been shown to
be the most effective. Nevertheless a drawback
of using bilingual dictionaries trained by IBM
Models in length-and-word-based methods is
the problem of out-of-vocabulary (OOV). We
propose using word similarity learned from
monolingual corpora to overcome the problem. Experimental results showed that our
method can reduce the OOV ratio and achieve
a better performance than some other lengthand-word-based methods. This implies that
using word similarity learned from monolingual data may help to deal with OOV problem
in sentence alignment.",196,https://aclanthology.org/Y16-2024/,1,
3095,A Two-Phase Approach for Building Vietnamese WordNet.,"Wordnets play an important role not only in
linguistics but also in natural language processing (NLP). This paper reports major results of a project which aims to construct a
wordnet for Vietnamese language. We propose a two-phase approach to the construction
of Vietnamese WordNet employing available
language resources and ensuring Vietnamese
specific linguistic and cultural characteristics.
We also give statistical results and analyses to
show characteristics of the wordnet.",196,https://aclanthology.org/2016.gwc-1.38/,1,
3096,Vietnamese treebank construction and entropy-based error detection.,"Treebanks, especially the Penn treebank for natural language processing (NLP) in English, play an essential role in both research into and the application of NLP. However, many languages still lack treebanks and building a treebank can be very complicated and difficult. This work has a twofold objective. Firstly, to share our results in constructing a large Vietnamese treebank (VTB) with three levels of annotation including word segmentation, part-of-speech tagging, and syntactic analysis. Major steps in the treebank construction process are described with particular regard to specific Vietnamese properties such as lack of word delimiter and isolation. Those properties make sentences highly syntactically ambiguous, and therefore it is difficult to ensure a high level of agreement among annotators. Various studies of Vietnamese syntax were employed not only to define annotations but also to systematically deal with ambiguities. Annotators were supported by automatic labelling tools, which are based on statistical machine learning methods, for sentence pre-processing and a tree editor for supporting manual annotation. As a result, an annotation agreement of around 90 % was achieved. Our second objective is to present our method for automatically finding errors and inconsistencies in treebank corpora and its application to the construction of the VTB. This method employs the Shannon entropy measure in a manner that the more reduced entropy the more corrected errors in a treebank. The method ranks error candidates by using a scoring function based on conditional entropy. Our experiments showed that this method detected high-error-density subsets of original error candidate sets, and that the corpus entropy was significantly reduced after error correction. The size of these subsets was only about one third of the whole set, while these subsets contained 80–90 % of the total errors. This method can also be applied to languages similar to Vietnamese.",196,https://doi.org/10.1007/s10579-015-9308-5,1,
3098,Automatic detection of problematic rules in Vietnamese Treebank.,"Vietnamese Treebank is a syntactically annotated corpus newly published in 2009. In this paper, we applied automated methods to detect errors in Vietnammese Treebank based on the concept of equivalence classes proposed by Dickinson. On this basis, we propose an improved method of error detection by transforming syntax trees based on vertical markovization. Our experimental results on Vietnamese Treebank showed that the scope of error detection was extended more than 2 times and the precision was improved more than 18.07% in comparison with the base line methods.",196,https://doi.org/10.1109/RIVF.2015.7049867,1,
3099,Identifying reduplicative words for Vietnamese word segmentation.,"This paper proposes a method based on linguistic word-formation rules and dictionaries for determining reduplicative words in Vietnamese. The key idea for identifying whether adjacent syllables in a text can form a reduplicative word based on its formation rules. For 2-syllable reduplicative words, this paper uses rules that describe the repeating and the opposing between pairs of initial consonants, rhymes and tones. Then the method is expanded to identify reduplicative words that have 3 or 4 syllables from 2-syllable ones for the Vietnamese word segmentation task. Experimental results showed that the F1-score was improved to 98.61% and that word segmentation errors were reduced significantly, 1.26%.",196,https://doi.org/10.1109/RIVF.2015.7049878,1,
3100,An Efficient Framework for Extracting Parallel Sentences from Non-Parallel Corpora.,"Automatically building a large bilingual corpus that contains millions of words is always a challenging task. In particular in case of low-resource languages, it is difficult to find an existing parallel corpus which is large enough for building a real statistical machine translation. However, comparable non-parallel corpora are richly available in the Internet environment, such as in Wikipedia, and from which we can extract valuable parallel texts. This work presents a framework for effectively extracting parallel sentences from that resource, which results in significantly improving the performance of statistical machine translation systems. Our framework is a bootstrapping-based method that is strengthened by using a new measurement for estimating the similarity between two bilingual sentences. We conduct experiment for the language pair of English and Vietnamese and obtain promising results on both constructing parallel corpora and improving the accuracy of machine translation from English to Vietnamese.",196,https://doi.org/10.3233/FI-2014-987,1,
3101,From Treebank Conversion to Automatic Dependency Parsing for Vietnamese.,"This paper presents a new conversion method to automatically transform a constituent-based Vietnamese Treebank into dependency trees. On a dependency Treebank created according to our new approach, we examine two state-of-the-art dependency parsers: the MSTParser and the MaltParser. Experiments show that the MSTParser outperforms the MaltParser. To the best of our knowledge, we report the highest performances published to date in the task of dependency parsing for Vietnamese. Particularly, on gold standard POS tags, we get an unlabeled attachment score of 79.08% and a labeled attachment score of 71.66%.",196,https://doi.org/10.1007/978-3-319-07983-7_26,1,
3102,Bootstrapping Phrase-based Statistical Machine Translation via WSD Integration.,"Beside the word order problem, word
choice is another major obstacle for machine translation. Though phrase-based
statistical machine translation (SMT) has
an advantage of word choice based on local context, exploiting larger context is an
interesting research topic. Recently, there
have been a number of studies on integrating word sense disambiguation (WSD)
into phrase-based SMT. The WSD score
has been used as a feature of translation. In
this paper, we will show that by bootstrapping WSD models using unlabeled data,
we can bootstrap an SMT system. Our experiments on English-Vietnamese translation showed that BLEU scores have been
improved significantly.",196,https://aclanthology.org/I13-1143/,1,
3103,Improving Moore's Sentence Alignment Method Using Bilingual Word Clustering.,"Sentence alignment plays an extremely important role in machine translation. Most of the hybrid approaches get either a bad recall or low precision. We tackle disadvantages of several novel sentence alignment approaches, which combine length-based and word correspondences. Word clustering is applied in our method in order to improve the quality of the sentence aligner, especially when dealing with the sparse data problem. Our approach overcomes the limits of previous hybrid methods and obtains both highly recall and reasonable precision rates.",196,https://doi.org/10.1007/978-3-319-02741-8_14,1,
3104,Word Sense Induction Using Correlated Topic Model.,"Word sense induction (WSI) is the problem of automatic identification of word senses given the corpus. This paper presents a method for solving WSI problem based on the context clustering approach. The idea behind this approach is that similar contexts indicate similar meanings. Specifically, we have successfully applied Correlated Topic Model (CTM) to partition contexts of a word into clusters, each representing a sense of that word. Different from some previous systems where a single model is built for all words, in our system, each word has its own model. Experimental result on the SemEval-2010 dataset shows that CTM is a strong tool for modelling the word's contexts. Our system has significantly better performance than all systems participated in the SemEval-2010 workshop. In comparison to the use of other topic models for WSI, our system can explore additional useful information which is the relationship between senses of a word. The prospect of using CTM for discovering the correlation between senses of multiple words is also discussed at the end of this paper.",196,https://doi.org/10.1109/IALP.2012.73,1,
3106,Building an Efficient Functional-Tag Labeling System for Vietnamese.,"Functional tags represent the role of syntactic constituents such as noun phrases and verb phrases in a tree. Functional-tag labeling has been studied for languages such as English and Chinese. In this paper, we present a new system for tagging Vietnamese sentences functionally. We used maximum entropy model for this task with six tree-based features. Besides, a new feature based on word cluster has also been made use of. Our experiments on Vietnamese tree bank showed that the system achieved a good performance and the word cluster feature brought a significant improvement.",196,https://doi.org/10.1109/KSE.2011.22,1,
3107,Two Entropy-Based Methods for Detecting Errors in POS-Tagged Treebank.,"This paper proposes two methods of employing conditional entropy to find errors and inconsistencies in tree bank corpora. These methods are based on two principles that high entropy implies high possibility of error and that entropy is reduced after error correction. The first method ranks error candidates using a scoring function based on conditional entropy. The second method uses beam search to find a subset of error candidates in which the change of labels leads to decreasing of conditional entropy. We carried out experiments with Vietnamese tree bank corpus at two levels of annotation including word segmentation and part-of-speech tagging. Our experiments showed that these methods detected high-error-density subsets of original error candidate sets. The size of these subsets is only one third the size of whole sets, while these subsets contain 80%-90% of errors in whole sets. Moreover, entropy was significantly reduced after error correction.",196,https://doi.org/10.1109/KSE.2011.30,1,
3108,Named Entity Recognition for Vietnamese.,"Named Entity Recognition is an important task but is still relatively new for Vietnamese. It is partly due to the lack of a large annotated corpus. In this paper, we present a systematic approach in building a named entity annotated corpus while at the same time building rules to recognize Vietnamese named entities. The resulting open source system achieves an F-measure of 83%, which is better compared to existing Vietnamese NER systems.",196,https://doi.org/10.1007/978-3-642-12101-2_22,1,
3110,An Empirical Study of Vietnamese Noun Phrase Chunking with Discriminative Sequence Models.,"This paper presents an empirical work
for Vietnamese NP chunking task. We
show how to build an annotation corpus of
NP chunking and how discriminative sequence models are trained using the corpus. Experiment results using 5 fold cross
validation test show that discriminative sequence learning are well suitable for Vietnamese chunking. In addition, by empirical experiments we show that the part
of speech information contribute significantly to the performance of there learning
models",196,https://aclanthology.org/W09-3402/,1,
3111,Building a Large Syntactically-Annotated Corpus of Vietnamese.,"Treebank is an important resource for
both research and application of natural
language processing. For Vietnamese, we
still lack such kind of corpora. This paper
presents up-to-date results of a project for
Vietnamese treebank construction. Since
Vietnamese is an isolating language and
has no word delimiter, there are many
ambiguities in sentence analysis. We systematically applied a lot of linguistic
techniques to handle such ambiguities.
Annotators are supported by automaticlabeling tools and a tree-editor tool. Raw
texts are extracted from Tuoi Tre
(Youth), an online Vietnamese daily
newspaper. The current annotation
agreement is around 90 percent.",196,https://aclanthology.org/W09-3035/,1,
3112,Improving a Lexicalized Hierarchical Reordering Model Using Maximum Entropy.,"In this paper, we present a reordering model
based on Maximum Entropy. This model
is extended from a hierarchical reordering
model with PBSMT (Galley and Manning,
2008), which integrates syntactic information
directly in decoder as features of MaxEnt
model. The advantages of this model are (1)
maintaining the strength of phrase based approach with a hierarchical reordering model,
(2) many kinds of linguistic information integrated in PBSMT as arbitrary features of MaxEntropy model. The experiment results with
English-Vietnamese pair showed that our approach achieves improvements over the system which use a lexical hierarchical reordering model (Galley and Manning, 2008).",196,https://aclanthology.org/2009.mtsummit-papers.10,1,
3113,A Tree-to-String Phrase-based Model for Statistical Machine Translation.,"Though phrase-based SMT has achieved high
translation quality, it still lacks of generalization ability to capture word order differences
between languages. In this paper we describe
a general method for tree-to-string phrasebased SMT. We study how syntactic transformation is incorporated into phrase-based
SMT and its effectiveness. We design syntactic transformation models using unlexicalized
form of synchronous context-free grammars.
These models can be learned from sourceparsed bitext. Our system can naturally make
use of both constituent and non-constituent
phrasal translations in the decoding phase. We
considered various levels of syntactic analysis ranging from chunking to full parsing.
Our experimental results of English-Japanese
and English-Vietnamese translation showed
a significant improvement over two baseline
phrase-based SMT systems",196,https://aclanthology.org/W08-2119/,1,
3114,Online Structured Learning for Semantic Parsing with Synchronous and lambda-Synchronous Context Free Grammars.,"We formulate semantic parsing as a parsing problem on a synchronous context free grammar (SCFG) which is automatically built on the corpus of natural language sentences and the representation of semantic outputs. We then present an online learning framework for estimating the synchronous SCFG grammar. In addition, our online learning methods for semantic parsing problems are also extended to deal with the case, in which the semantic representation could be represented under lambda-calculus. Experimental results in the domain of semantic parsing show advantages in comparison with previous works.",196,https://doi.org/10.1109/ICTAI.2008.96,1,
3115,Reordering pPhrase-based machine translation over chunks.,"The paper presents a new method for reordering in phrase based statistical machine translation (PBMT). Our method is based on previous chunk-level reordering methods for PBMT. First, we parse the source language sentence to a chunk tree, according to the method developed by [16]. Second, we apply a series of transformation rules which are learnt automatically from the parallel corpus to the chunk tree over chunk level. Finally, we integrate a global reordering model directly in a decoder as a graph of phrases, and solve the overlapping phrase and chunk problem. The experimental results with English-Vietnamese pairs show that our method outperforms the baseline PBMT in both accuracy and speed.",196,https://doi.org/10.1109/RIVF.2008.4586342,1,
3116,A Syntactic Transformation Model for Statistical Machine Translation.,"We describe a syntactic transformation model based on the probabilistic context-free grammar. This model is trained by using bilingual corpus and a broad coverage parser of the source language. Then we present two methods to solve the word-order problem using the transformational model. The first method deals with this problem in the preprocessing phase. There is no reordering in the decoding phase. The second method employs the syntactic transformation model in the decoding phase for phrase reordering within chunks. Speed is an advantage of this method. We considered translation from English to Vietnamese and from English to French. Our experiments showed significant BLEU-score improvements in comparison with Pharaoh, a state-of-the-art phrase-based SMT system.",196,https://doi.org/10.1142/S0219427907001603,1,
3117,A Multilingual Dependency Analysis System Using Online Passive-Aggressive Learning.,"This paper presents an online algorithm for
dependency parsing problems. We propose
an adaptation of the passive and aggressive
online learning algorithm to the dependency
parsing domain. We evaluate the proposed
algorithms on the 2007 CONLL Shared
Task, and report errors analysis. Experimental results show that the system score is better than the average score among the participating systems.",196,https://aclanthology.org/D07-1126/,1,
3118,Improving phrase-based statistical machine translation with morphosyntactic transformation.,"We present a phrase-based statistical machine translation approach which uses linguistic analysis in the preprocessing phase. The linguistic analysis includes morphological transformation and syntactic transformation. Since the word-order problem is solved using syntactic transformation, there is no reordering in the decoding phase. For morphological transformation, we use hand-crafted transformational rules. For syntactic transformation, we propose a transformational model based on a probabilistic context-free grammar. This model is trained using a bilingual corpus and a broad-coverage parser of the source language. This approach is applicable to language pairs in which the target language is poor in resources. We considered translation from English to Vietnamese and from English to French. Our experiments showed significant BLEU-score improvements in comparison with Pharaoh, a state-of-the-art phrase-based SMT system.",196,https://doi.org/10.1007/s10590-007-9022-1,1,
3119,iK-means: an improvement of the iterative k-means partitioning algorithm.,"The evolutionary processes vary among sites of an alignment that strongly affect the accuracy of phylogenetic tree reconstruction. Partitioning an alignment into sub-alignments of sites such that the evolutionary processes at sites in the same sub-alignment are highly similar is a proper strategy. Gene features might be used as reasonable indicators to partition an alignment. However, the gene feature information is not always available or efficient Computational partitioning methods like iterative k-means has been proposed to automatically partition sites into groups based on the similarity of evolutionary rates of sites. Despite obtaining compelling results in terms of AICc and BIC measurements, the k-means method forms a group of all and only invariant sites that results in bias/wrong phylogenetic trees. In this paper, we improve the k-means algorithm by re-classifying invariant sites into different sub-alignments based on their likelihood values. Experimental results on simulated and empirical DNA datasets showed that the new method, called iK-means, overcame the pitfall of the K-means method, consequently, helps improve the quality of the partitioning sub-alignments. We recommend using the iK-means method to level up the accuracy in inferring phylogenetic trees.",197,https://doi.org/10.1109/KSE50997.2020.9287221,1,
3120,An Efficient Algorithm to Extract Control Flow-Based Features for IoT Malware Detection.,"Control flow-based feature extraction method has the ability to detect malicious code with higher accuracy than traditional text-based methods. Unfortunately, this method has been encountered with the NP-hard problem, which is infeasible for the large-sized and high-complexity programs. To tackle this, we propose a control flow-based feature extraction dynamic programming algorithm for fast extraction of control flow-based features with polynomial time O(⁠N2⁠), where N is the number of basic blocks in decompiled executable codes. From the experimental results, it is demonstrated that the proposed algorithm is more efficient and effective in detecting malware than the existing ones. Applying our algorithm to an Internet of Things dataset gives better results on three measures: Accuracy = 99.05%, False Positive Rate = 1.31% and False Negative Rate = 0.66%.",199,https://doi.org/10.1093/comjnl/bxaa087,1,
3121,USLTG: Test Case Automatic Generation by Transforming Use Cases.,"This paper proposes a transformation-based method to automatically generate functional test cases from use cases named USLTG (Use case Specification Language (USL)-based Test Generation). We first focus on developing a modeling language named Test Case Specification Language (TCSL) in order to express test cases. Test cases in TCSL can contain detailed information including test steps, test objects within steps, actions of test objects, and test data. Such information is often ignored in currently available test case specifications. We then aim to generate test cases in a TCSL model by a transformation from use cases that are represented by a USL. The USLTG transformation includes three main steps in generating (1) scenarios, (2) test data, and (3) a TCSL model. Within our transformation, the OCL solver is employed in order to build system snapshots as the part of test cases and to identify other test data. We applied our method to two case studies and evaluated our method by comparing it with other recent works.",199,https://doi.org/10.1142/S0218194019500414,1,
3122,A Novel Framework to Classify Malware in MIPS Architecture-Based IoT Devices.,"Malware on devices connected to the Internet via the Internet of Things (IoT) is evolving and is a core component of the fourth industrial revolution. IoT devices use the MIPS architecture with a large proportion running on embedded Linux operating systems, but the automatic analysis of IoT malware has not been resolved. We proposed a framework to classify malware in IoT devices by using MIPS-based system behavior (system call—syscall) obtained from our F-Sandbox passive process and machine learning techniques. The F-Sandbox is a new type for IoT sandbox, automatically created from the real firmware of the specialized IoT devices, inheriting the specialized environment in the real firmware, therefore creating a diverse environment for sandboxing as an important characteristic of IoT sandbox. This framework classifies five families of IoT malware with F1-Weight = 97.44%.",199,https://doi.org/10.1155/2019/4073940,1,
3123,CFDVex: A Novel Feature Extraction Method for Detecting Cross-Architecture IoT Malware.,"The widespread adoption of Internet of Things (IoT) devices built on different architectures gave rise to the creation and development of multi-architecture malware for mass compromise. Cross-architecture malware detection plays an important role in detecting malware early on devices using new or strange architectures. Prior knowledge of malware detection on traditional architectures can be inherited for the same task on new and uncommon ones. Basing on CFD and Vex intermediate representation, we propose a feature selection method to detect cross-architecture malware, called CFDVex. Experimental evaluation of the proposed approach on our large IoT dataset achieved good results for cross-architecture malware detection. We only trained a SVM model by Intel 80386 architecture samples, our method could detect the IoT malware for the MIPS architecture samples with 95.72% of accuracy and 2.81% false positive rate.",199,https://doi.org/10.1145/3368926.3369702,1,
3124,A Transformation-Based Method for Test Case Automatic Generation from Use Cases.,"This paper proposes a method to automatically generate system test cases from use cases. Within our method, we take use cases as source model that are represented by the use case modeling language USL. The target model for test cases is precisely specified using a modeling language named Test Case Specification Language (TCSL). Comparing with current approaches, generated test cases within our approach contain more detailed information including test steps, test objects within steps, actions of test objects, and test data. We focus on defining a transformation between the USL source model and the TCSL target model. We realized our method with a support tool based on the OCL solver of USE and the model transformation framework in Eclipse.",199,https://doi.org/10.1109/KSE.2018.8573372,1,
3125,A New Approach to Embedded Software Optimization Based on Reverse Engineering.,"Optimizing embedded software is a problem having scientific and practical signification. Optimizing embedded software can be done in different phases of the software life cycle under different optimal conditions. Most studies of embedded software optimization are done in forward engineering and these studies have not given an overall model for the optimization problem of embedded software in both forward engineering and reverse engineering. Therefore, in this paper, we propose a new approach to embedded software optimization based on reverse engineering. First, we construct an overall model for the embedded software optimization in both forward engineering and reverse engineering and present a process of embedded software optimization in reverse engineering. The main idea of this approach is that decompiling executable code to source code, converting the source code to models and optimizing embedded software under different levels such as source code and model. Then, the optimal source code is recompiled. To develop this approach, we present two optimization techniques such as optimizing power consumption of assembly programs based on instruction schedule and optimizing performance based on alternating equivalent expressions.",199,https://doi.org/10.1587/transinf.2014EDP7152,1,
3126,Transformation Rule Learning without Rule Templates: A Case Study in Part of Speech Tagging.,"Part of speech (POS) tagging is an important problem and is one of the first steps included in many tasks in natural language processing. It affects directly on the accuracy of many other problems such as Syntax Parsing, WordSense Disambiguation, and Machine Translation. Stochastic models solve this problem relatively well, but they still make mistakes. Transformation-based learning (TBL) is a solution which can be used to improve stochastic taggers by learning a set of transformation rules. However, its rule learning algorithm has the disadvantages that rule templates must be prepared by hand and only rules are instances of rule templates can be generated. In this paper, we propose a model to learn transformation rules without rule templates. This model considers the rule learning problem as a feature selection problem. Experiments on PennTree Bank showed that the proposal model reduces errors of stochastic taggers with some tags.",199,https://doi.org/10.1109/ALPIT.2008.73,1,
3127,USL: A Domain-Specific Language for Precise Specification of Use Cases and Its Transformations.,"A use case model is often represented by a UML use case diagram and loosely structured textual descriptions. The use case model expressed in such a form contains ambiguous and imprecise parts. This prevents integrating it into model-driven approaches, where use case models are often taken as the source of transformations. In this paper, we introduce a domain-specific language named the Use case Specification Language (USL) to precisely specify use cases. We define the abstract syntax of USL using a metamodel together with OCL wellformedness rules and then provide a graphical concrete syntax for
the usability goal. We also define a precise semantics for USL by mapping USL models to Labelled Transition Systems (LTSs). It opens a possibility to transform USL models to software artifacts such as test cases and design models. We focus on a transformation from a USL model to a template-based use case description in order to illustrate our method. A language evaluation of USL is also performed in this paper.",199,https://doi.org/10.31449/inf.v42i3.2253,1,
3128,USL: Towards Precise Specification of Use Cases for Model-Driven Development.,"Use cases have been widely employed as an efficient means to capture and structure software requirements. A use case model is often represented by a loose combination between a UML use case diagram and a textual description in natural language. The use case model expressed in such a form often contains ambiguous and imprecise parts. This prevents integrating it into model-driven approaches, where use case models are often taken as the source of transformations. This paper introduces a domain specific language named the Use case Specification Language (USL) to precisely specify use cases with two main features: (1) The USL has a concrete syntax in graphical form that allows us to achieve the usability goal; (2) The precise semantics of USL that is defined by mapping the USL to a Labelled Transition System (LTS) opens a possibility for transformations from USL models to other artifacts such as test cases and analysis class models.",199,https://doi.org/10.1145/3155133.3155194,1,
3129,A Context - Aware Mobile Learning Adaptive System for Supporting Foreigner Learning English.,"This paper represents a personalized context - aware mobile learning architecture for supporting student to learn English as foreign language in order to prepare for TOEFL test. It provides adaptive content for different learners based on context - awareness. In our model, the context includes location, time, manner as well as learner's knowledge. Through suggested topics as well as test questions, the learners will be supported adaptive content meeting their demands as well as their knowledge. Besides, this paper also describes CAMLES system prototype that allows the learner to learn adaptive materials for TOEFL test anytime in anywhere with mobile phone.",201,https://doi.org/10.1109/RIVF.2010.5632316,1,
3130,ACGs: Adaptive Course Generation System - An Efficient Approach to Build E-Learning Course.,"Recently, adaptive course generation has been focusing by several researchers. We have built ACGs system to create adaptive courses for each learner based on evaluating demand, ability, background and learning style of them. ACGs is also separated content from model of course and extended learning objects in order to using intelligent agent to help them choice effectively adaptive course.",201,https://doi.org/10.1109/CIT.2006.32,1,
3132,Proactive Eavesdropping via Jamming in NOMA Network.,"Recently, non-orthogonal multiple access (NOMA) has been considered as a promising technique in 5G network, and many investigations have addressed on the physical layer security to improve the security performance. In this paper, an alternative problem, where the eavesdropping process is considered as a legal activity, will be analyzed to track suspicious communications. More specifically, we study a wireless surveillance system in which the legitimate monitor is equipped with multi-antenna to overhear the messages between the suspicious receiver and the suspicious transmitter. Suspicious users are grouped into pairs and use the NOMA technique to transmit signals to a suspicious base station. Meanwhile, the legitimate monitor (LM) simultaneously transmits jamming signals, listens to suspicious links, decode-and-forward (DF) the eavesdropped information to the legitimate eavesdropper (LE). Based on the proposed mechanism, we investigate the power allocation policies for jamming signals of the legitimate monitor under deterministic and non-deterministic interference channel. Accordingly, we derive the closed-form expression of the successful eavesdropping probability for the best and the worst user to evaluate the system performance. Monte Carlo simulations are provided to verify our analytical results.",206,https://doi.org/10.1109/ACCESS.2021.3131906,1,
3133,Secrecy Outage Probability and Fairness of Packet Transmission Time in a NOMA System.,"In this paper, we analyze the secrecy outage probability (SOP) and the fairness of average packet transmission time for a non-orthogonal multiple access (NOMA) system which consists of a base station (BS) and two legal NOMA users in the presence of an eavesdropper (Eve). In order to extract the superimposed signal, the Eve is considered in two modes, i.e., successive interference cancellation (SIC) mode and parallel interference cancellation (PIC) mode. Accordingly, we analyze the secrecy performance of the considered system by deriving a new exact expression for SOP. Furthermore, the optimal power allocation between two legal users is determined such that the average transmission time from BS to two legitimate users are approximately equal to achieve the fairness of average packet transmission time. Monte Carlo simulations are provided to verify our analytical results.",206,https://doi.org/10.1109/ACCESS.2020.2989531,1,
3134,DF-SWin: Sliding windows for multi-sensor data fusion in wireless sensor networks.,"When using multiple sensor nodes in wireless sensor networks (WSNs) for monitoring (or measuring) parameters of the target and sending the result to the base station (BS), data redundancy is an inevitable problem. The measured data often contains the same information, and sending redundant data to BS causes the waste of energy of sensor nodes and the risk of congestion. Multi-sensor data fusion in WSNs is a technology of gathering and processing data applied from node to BS. It improves the performance of surveillance systems by allowing the obtained sensed information from multiple sensor nodes aggregated to one unified format data packet to send to BS to make decision. In this paper, we propose a solution namely DF-SWin for sampling sensor node in the cluster to optimize the energy conservation of sensor nodes in clusters and cluster head.",206,https://doi.org/10.1109/KSE.2017.8119434,1,
3135,SSTBC: Sleep scheduled and tree-based clustering routing protocol for energy-efficient in wireless sensor networks.,"Since sensor nodes are battery power constrained devices in wireless sensor network (WSN), so how to use the energy of sensor nodes efficiently to prolong the network lifetime is a chief challenge for designing routing protocols. To solve this problem, in this paper, we propose sleep scheduled and tree-based clustering approach routing algorithm (SSTBC) for energy-efficient in WSN. SSTBC preserves energy by turning off radio (entering sleep mode) of either impossible or unnecessary nodes, which observe almost the same information, base on their location information to remove redundant data. In addition, to further reduce energy dissipation of communication in network, we build minimum spanning tree with the root as the cluster head (CH) from active nodes in a cluster to forward data packets to base station (BS). Our simulation results show that the network lifetime with using of our proposed protocol can be improved about 250% and 23% compared to low-energy adaptive clustering hierarchy (LEACH) and power-efficient gathering in sensor information system (PEGASIS), respectively.",206,https://doi.org/10.1109/RIVF.2015.7049896,1,
3137,Text classification based on semi-supervised learning.,"In this paper, we present our solution and experimental results of the application of semi-supervised machine learning techniques and the improvement of SVM algorithm to build text classification applications. Firstly, we create a features model which is based on labeled data, and then we will be improved it by the unlabeled data. The technique that is to be added a label into new data is based on binary classification. Our experiment is implemented on three data layers which are extracted from papers in three topics sports, entertainment and education on VNEXPRESS.NET. We experimented and compared the accuracy of the classification results between before and after improve features model through semi-supervised machine learning method and classification algorithm based on SVM model. Experiments show that classification quality is enhanced after improvement features model.",219,https://doi.org/10.1109/SOCPAR.2013.7054133,1,
3138,Initialising PSO with randomised low-discrepancy sequences: the comparative results.,"In this paper, we investigate the use of some wel-known randomised low-discrepancy sequences (Halton, Sobol, and Faure sequences) for initialising particle swarms. We experimented with the standard global-best particle swarm algorithm for function optimization on some benchmark problems, using randomised low-discrepancy sequences for initialisation, and the results were compared with the same particle swarm algorithm using uniform initialisation with a pseudo-random generator. The results show that, the former initialisation method could help the particle swarm algorithm improve its performance over the latter on the problems tried. Furthermore the comparisons also indicate that the use of different randomised low-discrepancy sequences in the initialisation phase could bring different effects on the performance of PSO.",219,https://doi.org/10.1109/CEC.2007.4424717,1,
3139,"Liver Cancer Detection by a Simple, Inexpensive and Effective Immunosensor with Zinc Oxide Nanoparticles.","Regular monitoring of blood α-fetoprotein (AFP) and/or carcino-embryonic antigen (CEA) levels is important for the routine screening of liver cancer. However, AFP and CEA have a much lower specificity than des-γ-carboxyprothrombin (DCP) to detect liver cancer. Therefore, the study reported here was designed, to develop a screen-printed DCP immunosensor incorporating zinc oxide nanoparticles, for accurate determination of DCP. The designed immunosensor shows low detection limits for the detection of DCP: 0.440 ng/mL (based on impedance measurement), 0.081 ng/mL (based on real part of impedance measurement) and 0.078 ng/mL (based on imaginary part of impedance measurement), within the range of 3.125 ng/mL to 2000 ng/mL. In addition, there was little interference to DCP determination by molecules such as Na+, K+, Ca2+, Cl−, glucose, urea, and uric acid. It is therefore concluded that the DCP immunosensor developed and reported here is simple, inexpensive and effective, and shows promise in the rapid screening of early-stage liver cancer at home with a point-of-care approach",222,https://doi.org/10.3390/s151129408,1,
3140,Evaluation of Sampling-Based Ensembles of Classifiers on Imbalanced Data for Software Defect Prediction Problems.,"Defect prediction in software projects plays a crucial role to reduce quality-based risk and increase the capability of detecting faulty program modules. Hence, classification approaches to anticipate software defect proneness based on static code characteristics have become a hot topic with a great deal of attention in recent years. While several novel studies show that the use of a single classifier causes the performance bottleneck, ensembles of classifiers might effectively enhance classification performance compared to a single classifier. However, the class imbalance property of software defect data severely hinders the classification efficiency of ensemble learning. To cope with this problem, resampling methods are usually combined into ensemble models. This paper empirically assesses the importance of sampling with regard to ensembles of various classifiers on imbalanced data in software defect prediction problems. Extensive experiments with the combination of seven different kinds of classification algorithms, three sampling methods, and two balanced data learning schemata were conducted over ten datasets. Empirical results indicated the positive effects of combining sampling techniques and the ensemble learning model on the performance of defect prediction regarding datasets with imbalanced class distributions.",225,https://doi.org/10.1007/s42979-020-0119-4,1,
3141,Binary teaching-learning-based optimization algorithm with a new update mechanism for sample subset optimization in software defect prediction.,"Software defect prediction has gained considerable attention in recent years. A broad range of computational methods has been developed for accurate prediction of faulty modules based on code and design metrics. One of the challenges in training classifiers is the highly imbalanced class distribution in available datasets, leading to an undesirable bias in the prediction performance for the minority class. Data sampling is a widespread technique to tackle this problem. However, traditional sampling methods, which depend mainly on random resampling from a given dataset, do not take advantage of useful information available in training sets, such as sample quality and representative instances. To cope with this limitation, evolutionary undersampling methods are usually used for identifying an optimal sample subset for the training dataset. This paper proposes a binary teaching–learning- based optimization algorithm employing a distribution-based solution update rule, namely BTLBOd, to generate a balanced subset of highly valuable examples. This subset is then applied to train a classifier for reliable prediction of potentially defective modules in a software system. Each individual in BTLBOd includes two vectors: a real-valued vector generated by the distribution-based update mechanism, and a binary vector produced from the corresponding real vector by a proposed mapping function. Empirical results showed that the optimal sample subset produced by BTLBOd might ameliorate the classification accuracy of the predictor on highly imbalanced software defect data. Obtained results also demonstrated the superior performance of the proposed sampling method compared to other popular sampling techniques.",225,https://doi.org/10.1007/s00500-018-3546-6,1,
3142,Towards data variation trends recommendation.,"Present study on recommender systems mainly focuses on the logical nature of the existence or non-existence of a priority relationship between the user and data item, regardless of the ratio or implicative relationship based on statistics between users and data items in a particular context. Therefore, this report proposes a new approach to recommender systems based on data variation trends; such method will help form a new approach to recommender systems on basis of knowledge available in the form of implicity by computation of partial derivatives for interestingness measurements. In addition, experiments aim at evaluating the effectiveness of the proposed model with traditional models based on using MSWeb dataset as empirical data, comparing and discussing the results obtained from the proposed model.",225,https://doi.org/10.1145/3310986.3311015,1,
3143,Experimental Study on Software Fault Prediction Using Machine Learning Model.,"Faults are the leading cause of time consuming and cost wasting during software life cycle. Predicting faults in early stage improves the quality and reliability of the system and also reduces cost for software development. Many researches proved that software metrics are effective elements for software fault prediction. In addition, many machine learning techniques have been developed for software fault prediction. It is important to determine which set of metrics are effective for predicting fault by using machine learning techniques. In this paper, we conduct an experimental study to evaluate the performance of seven popular techniques including Logistic Regression, K-nearest Neighbors, Decision Tree, Random Forest, Naïve Bayes, Support Vector Machine and Multilayer Perceptron using software metrics from Promise repository dataset usage. Our experiment is performed on both method-level and class-level datasets. The experimental results show that Support Vector Machine archives a higher performance in class-level datasets and Multilayer Perception produces a better accuracy in method-level datasets among seven techniques above.",225,https://doi.org/10.1109/KSE.2019.8919429,1,
3144,Applying the meta-heuristic algorithms for mutation-based test data generation for Simulink models.,"Test data generation is one of the most important steps in testing process in order to reveal the faults in software. This activity is time-consuming and labor intensive. With the development of modeling tools such as Simulink, testing is able to early realize at design level. Therefore, it is desirable to seek the effective techniques for automating the testing process for Simulink models in order to make sure that the accurateness of systems which are built from these models. Mutation testing could be used as criterion to generate test data for Simulink models. In this paper, we evaluate the application of different meta-heuristic algorithms, like genetic algorithm, simulated annealing and artificial immune system, to optimize mutation-based test data generation in terms of killing the number of generated mutants for Simulink models. We discuss the effectiveness of these approaches and propose also an improvement of the genetic algorithm. These approaches have been applied to some different case studies and the obtained results are very promising.",225,https://doi.org/10.1145/2676585.2676617,1,
3145,Mutation Operators for Simulink Models.,"Mutation testing is a fault-based technique widely used for testing software. Particularly, it allows the effectiveness of a set of test data to be evaluated in terms of the ability to reveal faults. Nowadays, many industrial complex systems are more and more developed. Such systems require more testing activities to ensure a good quality. Simulink is one of the most popular tools to develop this kind of systems. In this paper, we focus on studying the application of mutation testing technique to Simulink models. We propose a set of mutation operators by basing on investigating common faults in Simulink models. A process of mutation testing for Simulink models is also presented. Some first experimentations show the promising outcomes.",225,https://doi.org/10.1109/KSE.2012.22,1,
3146,Incorporating dynamic features into minimum generation error training for HMM-based speech synthesis.,"This paper describes new methods of minimum generation error (MGE) training in HMM-based speech synthesis by introducing the error component of dynamic features into the generation error function. We propose two methods for setting the weight associated with the additional error component. In fixed weighting approach, this weight is kept constant over the course of speech. In adaptive weighting approach, it is adjusted according to the degree of dynamic of speech segments. Objective evaluation shows that the newly derived MGE criterion with adaptive weighting method obtains comparable performance on static feature and better performance on delta feature compared to the baseline MGE criterion. Subjective evaluation exhibits an improvement in the quality of synthesized speech with the proposed technique. The newly derived criterion improves the capability of the HMMs in capturing dynamic properties of speech without increasing the computational complexity of training process compared to the baseline criterion.",232,https://doi.org/10.1109/ISCSLP.2012.6423486,1,
3147,Robust Discriminant Network for Gait Recognition on Low-Resolution Input.,"Gait recognition is one of the main challenges in surveillance systems due to the high diversity of possible factors affecting the accuracy. We propose a convolutional neural network addressing this task that requires only inputs of small resolution. Our method can also adapt with scenarios where there are many identified subjects in the system’s database but each subject has very few gait samples for generalizing a recognition model. Our network can be trained end-to-end with an optimization strategy that processes just a small portion of training data while still provides good results. The experiments are performed on the largest and most challenged OU-MVLP Gait Database under 4 different major camera angles (0∘, 30∘, 60∘, 90∘) and demonstrate a competitive performance with respect to related studies.",235,https://doi.org/10.1007/978-3-030-63007-2_34,1,
3148,Recommendation with quantitative implication rules.,"Association rules based recommendation is one of approaches to develop recommendation systems. However, such systems just focus on binary dataset, whereas many datasets are in the quantitative form. There are many solutions proposed for this problem such as combining the association rules mining with fuzzy logic, binarizing quantitative data, etc. These proposals have contributed to improving the performance of traditional association rules mining, however, they have to deal with the trade-off between the processing performance and the loss of information. In this paper, we propose a new approach to make recommendations based on implication rules. The experimental results show that our proposed solution can be implemented on quantitative dataset well as well as improve the accuracy and performance of the recommendation systems.",235,https://doi.org/10.4108/eai.13-7-2018.156837,1,
3149,A Scalable IoT Video Data Analytics for Smart Cities.,"The smart city is a comprehensive application of information resources and a high degree of information technology integration. With the technical support from IoT (Internet of things), smart city need to have three features of being instrumented, interconnected and intelligent. IoT provides the ability to manage, remotely monitor and control devices from massive streams of real-time data.Our model offers a scalable IoT video data analytics applications for Smart cities to end users, who can exploit scalability in both data storage and processing power to execute analysis on large or complex datasets. This model provides data analytics programming suites and environments in which developers and researchers can design scalable analytics services and applications. A cloud/edge-based automated video analysis system to process large numbers of video streams, where the underlying infrastructure is able to scale based on the number of camera devices and easy to integrate analytic application. The system automates the video analysis process and reduces manual intervention. The design of our model is developed to be easily extended for new kinds of IoT devices, message routing and queueing, and data analytics, to permit specific application to be programmed via the paradigm to be flexible yet simple.",235,https://doi.org/10.4108/eai.13-7-2018.163136,1,
3150,Measurement of Human Gait Symmetry using Body Surface Normals Extracted from Depth Maps.,"In this paper, we introduce an approach for measuring human gait symmetry where the input is a sequence of depth maps of subject walking on a treadmill. Body surface normals are used to describe 3D information of the walking subject in each frame. Two different schemes for embedding the temporal factor into a symmetry index are proposed. Experiments on the whole body, as well as the lower limbs, were also considered to assess the usefulness of upper body information in this task. The potential of our method was demonstrated with a dataset of 97,200 depth maps of nine different walking gaits. An ROC analysis for abnormal gait detection gave the best result ( AUC=0.958 ) compared with other related studies. The experimental results provided by our method confirm the contribution of upper body in gait analysis as well as the reliability of approximating average gait symmetry index without explicitly considering individual gait cycles for asymmetry detection. ",235,https://doi.org/10.3390/s19040891,1,
3151,Improved collaborative filtering recommendations using quantitative implication rules mining in implication field.,"Collaborative filtering recommendation based on association rule mining has become a research trend in the field of recommender systems. However, most research results only focus on binary data, whereas in practice sets of transactions are usually quantitative data. Moreover, association rule mining algorithms are designed to focus on optimizing for basket analysis, so that in order to better serve for recommendation, they need to be adjusted. Therefore, a solution for recommender systems to deal with association rules on both binary and quantitative data as well as improve the quality of recommendation based on the rule set is a challenge today. This paper proposes a new approach to improve the accuracy, the performance and the time of recommendation by the model based on quantitative implication rules mining in the implication field.",235,https://doi.org/10.1145/3310986.3310996,1,
3152,Direction-Free Person Identification with Distance Transform and Random Forest.,"The problem of person identification is usually dealt with by explicitly extracting gait-related characteristics from input silhouettes. This paper describes another approach that does not perform such pre-defined and/or hand-crafted features to highlight useful properties. Unlike related works where the task of gait cycle extraction is required, our method directly employs a sliding window to process the temporal factor. In addition, we also introduce a scheme of combining identification results obtained under different camera directions in order to improve the system confidence. Our experiments on the CASIA-A gait dataset demonstrated the potential of the proposed approach and the obtained results were promising compared with related methods.",235,https://doi.org/10.1109/RIVF.2019.8713735,1,
3153,Matching-based Depth Camera and Mirrors for 3D Reconstruction.,"Reconstructing 3D object models is playing an important role in many applications in the field of computer vision. Instead of employing a collection of cameras and/or sensors as in many studies, this paper proposes a simple way to build a cheaper system for 3D reconstruction using only one depth camera and 2 or more mirrors. Each mirror is equivalently considered as a depth camera at another viewpoint. Since all scene data are provided by only one depth sensor, our approach can be applied to moving objects and does not require any synchronization protocol as with a set of cameras. Some experiments were performed on easy-to-evaluate objects to confirm the reconstruction accuracy of our proposed system.",235,http://arxiv.org/abs/1908.06342,1,
3154,Collaborative filtering recommendation with threshold value of the equipotential plane in implication field.,"Collaborative filtering is one of the most popular and effective techniques available today in the recommender system. However, most of them use symmetric similarity measures. Therefore, the default effect and the role of the pair of users are the same, but in practice this may not be true. In addition, they only logically demonstrate the existence of a priority relationship between two users rather than the level of the relationship in practice. In this paper, we propose a new approach for the collaborative filtering based on the variation analysis of the implication index. An asymmetric measure is developed which can be used to rank or filter information based on the variation of the implication index by a counter-example. This measure provides a meaningful recommendation with a certain level of implication. Experimental results shown that the proposed approach can overcome the drawbacks in the traditional recommender systems.",235,https://doi.org/10.1145/3184066.3184072,1,
3155,Collaborative filtering recommender system base on the interaction multi-criteria decision with ordered weighted averaging operator.,"In the recommender system, the most important is the decision-making solutionto consulte for user. Depending on the type and size of data stored, decision-making will always be improved to produce the best possible result.. The main task in implementing the model is to use methods to find the most valuable product or service for the user. In this paper, we propose a new approach to building a multi-user based collaborative filtering model using the interaction multi-criteria decision with ordered weighted averaging operator. This model demonstrates the synergy and interplay between user criteria for decision making. The model was evaluated through experimentation with the multirecsys tool on three datasets: MovieLense 100K, MSWeb and Jester5k. The experiment illustrated the model comparison with some other interactive multi-criteria counseling methods that have been researchedon both sparse datasets and thick datasets. In addition, the model is compared and evaluated with item-base collaborative filtering model using the interaction multi-criteria decision with ordered weighted averaging operator on two types of datasets. Consultancy results of the proposed model are quite effective compared to some traditional consulting models and some models with other operator. This counseling model can be applied well in a variety of contexts, especially in the case of sparse data, this model will give result in improved counseling. In addition, with the above method, the user-base model is always more efficient than item-base on all datasets.",235,https://doi.org/10.1145/3184066.3184075,1,
3156,Hybrid recommendation based on implicative rating measures.,"This paper proposes the implicative rating measures and the hybrid recommendation model based on those measures to suggest a list of top N items to an active user. The proposed recommendation model is the combination of the user-based collaborative filtering approach and the association rule based approach. This hybrid model are compared to some existing models such as the popular model, the item based collaborative filtering using the Jaccard measure, the user based collaborative filtering using the Jaccard measure, the latent factor model, and the association rule based model using the Confidence measure on two datasets CourseRegistration and MSWeb. The experimental results show that the performance of the proposed model is better than that of the compared models.",235,https://doi.org/10.1145/3184066.3184076,1,
3157,Design and optimization of the Automatic Brown PlantHopper (BPH) light trap surveillance network.,"Communicating coverage of automatic BPH light trap surveillance network characterizes how well an area is monitored or tracked by automatic light traps. Connectivity is an important required that shows how nodes in a automatic BPH light trap surveillance network (BSNET) can effectively communicate. Some areas in the deployment region are more important than other areas and need to be covered. In this paper, we propose a new approach based hexagonal cellular automata to find the automatic light trap node distribution. This approach are ensure the deployment region that maximizes the coverage area of BSNET, and preserve connectivity between nodes provided.",235,https://doi.org/10.4108/eai.2-5-2016.151211,1,
3158,Clustering the objective interestingness measures based on tendency of variation in statistical implications.,"In recent years, the research cluster of objective interestingness measures has rapidly developed in order to assist users to choose the appropriate measure for their application. Researchers in this field mainly focus on three main directions: clustering based on the properties of the measures, clustering based on the behavior of measures and clustering tendency of variation in statistical implications. In this paper we propose a new approach to cluster the objective interestingness measures based on tendency of variation in statistical implications. In this proposal, we built the statistical implication data of 31 objective interestingness measures based on the examination of the partial derivatives on four parameters. From this data, two distance matrices of interestingness measures are established based on Euclidean and Manhattan distance. The similarity trees are built based on distance matrix that gave results of 31 measures clustering with two different clustering thresholds.",235,https://doi.org/10.4108/eai.2-5-2016.151212,1,
3159,Classification of objective interestingness measures.,"The creation of the interestingness measures for evaluating the quality of the association rule - based knowledge plays an important role in the post-processing of the Knowledge Discovery from Databases. More and more interestingness measures are proposed by two approaches (subjective assessment and objective assessment), studying the properties or the attributes of the interestingness measures is important in understanding the nature of the objective interestingness measures. In this paper, we focus primarily on the objective interestingness measures to obtain a general view of recent researches on the nature of the objective interestingness measures, as well as complete a new classification on 109 selected objective interestingness measures on 6 criterions (independence, equilibrium, symmetry, variation, description, and statistics).",235,https://doi.org/10.4108/eai.12-9-2016.151678,1,
3160,A Synchronous Network for Brown Planthopper Surveillance Based on Hexagonal Cellular Automata.,"The paper proposes a new approach to model the Brown Planthopper surveillance as a synchronous network, network consists of components running simultaneously, based on hexagonal cellular automata. In the surveillance network, working space of hoppers is divided as a hexagonal cell system of which cell is a node of a graph G and two neighborhood cells compose an edge of the graph. Sensor nodes are deployed at some cells to measure surrounding conditions as well as hopper density. Simulation results of the hopper surveillance network model with data collection in Cantho, Mekong Delta may provide some useful information in managing pest insects as well as in sensing and collecting data from observation wireless sensor network.",235,https://doi.org/10.1007/978-3-319-46909-6_10,1,
3161,Modeling and Optimizing of Connections for Dynamic Sensor Fields Based on BT-Graph.,"In this paper, we propose a new approach to model and optimize the dynamic sensor field for both internal network connections and LEO satellite connection based on BT-Graph. Due to the shift of LEO satellite’s orbit at each revolution, a dynamic sensor field (DSF), which is able to redetermine its gateways, is suitable to improve successful data communications. It is convenient to present a DSF as a BT-Graph that aims to utilize optimization algorithms. The simulation experiments are performed on a forest fire surveillance network to validate our proposed approach.",235,https://doi.org/10.1007/978-3-319-46909-6_27,1,
3162,Automatic hand gesture segmentation for recognition of Vietnamese sign language.,"In this paper, we propose a new solution to identify hand gestures corresponding to alphabetic characters of Vietnamese Sign Language (VSL) via a sequence of images (video) collected from the depth sensor in a Microsoft Kinect. First, a preprocessing is performed to localize and separate the hand from each image and then remove possible noise. In the next stage, the object is extracted to select key frames, which support to represent a segment of the video. Each determined key frame is then converted to a binary image and estimate some biological information such as the hand boundary, finger positions and the palm center. The position of palm centre and fingertips are also localized in 3D space. The process of recognition is performed using Support Vector Machine (SVM) method. The experiments show that the proposed approach is promising since the recognition accuracy is about 91%.",235,https://doi.org/10.1145/3011077.3011135,1,
3163,Pomelo's Quality Classification Based on Combination of Color Information and Gabor Filter.,"Vietnam is a country with strength in fruit trees, including many fruits well-known to the world, such as pomelo, dragon fruit, star apple, mango, durian, rambutan, longan, litchi and watermelon. However, the competitiveness and export of these fruits are low and incommensurate with the existing potential. To solve this problem, Vietnam is studying sustainable directions by investing in machinery for automation process to meet international standards. In this paper, we introduce an effective method for detecting surface defects of the pomelo automatically based on the combination of color information and Gabor filter. Our approach begins by representing the input image in HSV color space, computing the compactness based on the H channel, extracting texture parameters and using the K-nearest neighbor algorithm for quality classification. The proposed approach has been tested with high accuracy and is promising.

",235,https://doi.org/10.1007/978-3-319-02741-8_33,1,
3164,Text classification based on semi-supervised learning.,"In this paper, we present our solution and experimental results of the application of semi-supervised machine learning techniques and the improvement of SVM algorithm to build text classification applications. Firstly, we create a features model which is based on labeled data, and then we will be improved it by the unlabeled data. The technique that is to be added a label into new data is based on binary classification. Our experiment is implemented on three data layers which are extracted from papers in three topics sports, entertainment and education on VNEXPRESS.NET. We experimented and compared the accuracy of the classification results between before and after improve features model through semi-supervised machine learning method and classification algorithm based on SVM model. Experiments show that classification quality is enhanced after improvement features model.",237,https://doi.org/10.1109/SOCPAR.2013.7054133,1,
3165,Initialising PSO with randomised low-discrepancy sequences: the comparative results.,"In this paper, we investigate the use of some wel-known randomised low-discrepancy sequences (Halton, Sobol, and Faure sequences) for initialising particle swarms. We experimented with the standard global-best particle swarm algorithm for function optimization on some benchmark problems, using randomised low-discrepancy sequences for initialisation, and the results were compared with the same particle swarm algorithm using uniform initialisation with a pseudo-random generator. The results show that, the former initialisation method could help the particle swarm algorithm improve its performance over the latter on the problems tried. Furthermore the comparisons also indicate that the use of different randomised low-discrepancy sequences in the initialisation phase could bring different effects on the performance of PSO.",237,https://doi.org/10.1109/CEC.2007.4424717,1,
3166,LoRa Link Quality Estimation Based on Support Vector Machine.,"Nowadays, the LoRa technology is one of the promising technologies used for the Internet of Things (IoT) networks. Over the LoRa transmission link, two devices can communicate with each other over a long distance. As perspective research on LoRa mesh network, it is necessary to consider the link quality estimation (LQE) between neighbor nodes to choose reliable routes. In this paper, we propose a LQE method to classify the connection level between two nodes. The LQE method is developed based on the kernel support vector machine (kSVM), which is one of the machine learning techniques used in classification problems. Series of experiments were performed to collect a dataset consisting of received signal strength indicator (RSSI), signal-to-noise ratio (SNR) of received packets, and packet reception rate (PRR). The trained model shows a high prediction accuracy (mean = 95%) while using 10% of the dataset for training.",238,https://doi.org/10.1007/978-3-030-92507-9_9,1,
3167,A Combined Attack Scenario to Exploit the Join Procedure of LoRaWAN.,"In the LoRaWAN network, end devices must initiate the join operation by sending packets to the Network Server. This process contains several flaws that attackers can use to disrupt network functioning. We will study, analyze, and compare several techniques for exploiting vulnerabilities that are typically conducted during the join procedure in LoRaWAN. Additionally, we propose considering a possible attack scenario, which may occur when the attacker combines some attack techniques to exploit the join procedure.",238,https://doi.org/10.1109/ICUMT54235.2021.9631602,1,
3168,The multipolling mechanism based on the prioritization for WLAN network with multiple access points.,"In this paper, we propose a new polling-based mechanism for the point coordination function (PCF) in IEEE 802.11 wireless local area network (WLAN). The proposed mechanism uses prioritization on two stages to enhance the ability to support quality of service (QoS) in networks with a high density of access points (APs). First step: in the competition between APs, only one AP can get the right to access the environment. This step helps reduce the collision and influence of hidden node problems. Second step: that AP uses a multipolling mechanism to poll all stations (STAs) in a prioritized group m. This step helps reduce delay and polling overhead. The major innovation is the use of multipolling mechanism for WLAN network with multiple APs instead of WLAN network with single AP. We propose Polling List Update (PLU) and Multi Polling Prioritized period (MPP) frames. The PLU frame is used to collect information from STAs and update polling list. The MPP frame coordinates data transmissions without collision. Comparing with the single polling scheme used in the conventional IEEE802.11 MAC protocol, the proposed scheme can reduce the overhead caused by the polling frames. We evaluate the efficacy of the proposed mechanism in terms of polling overhead, delay, data dropped and retransmission attempts.",238,https://doi.org/10.1109/ICOIN.2017.7899447,1,
3169,Adversarial Examples Identification in an End-to-End System With Image Transformation and Filters.,"Deep learning has been receiving great attention in recent years because of its impressive performance in many tasks. However, the widespread adoption of deep learning also becomes a major security risk for those systems as recent researches have pointed out the vulnerabilities of deep learning models. And one of the security issues related to deep learning models is adversarial examples that are an instance with very small, intentional feature perturbations that cause a machine learning model to make a wrong prediction. There have been many proposed defensive methods to combat or detect adversarial examples but still not perfect, powerful and still need a lot of fine-tuning in the process of installing security systems. In this work, we introduce a completely automated method of identifying adversarial examples by using image transformation and filter techniques in an end-to-end system. By exploring the adversarial features that are sensitive to geometry and frequency, we integrate the geometric transformation and denoising based on the frequency domain for identifying adversarial examples. Our proposed detection system is evaluated on popular data sets such as ImageNet or MNIST and gives accurate results up to 99.9% with many optimizations.",244,https://doi.org/10.1109/ACCESS.2020.2978056,1,
3170,Search Space of Adversarial Perturbations against Image Filters.,"The superiority of deep learning performance is threatened by safety issues for itself. Recent findings have shown that deep learning systems are very weak to adversarial examples, an attack form that was altered by the attacker's intent to deceive the deep learning system. There are many proposed defensive methods to protect deep learning systems against adversarial examples. However, there is still a lack of principal strategies to deceive those defensive methods. Any time a particular countermeasure is proposed, a new powerful adversarial attack will be invented to deceive that countermeasure. In this study, we focus on investigating the ability to create adversarial patterns in search space against defensive methods that use image filters. Experimental results conducted on the ImageNet dataset with image classification tasks showed the correlation between the search space of adversarial perturbation and filters. These findings open a new direction for building stronger offensive methods towards deep learning systems.",244,https://arxiv.org/abs/2003.02750,1,
3171,Automated Detection System for Adversarial Examples with High-Frequency Noises Sieve.,"Deep neural networks are being applied in many tasks with encouraging results, and have often reached human-level performance. However, deep neural networks are vulnerable to well-designed input samples called adversarial examples. In particular, neural networks tend to misclassify adversarial examples that are imperceptible to humans. This paper introduces a new detection system that automatically detects adversarial examples on deep neural networks. Our proposed system can mostly distinguish adversarial samples and benign images in an end-to-end manner without human intervention. We exploit the important role of the frequency domain in adversarial samples, and propose a method that detects malicious samples in observations. When evaluated on two standard benchmark datasets (MNIST and ImageNet), our method achieved an out-detection rate of 99.7–100% in many settings.",244,https://doi.org/10.1007/978-3-030-37337-5_28,1,
3172,A Label-Based Approach for Automatic Identifying Adversarial Examples with Image Transformation.,"Besides extraordinary results that deep neural networks are widely deployed in many fields such as computer vision, speech recognition, and natural language processing, in recent years deep neural networks have been recognized vulnerable to intentional modification of legitimate inputs called adversarial examples. These patterns are almost indistinguishable from AI models and human perception. Adversarial examples' concern is rising dramatically and is attracted by many research's apprehensiveness because of its tremendous impact. Unfortunately, until now there is none of the defenses has been shown to be very effective. In this paper, we introduce a new defense strategy against adversarial examples by using a label-based end-to-end system. Our proposed defense system can mostly distinguish adversarial samples and benign images without human intervention. We exploit the important role of spatial domain in adversarial attacks and proposing a state-of-the-art method for detecting adversarial examples based on our observation. We evaluate our system's performance on a variety of standard benchmark datasets including MNIST and ImageNet. Our proposed method reached out detection rates in a range from 94.6% to 99.2% in many settings.",244,https://doi.org/10.1109/CANDAR.2019.00021,1,
3173,Image Transformation can make Neural Networks more robust against Adversarial Examples.,"Neural networks are being applied in many tasks related to IoT with encouraging results. For example, neural networks can precisely detect human, objects and animal via surveillance camera for security purpose. However, neural networks have been recently found vulnerable to well-designed input samples that called adversarial examples. Such issue causes neural networks to misclassify adversarial examples that are imperceptible to humans. We found giving a rotation to an adversarial example image can defeat the effect of adversarial examples. Using MNIST number images as the original images, we first generated adversarial examples to neural network recognizer, which was completely fooled by the forged examples. Then we rotated the adversarial image and gave them to the recognizer to find the recognizer to regain the correct recognition. Thus, we empirically confirmed rotation to images can protect pattern recognizer based on neural networks from adversarial example attacks.",244,http://arxiv.org/abs/1901.03037,1,
3174,A Spatial-Pyramid Scene Categorization Algorithm based on Locality-aware Sparse Coding.,"Scene recognition has a wide range of applications, such as object recognition and detection, content-based image indexing and retrieval, and intelligent vehicle and robot navigation. In particular, natural scene images tend to be very complex and are difficult to analyze due to changes of illumination and transformation. In this study, we investigate a novel model to learn and recognize scenes in nature by combining locality constrained sparse coding (LCSP), Spatial Pyramid Pooling, and linear SVM in end-to-end model. First, interesting points for each image in the training set are characterized by a collection of local features, known as codewords, obtained using dense SIFT descriptor. Each codeword is represented as part of a topic. Then, we employ LCSP algorithm to learn the codeword distribution of those local features from the training images. Next, a modified Spatial Pyramid Pooling model is employed to encode the spatial distribution of the local features. For the final stage, a linear SVM is employed to classify local features encoded by Spatial Pyramid Pooling. Experimental evaluations on several benchmarks well demonstrate the effectiveness and robustness of the proposed method compared to several state-of-the-art visual descriptors.",244,https://doi.org/10.1109/BigMM.2016.93,1,
3175,Poster: Developing an Intrusion Detection System for Cloud Computing.,"Intrusion Detection System (IDS) has been used widely in network systems to detect malicious behaviors which can harm system or computers. However, in practical implementations, an IDS is a complex work and needs investing in the new infrastructure or in training new personnel. Therefore, it is inconsistent with the users who has a small network system. In this paper, we introduce an IDSCloud model as a solution to implementing a network IDS based on cloud computing. The IDSCloud provides network IDS as a service over the internet which can be simple in deployment, maintenance, scalability without investing in the new infrastructure.",244,https://doi.org/10.1145/2938559.2948834,1,
3176,A Pessimistic Approach for Solving a Multi-criteria Decision Making.,"An extension of the DS/AHP method in the paper. The extension assumes that expert judgments concerning the criteria are often imprecise and incomplete. The proposed extension also uses groups of experts or decision makers for comparing decision alternatives and criteria. However, it does not require assigning favorable values for different groups of decision alternatives and criteria. The computation procedure for processing and aggregating the incomplete information about criteria and decision alternatives is reduced to solving a finite set of linear programming problems. Main results are explained and illustrated by numerical examples.",244,https://doi.org/10.1109/KSE.2012.12,1,
3177,An integrated data collection and remote monitoring of power transmission grid using Internet of Things (IoT).,"The Internet of Things (IoT) refers to a heterogeneous network of physical and virtual objects embedded. Most of the data collection solution today relies on specialized electronic devices located far apart, then perform data collection and analysis based on a fixed schedule. This paper proposes an improved IoT architecture takes into consideration the environment surrounding the devices, including the objects and fires to be detected, and other factors affecting the remote management and monitoring. We present the results of research and build an Integrated Monitoring System (IMS) based on IoT technology. The collected data in IMS can be used to cater to the different requirements from the various environment. The analysis results indicate that the designed platform is suitable for the resource-constrained IoT architecture and is scalable to be extended in various IoT scenarios.",245,https://doi.org/10.1145/3440749.3442658,1,
3178,Experience Report on Developing a Crowdsourcing Test Platform for Mobile Applications.,"Crowdsourcing-based testing is a recent approach where testing is operated by volunteer users through the cloud. This approach is particularly suited for mobile applications since various users operating in various contexts can be involved. In the field of software engineering, crowd-testing has acquired a reputation for supporting the testing tasks, not only by professional testers, but also by end users. In this paper, we present TMACSTest (Testing of Mobile Applications using Crowdsourcing). This platform provides the important features for crowdsourcing testing of mobile apps by means of the following functionalities: It allows mobile app providers to register and upload mobile apps for testing, and it allows volunteering Internet users to register and test uploaded mobile apps. Expected behavior is that uploaded mobile apps are tested by many different Internet users in order to cover different runtime platforms and meaningful geographical locations.",246,https://doi.org/10.1007/978-3-030-63119-2_53,1,
3179,Experimental Study on Software Fault Prediction Using Machine Learning Model.,"Faults are the leading cause of time consuming and cost wasting during software life cycle. Predicting faults in early stage improves the quality and reliability of the system and also reduces cost for software development. Many researches proved that software metrics are effective elements for software fault prediction. In addition, many machine learning techniques have been developed for software fault prediction. It is important to determine which set of metrics are effective for predicting fault by using machine learning techniques. In this paper, we conduct an experimental study to evaluate the performance of seven popular techniques including Logistic Regression, K-nearest Neighbors, Decision Tree, Random Forest, Naïve Bayes, Support Vector Machine and Multilayer Perceptron using software metrics from Promise repository dataset usage. Our experiment is performed on both method-level and class-level datasets. The experimental results show that Support Vector Machine archives a higher performance in class-level datasets and Multilayer Perception produces a better accuracy in method-level datasets among seven techniques above.",246,https://doi.org/10.1109/KSE.2019.8919429,1,
3180,Neural Network-Based Generation of Test Data for Interactive Multimodal Applications.,"Testing activity plays an important role in the software development process to ensure product quality and this is even more crucial for safety-critical applications. Interactive multimodal applications are currently present in many such critical systems. An interactive multimodal application processes two or more combined user input modes. Testing such applications requires suitable methods and tools. In this paper, we propose to extend a previously proposed approach to generate test data for interactive multimodal applications by adding the use of neural networks. In this new approach, the application is modeled with the TTT language (as previously), but test data are generated by analyzing not only the TTT model but also the test cases previously obtained. Preliminary experimental results show the approach is promising.",246,https://doi.org/10.1109/RIVF.2019.8713675,1,
3181,A Solution For Improving The Effectiveness of Higher Order Mutation Testing.,"Testing plays a crucial role in software development for ensuring quality. Mutation testing in general and higher order mutation in particular are the good techniques to evaluate the quality of test data, i.e. determining if test data can uncover errors. However, higher order mutation is often very costly because of huge number of generated mutants. In this work, we focus on reducing the cost of higher order mutation testing. We propose different strategies to combine first order mutants to generate less number of higher order mutants for a program under test, but keep the quality of generated mutants. The proposed strategies are experimented on a set of different programs and the results show the effectiveness in terms of number of generated mutants and mutation score.",246,https://doi.org/10.1109/RIVF.2019.8713650,1,
3182,Mutants Generation For Testing Lustre Programs.,"Lustre is synchronous language, widely used for the development of reactive systems, control systems and monitoring systems, such as nuclear reactors, civil aircraft, automobile vehicles... In particular, Lustre is suitable for developing real-time systems. In such applications, testing activities for fault detection play a very important role. Mutation testing is one of the most commonly used techniques for evaluating the probability of fault detection of test data. Typically, the mutants generated by a set of mutation operators of a programming language are very large, so the manual mutant generation is often very costly. In this paper. we present a mutants generator by using the set of mutation operators defined for the Lustre language. Automatic mutants generation strategy is implemented in the generator in order to reduce test cost. Mutant generation and random test data generation are also experimented on different Lustre programs.",246,https://doi.org/10.1145/3155133.3155155,1,
3183,Testing Multimodal Interactive Applications by Means of the TTT Language.," Developing interactive applications is a complex activity as they must deal with various kinds of human-computer interactions. This is especially true when these interactions use multiple modalities (voice, gesture…). As a result, thoroughly testing such applications is particularly important and requires more effort than for traditional interactive applications. In this paper, we propose an approach for automating the test generation of such multimodal applications. This approach is based on the definition of a new test modelling language, TTT. Test models provided in TTT can be translated intest generators.TTT deals with a well-known class of multimodality properties: the CARE properties. The whole approach is illustrated on a case study.",246,https://doi.org/10.5220/0005842500230032,1,
3184,Automatically Testing of Multimodal Interactive Applications.,"Testing interactive multimodal applications is particularly important and requires a lot of effort. Automating this activity can result to significant development cost reduction and quality improvement. In this paper, we propose an approach for automating the test generation of such multimodal applications. This approach is based on the definition of a test modeling language, TTT. The objective of the TTT language is to provide a means for expressing abstract test scenarios for interactive multimodal applications, including non-deterministic choices and action occurrence probabilities that can be used to automate the test generation. Then, we built the TTTEST tool that supports to generate tests for multimodal events and to check the validity of CARE properties of this kind of applications. The approach is illustrated on a case study.",246,https://doi.org/10.1007/978-3-319-66302-9_5,1,
3185,A regression testing approach for Lustre/SCADE programs.,"Software maintenance is an activity which includes enhancements, error corrections, optimization and deletion of existing features. These modifications may cause the system to work incorrectly. Therefore, regression testing becomes necessary. Regression testing is any type of software testing that seeks to uncover new software bugs, or regressions, in existing functional and non-functional areas of a system after changes, such as enhancements, patches or configuration changes, have been made to them. During the software maintenance phase, regression testing is certainly an expensive but necessary activity to make sure the new versions of the system do not ""regress"". As software evolves, chances are that not only the implementation changes, but that the specification of the system changes too. We argue that guiding regression testing by the system specifications will be more accurate and cost effective. Lustre is a formal synchronous declarative language widely used for modeling and specifying safety critical applications in the fields of avionics, transportation, and energy production. In such applications, the testing activity to ensure correctness of the system plays a crucial role. During the development process, Lustre programs are often upgraded, so regression testing should be performed to detect bugs. In this paper, we present an approach to generate test cases in regression testing of Lustre programs. In this approach, a Lustre program is modeled by an operator network, then we determine the set of paths and compute symbolically the path activation conditions for each version. Test cases for regression are generated by comparing paths between versions. We apply this approach on a case study for regression testing of a Heater Controller System.",246,https://doi.org/10.1145/2833258.2833278,1,
3186,Indexing Based on Topic Modeling and MATHML for Building Vietnamese Technical Document Retrieval Effectively.,"The grow of data on the Internet has brought to people many information and it also opened some important problem in Information retrieval…Along with it, some search engines have developed for user’s purpose. User can retrieve information by content, keyword or anything what they need. However, data on the Internet is too huge, the results feedback is often millions or hundreds millions for each query. Therefore, with the narrow field, we will meet a difficult to find related information, especially technical information that contain formulas. In this paper, we present a method for building Vietnamese technical text based on topic modeling and MathML for indexing. System has built and tested with over 500 Vietnamese technical text shown that, this system satisfied users’ requires in accuracy and speed.",250,https://doi.org/10.1007/978-3-319-29236-6_31,1,
3188,Item-based recommendation with Shapley value.,"Discovering knowledge in archival data is the goal of researchers. One of them is collaborative filtering recommender system is developing fastly today. It may be rather effective in sparse and ""long tail"" datasets. Calculating to make decision based on many criteria is really necessary. Relationships, interactions between criteria need to have been fully considered, decision will be more reliable and feasible. In this paper, we propose a new approach that builds a recommender decision-making model based on importance of item, set of items with Shapley value. This model also incorporates traditional techniques and some our new approaches and was tested, evaluated on multirecsys tool we develope from some available tools and uses standardized datasets to experiment. Experimental results show that the proposed model is always satisfactory and reliable. They can be applied in appropriate contexts to minimize limitations of recommender system today and is a research way next time.",251,https://doi.org/10.4108/eai.18-3-2019.159341,1,
3189,A Scalable IoT Video Data Analytics for Smart Cities.,"The smart city is a comprehensive application of information resources and a high degree of information technology integration. With the technical support from IoT (Internet of things), smart city need to have three features of being instrumented, interconnected and intelligent. IoT provides the ability to manage, remotely monitor and control devices from massive streams of real-time data.Our model offers a scalable IoT video data analytics applications for Smart cities to end users, who can exploit scalability in both data storage and processing power to execute analysis on large or complex datasets. This model provides data analytics programming suites and environments in which developers and researchers can design scalable analytics services and applications. A cloud/edge-based automated video analysis system to process large numbers of video streams, where the underlying infrastructure is able to scale based on the number of camera devices and easy to integrate analytic application. The system automates the video analysis process and reduces manual intervention. The design of our model is developed to be easily extended for new kinds of IoT devices, message routing and queueing, and data analytics, to permit specific application to be programmed via the paradigm to be flexible yet simple.",251,https://doi.org/10.4108/eai.13-7-2018.163136,1,
3190,Collaborative filtering recommender system base on the interaction multi-criteria decision with ordered weighted averaging operator.,"In the recommender system, the most important is the decision-making solutionto consulte for user. Depending on the type and size of data stored, decision-making will always be improved to produce the best possible result.. The main task in implementing the model is to use methods to find the most valuable product or service for the user. In this paper, we propose a new approach to building a multi-user based collaborative filtering model using the interaction multi-criteria decision with ordered weighted averaging operator. This model demonstrates the synergy and interplay between user criteria for decision making. The model was evaluated through experimentation with the multirecsys tool on three datasets: MovieLense 100K, MSWeb and Jester5k. The experiment illustrated the model comparison with some other interactive multi-criteria counseling methods that have been researchedon both sparse datasets and thick datasets. In addition, the model is compared and evaluated with item-base collaborative filtering model using the interaction multi-criteria decision with ordered weighted averaging operator on two types of datasets. Consultancy results of the proposed model are quite effective compared to some traditional consulting models and some models with other operator. This counseling model can be applied well in a variety of contexts, especially in the case of sparse data, this model will give result in improved counseling. In addition, with the above method, the user-base model is always more efficient than item-base on all datasets.",251,https://doi.org/10.1145/3184066.3184075,1,
3191,IoT Services for Solving Critical Problems in Vietnam: A Research Landscape and Directions.,"Critical problems in Vietnam - such as food safety, traffic management, saltwater intrusion, and wastewater management - require novel Internet of Things (IoT) solutions. Here, the authors analyze current research and developments and propose specific IoT services as solutions.",251,https://doi.org/10.1109/MIC.2016.97,1,
3192,Toward Real-Time Multi-criteria Decision Making for Bus Service Reliability Optimization.,"This paper addresses issues associated with the real-time control of public transit operations to minimize passenger wait time: namely vehicle headway, maintenance of passenger comfort, and reducing the impact of control strategies. The randomness of passenger arrivals at bus stops and external factors (such as traffic congestion and bad weather) in high frequency transit operations often cause irregular headway that can result in decreased service reliability. The approach proposed in this paper, which has the capability of handling the uncertainty of transit operations based on Multi-objective evolutionary algorithm using a dynamic Bayesian network, applies preventive strategies to forestall bus unreliability and, where unreliability is evident, restore reliability using corrective strategies. “Holding”, “expressing”, “short-turning” and “deadheading” are the corrective strategies considered in this paper.",251,https://doi.org/10.1007/978-3-319-25252-0_40,1,
3193,Learning Diagnostic Diagrams in Transport-Based Data-Collection Systems.,"Insights about service improvement in a transit network can be gained by studying transit service reliability. In this paper, a general procedure for constructing a transit service reliability diagnostic (TSRD) diagram based on a Bayesian network is proposed to automatically build a behavioural model from Automatic Vehicle Location (AVL) and Automatic Passenger Counters (APC) data. Our purpose is to discover the variability of transit service attributes and their effects on traveller behaviour. A TSRD diagram describes and helps to analyse factors affecting public transport by combining domain knowledge with statistical data.",251,https://doi.org/10.1007/978-3-319-08326-1_61,1,
3194,Evolutionary simulation for a public transit digital ecosystem: a case study.,"This paper presents an integrated simulation environment Connected Mobility Digital Ecosystem Simulation (CmdeSim) designed for testing and evaluating traffic planning and management systems using multi-objective Evolutionary Algorithm optimization and Bayesian network for bus network prediction. CmdeSim represents traffic data and flows in a local bus network and predicts bus service reliability. A graphical user interface (GUI) allows visualization of the simulation: including animation of vehicle movements on the map route. Simulation is an important element for assessing the performance of alternative designs and is a cost effective alternative to operational testing since it allows the analysis of the robustness of a design in a shorter time frame. CmdeSim, which is an simulation based evaluation, is an off-line tool for testing system designs before implementation and for studying the complex interactions among the components of the public transit system. The developed model in CmdeSim is used for simulation of different bus scenarios and strategies to identify their strengths and weaknesses for optimizing bus service reliability. CmdeSim describes how a public transit system is composed and this aids an understanding of the most important factors and actors that should be included in the Digital Ecosystem in order for Public Transit Operations in the system to be self-sustaining.",251,https://doi.org/10.1145/2536146.2536155,1,
3195,A Digital Ecosystem For Optimizing Service Reliability In Public Transport.,"Automatic vehicle location (AVL) and automatic passenger counting (APC) systems can generate a huge quantity and variety of operational, spatial, and temporal data. This potentially allows the discovery of new ways to enhance service quality and transport efficiency by utilizing AVL-APC inputs. There is currently no framework for implementing full service quality improvement cycles from automated data (Boyle 2008) and this motivates our case study. The objective of this research is to apply a digital ecosystem metaphor that extends the use of AVL and APC data for the benefit of transit agencies. It is intended that this framework will capture relevant data for stakeholders and enable the discovery of otherwise hidden trends that help explain irregularities in operations and suggest new avenues for service improvement. The framework is divided into two components: proactive adaptation and reactive adaptation.",251,http://aisel.aisnet.org/pacis2012/191,1,
